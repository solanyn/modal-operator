Directory structure:
â””â”€â”€ modal-labs-modal-client/
    â”œâ”€â”€ README.md
    â”œâ”€â”€ AGENTS.md
    â”œâ”€â”€ conftest.py
    â”œâ”€â”€ LICENSE
    â”œâ”€â”€ pyproject.toml
    â”œâ”€â”€ requirements.dev.txt
    â”œâ”€â”€ SECURITY.md
    â”œâ”€â”€ tasks.py
    â”œâ”€â”€ .pre-commit-config.yaml
    â”œâ”€â”€ CLAUDE.md -> AGENTS.md
    â”œâ”€â”€ modal/
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ __main__.py
    â”‚   â”œâ”€â”€ _clustered_functions.py
    â”‚   â”œâ”€â”€ _container_entrypoint.py
    â”‚   â”œâ”€â”€ _ipython.py
    â”‚   â”œâ”€â”€ _location.py
    â”‚   â”œâ”€â”€ _object.py
    â”‚   â”œâ”€â”€ _output.py
    â”‚   â”œâ”€â”€ _partial_function.py
    â”‚   â”œâ”€â”€ _pty.py
    â”‚   â”œâ”€â”€ _resolver.py
    â”‚   â”œâ”€â”€ _resources.py
    â”‚   â”œâ”€â”€ _serialization.py
    â”‚   â”œâ”€â”€ _traceback.py
    â”‚   â”œâ”€â”€ _tunnel.py
    â”‚   â”œâ”€â”€ _type_manager.py
    â”‚   â”œâ”€â”€ _watcher.py
    â”‚   â”œâ”€â”€ app.py
    â”‚   â”œâ”€â”€ call_graph.py
    â”‚   â”œâ”€â”€ client.py
    â”‚   â”œâ”€â”€ cloud_bucket_mount.py
    â”‚   â”œâ”€â”€ cls.py
    â”‚   â”œâ”€â”€ config.py
    â”‚   â”œâ”€â”€ container_process.py
    â”‚   â”œâ”€â”€ dict.py
    â”‚   â”œâ”€â”€ environments.py
    â”‚   â”œâ”€â”€ exception.py
    â”‚   â”œâ”€â”€ file_io.py
    â”‚   â”œâ”€â”€ file_pattern_matcher.py
    â”‚   â”œâ”€â”€ functions.py
    â”‚   â”œâ”€â”€ gpu.py
    â”‚   â”œâ”€â”€ io_streams.py
    â”‚   â”œâ”€â”€ mount.py
    â”‚   â”œâ”€â”€ network_file_system.py
    â”‚   â”œâ”€â”€ object.py
    â”‚   â”œâ”€â”€ output.py
    â”‚   â”œâ”€â”€ partial_function.py
    â”‚   â”œâ”€â”€ proxy.py
    â”‚   â”œâ”€â”€ py.typed
    â”‚   â”œâ”€â”€ queue.py
    â”‚   â”œâ”€â”€ retries.py
    â”‚   â”œâ”€â”€ runner.py
    â”‚   â”œâ”€â”€ running_app.py
    â”‚   â”œâ”€â”€ sandbox.py
    â”‚   â”œâ”€â”€ schedule.py
    â”‚   â”œâ”€â”€ scheduler_placement.py
    â”‚   â”œâ”€â”€ secret.py
    â”‚   â”œâ”€â”€ serving.py
    â”‚   â”œâ”€â”€ snapshot.py
    â”‚   â”œâ”€â”€ stream_type.py
    â”‚   â”œâ”€â”€ token_flow.py
    â”‚   â”œâ”€â”€ _runtime/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ asgi.py
    â”‚   â”‚   â”œâ”€â”€ execution_context.py
    â”‚   â”‚   â”œâ”€â”€ gpu_memory_snapshot.py
    â”‚   â”‚   â”œâ”€â”€ telemetry.py
    â”‚   â”‚   â””â”€â”€ user_code_imports.py
    â”‚   â”œâ”€â”€ _utils/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ app_utils.py
    â”‚   â”‚   â”œâ”€â”€ async_utils.py
    â”‚   â”‚   â”œâ”€â”€ auth_token_manager.py
    â”‚   â”‚   â”œâ”€â”€ blob_utils.py
    â”‚   â”‚   â”œâ”€â”€ bytes_io_segment_payload.py
    â”‚   â”‚   â”œâ”€â”€ deprecation.py
    â”‚   â”‚   â”œâ”€â”€ docker_utils.py
    â”‚   â”‚   â”œâ”€â”€ function_utils.py
    â”‚   â”‚   â”œâ”€â”€ git_utils.py
    â”‚   â”‚   â”œâ”€â”€ grpc_testing.py
    â”‚   â”‚   â”œâ”€â”€ grpc_utils.py
    â”‚   â”‚   â”œâ”€â”€ hash_utils.py
    â”‚   â”‚   â”œâ”€â”€ http_utils.py
    â”‚   â”‚   â”œâ”€â”€ jwt_utils.py
    â”‚   â”‚   â”œâ”€â”€ logger.py
    â”‚   â”‚   â”œâ”€â”€ mount_utils.py
    â”‚   â”‚   â”œâ”€â”€ name_utils.py
    â”‚   â”‚   â”œâ”€â”€ package_utils.py
    â”‚   â”‚   â”œâ”€â”€ pattern_utils.py
    â”‚   â”‚   â”œâ”€â”€ rand_pb_testing.py
    â”‚   â”‚   â”œâ”€â”€ shell_utils.py
    â”‚   â”‚   â””â”€â”€ time_utils.py
    â”‚   â”œâ”€â”€ _vendor/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ a2wsgi_wsgi.py
    â”‚   â”‚   â””â”€â”€ tblib.py
    â”‚   â”œâ”€â”€ builder/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ 2023.12.312.txt
    â”‚   â”‚   â”œâ”€â”€ 2023.12.txt
    â”‚   â”‚   â”œâ”€â”€ 2024.04.txt
    â”‚   â”‚   â”œâ”€â”€ 2024.10.txt
    â”‚   â”‚   â”œâ”€â”€ 2025.06.txt
    â”‚   â”‚   â”œâ”€â”€ base-images.json
    â”‚   â”‚   â””â”€â”€ PREVIEW.txt
    â”‚   â”œâ”€â”€ cli/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ _download.py
    â”‚   â”‚   â”œâ”€â”€ _traceback.py
    â”‚   â”‚   â”œâ”€â”€ app.py
    â”‚   â”‚   â”œâ”€â”€ cluster.py
    â”‚   â”‚   â”œâ”€â”€ config.py
    â”‚   â”‚   â”œâ”€â”€ container.py
    â”‚   â”‚   â”œâ”€â”€ dict.py
    â”‚   â”‚   â”œâ”€â”€ entry_point.py
    â”‚   â”‚   â”œâ”€â”€ environment.py
    â”‚   â”‚   â”œâ”€â”€ import_refs.py
    â”‚   â”‚   â”œâ”€â”€ launch.py
    â”‚   â”‚   â”œâ”€â”€ network_file_system.py
    â”‚   â”‚   â”œâ”€â”€ profile.py
    â”‚   â”‚   â”œâ”€â”€ queues.py
    â”‚   â”‚   â”œâ”€â”€ run.py
    â”‚   â”‚   â”œâ”€â”€ secret.py
    â”‚   â”‚   â”œâ”€â”€ token.py
    â”‚   â”‚   â”œâ”€â”€ utils.py
    â”‚   â”‚   â”œâ”€â”€ volume.py
    â”‚   â”‚   â””â”€â”€ programs/
    â”‚   â”‚       â”œâ”€â”€ __init__.py
    â”‚   â”‚       â”œâ”€â”€ launch_instance_ssh.py
    â”‚   â”‚       â”œâ”€â”€ run_jupyter.py
    â”‚   â”‚       â”œâ”€â”€ run_marimo.py
    â”‚   â”‚       â””â”€â”€ vscode.py
    â”‚   â””â”€â”€ experimental/
    â”‚       â”œâ”€â”€ __init__.py
    â”‚       â”œâ”€â”€ flash.py
    â”‚       â””â”€â”€ ipython.py
    â”œâ”€â”€ modal_docs/
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ gen_cli_docs.py
    â”‚   â”œâ”€â”€ gen_reference_docs.py
    â”‚   â””â”€â”€ mdmd/
    â”‚       â”œâ”€â”€ __init__.py
    â”‚       â”œâ”€â”€ mdmd.py
    â”‚       â””â”€â”€ signatures.py
    â”œâ”€â”€ modal_global_objects/
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ images/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â””â”€â”€ base_images.py
    â”‚   â””â”€â”€ mounts/
    â”‚       â”œâ”€â”€ __init__.py
    â”‚       â”œâ”€â”€ modal_client_dependencies.py
    â”‚       â”œâ”€â”€ modal_client_package.py
    â”‚       â””â”€â”€ python_standalone.py
    â”œâ”€â”€ modal_proto/
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ options.proto
    â”‚   â”œâ”€â”€ py.typed
    â”‚   â””â”€â”€ sandbox_router.proto
    â”œâ”€â”€ modal_version/
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â””â”€â”€ __main__.py
    â”œâ”€â”€ protoc_plugin/
    â”‚   â””â”€â”€ plugin.py
    â”œâ”€â”€ test/
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ aio_test.py
    â”‚   â”œâ”€â”€ app_composition_test.py
    â”‚   â”œâ”€â”€ app_test.py
    â”‚   â”œâ”€â”€ asgi_wrapper_test.py
    â”‚   â”œâ”€â”€ async_utils_test.py
    â”‚   â”œâ”€â”€ auth_token_manager_test.py
    â”‚   â”œâ”€â”€ blob_test.py
    â”‚   â”œâ”€â”€ cli_imports_test.py
    â”‚   â”œâ”€â”€ client_test.py
    â”‚   â”œâ”€â”€ cloud_bucket_mount_test.py
    â”‚   â”œâ”€â”€ cls_test.py
    â”‚   â”œâ”€â”€ config_test.py
    â”‚   â”œâ”€â”€ container_app_test.py
    â”‚   â”œâ”€â”€ container_buffer_test.py
    â”‚   â”œâ”€â”€ decorator_test.py
    â”‚   â”œâ”€â”€ deprecation_test.py
    â”‚   â”œâ”€â”€ dict_test.py
    â”‚   â”œâ”€â”€ docker_utils_test.py
    â”‚   â”œâ”€â”€ e2e_test.py
    â”‚   â”œâ”€â”€ error_test.py
    â”‚   â”œâ”€â”€ experimental_test.py
    â”‚   â”œâ”€â”€ file_io_test.py
    â”‚   â”œâ”€â”€ file_pattern_matcher_test.py
    â”‚   â”œâ”€â”€ flash_test.py
    â”‚   â”œâ”€â”€ fork_test.py
    â”‚   â”œâ”€â”€ function_retry_test.py
    â”‚   â”œâ”€â”€ function_serialization_test.py
    â”‚   â”œâ”€â”€ function_utils_test.py
    â”‚   â”œâ”€â”€ git_utils_test.py
    â”‚   â”œâ”€â”€ gpu_fallbacks_test.py
    â”‚   â”œâ”€â”€ gpu_test.py
    â”‚   â”œâ”€â”€ grpc_utils_test.py
    â”‚   â”œâ”€â”€ helpers.py
    â”‚   â”œâ”€â”€ i6pn_clustered_test.py
    â”‚   â”œâ”€â”€ input_plane_test.py
    â”‚   â”œâ”€â”€ io_streams_test.py
    â”‚   â”œâ”€â”€ live_reload_test.py
    â”‚   â”œâ”€â”€ logging_test.py
    â”‚   â”œâ”€â”€ lookup_test.py
    â”‚   â”œâ”€â”€ map_item_context_test.py
    â”‚   â”œâ”€â”€ map_item_mananger_test.py
    â”‚   â”œâ”€â”€ mdmd_test.py
    â”‚   â”œâ”€â”€ mount_test.py
    â”‚   â”œâ”€â”€ mount_utils_test.py
    â”‚   â”œâ”€â”€ mounted_files_test.py
    â”‚   â”œâ”€â”€ network_file_system_test.py
    â”‚   â”œâ”€â”€ notebook_test.py
    â”‚   â”œâ”€â”€ object_test.py
    â”‚   â”œâ”€â”€ package_utils_test.py
    â”‚   â”œâ”€â”€ queue_test.py
    â”‚   â”œâ”€â”€ resolver_test.py
    â”‚   â”œâ”€â”€ retries_test.py
    â”‚   â”œâ”€â”€ runner_test.py
    â”‚   â”œâ”€â”€ sandbox_test.py
    â”‚   â”œâ”€â”€ schedule_test.py
    â”‚   â”œâ”€â”€ scheduler_placement_test.py
    â”‚   â”œâ”€â”€ secret_test.py
    â”‚   â”œâ”€â”€ serialization_test.py
    â”‚   â”œâ”€â”€ should_upload_test.py
    â”‚   â”œâ”€â”€ shutdown_test.py
    â”‚   â”œâ”€â”€ slow_dependencies_test.py
    â”‚   â”œâ”€â”€ static_types_test.py
    â”‚   â”œâ”€â”€ telemetry_test.py
    â”‚   â”œâ”€â”€ token_flow_test.py
    â”‚   â”œâ”€â”€ traceback_test.py
    â”‚   â”œâ”€â”€ tunnel_test.py
    â”‚   â”œâ”€â”€ user_code_import_test.py
    â”‚   â”œâ”€â”€ utils_test.py
    â”‚   â”œâ”€â”€ version_test.py
    â”‚   â”œâ”€â”€ volume_test.py
    â”‚   â”œâ”€â”€ watcher_test.py
    â”‚   â”œâ”€â”€ web_server_proxy_test.py
    â”‚   â”œâ”€â”€ webhook_test.py
    â”‚   â”œâ”€â”€ mdmd_data/
    â”‚   â”‚   â”œâ”€â”€ foo-expected.md
    â”‚   â”‚   â””â”€â”€ foo.py
    â”‚   â”œâ”€â”€ supports/
    â”‚   â”‚   â”œâ”€â”€ assert_package.py
    â”‚   â”‚   â”œâ”€â”€ base_class.py
    â”‚   â”‚   â”œâ”€â”€ batching_config.py
    â”‚   â”‚   â”œâ”€â”€ class_hierarchy.py
    â”‚   â”‚   â”œâ”€â”€ class_with_image.py
    â”‚   â”‚   â”œâ”€â”€ common.py
    â”‚   â”‚   â”œâ”€â”€ concurrency_config.py
    â”‚   â”‚   â”œâ”€â”€ consumed_map.py
    â”‚   â”‚   â”œâ”€â”€ forking.py
    â”‚   â”‚   â”œâ”€â”€ function_without_app.py
    â”‚   â”‚   â”œâ”€â”€ functions.py
    â”‚   â”‚   â”œâ”€â”€ hello.py
    â”‚   â”‚   â”œâ”€â”€ image_run_function.py
    â”‚   â”‚   â”œâ”€â”€ import_and_filter_source.py
    â”‚   â”‚   â”œâ”€â”€ import_modal_from_thread.py
    â”‚   â”‚   â”œâ”€â”€ imports_ast.py
    â”‚   â”‚   â”œâ”€â”€ imports_six.py
    â”‚   â”‚   â”œâ”€â”€ lazy_hydration.py
    â”‚   â”‚   â”œâ”€â”€ map_item_test_utils.py
    â”‚   â”‚   â”œâ”€â”€ missing_main_conditional.py
    â”‚   â”‚   â”œâ”€â”€ modal_run_from_function.py
    â”‚   â”‚   â”œâ”€â”€ module_1.py
    â”‚   â”‚   â”œâ”€â”€ module_2.py
    â”‚   â”‚   â”œâ”€â”€ mount_dedupe.py
    â”‚   â”‚   â”œâ”€â”€ multiapp.py
    â”‚   â”‚   â”œâ”€â”€ multiapp_privately_decorated.py
    â”‚   â”‚   â”œâ”€â”€ multiapp_privately_decorated_named_app.py
    â”‚   â”‚   â”œâ”€â”€ multiapp_same_name.py
    â”‚   â”‚   â”œâ”€â”€ multiapp_serialized_func.py
    â”‚   â”‚   â”œâ”€â”€ package_mount.py
    â”‚   â”‚   â”œâ”€â”€ progress_info.py
    â”‚   â”‚   â”œâ”€â”€ pyproject.toml
    â”‚   â”‚   â”œâ”€â”€ raise_error.py
    â”‚   â”‚   â”œâ”€â”€ sandbox.py
    â”‚   â”‚   â”œâ”€â”€ script.py
    â”‚   â”‚   â”œâ”€â”€ serialize_class.py
    â”‚   â”‚   â”œâ”€â”€ sibling_hydration_app.py
    â”‚   â”‚   â”œâ”€â”€ skip.py
    â”‚   â”‚   â”œâ”€â”€ slow_dependencies_container.py
    â”‚   â”‚   â”œâ”€â”€ slow_dependencies_local.py
    â”‚   â”‚   â”œâ”€â”€ special_poetry.lock
    â”‚   â”‚   â”œâ”€â”€ standalone_file.py
    â”‚   â”‚   â”œâ”€â”€ startup_failure.py
    â”‚   â”‚   â”œâ”€â”€ startup_failure_bigexception.py
    â”‚   â”‚   â”œâ”€â”€ test-conda-environment.yml
    â”‚   â”‚   â”œâ”€â”€ test-dockerfile
    â”‚   â”‚   â”œâ”€â”€ test-pyproject.toml
    â”‚   â”‚   â”œâ”€â”€ test-requirements.txt
    â”‚   â”‚   â”œâ”€â”€ type_assertions.py
    â”‚   â”‚   â”œâ”€â”€ type_assertions_negative.py
    â”‚   â”‚   â”œâ”€â”€ unconsumed_map.py
    â”‚   â”‚   â”œâ”€â”€ volume_local.py
    â”‚   â”‚   â”œâ”€â”€ webhook_forgot_function.py
    â”‚   â”‚   â”œâ”€â”€ app_run_tests/
    â”‚   â”‚   â”‚   â”œâ”€â”€ app_with_lookups.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ app_with_multiple_functions.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ async_app.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ cli_args.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ cls.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ custom_app.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ default_app.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ file_with_global_lookups.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ generator.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ local_entrypoint.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ local_entrypoint_async.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ local_entrypoint_invalid.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ main_thread_assertion.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ prints_desc_app.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ raises_error.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ returns_data.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ uses_experimental_options.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ uses_with_options.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ variadic_args.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ webhook.py
    â”‚   â”‚   â”‚   â””â”€â”€ multifile/
    â”‚   â”‚   â”‚       â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚       â”œâ”€â”€ main.py
    â”‚   â”‚   â”‚       â””â”€â”€ util.py
    â”‚   â”‚   â”œâ”€â”€ multifile_project/
    â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ a.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ b.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ c.py
    â”‚   â”‚   â”‚   â””â”€â”€ main.py
    â”‚   â”‚   â”œâ”€â”€ notebooks/
    â”‚   â”‚   â”‚   â””â”€â”€ simple.notebook.py
    â”‚   â”‚   â”œâ”€â”€ pkg_a/
    â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ a.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ d.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ package.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ script.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ serialized_fn.py
    â”‚   â”‚   â”‚   â””â”€â”€ b/
    â”‚   â”‚   â”‚       â”œâ”€â”€ c.py
    â”‚   â”‚   â”‚       â””â”€â”€ e.py
    â”‚   â”‚   â”œâ”€â”€ pkg_b/
    â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ f.py
    â”‚   â”‚   â”‚   â””â”€â”€ g/
    â”‚   â”‚   â”‚       â””â”€â”€ h.py
    â”‚   â”‚   â”œâ”€â”€ pkg_c/
    â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ i.py
    â”‚   â”‚   â”‚   â””â”€â”€ j/
    â”‚   â”‚   â”‚       â””â”€â”€ k.py
    â”‚   â”‚   â”œâ”€â”€ pkg_d/
    â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ main.py
    â”‚   â”‚   â”‚   â””â”€â”€ sibling.py
    â”‚   â”‚   â”œâ”€â”€ user_code_import_samples/
    â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ cls.py
    â”‚   â”‚   â”‚   â””â”€â”€ func.py
    â”‚   â”‚   â”œâ”€â”€ uv_lock_no_modal/
    â”‚   â”‚   â”‚   â””â”€â”€ pyproject.toml
    â”‚   â”‚   â”œâ”€â”€ uv_lock_project/
    â”‚   â”‚   â”‚   â””â”€â”€ pyproject.toml
    â”‚   â”‚   â”œâ”€â”€ uv_lock_workspace/
    â”‚   â”‚   â”‚   â”œâ”€â”€ pyproject.toml
    â”‚   â”‚   â”‚   â””â”€â”€ packages/
    â”‚   â”‚   â”‚       â”œâ”€â”€ mod1/
    â”‚   â”‚   â”‚       â”‚   â””â”€â”€ pyproject.toml
    â”‚   â”‚   â”‚       â””â”€â”€ mod2/
    â”‚   â”‚   â”‚           â””â”€â”€ pyproject.toml
    â”‚   â”‚   â””â”€â”€ uv_sync_just_pyproject/
    â”‚   â”‚       â””â”€â”€ pyproject.toml
    â”‚   â””â”€â”€ telemetry/
    â”‚       â”œâ”€â”€ tracing_module_1.py
    â”‚       â””â”€â”€ tracing_module_2.py
    â””â”€â”€ .github/
        â”œâ”€â”€ CONTRIBUTING.md
        â”œâ”€â”€ pull_request_template.md
        â”œâ”€â”€ actions/
        â”‚   â””â”€â”€ setup-cached-python/
        â”‚       â””â”€â”€ action.yml
        â”œâ”€â”€ ISSUE_TEMPLATE/
        â”‚   â”œâ”€â”€ 1_bug_report.yaml
        â”‚   â””â”€â”€ config.yml
        â””â”€â”€ workflows/
            â”œâ”€â”€ check.yml
            â”œâ”€â”€ ci-cd.yml
            â”œâ”€â”€ docs.yml
            â””â”€â”€ sast-codeql.yml

================================================
FILE: README.md
================================================
# Modal Python Library

[![PyPI Version](https://img.shields.io/pypi/v/modal.svg)](https://pypi.org/project/modal/)
[![License](https://img.shields.io/badge/license-apache_2.0-darkviolet.svg)](https://github.com/modal-labs/modal-client/blob/master/LICENSE)
[![Tests](https://github.com/modal-labs/modal-client/actions/workflows/ci-cd.yml/badge.svg)](https://github.com/modal-labs/modal-client/actions/workflows/ci-cd.yml)
[![Slack](https://img.shields.io/badge/slack-join-blue.svg?logo=slack)](https://modal.com/slack)

The [Modal](https://modal.com/) Python library provides convenient, on-demand
access to serverless cloud compute from Python scripts on your local computer.

## Documentation

See the [online documentation](https://modal.com/docs/guide) for many
[example applications](https://modal.com/docs/examples),
a [user guide](https://modal.com/docs/guide), and the detailed
[API reference](https://modal.com/docs/reference).

## Installation

**This library requires Python 3.9 â€“ 3.13.**

Install the package with `pip`:

```bash
pip install modal
```

You can create a Modal account (or link your existing one) directly on the
command line:

```bash
python3 -m modal setup
```

## Support

For usage questions and other support, please reach out on the
[Modal Slack](https://modal.com/slack).



================================================
FILE: AGENTS.md
================================================
# Rules for working with LLMs on the Modal client codebase

This file provides guidance to LLMs when working with code in this repository.

## Common Development Commands

**Code Quality:**

Before making a pull request, the following checks must pass:

- `inv lint --fix` - Run ruff with auto-fix.
- `inv lint-protos` - Lint protocol buffer definitions.
- `inv type-check` - Run mypy and pyright type checking.
- `inv check-copyright --fix` - Ensure that all files have a copyright header.

**Development Tasks:**

- `inv protoc` - Compile protocol buffer files.
- `inv lint-protos` - Lint protocol buffer definitions.
- `inv type-stubs` - Regenerate the *.pyi type stub files using the synchronicity library.

**Running tests:**
- `inv test` - Run all tests
- `inv test --pytest-args "test_file.py::specific_test` - Pass arguments to pytest directly, e.g. to run specific tests.

## Code Quality Standards

**Linting**: ruff configured for 120 character line length with import sorting.

**Type Checking**: Both MyPy and Pyright used for different file sets (see `inv type-check` in `tasks.py`).

**Formatting**: ruff handles both linting and formatting.

**Copyright**: All Python files must start with `# Copyright Modal Labs {year}` (see `inv check-copyright --fix` in `tasks.py`).

**Imports**: Follow isort configuration with Modal packages as first-party.

## Key Development Considerations

**Synchronicity Wrappers**: When adding new async methods, ensure they're properly wrapped for sync API generation.

**Protocol Buffers**: Changes to `.proto` files require running `inv protoc` to regenerate Python stubs.

**Type stubs**: All `*.pyi` files are auto-generated and should never be manually edited.

**Object Lifecycle**: New Modal resource types should inherit from `_Object` and follow established patterns.

**Error Handling**: Use Modal's exception hierarchy and ensure proper error propagation between client/server.

**Testing Async Code**: Use pytest-asyncio and ensure proper async test setup in conftest.py.



================================================
FILE: conftest.py
================================================
# Copyright Modal Labs 2022
import ast
import pytest
import traceback
from typing import Any

from pytest_markdown_docs._runners import DefaultRunner, register_runner
from pytest_markdown_docs.definitions import FenceTestDefinition

import modal
from modal import enable_output
from modal.app import LocalEntrypoint
from modal.cli.import_refs import infer_runnable, list_cli_commands
from modal.functions import Function


def pytest_markdown_docs_globals():
    import math

    return {
        "modal": modal,
        "app": modal.App("pytest-markdown-docs-app"),
        "math": math,
        "__name__": "runtest",
        "fastapi_endpoint": modal.fastapi_endpoint,
        "asgi_app": modal.asgi_app,
        "wsgi_app": modal.wsgi_app,
        "__file__": "xyz.py",
    }


@pytest.fixture()
def running_app():
    return modal.App.lookup("pytest-markdown-docs-running-app", create_if_missing=True)


@register_runner()
class ModalRunner(DefaultRunner):
    def runtest(self, test, args):
        try:
            module_name = "markdown_code_fence.py"
            try:
                tree = ast.parse(test.source, filename=module_name)
            except SyntaxError:
                raise

            try:
                # if we don't compile the code, it seems we get name lookup errors
                # for functions etc. when doing cross-calls across inline functions
                compiled = compile(tree, filename=module_name, mode="exec", dont_inherit=True)
            except SyntaxError:
                raise

            exec_globals = args.copy()
            exec_locals: dict[str, Any] = {}  # this will contain anything defined in the fence, like @app.function
            exec(compiled, exec_globals, exec_locals)

            # TODO (elias): add support for runnable "arguments" in pytest-markdown-docs so the code fence
            #  can specify which function to run and what to pass as arguments to it

            # for now, use "modal run" logic:
            cli_commands = list_cli_commands(exec_locals)

            # all commands that satisfy local entrypoint/accept webhook limitations AND object path prefix
            runnable = infer_runnable(cli_commands, "", True, False)

            with enable_output():
                if isinstance(runnable, LocalEntrypoint):
                    with runnable.app.run():
                        runnable()
                elif isinstance(runnable, Function):
                    with runnable.app.run():
                        runnable.remote()
                else:
                    with runnable.cls.app.run():
                        getattr(runnable.cls(), runnable.method_name).remote()
        except:
            traceback.print_exc()
            raise

    def repr_failure(self, test: FenceTestDefinition, excinfo: pytest.ExceptionInfo[BaseException], style=None):
        return "Error during app run, see logs"



================================================
FILE: LICENSE
================================================
                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS



================================================
FILE: pyproject.toml
================================================
[build-system]
requires = ["setuptools~=77.0.3", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "modal"
description = "Python client library for Modal"
readme = "README.md"
dynamic = ["version"]
requires-python = ">=3.9"
license = {text = "Apache-2.0"}

authors = [
    { name = "Modal Labs", email = "support@modal.com" }
]
dependencies = [
    "aiohttp",
    "cbor2",
    "certifi",
    "click~=8.1",
    "grpclib>=0.4.7,<0.4.9",
    "protobuf>=3.19,<7.0,!=4.24.0",
    "rich>=12.0.0",
    "synchronicity~=0.10.2",
    "toml",
    "typer>=0.9",
    "types-certifi",
    "types-toml",
    "watchfiles",
    "typing_extensions~=4.6"
]
keywords = ["modal", "client", "cloud", "serverless", "infrastructure"]
classifiers = [
    "Topic :: System :: Distributed Computing",
    "Operating System :: OS Independent",
    "License :: OSI Approved :: Apache Software License",
    "Programming Language :: Python :: 3"
]

[project.urls]
Homepage = "https://modal.com"
Source = "https://github.com/modal-labs/modal-client"
Documentation = "https://modal.com/docs"
"Issue Tracker" = "https://github.com/modal-labs/modal-client/issues"

[project.scripts]
modal = "modal.__main__:main"

[tool.setuptools.packages.find]
include = ["modal", "modal.*", "modal_docs", "modal_docs.*", "modal_version", "modal_proto"]
exclude = ["test*", "modal_global_objects"]

[tool.setuptools.package-data]
modal = ["builder/*.md", "builder/*.txt", "builder/*.json", "py.typed", "*.pyi"]
modal_proto = ["*.proto", "py.typed", "*.pyi"]

[tool.setuptools.dynamic]
version = {attr = "modal_version.__version__"}

[tool.mypy]
python_version = "3.11"
exclude = "build"
ignore_missing_imports = true
check_untyped_defs = true
no_strict_optional = true
namespace_packages = true

[[tool.mypy.overrides]]
module = [
    "modal/_vendor/cloudpickle",
    "modal/_vendor/tblib",
    "modal/_vendor/a2wsgi_wsgi",
]
ignore_errors = true

[tool.pytest.ini_options]
timeout = 300
addopts = "--ignore=modal/cli/programs"
filterwarnings = [
    "error::DeprecationWarning",
    "ignore:Type google._upb.*MapContainer uses PyType_Spec.*Python 3.14:DeprecationWarning",
    "error::modal.exception.DeprecationError",
    "ignore::DeprecationWarning:pytest.*:",
    "ignore::DeprecationWarning:pkg_resources.*:",
    "ignore::DeprecationWarning:google.rpc.*:",
    "ignore:.*pkg_resources.*:DeprecationWarning::",
]

[tool.ruff]
extend-include = ["*.pyi"]
exclude = [
    '.venv',
    '.git',
    '__pycache__',
    'proto',
    'build',
    'modal_proto',
    'modal/_vendor',
]
line-length = 120

[tool.ruff.lint]
ignore = ['E741', 'F841']
select = [
    'E',
    'F',
    'W',
    'I',
    'RUF006', # asyncio-dangling-task
]
preview = true
explicit-preview-rules = true
extend-select = ['PLR1702']

[tool.ruff.lint.per-file-ignores]
"*_test.py" = ['E712']
"test/supports/notebooks/*.py" = ['E402']

[tool.ruff.lint.isort]
combine-as-imports = true
known-first-party = [
    "modal",
    "modal_global_objects",
    "modal_proto",
    "modal_version",
]
extra-standard-library = ["pytest"]

[tool.ruff.lint.pylint]
max-nested-blocks = 7

[tool.pyright]
reportUnnecessaryComparison = true
venvPath = ".venv/"



================================================
FILE: requirements.dev.txt
================================================
# Development requirements

black==23.11.0
cbor2==5.7.0
fastapi~=0.115
flaky~=3.7
grpcio-tools==1.48.0;python_version<'3.11'  # TODO: remove when we drop client support for Protobuf 3.19
grpcio-tools==1.59.2;python_version>='3.11' and python_version<'3.13'
grpcio-tools==1.66.2;python_version>='3.13'
grpclib==0.4.7
httpx~=0.23.0
invoke~=2.2
pyjwt==2.10.1
mypy~=1.11.2
mypy-protobuf~=3.3.0  # TODO: can't use mypy-protobuf>=3.4 because of protobuf==3.19 support
pre-commit>=2.21,<4
packaging>=24.2
pytest~=8.4.0
pytest-asyncio @ git+https://github.com/modal-labs/pytest-asyncio.git@706005ef8ca1b0a50fb17c9780b32004ab2d16ae
pytest-env~=1.1.5
pytest-markdown-docs==0.9.0
pytest-timeout~=2.4.0
python-dotenv~=1.0.0;python_version>='3.8'
requests~=2.32.4
ruff==0.9.6
types-croniter~=1.0.8
types-python-dateutil~=2.8.10
types-requests~=2.31.0
types-setuptools~=57.4.11
types-six==1.16.21
types-toml~=0.10.4
twine~=6.1.0
wheel~=0.38.1
nbclient==0.6.8
notebook==6.5.1
jupytext==1.14.1
pyright==1.1.351
python-json-logger==2.0.7  # unpinned transitive dependency of jupytext breaking on later versions
console-ctrl==0.1.0



================================================
FILE: SECURITY.md
================================================
# Reporting a Vulnerability

If you discover a security vulnerability, please report it to security@modal.com.

We take security seriously and will acknowledge your report within one business day. Resolution times vary based on severity and impact.
Read more about our Bug Bounty program here: https://modal.com/docs/guide/security#bug-bounty-program.

Thank you for helping us keep our platform secure.



================================================
FILE: tasks.py
================================================
# Copyright Modal Labs 2022

import ast
import datetime
import importlib
import os
import pkgutil
import re
import subprocess
import sys
from collections.abc import Generator
from contextlib import contextmanager
from datetime import date
from pathlib import Path
from tempfile import NamedTemporaryFile

import requests
from invoke import call, task
from packaging.version import Version
from rich.console import Console
from rich.table import Table

# Set working directory to the root of the client repository.
original_cwd = Path.cwd()
project_root = Path(os.path.dirname(__file__))
os.chdir(project_root)


year = datetime.date.today().year
copyright_header_start = "# Copyright Modal Labs"
copyright_header_full = f"{copyright_header_start} {year}"


@contextmanager
def python_file_as_executable(path: Path) -> Generator[Path, None, None]:
    if sys.platform == "win32":
        # windows can't just run shebang:ed python files, so we create a .bat file that calls it
        src = f"""@echo off
{sys.executable} {path}
"""
        with NamedTemporaryFile(mode="w", suffix=".bat", encoding="ascii", delete=False) as f:
            f.write(src)

        try:
            yield Path(f.name)
        finally:
            Path(f.name).unlink()
    else:
        yield path


@task
def protoc(ctx):
    """Compile protocol buffer files for gRPC and Modal-specific wrappers.

    Generates Python stubs for api.proto and options.proto."""
    protoc_cmd = f"{sys.executable} -m grpc_tools.protoc"
    client_proto_files = "modal_proto/api.proto modal_proto/options.proto"
    sandbox_router_proto_file = "modal_proto/sandbox_router.proto"
    py_protoc = (
        protoc_cmd + " --python_out=. --grpclib_python_out=." + " --grpc_python_out=. --mypy_out=. --mypy_grpc_out=."
    )
    print(py_protoc)
    # generate grpcio and grpclib proto files:
    ctx.run(f"{py_protoc} -I . {client_proto_files} {sandbox_router_proto_file}")

    # generate modal-specific wrapper around grpclib api stub using custom plugin:
    grpc_plugin_pyfile = Path("protoc_plugin/plugin.py")

    with python_file_as_executable(grpc_plugin_pyfile) as grpc_plugin_executable:
        ctx.run(
            f"{protoc_cmd} --plugin=protoc-gen-modal-grpclib-python={grpc_plugin_executable}"
            + f" --modal-grpclib-python_out=. -I . {client_proto_files}"
        )


@task(
    help={
        "fix": "Auto-fix issues if possible",
    },
)
def lint(ctx, fix=False):
    """Run linter on all files."""
    ctx.run(f"ruff check {'--fix' if fix else ''}", pty=True, echo=True)
    ctx.run(f"ruff format {'' if fix else '--diff'}", pty=True, echo=True)


def lint_protos_impl(ctx, proto_fname: str):
    with open(proto_fname) as f:
        proto_text = f.read()

    sections = ["import", "enum", "message", "service"]
    section_regex = "|".join(sections)
    matches = re.findall(rf"^((?:{section_regex})\s+(?:\w+))", proto_text, flags=re.MULTILINE)
    entities = [tuple(e.split()) for e in matches]

    console = Console()

    def get_first_lineno_with_prefix(text: str, prefix: str) -> int:
        lines = text.split("\n")
        for lineno, line in enumerate(lines):
            if re.match(rf"^{prefix}", line):
                return lineno
        raise RuntimeError(f"Failed to find line starting with `{prefix}` (this shouldn't happen)")

    section_order = {key: i for i, key in enumerate(sections)}
    for (a_type, a_name), (b_type, b_name) in zip(entities[:-1], entities[1:]):
        if (section_order[a_type] > section_order[b_type]) or (a_type == b_type and a_name > b_name):
            # This is a simplistic and sort of hacky of way of identifying the "out of order" entity,
            # as the latter one may be the one that is misplaced. Doesn't seem worth the effort though.
            lineno = get_first_lineno_with_prefix(proto_text, f"{a_type} {a_name}")
            console.print(f"[bold red]Proto lint error:[/bold red] {proto_fname}:{lineno}")
            console.print(f"\nThe {a_name} {a_type} proto is out of order relative to the {b_name} {b_type}.")
            console.print(
                "\nProtos should be organized into the following sections:", *sections, sep="\n - ", style="dim"
            )
            console.print("\nWithin sections, protos should be lexicographically sorted by name.", style="dim")
            sys.exit(1)

    service_chunks = re.findall(r"service \w+ {(.+)}", proto_text, flags=re.DOTALL)
    for service_text in service_chunks:
        rpcs = re.findall(r"^\s*rpc\s+(\w+)", service_text, flags=re.MULTILINE)
        for rpc_a, rpc_b in zip(rpcs[:-1], rpcs[1:]):
            if rpc_a > rpc_b:
                lineno = get_first_lineno_with_prefix(proto_text, rf"\s*rpc\s+{rpc_a}")
                console.print(f"[bold red]Proto lint error:[/bold red] {proto_fname}:{lineno}")
                console.print(f"\nThe {rpc_a} rpc proto is out of order relative to the {rpc_b} rpc.")
                console.print("\nRPC definitions should be ordered within each service proto.", style="dim")
                sys.exit(1)


@task
def lint_protos(ctx):
    """Lint protocol buffer files.

    Ensures imports/enums/messages/services are ordered correctly and RPCs are alphabetized.
    """
    lint_protos_impl(ctx, "modal_proto/api.proto")
    lint_protos_impl(ctx, "modal_proto/sandbox_router.proto")


@task
def type_stubs(ctx):
    """Generate type stub files (.pyi) for synchronicity-wrapped Modal modules.

    We only generate type stubs for modules that contain synchronicity wrapped types.
    """
    from synchronicity.synchronizer import SYNCHRONIZER_ATTR

    stubs_to_remove = []
    for root, _, files in os.walk("modal"):
        for file in files:
            if file.endswith(".pyi"):
                stubs_to_remove.append(os.path.abspath(os.path.join(root, file)))
    for path in sorted(stubs_to_remove):
        os.remove(path)
        print(f"Removed {path}")

    def find_modal_modules(root: str = "modal"):
        modules = []
        path = importlib.import_module(root).__path__
        for _, name, is_pkg in pkgutil.iter_modules(path):
            full_name = f"{root}.{name}"
            if is_pkg:
                modules.extend(find_modal_modules(full_name))
            else:
                modules.append(full_name)
        return modules

    def get_wrapped_types(module_name: str) -> list[str]:
        module = importlib.import_module(module_name)
        return [
            name
            for name, obj in vars(module).items()
            if not module_name.startswith("modal.cli.")  # TODO we don't handle typer-wrapped functions well
            and hasattr(obj, "__module__")
            and obj.__module__ == module_name
            and not name.startswith("_")  # Avoid deprecation of _App.__getattr__
            and hasattr(obj, SYNCHRONIZER_ATTR)
        ]

    modules = [m for m in find_modal_modules() if len(get_wrapped_types(m))]
    subprocess.check_call(["python", "-m", "synchronicity.type_stubs", *modules])
    ctx.run("ruff format modal/ --exclude=*.py --no-respect-gitignore", pty=True)


@task(type_stubs)
def type_check(ctx):
    """Run static type checking.

    Uses mypy for most files, but since mypy will not check the *implementation* (.py) for files that also have .pyi
    type stubs, we use pyright for checking the implementation of those files.
    """
    mypy_exclude_list = [
        "playground",
        "venv312",
        "venv311",
        "venv310",
        "venv39",
        "venv38",
        "test/cls_test.py",  # blocked by mypy bug: https://github.com/python/mypy/issues/16527
        "test/supports/sibling_hydration_app.py",  # blocked by mypy bug: https://github.com/python/mypy/issues/16527
        "test/supports/type_assertions_negative.py",
    ]
    excludes = " ".join(f"--exclude {path}" for path in mypy_exclude_list)
    ctx.run(f"mypy . {excludes}", pty=True)

    pyright_allowlist = [
        "modal/_functions.py",
        "modal/_runtime/asgi.py",
        "modal/_runtime/user_code_imports.py",
        "modal/_utils/__init__.py",
        "modal/_utils/async_utils.py",
        "modal/_utils/grpc_testing.py",
        "modal/_utils/hash_utils.py",
        "modal/_utils/http_utils.py",
        "modal/_utils/name_utils.py",
        "modal/_utils/logger.py",
        "modal/_utils/mount_utils.py",
        "modal/_utils/package_utils.py",
        "modal/_utils/rand_pb_testing.py",
        "modal/_utils/shell_utils.py",
        "test/cls_test.py",  # see mypy bug above - but this works with pyright, so we run that instead
        "modal/_runtime/container_io_manager.py",
        "modal/io_streams.py",
        "modal/image.py",
        "modal/file_io.py",
        "modal/cli/import_refs.py",
        "modal/snapshot.py",
        "modal/config.py",
        "modal/object.py",
        "modal/_type_manager.py",
    ]
    ctx.run(f"pyright {' '.join(pyright_allowlist)}", pty=True)


@task(
    help={
        "pytest_args": "Arguments to pass to pytest",
    },
)
def test(ctx, pytest_args="-v"):
    """Run all tests."""
    ctx.run(f"pytest {pytest_args}", pty=sys.platform != "win32")  # win32 doesn't support the 'pty' module


@task(
    help={
        "fix": "Automatically add missing headers",
    },
)
def check_copyright(ctx, fix=False):
    """Verify all Python files have correct copyright headers.

    Excludes generated, vendored, and third-party code."""
    invalid_files = []
    for root, dirs, files in os.walk("."):
        fns = [
            os.path.join(root, fn)
            for fn in files
            if (
                fn.endswith(".py")
                # jupytext notebook formatted .py files can't be detected as notebooks if we put a
                # copyright comment at the top
                and not fn.endswith(".notebook.py")
                # ignore generated protobuf code
                and "/modal_proto" not in root
                # vendored code has a different copyright
                and "_vendor" not in root
                and "protoc_plugin" not in root
                # third-party code (i.e., in a local venv) has a different copyright
                and "/site-packages/" not in root
                and "/build/" not in root
                and "/.venv/" not in root
                and not re.search(r"/venv[0-9]*/", root)
            )
        ]
        for fn in fns:
            first_line = open(fn).readline()
            if not first_line.startswith(copyright_header_start):
                if fix:
                    print(f"Fixing {fn}...")
                    content = copyright_header_full + "\n" + open(fn).read()
                    with open(fn, "w") as g:
                        g.write(content)
                else:
                    invalid_files.append(fn)

    if invalid_files:
        for fn in invalid_files:
            print("Missing copyright:", fn)

        raise Exception(f"{len(invalid_files)} are missing copyright headers! Please run `inv check-copyright --fix`")


@task(
    pre=[
        call(check_copyright, fix=True),
        call(lint, fix=True),
        lint_protos,
        type_check,
    ]
)
def pre_pr_checks(ctx):
    """Run all pre-PR validation checks.

    Auto-fixes anything that can be auto-fixed."""
    ...


def _check_prod(no_confirm: bool):
    from urllib.parse import urlparse

    from modal import config

    server_url = config.config["server_url"]
    if "localhost" not in urlparse(server_url).netloc and not no_confirm:
        answer = input(f"ðŸš¨ Modal server URL is '{server_url}' not localhost. Continue operation? [y/N]: ")
        if answer.upper() not in ["Y", "YES"]:
            exit("Aborting task.")
    return server_url


@task
def publish_base_mounts(ctx, no_confirm: bool = False):
    """Publish the client mount and other mounts."""
    _check_prod(no_confirm)
    for mount in ["modal_client_package", "python_standalone", "modal_client_dependencies"]:
        ctx.run(f"{sys.executable} modal_global_objects/mounts/{mount}.py", pty=True)


@task(
    help={
        "name": "Image name (e.g. 'debian_slim')",
        "builder_version": "Docker builder version",
        "allow_global_deployment": "Required flag to confirm global deployment",
    },
)
def publish_base_images(
    ctx,
    name: str,
    builder_version: str = "2024.10",
    allow_global_deployment: bool = False,
    no_confirm: bool = False,
) -> None:
    """Publish base images. For example, `inv publish-base-images debian_slim`.

    These should be published as global deployments. However, publishing global
    deployments is *risky* because it would affect all workspaces. Pass the
    `--allow-global-deployment` flag to confirm this behavior."""
    if not allow_global_deployment:
        console = Console()
        console.print("This is a dry run. Rerun with `--allow-global-deployment` to publish.", style="yellow")

    _check_prod(no_confirm)
    ctx.run(
        f"python -m modal_global_objects.images.base_images {name}",
        pty=True,
        env={
            "MODAL_IMAGE_ALLOW_GLOBAL_DEPLOYMENT": "1" if allow_global_deployment else "",
            "MODAL_IMAGE_BUILDER_VERSION": builder_version,
        },
    )


version_file_contents_template = '''\
# Copyright Modal Labs 2025
"""Supplies the current version of the modal client library."""

__version__ = "{}"
'''


@task(
    help={
        "force": "Bump even if version file was modified in last commit",
    },
)
def bump_dev_version(ctx, dry_run: bool = False, force: bool = False):
    """Automatically increment the modal version, handling dev releases (but not other pre-releases).

    This only has an effect when the version file was not modified by the most recent git commit
    (unless `force` is True).

    The version will always be in development after this runs. In the context of the modal client
    release process, manually updating the version file to a non-development version will trigger
    a "real" release. Otherwise we'll push the development version to PyPI.
    """
    version_file = "modal_version/__init__.py"
    commit_files = ctx.run("git diff --name-only HEAD~1 HEAD", hide="out").stdout.splitlines()
    if version_file in commit_files and not force:
        print(f"Aborting: {version_file} was modified by the most recent commit")
        return

    from modal_version import __version__

    v = Version(__version__)

    if v.is_prerelease:
        if not v.is_devrelease:
            raise RuntimeError("We only know how to auto-bump dev versions")
        # For dev releases, increment the dev suffix
        next_version = f"{v.major}.{v.minor}.{v.micro}.dev{v.dev + 1}"
    else:
        # If the most recent commit was *not* a dev release, start the next cycle
        next_version = f"{v.major}.{v.minor}.{v.micro + 1}.dev0"

    version_file_contents = version_file_contents_template.format(next_version)
    if dry_run:
        print(f"Would update {version_file} to the following:")
        print(version_file_contents)
        return

    with open(version_file, "w") as f:
        f.write(version_file_contents)


@task
def get_release_tag(ctx):
    """Optionally print a tag name for the current modal client version."""
    from modal_version import __version__

    v = Version(__version__)
    if not v.is_devrelease:
        print(f"v{v}")


@task(
    help={
        "sha": "Commit SHA (defaults to the most recent commit)",
    },
)
def update_changelog(ctx, sha: str = ""):
    """Update CHANGELOG.md from GitHub PR description.

    Parse a commit message for a GitHub PR number. Requires GITHUB_TOKEN environment variable."""
    res = ctx.run(f"git log --pretty=format:%s -n 1 {sha}", hide="stdout", warn=True)
    if res.exited:
        print("Failed to extract changelog update!")
        print("Last 5 commits:")
        res = ctx.run("git log --pretty=oneline -n 5")
        return
    m = re.search(r"\(#(\d+)\)$", res.stdout)
    if m:
        pull_number = m.group(1)
    else:
        print("Aborting: No PR number in commit message")
        return

    # Get the corresponding PR description via the GitHub API
    url = f"https://api.github.com/repos/modal-labs/modal-client/pulls/{pull_number}"
    headers = {"Authorization": f"Bearer {os.environ['GITHUB_TOKEN']}", "Accept": "application/vnd.github.v3+json"}
    response = requests.get(url, headers=headers).json()
    pr_description = response.get("body")
    if pr_description is None:
        print("Aborting: No PR description in response from GitHub API")
        return

    # Parse the PR description to get a changelog update, which is all text between
    # the changelog header and any auto comments appended by Cursor

    changelog_pattern = r"## Changelog\s*(.+?)(?:<!--\s*\w*CURSOR\w*\s*-->|$)"
    m = re.search(changelog_pattern, pr_description, flags=re.DOTALL)
    if m:
        update = m.group(1).strip()
    else:
        print("Aborting: No changelog section in PR description")
        return
    if not update:
        print("Aborting: Empty changelog in PR description")
        return

    # Remove any HTML comments
    comment_pattern = r"<!--.+?-->"
    update = re.sub(comment_pattern, "", update, flags=re.DOTALL)

    # Read the existing changelog and split after the header so we can prepend new content
    with open("CHANGELOG.md") as fid:
        content = fid.read()
    token_pattern = "<!-- NEW CONTENT GENERATED BELOW. PLEASE PRESERVE THIS COMMENT. -->"
    m = re.search(token_pattern, content)
    if m:
        break_idx = m.span()[1]
        header = content[:break_idx]
        previous_changelog = content[break_idx:]
    else:
        print("Aborting: Could not find token in existing changelog to mark insertion spot")
        return

    # Build the new changelog and write it out
    from modal_version import __version__

    date = datetime.datetime.now().strftime("%Y-%m-%d")
    new_section = f"#### {__version__} ({date})\n\n{update}"
    final_content = f"{header}\n\n{new_section}\n{previous_changelog}"
    with open("CHANGELOG.md", "w") as fid:
        fid.write(final_content)


@task
def show_deprecations(ctx):
    """Analyze Modal source code and display all deprecation warnings/errors.

    Shows deprecation date, level, location, function, and message in a formatted table."""

    def get_modal_source_files() -> list[str]:
        source_files: list[str] = []
        for root, _, files in os.walk("modal"):
            for file in files:
                if file.endswith(".py"):
                    source_files.append(os.path.join(root, file))
        return source_files

    class FunctionCallVisitor(ast.NodeVisitor):
        def __init__(self, fname):
            self.fname = fname
            self.deprecations = []
            self.assignments = {}
            self.current_class = None
            self.current_function = None

        def visit_ClassDef(self, node):
            self.current_class = node.name
            self.generic_visit(node)
            self.current_class = None

        def visit_FunctionDef(self, node):
            self.current_function = node.name
            self.assignments["__doc__"] = ast.get_docstring(node)
            self.generic_visit(node)
            self.current_function = None
            self.assignments.pop("__doc__", None)

        def visit_Assign(self, node):
            for target in node.targets:
                if isinstance(target, ast.Name):
                    self.assignments[target.id] = node.value
            self.generic_visit(node)

        def visit_Attribute(self, node):
            self.assignments[node.attr] = node.value
            self.generic_visit(node)

        def visit_Call(self, node):
            func_name_to_level = {
                "deprecation_warning": "[yellow]warning[/yellow]",
                "deprecation_error": "[red]error[/red]",
                # We may add a flag to make renamed_parameter error instead of warn
                # in which case this would get a little bit more complicated.
                "renamed_parameter": "[yellow]warning[/yellow]",
            }
            if (
                isinstance(node.func, ast.Name)
                and node.func.id in func_name_to_level
                and isinstance(node.args[0], ast.Tuple)
            ):
                depr_date = date(*(getattr(elt, "n") for elt in node.args[0].elts))
                function = (
                    f"{self.current_class}.{self.current_function}" if self.current_class else self.current_function
                )
                if node.func.id == "renamed_parameter":
                    old_name = getattr(node.args[1], "s")
                    new_name = getattr(node.args[2], "s")
                    message = f"Renamed parameter: {old_name} -> {new_name}"
                else:
                    message = node.args[1]
                    # Handle a few different ways that the message can get passed to the deprecation helper
                    # since it's not always a literal string (e.g. it's often a functions .__doc__ attribute)
                    if isinstance(message, ast.Name):
                        message = self.assignments.get(message.id, "")
                    if isinstance(message, ast.Attribute):
                        message = self.assignments.get(message.attr, "")
                    if isinstance(message, ast.Constant):
                        message = message.s
                    elif isinstance(message, ast.JoinedStr):
                        message = "".join(v.s for v in message.values if isinstance(v, ast.Constant))
                    else:
                        message = str(message)
                    message = message.replace("\n", " ")
                    if len(message) > (max_length := 80):
                        message = message[:max_length] + "..."

                level = func_name_to_level[node.func.id]
                self.deprecations.append((str(depr_date), level, f"{self.fname}:{node.lineno}", function, message))

    files = get_modal_source_files()
    deprecations = []
    for fname in files:
        with open(fname) as f:
            tree = ast.parse(f.read())
        visitor = FunctionCallVisitor(fname)
        visitor.visit(tree)
        deprecations.extend(visitor.deprecations)

    console = Console()
    table = Table("Date", "Level", "Location", "Function", "Message")
    for row in sorted(deprecations, key=lambda r: r[0]):
        table.add_row(*row)
    console.print(table)



================================================
FILE: .pre-commit-config.yaml
================================================
repos:
  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: "v0.9.6"
    hooks:
      - id: ruff
        # Autofix, and respect `exclude` and `extend-exclude` settings.
        args: [--fix, --exit-non-zero-on-fix]
      - id: ruff-format



================================================
SYMLINK: CLAUDE.md -> AGENTS.md
================================================



================================================
FILE: modal/__init__.py
================================================
# Copyright Modal Labs 2022
import sys

if sys.version_info[:2] < (3, 9):
    raise RuntimeError("This version of Modal requires at least Python 3.9")
if sys.version_info[:2] >= (3, 14):
    raise RuntimeError("This version of Modal does not support Python 3.14+")

from modal_version import __version__

try:
    from ._runtime.execution_context import current_function_call_id, current_input_id, interact, is_local
    from ._tunnel import Tunnel, forward
    from .app import App
    from .client import Client
    from .cloud_bucket_mount import CloudBucketMount
    from .cls import Cls, parameter
    from .dict import Dict
    from .exception import Error
    from .file_pattern_matcher import FilePatternMatcher
    from .functions import Function, FunctionCall
    from .image import Image
    from .network_file_system import NetworkFileSystem
    from .output import enable_output
    from .partial_function import (
        asgi_app,
        batched,
        concurrent,
        enter,
        exit,
        fastapi_endpoint,
        method,
        web_endpoint,
        web_server,
        wsgi_app,
    )
    from .proxy import Proxy
    from .queue import Queue
    from .retries import Retries
    from .sandbox import Sandbox
    from .schedule import Cron, Period
    from .scheduler_placement import SchedulerPlacement
    from .secret import Secret
    from .snapshot import SandboxSnapshot
    from .volume import Volume
except Exception:
    print()
    print("#" * 80)
    print("#" + "Something with the Modal installation seems broken.".center(78) + "#")
    print("#" + "Please email support@modal.com and we will try to help!".center(78) + "#")
    print("#" * 80)
    print()
    raise


__all__ = [
    "__version__",
    "App",
    "Client",
    "Cls",
    "Cron",
    "Dict",
    "Error",
    "FilePatternMatcher",
    "Function",
    "FunctionCall",
    "Image",
    "NetworkFileSystem",
    "Period",
    "Proxy",
    "Queue",
    "Retries",
    "CloudBucketMount",
    "Sandbox",
    "SandboxSnapshot",
    "SchedulerPlacement",
    "Secret",
    "Tunnel",
    "Volume",
    "asgi_app",
    "batched",
    "concurrent",
    "current_function_call_id",
    "current_input_id",
    "enable_output",
    "enter",
    "exit",
    "fastapi_endpoint",
    "forward",
    "is_local",
    "interact",
    "method",
    "parameter",
    "web_endpoint",
    "web_server",
    "wsgi_app",
]


def __getattr__(name):
    if name == "Stub":
        raise AttributeError(
            "Module 'modal' has no attribute 'Stub'. Use `modal.App` instead. This is a simple name change."
        )
    raise AttributeError(f"module 'modal' has no attribute '{name}'")



================================================
FILE: modal/__main__.py
================================================
# Copyright Modal Labs 2022
import sys

from ._output import make_console
from ._traceback import reduce_traceback_to_user_code
from .cli._traceback import highlight_modal_warnings, setup_rich_traceback
from .cli.entry_point import entrypoint_cli
from .cli.import_refs import _CliUserExecutionError
from .config import config


def main():
    # Setup rich tracebacks, but only on user's end, when using the Modal CLI.
    setup_rich_traceback()
    highlight_modal_warnings()

    try:
        entrypoint_cli()

    except _CliUserExecutionError as exc:
        if config.get("traceback"):
            raise

        assert exc.__cause__  # We should always raise this class from another error
        tb = reduce_traceback_to_user_code(exc.__cause__.__traceback__, exc.user_source)
        sys.excepthook(type(exc.__cause__), exc.__cause__, tb)
        sys.exit(1)

    except Exception as exc:
        if (
            # User has asked to alway see full tracebacks
            config.get("traceback")
            # The exception message is empty, so we need to provide _some_ actionable information
            or not str(exc)
        ):
            raise

        from grpclib import GRPCError, Status
        from rich.panel import Panel

        if isinstance(exc, GRPCError):
            status_map = {
                Status.ABORTED: "Aborted",
                Status.ALREADY_EXISTS: "Already exists",
                Status.CANCELLED: "Cancelled",
                Status.DATA_LOSS: "Data loss",
                Status.DEADLINE_EXCEEDED: "Deadline exceeded",
                Status.FAILED_PRECONDITION: "Failed precondition",
                Status.INTERNAL: "Internal",
                Status.INVALID_ARGUMENT: "Invalid",
                Status.NOT_FOUND: "Not found",
                Status.OUT_OF_RANGE: "Out of range",
                Status.PERMISSION_DENIED: "Permission denied",
                Status.RESOURCE_EXHAUSTED: "Resource exhausted",
                Status.UNAUTHENTICATED: "Unauthenticaed",
                Status.UNAVAILABLE: "Unavailable",
                Status.UNIMPLEMENTED: "Unimplemented",
                Status.UNKNOWN: "Unknown",
            }
            title = f"Error: {status_map.get(exc.status, 'Unknown')}"
            content = str(exc.message)
            if exc.details:
                content += f"\n\nDetails: {exc.details}"
        else:
            title = "Error"
            content = str(exc)
            if notes := getattr(exc, "__notes__", []):
                content = f"{content}\n\nNote: {' '.join(notes)}"

        console = make_console(stderr=True)
        panel = Panel(content, title=title, title_align="left", border_style="red")
        console.print(panel, highlight=False)
        sys.exit(1)


if __name__ == "__main__":
    main()



================================================
FILE: modal/_clustered_functions.py
================================================
# Copyright Modal Labs 2024
import os
import socket
from dataclasses import dataclass
from typing import Optional

from modal._utils.async_utils import synchronize_api
from modal._utils.grpc_utils import retry_transient_errors
from modal.client import _Client
from modal.exception import InvalidError
from modal_proto import api_pb2


@dataclass
class ClusterInfo:
    rank: int
    cluster_id: str
    container_ips: list[str]
    container_ipv4_ips: list[str]


cluster_info: Optional[ClusterInfo] = None


def get_cluster_info() -> ClusterInfo:
    if cluster_info is None:
        raise InvalidError(
            "Cluster info not initialized. Please ensure that you are "
            "calling get_cluster_info() from a clustered function."
        )
    return cluster_info


async def _initialize_clustered_function(client: _Client, task_id: str, world_size: int):
    global cluster_info

    def get_i6pn():
        """Returns the ipv6 address assigned to this container."""
        return socket.getaddrinfo("i6pn.modal.local", None, socket.AF_INET6)[0][4][0]

    hostname = socket.gethostname()
    container_ip = get_i6pn()

    # nccl's default host ID is $(hostname)$(cat /proc/sys/kernel/random/boot_id).
    # on runc, if two i6pn-linked containers get scheduled on the same worker,
    # their boot ID and hostname will both be identical, causing nccl to break.
    # As a workaround, we can explicitly specify a unique host ID here.
    # See MOD-4067.
    os.environ["NCCL_HOSTID"] = f"{hostname}{container_ip}"

    # We found these settings to work well in most cases. You may be able to achieve
    # better performance by tuning these settings.
    if os.environ["MODAL_CLOUD_PROVIDER"] in ("CLOUD_PROVIDER_GCP", "CLOUD_PROVIDER_OCI"):
        os.environ["NCCL_SOCKET_NTHREADS"] = "4"
        os.environ["NCCL_NSOCKS_PERTHREAD"] = "1"
    elif os.environ["MODAL_CLOUD_PROVIDER"] == "CLOUD_PROVIDER_AWS":
        os.environ["NCCL_SOCKET_NTHREADS"] = "2"
        os.environ["NCCL_NSOCKS_PERTHREAD"] = "8"
    else:
        os.environ["NCCL_SOCKET_NTHREADS"] = "1"
        os.environ["NCCL_NSOCKS_PERTHREAD"] = "1"

    if world_size > 1:
        resp: api_pb2.TaskClusterHelloResponse = await retry_transient_errors(
            client.stub.TaskClusterHello,
            api_pb2.TaskClusterHelloRequest(
                task_id=task_id,
                container_ip=container_ip,
            ),
        )
        cluster_info = ClusterInfo(
            rank=resp.cluster_rank,
            cluster_id=resp.cluster_id,
            container_ips=resp.container_ips,
            container_ipv4_ips=resp.container_ipv4_ips,
        )
    else:
        cluster_info = ClusterInfo(
            rank=0,
            cluster_id="",  # No cluster ID for single-node  # TODO(irfansharif): Is this right?
            container_ips=[container_ip],
            container_ipv4_ips=[],  # No IPv4 IPs for single-node
        )


initialize_clustered_function = synchronize_api(_initialize_clustered_function)



================================================
FILE: modal/_container_entrypoint.py
================================================
# Copyright Modal Labs 2022
# ruff: noqa: E402
import os

from modal._runtime.user_code_imports import (
    Service,
    import_class_service,
    import_single_function_service,
)

telemetry_socket = os.environ.get("MODAL_TELEMETRY_SOCKET")
if telemetry_socket:
    from ._runtime.telemetry import instrument_imports

    instrument_imports(telemetry_socket)

import asyncio
import inspect
import queue
import signal
import sys
import threading
import time
from collections.abc import Sequence
from typing import TYPE_CHECKING, Any, Callable, Optional

from google.protobuf.message import Message

from modal._clustered_functions import initialize_clustered_function
from modal._partial_function import (
    _find_callables_for_obj,
    _PartialFunctionFlags,
)
from modal._serialization import deserialize, deserialize_params
from modal._utils.async_utils import TaskContext, aclosing, synchronizer
from modal._utils.function_utils import (
    callable_has_non_self_params,
)
from modal.app import App, _App
from modal.client import Client, _Client
from modal.config import logger
from modal.exception import ExecutionError, InputCancellation
from modal.running_app import RunningApp, running_app_from_layout
from modal_proto import api_pb2

from ._runtime import execution_context
from ._runtime.container_io_manager import (
    ContainerIOManager,
    IOContext,
    UserException,
)

if TYPE_CHECKING:
    import modal._object
    import modal._runtime.container_io_manager


class DaemonizedThreadPool:
    # Used instead of ThreadPoolExecutor, since the latter won't allow
    # the interpreter to shut down before the currently running tasks
    # have finished
    def __init__(self, max_threads: int):
        self.max_threads = max_threads

    def __enter__(self):
        self.spawned_workers = 0
        self.inputs: queue.Queue[Any] = queue.Queue()
        self.finished = threading.Event()
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        self.finished.set()

        if exc_type is None:
            self.inputs.join()
        else:
            # special case - allows us to exit the
            if self.inputs.unfinished_tasks:
                logger.info(
                    f"Exiting DaemonizedThreadPool with {self.inputs.unfinished_tasks} active "
                    f"inputs due to exception: {repr(exc_type)}"
                )

    def submit(self, func, *args):
        def worker_thread():
            while not self.finished.is_set():
                try:
                    _func, _args = self.inputs.get(timeout=1)
                except queue.Empty:
                    continue
                try:
                    _func(*_args)
                except BaseException:
                    logger.exception(f"Exception raised by {_func} in DaemonizedThreadPool worker!")
                self.inputs.task_done()

        if self.spawned_workers < self.max_threads:
            threading.Thread(target=worker_thread, daemon=True).start()
            self.spawned_workers += 1

        self.inputs.put((func, args))


class UserCodeEventLoop:
    """Run an async event loop as a context manager and handle signals.

    This will run all *user supplied* async code, i.e. async functions, as well as async enter/exit managers

    The following signals are handled while a coroutine is running on the event loop until
    completion (and then handlers are deregistered):

    - `SIGUSR1`: converted to an async task cancellation. Note that this only affects the event
      loop, and the signal handler defined here doesn't run for sync functions.
    - `SIGINT`: Unless the global signal handler has been set to SIGIGN, the loop's signal handler
        is set to cancel the current task and raise KeyboardInterrupt to the caller.
    """

    def __enter__(self):
        self.loop = asyncio.new_event_loop()
        self.tasks = set()
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        self.loop.run_until_complete(self.loop.shutdown_asyncgens())
        if sys.version_info[:2] >= (3, 9):
            self.loop.run_until_complete(self.loop.shutdown_default_executor())  # Introduced in Python 3.9

        for task in self.tasks:
            task.cancel()

        self.loop.close()

    def create_task(self, coro):
        task = self.loop.create_task(coro)
        self.tasks.add(task)
        task.add_done_callback(self.tasks.discard)
        return task

    def run(self, coro):
        task = asyncio.ensure_future(coro, loop=self.loop)
        self._sigints = 0

        def _sigint_handler():
            # cancel the task in order to have run_until_complete return soon and
            # prevent a bunch of unwanted tracebacks when shutting down the
            # event loop.

            # this basically replicates the sigint handler installed by asyncio.run()
            self._sigints += 1
            if self._sigints == 1:
                # first sigint is graceful
                task.cancel()
                return

            # this should normally not happen, but the second sigint would "hard kill" the event loop!
            raise KeyboardInterrupt()

        ignore_sigint = signal.getsignal(signal.SIGINT) == signal.SIG_IGN
        if not ignore_sigint:
            self.loop.add_signal_handler(signal.SIGINT, _sigint_handler)

        # Before Python 3.9 there is no argument to Task.cancel
        if sys.version_info[:2] >= (3, 9):
            self.loop.add_signal_handler(signal.SIGUSR1, task.cancel, "Input was cancelled by user")
        else:
            self.loop.add_signal_handler(signal.SIGUSR1, task.cancel)

        try:
            return self.loop.run_until_complete(task)
        except asyncio.CancelledError:
            if self._sigints > 0:
                raise KeyboardInterrupt()
        finally:
            self.loop.remove_signal_handler(signal.SIGUSR1)
            if not ignore_sigint:
                self.loop.remove_signal_handler(signal.SIGINT)


def call_function(
    user_code_event_loop: UserCodeEventLoop,
    container_io_manager: "modal._runtime.container_io_manager.ContainerIOManager",
    finalized_functions: dict[str, "modal._runtime.user_code_imports.FinalizedFunction"],
    batch_max_size: int,
    batch_wait_ms: int,
):
    async def run_input_async(io_context: IOContext) -> None:
        reset_context = execution_context._set_current_context_ids(
            io_context.input_ids, io_context.function_call_ids, io_context.attempt_tokens
        )
        started_at = time.time()
        async with container_io_manager.handle_input_exception.aio(io_context, started_at):
            # TODO(erikbern): any exception below shouldn't be considered a user exception
            if io_context.finalized_function.is_generator:
                # Send up to this many outputs at a time.
                current_function_call_id = execution_context.current_function_call_id()
                assert current_function_call_id is not None  # Set above.
                current_attempt_token = execution_context.current_attempt_token()
                assert current_attempt_token is not None  # Set above, but can be empty string.
                generator_queue: asyncio.Queue[Any] = await container_io_manager._queue_create.aio(1024)
                async with container_io_manager.generator_output_sender(
                    current_function_call_id,
                    current_attempt_token,
                    io_context._generator_output_format(),
                    generator_queue,
                ):
                    item_count = 0
                    async with aclosing(io_context.call_generator_async()) as gen:
                        async for value in gen:
                            await container_io_manager._queue_put.aio(generator_queue, value)
                            item_count += 1

                await container_io_manager._send_outputs.aio(
                    started_at, io_context.output_items_generator_done(started_at, item_count)
                )
            else:
                value = await io_context.call_function_async()
                await container_io_manager.push_outputs.aio(
                    io_context,
                    started_at,
                    value,
                )
        reset_context()

    def run_input_sync(io_context: IOContext) -> None:
        started_at = time.time()
        reset_context = execution_context._set_current_context_ids(
            io_context.input_ids, io_context.function_call_ids, io_context.attempt_tokens
        )
        with container_io_manager.handle_input_exception(io_context, started_at):
            # TODO(erikbern): any exception below shouldn't be considered a user exception
            if io_context.finalized_function.is_generator:
                gen = io_context.call_generator_sync()
                # Send up to this many outputs at a time.
                current_function_call_id = execution_context.current_function_call_id()
                assert current_function_call_id is not None  # Set above.
                current_attempt_token = execution_context.current_attempt_token()
                assert current_attempt_token is not None  # Set above, but can be empty string.
                generator_queue: asyncio.Queue[Any] = container_io_manager._queue_create(1024)
                with container_io_manager.generator_output_sender(
                    current_function_call_id,
                    current_attempt_token,
                    io_context._generator_output_format(),
                    generator_queue,
                ):
                    item_count = 0
                    for value in gen:
                        container_io_manager._queue_put(generator_queue, value)
                        item_count += 1

                container_io_manager._send_outputs(
                    started_at, io_context.output_items_generator_done(started_at, item_count)
                )
            else:
                values = io_context.call_function_sync()
                container_io_manager.push_outputs(io_context, started_at, values)
        reset_context()

    if container_io_manager.input_concurrency_enabled:
        with DaemonizedThreadPool(max_threads=container_io_manager.max_concurrency) as thread_pool:

            def make_async_cancel_callback(task):
                def f():
                    user_code_event_loop.loop.call_soon_threadsafe(task.cancel)

                return f

            did_sigint = False

            def cancel_callback_sync():
                nonlocal did_sigint
                # We only want one sigint even if multiple inputs are cancelled
                # A second sigint would forcibly shut down the event loop and spew
                # out a bunch of tracebacks, which we only want to happen in case
                # the worker kills this process after a failed self-termination
                if not did_sigint:
                    did_sigint = True
                    logger.warning(
                        "User cancelling input of non-async functions with input concurrency enabled.\n"
                        "This shuts down the container, causing concurrently running inputs to be "
                        "rescheduled in other containers."
                    )
                    os.kill(os.getpid(), signal.SIGINT)

            async def run_concurrent_inputs():
                # all run_input coroutines will have completed by the time we leave the execution context
                # but the wrapping *tasks* may not yet have been resolved, so we add a 0.01s
                # for them to resolve gracefully:
                async with TaskContext(0.01) as task_context:
                    async for io_context in container_io_manager.run_inputs_outputs.aio(
                        finalized_functions, batch_max_size, batch_wait_ms
                    ):
                        # Note that run_inputs_outputs will not return until all the input slots are released
                        # so that they can be acquired by the run_inputs_outputs finalizer
                        # This prevents leaving the task_context before outputs have been created
                        # TODO: refactor to make this a bit more easy to follow?
                        if io_context.finalized_function.is_async:
                            input_task = task_context.create_task(run_input_async(io_context))
                            io_context.set_cancel_callback(make_async_cancel_callback(input_task))
                        else:
                            # run sync input in thread
                            thread_pool.submit(run_input_sync, io_context)
                            io_context.set_cancel_callback(cancel_callback_sync)

            user_code_event_loop.run(run_concurrent_inputs())
    else:
        for io_context in container_io_manager.run_inputs_outputs(finalized_functions, batch_max_size, batch_wait_ms):
            # This goes to a registered signal handler for sync Modal functions, or to the
            # `UserCodeEventLoop` for async functions.
            #
            # We only send this signal on functions that do not have concurrent inputs enabled.
            # This allows us to do fine-grained input cancellation. On sync functions, the
            # SIGUSR1 signal should interrupt the main thread where user code is running,
            # raising an InputCancellation() exception. On async functions, the signal should
            # reach a handler in UserCodeEventLoop, which cancels the task.
            io_context.set_cancel_callback(lambda: os.kill(os.getpid(), signal.SIGUSR1))

            if io_context.finalized_function.is_async:
                user_code_event_loop.run(run_input_async(io_context))
            else:
                # Set up a custom signal handler for `SIGUSR1`, which gets translated to an InputCancellation
                # during function execution. This is sent to cancel inputs from the user
                def _cancel_input_signal_handler(signum, stackframe):
                    raise InputCancellation("Input was cancelled by user")

                usr1_handler = signal.signal(signal.SIGUSR1, _cancel_input_signal_handler)
                # run this sync code in the main thread, blocking the "userland" event loop
                # this lets us cancel it using a signal handler that raises an exception
                try:
                    run_input_sync(io_context)
                finally:
                    signal.signal(signal.SIGUSR1, usr1_handler)  # reset signal handler


def call_lifecycle_functions(
    event_loop: UserCodeEventLoop,
    container_io_manager,  #: ContainerIOManager,  TODO: this type is generated at runtime
    funcs: Sequence[Callable[..., Any]],
) -> None:
    """Call function(s), can be sync or async, but any return values are ignored."""
    with container_io_manager.handle_user_exception():
        for func in funcs:
            # We are deprecating parametrized exit methods but want to gracefully handle old code.
            # We can remove this once the deprecation in the actual @exit decorator is enforced.
            args = (None, None, None) if callable_has_non_self_params(func) else ()
            # in case func is non-async, it's executed here and sigint will by default
            # interrupt it using a KeyboardInterrupt exception
            res = func(*args)
            if inspect.iscoroutine(res):
                # if however func is async, we have to jump through some hoops
                event_loop.run(res)


def main(container_args: api_pb2.ContainerArguments, client: Client):
    # This is a bit weird but we need both the blocking and async versions of ContainerIOManager.
    # At some point, we should fix that by having built-in support for running "user code"
    container_io_manager = ContainerIOManager(container_args, client)
    active_app: _App
    service: Service
    function_def = container_args.function_def
    is_auto_snapshot: bool = function_def.is_auto_snapshot
    # The worker sets this flag to "1" for snapshot and restore tasks. Otherwise, this flag is unset,
    # in which case snapshots should be disabled.
    is_snapshotting_function = (
        function_def.is_checkpointing_function and os.environ.get("MODAL_ENABLE_SNAP_RESTORE") == "1"
    )

    _client: _Client = synchronizer._translate_in(client)  # TODO(erikbern): ugly

    # Call ContainerHello - currently a noop but might be used later for things
    container_io_manager.hello()

    with container_io_manager.heartbeats(is_snapshotting_function), UserCodeEventLoop() as event_loop:
        # If this is a serialized function, fetch the definition from the server
        if function_def.definition_type == api_pb2.Function.DEFINITION_TYPE_SERIALIZED:
            assert function_def.function_serialized or function_def.class_serialized

            if function_def.function_serialized:
                ser_fun = deserialize(function_def.function_serialized, _client)
            else:
                ser_fun = None

            if function_def.class_serialized:
                ser_usr_cls = deserialize(function_def.class_serialized, _client)
            else:
                ser_usr_cls = None
        else:
            ser_usr_cls, ser_fun = None, None

        # Initialize the function, importing user code.
        with container_io_manager.handle_user_exception():
            if container_args.serialized_params:
                param_args, param_kwargs = deserialize_params(container_args.serialized_params, function_def, _client)
            else:
                param_args = ()
                param_kwargs = {}

            with execution_context._import_context():
                if function_def.is_class:
                    # this is a bit ugly - match the function and class based on function name to get metadata
                    # This metadata is required in order to hydrate the class in case it's not globally
                    # decorated (or serialized)
                    service_base_function_id = container_args.app_layout.function_ids[function_def.function_name]
                    service_function_hydration_data = [
                        o for o in container_args.app_layout.objects if o.object_id == service_base_function_id
                    ][0]
                    class_id = container_args.app_layout.class_ids[function_def.function_name.removesuffix(".*")]

                    service = import_class_service(
                        function_def,
                        service_function_hydration_data,
                        class_id,
                        client,
                        ser_usr_cls,
                        param_args,
                        param_kwargs,
                    )
                else:
                    assert ser_usr_cls is None
                    service = import_single_function_service(
                        function_def,
                        ser_fun,
                    )

            active_app = service.app

            if function_def.pty_info.pty_type == api_pb2.PTYInfo.PTY_TYPE_SHELL:
                # Concurrency and batching doesn't apply for `modal shell`.
                batch_max_size = 0
                batch_wait_ms = 0
            else:
                batch_max_size = function_def.batch_max_size or 0
                batch_wait_ms = function_def.batch_linger_ms or 0

        # Get ids and metadata for objects (primarily functions and classes) on the app
        container_app: RunningApp = running_app_from_layout(container_args.app_id, container_args.app_layout)

        # Initialize objects on the app.
        # This is basically only functions and classes - anything else is deprecated and will be unsupported soon
        app: App = synchronizer._translate_out(active_app)
        app._init_container(client, container_app)

        # Hydrate all function dependencies.
        # TODO(erikbern): we an remove this once we
        # 1. Enable lazy hydration for all objects
        # 2. Fully deprecate .new() objects
        if service.service_deps is not None:  # this is not set for serialized or non-global scope functions
            dep_object_ids: list[str] = [dep.object_id for dep in function_def.object_dependencies]
            if len(service.service_deps) != len(dep_object_ids):
                raise ExecutionError(
                    f"Function has {len(service.service_deps)} dependencies"
                    f" but container got {len(dep_object_ids)} object ids.\n"
                    f"Code deps: {service.service_deps}\n"
                    f"Object ids: {dep_object_ids}"
                )
            for object_id, obj in zip(dep_object_ids, service.service_deps):
                metadata: Message = container_app.object_handle_metadata[object_id]
                obj._hydrate(object_id, _client, metadata)

        # Initialize clustered functions.
        if function_def._experimental_group_size > 0:
            initialize_clustered_function(
                client,
                container_args.task_id,
                function_def._experimental_group_size,
            )

        # Identify all "enter" methods that need to run before we snapshot.
        if service.user_cls_instance is not None and not is_auto_snapshot:
            pre_snapshot_methods = _find_callables_for_obj(
                service.user_cls_instance, _PartialFunctionFlags.ENTER_PRE_SNAPSHOT
            )
            call_lifecycle_functions(event_loop, container_io_manager, list(pre_snapshot_methods.values()))

        # If this container is being used to create a checkpoint, checkpoint the container after
        # global imports and initialization. Checkpointed containers run from this point onwards.
        if is_snapshotting_function:
            container_io_manager.memory_snapshot()

        # Install hooks for interactive functions.
        def breakpoint_wrapper():
            # note: it would be nice to not have breakpoint_wrapper() included in the backtrace
            container_io_manager.interact(from_breakpoint=True)
            import pdb

            frame = inspect.currentframe().f_back

            pdb.Pdb().set_trace(frame)

        sys.breakpointhook = breakpoint_wrapper

        # Identify the "enter" methods to run after resuming from a snapshot.
        if service.user_cls_instance is not None and not is_auto_snapshot:
            post_snapshot_methods = _find_callables_for_obj(
                service.user_cls_instance, _PartialFunctionFlags.ENTER_POST_SNAPSHOT
            )
            call_lifecycle_functions(event_loop, container_io_manager, list(post_snapshot_methods.values()))

        with container_io_manager.handle_user_exception():
            finalized_functions = service.get_finalized_functions(function_def, container_io_manager)
        # Execute the function.
        lifespan_background_tasks = []
        try:
            for finalized_function in finalized_functions.values():
                if finalized_function.lifespan_manager:
                    lifespan_background_tasks.append(
                        event_loop.create_task(finalized_function.lifespan_manager.background_task())
                    )
                    with container_io_manager.handle_user_exception():
                        event_loop.run(finalized_function.lifespan_manager.lifespan_startup())
            call_function(
                event_loop,
                container_io_manager,
                finalized_functions,
                batch_max_size,
                batch_wait_ms,
            )
        finally:
            # Run exit handlers. From this point onward, ignore all SIGINT signals that come from
            # graceful shutdowns originating on the worker, as well as stray SIGUSR1 signals that
            # may have been sent to cancel inputs.
            int_handler = signal.signal(signal.SIGINT, signal.SIG_IGN)
            usr1_handler = signal.signal(signal.SIGUSR1, signal.SIG_IGN)

            try:
                try:
                    # run lifespan shutdown for asgi apps
                    for finalized_function in finalized_functions.values():
                        if finalized_function.lifespan_manager:
                            with container_io_manager.handle_user_exception():
                                event_loop.run(finalized_function.lifespan_manager.lifespan_shutdown())
                finally:
                    # no need to keep the lifespan asgi call around - we send it no more messages
                    for lifespan_background_task in lifespan_background_tasks:
                        lifespan_background_task.cancel()  # prevent dangling tasks

                    # Identify "exit" methods and run them.
                    # want to make sure this is called even if the lifespan manager fails
                    if service.user_cls_instance is not None and not is_auto_snapshot:
                        exit_methods = _find_callables_for_obj(service.user_cls_instance, _PartialFunctionFlags.EXIT)
                        call_lifecycle_functions(event_loop, container_io_manager, list(exit_methods.values()))

                # Finally, commit on exit to catch uncommitted volume changes and surface background
                # commit errors.
                container_io_manager.volume_commit(
                    [v.volume_id for v in function_def.volume_mounts if v.allow_background_commits]
                )
            finally:
                # Restore the original signal handler, needed for container_test hygiene since the
                # test runs `main()` multiple times in the same process.
                signal.signal(signal.SIGINT, int_handler)
                signal.signal(signal.SIGUSR1, usr1_handler)


if __name__ == "__main__":
    logger.debug("Container: starting")

    container_args = api_pb2.ContainerArguments()
    container_arguments_path: Optional[str] = os.environ.get("MODAL_CONTAINER_ARGUMENTS_PATH")
    if container_arguments_path is None:
        raise RuntimeError("No path to the container arguments file provided!")
    container_args.ParseFromString(open(container_arguments_path, "rb").read())

    # Note that we're creating the client in a synchronous context, but it will be running in a separate thread.
    # This is good because if the function is long running then we the client can still send heartbeats
    # The only caveat is a bunch of calls will now cross threads, which adds a bit of overhead?
    client = Client.from_env()

    try:
        main(container_args, client)
    except UserException:
        logger.info("User exception caught, exiting")
    except KeyboardInterrupt:
        logger.debug("Container: interrupted")

    # Detect if any non-daemon threads are still running, which will prevent the Python interpreter
    # from shutting down. The sleep(0) here is needed for finished ThreadPoolExecutor resources to
    # shut down without triggering this warning (e.g., `@wsgi_app()`).
    time.sleep(0)
    lingering_threads: list[threading.Thread] = []
    for thread in threading.enumerate():
        current_thread = threading.get_ident()
        if thread.ident is not None and thread.ident != current_thread and not thread.daemon and thread.is_alive():
            lingering_threads.append(thread)
    if lingering_threads:
        thread_names = ", ".join(t.name for t in lingering_threads)
        logger.warning(
            f"Detected {len(lingering_threads)} background thread(s) [{thread_names}] still running "
            "after container exit. This will prevent runner shutdown for up to 30 seconds."
        )

    logger.debug("Container: done")



================================================
FILE: modal/_ipython.py
================================================
# Copyright Modal Labs 2022
import sys


def is_notebook(stdout=None):
    ipykernel_iostream = sys.modules.get("ipykernel.iostream")
    if ipykernel_iostream is None:
        return False
    if stdout is None:
        stdout = sys.stdout
    return isinstance(stdout, ipykernel_iostream.OutStream)



================================================
FILE: modal/_location.py
================================================
# Copyright Modal Labs 2022
import modal_proto.api_pb2


def display_location(cloud_provider: "modal_proto.api_pb2.CloudProvider.V") -> str:
    if cloud_provider == modal_proto.api_pb2.CLOUD_PROVIDER_GCP:
        return "GCP (us-east1)"
    elif cloud_provider == modal_proto.api_pb2.CLOUD_PROVIDER_AWS:
        return "AWS (us-east-1)"
    else:
        return ""



================================================
FILE: modal/_object.py
================================================
# Copyright Modal Labs 2022
import typing
import uuid
from collections.abc import Awaitable, Hashable, Sequence
from functools import wraps
from typing import Callable, ClassVar, Optional

from google.protobuf.message import Message
from typing_extensions import Self

from modal._traceback import suppress_tb_frames

from ._resolver import Resolver
from ._utils.async_utils import aclosing
from ._utils.deprecation import deprecation_warning
from .client import _Client
from .config import config, logger
from .exception import ExecutionError, InvalidError

EPHEMERAL_OBJECT_HEARTBEAT_SLEEP: int = 300


def _get_environment_name(environment_name: Optional[str] = None, resolver: Optional[Resolver] = None) -> Optional[str]:
    if environment_name:
        return environment_name
    elif resolver and resolver.environment_name:
        return resolver.environment_name
    else:
        return config.get("environment")


class _Object:
    _type_prefix: ClassVar[Optional[str]] = None
    _prefix_to_type: ClassVar[dict[str, type]] = {}

    # For constructors
    _load: Optional[Callable[[Self, Resolver, Optional[str]], Awaitable[None]]]
    _preload: Optional[Callable[[Self, Resolver, Optional[str]], Awaitable[None]]]
    _rep: str
    _is_another_app: bool
    _hydrate_lazily: bool
    _deps: Optional[Callable[..., Sequence["_Object"]]]
    _deduplication_key: Optional[Callable[[], Awaitable[Hashable]]] = None

    # For hydrated objects
    _object_id: Optional[str]
    _client: Optional[_Client]
    _is_hydrated: bool
    _is_rehydrated: bool

    # Not all object subclasses have a meaningful "name" concept
    # So whether they expose this is a matter of having a name property
    _name: Optional[str]

    @classmethod
    def __init_subclass__(cls, type_prefix: Optional[str] = None):
        super().__init_subclass__()
        if type_prefix is not None:
            cls._type_prefix = type_prefix
            cls._prefix_to_type[type_prefix] = cls

    def __init__(self, *args, **kwargs):
        """mdmd:hidden"""
        raise InvalidError(f"Class {type(self).__name__} has no constructor. Use class constructor methods instead.")

    def _init(
        self,
        rep: str,
        load: Optional[Callable[[Self, Resolver, Optional[str]], Awaitable[None]]] = None,
        is_another_app: bool = False,
        preload: Optional[Callable[[Self, Resolver, Optional[str]], Awaitable[None]]] = None,
        hydrate_lazily: bool = False,
        deps: Optional[Callable[..., Sequence["_Object"]]] = None,
        deduplication_key: Optional[Callable[[], Awaitable[Hashable]]] = None,
        name: Optional[str] = None,
    ):
        self._local_uuid = str(uuid.uuid4())
        self._load = load
        self._preload = preload
        self._rep = rep
        self._is_another_app = is_another_app
        self._hydrate_lazily = hydrate_lazily
        self._deps = deps
        self._deduplication_key = deduplication_key

        self._object_id = None
        self._client = None
        self._is_hydrated = False
        self._is_rehydrated = False

        self._name = name

        self._initialize_from_empty()

    def _unhydrate(self):
        self._object_id = None
        self._client = None
        self._is_hydrated = False

    def _initialize_from_empty(self):
        # default implementation, can be overriden in subclasses
        pass

    def _initialize_from_other(self, other):
        # default implementation, can be overriden in subclasses
        self._object_id = other._object_id
        self._is_hydrated = other._is_hydrated
        self._client = other._client

    def _hydrate(self, object_id: str, client: _Client, metadata: Optional[Message]):
        assert isinstance(object_id, str) and self._type_prefix is not None
        if not object_id.startswith(self._type_prefix):
            raise ExecutionError(
                f"Can not hydrate {type(self)}:"
                f" it has type prefix {self._type_prefix}"
                f" but the object_id starts with {object_id[:3]}"
            )
        self._object_id = object_id
        self._client = client
        self._hydrate_metadata(metadata)
        self._is_hydrated = True

    def _hydrate_metadata(self, metadata: Optional[Message]):
        # override this is subclasses that need additional data (other than an object_id) for a functioning Handle
        pass

    def _get_metadata(self) -> Optional[Message]:
        # return the necessary metadata from this handle to be able to re-hydrate in another context if one is needed
        # used to provide a handle's handle_metadata for serializing/pickling a live handle
        # the object_id is already provided by other means
        return None

    def _validate_is_hydrated(self):
        if not self._is_hydrated:
            object_type = self.__class__.__name__.strip("_")
            if hasattr(self, "_app") and getattr(self._app, "_running_app", "") is None:  # type: ignore
                # The most common cause of this error: e.g., user called a Function without using App.run()
                reason = ", because the App it is defined on is not running"
            else:
                # Technically possible, but with an ambiguous cause.
                reason = ""
            raise ExecutionError(
                f"{object_type} has not been hydrated with the metadata it needs to run on Modal{reason}."
            )

    def clone(self) -> Self:
        """mdmd:hidden Clone a given hydrated object.

        Note: This is not intended to be public API and has no public use. It will be removed in a future release.
        """
        object_class = self.__class__.__name__.strip("_")
        deprecation_warning(
            (2025, 6, 30),
            f"{object_class}.clone() is not intended to be public API and will be removed in a future release.",
        )

        # Object to clone must already be hydrated, otherwise from_loader is more suitable.
        self._validate_is_hydrated()
        obj = type(self).__new__(type(self))
        obj._initialize_from_other(self)
        return obj

    @classmethod
    def _from_loader(
        cls,
        load: Callable[[Self, Resolver, Optional[str]], Awaitable[None]],
        rep: str,
        is_another_app: bool = False,
        preload: Optional[Callable[[Self, Resolver, Optional[str]], Awaitable[None]]] = None,
        hydrate_lazily: bool = False,
        deps: Optional[Callable[..., Sequence["_Object"]]] = None,
        deduplication_key: Optional[Callable[[], Awaitable[Hashable]]] = None,
        name: Optional[str] = None,
    ):
        # TODO(erikbern): flip the order of the two first arguments
        obj = _Object.__new__(cls)
        obj._init(rep, load, is_another_app, preload, hydrate_lazily, deps, deduplication_key, name)
        return obj

    @staticmethod
    def _get_type_from_id(object_id: str) -> type["_Object"]:
        parts = object_id.split("-")
        if len(parts) != 2:
            raise InvalidError(f"Object id {object_id} has no dash in it")
        prefix = parts[0]
        if prefix not in _Object._prefix_to_type:
            raise InvalidError(f"Object prefix {prefix} does not correspond to a type")
        return _Object._prefix_to_type[prefix]

    @classmethod
    def _is_id_type(cls, object_id) -> bool:
        return cls._get_type_from_id(object_id) == cls

    @classmethod
    def _repr(cls, name: str, environment_name: Optional[str] = None) -> str:
        public_cls = cls.__name__.strip("_")
        environment_repr = f", environment_name={environment_name!r}" if environment_name else ""
        return f"modal.{public_cls}.from_name({name!r}{environment_repr})"

    @classmethod
    def _new_hydrated(
        cls,
        object_id: str,
        client: _Client,
        handle_metadata: Optional[Message],
        is_another_app: bool = False,
        rep: Optional[str] = None,
    ) -> Self:
        obj_cls: type[Self]
        if cls._type_prefix is not None:
            # This is called directly on a subclass, e.g. Secret.from_id
            # validate the id matching the expected id type of the Object subclass
            if not object_id.startswith(cls._type_prefix + "-"):
                raise InvalidError(f"Object {object_id} does not start with {cls._type_prefix}")

            obj_cls = cls
        else:
            # this means the method is used directly on _Object
            # typically during deserialization of objects
            obj_cls = typing.cast(type[Self], cls._get_type_from_id(object_id))

        # Instantiate provider
        obj = _Object.__new__(obj_cls)
        rep = rep or f"modal.{obj_cls.__name__.strip('_')}.from_id({object_id!r})"
        obj._init(rep, is_another_app=is_another_app)
        obj._hydrate(object_id, client, handle_metadata)

        return obj

    def _hydrate_from_other(self, other: Self):
        self._hydrate(other.object_id, other.client, other._get_metadata())

    def __repr__(self):
        return self._rep

    @property
    def local_uuid(self):
        """mdmd:hidden"""
        return self._local_uuid

    @property
    def object_id(self) -> str:
        """mdmd:hidden"""
        if self._object_id is None:
            raise AttributeError(f"Attempting to get object_id of unhydrated {self}")
        return self._object_id

    @property
    def client(self) -> _Client:
        """mdmd:hidden"""
        if self._client is None:
            raise AttributeError(f"Attempting to get client of unhydrated {self}")
        return self._client

    @property
    def is_hydrated(self) -> bool:
        """mdmd:hidden"""
        return self._is_hydrated

    @property
    def deps(self) -> Callable[..., Sequence["_Object"]]:
        """mdmd:hidden"""

        def default_deps(*args, **kwargs) -> Sequence["_Object"]:
            return []

        return self._deps if self._deps is not None else default_deps

    async def hydrate(self, client: Optional[_Client] = None) -> Self:
        """Synchronize the local object with its identity on the Modal server.

        It is rarely necessary to call this method explicitly, as most operations
        will lazily hydrate when needed. The main use case is when you need to
        access object metadata, such as its ID.

        *Added in v0.72.39*: This method replaces the deprecated `.resolve()` method.
        """
        if self._is_hydrated:
            if self.client._snapshotted and not self._is_rehydrated:
                # memory snapshots capture references which must be rehydrated
                # on restore to handle staleness.
                logger.debug(f"rehydrating {self} after snapshot")
                self._is_hydrated = False  # un-hydrate and re-resolve
                c = client if client is not None else await _Client.from_env()
                resolver = Resolver(c)
                await resolver.load(typing.cast(_Object, self))
                self._is_rehydrated = True
                logger.debug(f"rehydrated {self} with client {id(c)}")
        elif not self._hydrate_lazily:
            # TODO(michael) can remove _hydrate lazily? I think all objects support it now?
            self._validate_is_hydrated()
        else:
            c = client if client is not None else await _Client.from_env()
            resolver = Resolver(c)
            with suppress_tb_frames(1):  # skip this frame by default
                await resolver.load(self)
        return self


def live_method(method):
    @wraps(method)
    async def wrapped(self, *args, **kwargs):
        await self.hydrate()
        return await method(self, *args, **kwargs)

    return wrapped


def live_method_gen(method):
    @wraps(method)
    async def wrapped(self, *args, **kwargs):
        await self.hydrate()
        async with aclosing(method(self, *args, **kwargs)) as stream:
            async for item in stream:
                yield item

    return wrapped



================================================
FILE: modal/_output.py
================================================
# Copyright Modal Labs 2022
from __future__ import annotations

import asyncio
import contextlib
import functools
import platform
import re
import socket
import sys
from collections.abc import Generator
from datetime import timedelta
from typing import Callable, ClassVar

from grpclib.exceptions import GRPCError, StreamTerminatedError
from rich.console import Console, Group, RenderableType
from rich.live import Live
from rich.panel import Panel
from rich.progress import (
    BarColumn,
    DownloadColumn,
    MofNCompleteColumn,
    Progress,
    ProgressColumn,
    TaskID,
    TextColumn,
    TimeElapsedColumn,
    TimeRemainingColumn,
    TransferSpeedColumn,
)
from rich.spinner import Spinner
from rich.text import Text

from modal._utils.time_utils import timestamp_to_localized_str
from modal_proto import api_pb2

from ._utils.grpc_utils import RETRYABLE_GRPC_STATUS_CODES, retry_transient_errors
from ._utils.shell_utils import stream_from_stdin, write_to_fd
from .client import _Client
from .config import logger

if platform.system() == "Windows":
    default_spinner = "line"
else:
    default_spinner = "dots"


def make_console(*, stderr: bool = False, highlight: bool = True) -> Console:
    """Create a rich Console tuned for Modal CLI output."""
    return Console(
        stderr=stderr,
        highlight=highlight,
        # CLI does not work with auto-detected Jupyter HTML display_data.
        force_jupyter=False,
    )


class FunctionQueuingColumn(ProgressColumn):
    """Renders time elapsed, including task.completed as additional elapsed time."""

    def __init__(self):
        self.lag = 0
        super().__init__()

    def render(self, task) -> Text:
        self.lag = max(task.completed - task.elapsed, self.lag)
        if task.finished:
            elapsed = max(task.finished_time, task.completed)
        else:
            elapsed = task.elapsed + self.lag
        delta = timedelta(seconds=int(elapsed))
        return Text(str(delta), style="progress.elapsed")


class LineBufferedOutput:
    """Output stream that buffers lines and passes them to a callback."""

    LINE_REGEX = re.compile("(\r\n|\r|\n)")

    def __init__(self, callback: Callable[[str], None], show_timestamps: bool):
        self._callback = callback
        self._buf = ""
        self._show_timestamps = show_timestamps

    def write(self, log: api_pb2.TaskLogs):
        chunks = self.LINE_REGEX.split(self._buf + log.data)

        # re.split("(<exp>)") returns the matched groups, and also the separators.
        # e.g. re.split("(+)", "a+b") returns ["a", "+", "b"].
        # This means that chunks is guaranteed to be odd in length.

        if self._show_timestamps:
            for i in range(0, len(chunks) - 1, 2):
                chunks[i] = f"{timestamp_to_localized_str(log.timestamp)} {chunks[i]}"

        completed_lines = "".join(chunks[:-1])
        remainder = chunks[-1]

        # Partially completed lines end with a carriage return. Append a newline so that they
        # are not overwritten by the `rich.Live` and prefix the inverse operation to the remaining
        # buffer. Note that this is not perfect -- when stdout and stderr are interleaved, the results
        # can have unexpected spacing.
        if completed_lines.endswith("\r"):
            completed_lines = completed_lines[:-1] + "\n"
            # Prepend cursor up + carriage return.
            remainder = "\x1b[1A\r" + remainder

        self._callback(completed_lines)
        self._buf = remainder

    def flush(self):
        pass

    def finalize(self):
        if self._buf:
            self._callback(self._buf)
            self._buf = ""


class OutputManager:
    _instance: ClassVar[OutputManager | None] = None

    _console: Console
    _task_states: dict[str, int]
    _task_progress_items: dict[tuple[str, int], TaskID]
    _current_render_group: Group | None
    _function_progress: Progress | None
    _function_queueing_progress: Progress | None
    _snapshot_progress: Progress | None
    _line_buffers: dict[int, LineBufferedOutput]
    _status_spinner: Spinner
    _app_page_url: str | None
    _show_image_logs: bool
    _status_spinner_live: Live | None
    _show_timestamps: bool

    def __init__(
        self,
        *,
        status_spinner_text: str = "Running app...",
        show_timestamps: bool = False,
    ):
        self._stdout = sys.stdout
        self._console = make_console(highlight=False)
        self._task_states = {}
        self._task_progress_items = {}
        self._current_render_group = None
        self._function_progress = None
        self._function_queueing_progress = None
        self._snapshot_progress = None
        self._line_buffers = {}
        self._status_spinner = OutputManager.step_progress(status_spinner_text)
        self._app_page_url = None
        self._show_image_logs = False
        self._status_spinner_live = None
        self._show_timestamps = show_timestamps

    @classmethod
    def disable(cls):
        cls._instance.flush_lines()
        if cls._instance._status_spinner_live:
            cls._instance._status_spinner_live.stop()
        cls._instance = None

    @classmethod
    def get(cls) -> OutputManager | None:
        return cls._instance

    @classmethod
    @contextlib.contextmanager
    def enable_output(cls, show_progress: bool = True) -> Generator[None]:
        if show_progress:
            cls._instance = OutputManager()
        try:
            yield
        finally:
            cls._instance = None

    @staticmethod
    def step_progress(text: str = "") -> Spinner:
        """Returns the element to be rendered when a step is in progress."""
        return Spinner(default_spinner, text, style="blue")

    @staticmethod
    def step_completed(message: str) -> RenderableType:
        return f"[green]âœ“[/green] {message}"

    @staticmethod
    def substep_completed(message: str) -> RenderableType:
        return f"ðŸ”¨ {message}"

    def print(self, renderable) -> None:
        self._console.print(renderable)

    def make_live(self, renderable: RenderableType) -> Live:
        """Creates a customized `rich.Live` instance with the given renderable. The renderable
        is placed in a `rich.Group` to allow for dynamic additions later."""
        self._function_progress = None
        self._current_render_group = Group(renderable)
        return Live(self._current_render_group, console=self._console, transient=True, refresh_per_second=4)

    def enable_image_logs(self):
        self._show_image_logs = True

    @property
    def function_progress(self) -> Progress:
        """Creates a `rich.Progress` instance with custom columns for function progress,
        and adds it to the current render group."""
        if not self._function_progress:
            self._function_progress = Progress(
                TextColumn("[progress.description][white]{task.description}[/white]"),
                BarColumn(),
                MofNCompleteColumn(),
                TimeRemainingColumn(),
                console=self._console,
            )
            if self._current_render_group:
                self._current_render_group.renderables.append(Panel(self._function_progress, style="gray50"))
        return self._function_progress

    @property
    def snapshot_progress(self) -> Progress:
        """Creates a `rich.Progress` instance with custom columns for image snapshot progress,
        and adds it to the current render group."""
        if not self._snapshot_progress:
            self._snapshot_progress = Progress(
                TextColumn("[progress.description]{task.description}"),
                BarColumn(),
                DownloadColumn(),
                TimeElapsedColumn(),
                console=self._console,
                transient=True,
            )
            if self._current_render_group:
                # Appear above function progress renderables.
                self._current_render_group.renderables.insert(0, self._snapshot_progress)
        return self._snapshot_progress

    @property
    def function_queueing_progress(self) -> Progress:
        """Creates a `rich.Progress` instance with custom columns for function queue waiting progress
        and adds it to the current render group."""
        if not self._function_queueing_progress:
            self._function_queueing_progress = Progress(
                TextColumn("[progress.description]{task.description}"),
                FunctionQueuingColumn(),
                console=self._console,
                transient=True,
            )
            if self._current_render_group:
                self._current_render_group.renderables.append(self._function_queueing_progress)
        return self._function_queueing_progress

    def function_progress_callback(self, tag: str, total: int | None) -> Callable[[int, int], None]:
        """Adds a task to the current function_progress instance, and returns a callback
        to update task progress with new completed and total counts."""

        progress_task = self.function_progress.add_task(tag, total=total)

        def update_counts(completed: int, total: int):
            self.function_progress.update(progress_task, completed=completed, total=total)

        return update_counts

    def _print_log(self, fd: int, data: str) -> None:
        if fd == api_pb2.FILE_DESCRIPTOR_STDOUT:
            style = "blue"
        elif fd == api_pb2.FILE_DESCRIPTOR_STDERR:
            style = "red"
        elif fd == api_pb2.FILE_DESCRIPTOR_INFO:
            style = "yellow"
        else:
            raise Exception(f"Weird file descriptor {fd} for log output")

        self._console.out(data, style=style, end="")

    def update_app_page_url(self, app_page_url: str) -> None:
        self._app_page_url = app_page_url

    def update_task_state(self, task_id: str, state: int):
        """Updates the state of a task, sets the new task status string."""
        self._task_states[task_id] = state

        all_states = self._task_states.values()
        states_set = set(all_states)

        def tasks_at_state(state):
            return sum(x == state for x in all_states)

        # The most advanced state that's present informs the message.
        if api_pb2.TASK_STATE_ACTIVE in states_set or api_pb2.TASK_STATE_IDLE in states_set:
            # Note that as of writing the server no longer uses TASK_STATE_ACTIVE, but we'll
            # make the numerator the sum of active / idle in case that is revived at some point in the future.
            tasks_running = tasks_at_state(api_pb2.TASK_STATE_ACTIVE) + tasks_at_state(api_pb2.TASK_STATE_IDLE)
            tasks_not_completed = len(self._task_states) - tasks_at_state(api_pb2.TASK_STATE_COMPLETED)
            message = f"Running ({tasks_running}/{tasks_not_completed} containers active)..."
        elif api_pb2.TASK_STATE_LOADING_IMAGE in states_set:
            tasks_loading = tasks_at_state(api_pb2.TASK_STATE_LOADING_IMAGE)
            message = f"Loading images ({tasks_loading} containers initializing)..."
        elif api_pb2.TASK_STATE_WORKER_ASSIGNED in states_set:
            message = "Worker assigned..."
        elif api_pb2.TASK_STATE_COMPLETED in states_set:
            tasks_completed = tasks_at_state(api_pb2.TASK_STATE_COMPLETED)
            message = f"Running ({tasks_completed} containers finished)..."
        else:
            message = "Running..."

        message = f"[blue]{message}[/blue] [grey70]View app at [underline]{self._app_page_url}[/underline][/grey70]"

        # Set the new message
        self._status_spinner.update(text=message)

    def update_snapshot_progress(self, image_id: str, task_progress: api_pb2.TaskProgress) -> None:
        # TODO(erikbern): move this to sit on the resolver object, mostly
        completed = task_progress.pos
        total = task_progress.len

        task_key = (image_id, api_pb2.IMAGE_SNAPSHOT_UPLOAD)
        if task_key in self._task_progress_items:
            progress_task_id = self._task_progress_items[task_key]
        else:
            progress_task_id = self.snapshot_progress.add_task("[yellow]Uploading image snapshotâ€¦", total=total)
            self._task_progress_items[task_key] = progress_task_id

        try:
            self.snapshot_progress.update(progress_task_id, completed=completed, total=total)
            if completed == total:
                self.snapshot_progress.remove_task(progress_task_id)
        except KeyError:
            # Rich throws a KeyError if the task has already been removed.
            pass

    def update_queueing_progress(
        self, *, function_id: str, completed: int, total: int | None, description: str | None
    ) -> None:
        """Handle queueing updates, ignoring completion updates for functions that have no queue progress bar."""
        task_key = (function_id, api_pb2.FUNCTION_QUEUED)
        task_description = description or f"'{function_id}' function waiting on worker"
        task_desc = f"[yellow]{task_description}. Time in queue:"
        if task_key in self._task_progress_items:
            progress_task_id = self._task_progress_items[task_key]
            try:
                self.function_queueing_progress.update(progress_task_id, completed=completed, total=total)
                if completed == total:
                    del self._task_progress_items[task_key]
                    self.function_queueing_progress.remove_task(progress_task_id)
            except KeyError:
                pass
        elif completed != total:  # Create new bar for queued function
            progress_task_id = self.function_queueing_progress.add_task(task_desc, start=True, total=None)
            self._task_progress_items[task_key] = progress_task_id

    async def put_log_content(self, log: api_pb2.TaskLogs):
        stream = self._line_buffers.get(log.file_descriptor)
        if stream is None:
            stream = LineBufferedOutput(functools.partial(self._print_log, log.file_descriptor), self._show_timestamps)
            self._line_buffers[log.file_descriptor] = stream
        stream.write(log)

    def flush_lines(self):
        for stream in self._line_buffers.values():
            stream.finalize()

    @contextlib.contextmanager
    def show_status_spinner(self):
        self._status_spinner_live = self.make_live(self._status_spinner)
        with self._status_spinner_live:
            yield


class ProgressHandler:
    live: Live
    _type: str
    _spinner: Spinner
    _overall_progress: Progress
    _download_progress: Progress
    _overall_progress_task_id: TaskID
    _total_tasks: int
    _completed_tasks: int

    def __init__(self, type: str, console: Console):
        self._type = type

        if self._type == "download":
            title = "Downloading file(s) to local..."
        elif self._type == "upload":
            title = "Uploading file(s) to volume..."
        else:
            raise NotImplementedError(f"Progress handler of type: `{type}` not yet implemented")

        self._spinner = OutputManager.step_progress(title)

        self._overall_progress = Progress(
            TextColumn(f"[bold white]{title}", justify="right"),
            TimeElapsedColumn(),
            BarColumn(bar_width=None),
            TextColumn("[bold white]{task.description}"),
            transient=True,
            console=console,
        )
        self._download_progress = Progress(
            TextColumn("[bold white]{task.fields[path]}", justify="right"),
            BarColumn(bar_width=None),
            "[progress.percentage]{task.percentage:>3.1f}%",
            "â€¢",
            DownloadColumn(),
            "â€¢",
            TransferSpeedColumn(),
            "â€¢",
            TimeRemainingColumn(),
            transient=True,
            console=console,
        )

        self.live = Live(
            Group(self._spinner, self._overall_progress, self._download_progress), transient=True, refresh_per_second=4
        )

        self._overall_progress_task_id = self._overall_progress.add_task(".", start=True)
        self._total_tasks = 0
        self._completed_tasks = 0

    def _add_sub_task(self, name: str, size: float) -> TaskID:
        task_id = self._download_progress.add_task(self._type, path=name, start=True, total=size)
        self._total_tasks += 1
        self._overall_progress.update(self._overall_progress_task_id, total=self._total_tasks)
        return task_id

    def _reset_sub_task(self, task_id: TaskID):
        self._download_progress.reset(task_id)

    def _complete_progress(self):
        # TODO: we could probably implement some callback progression from the server
        # to get progress reports for the post processing too
        # so we don't have to just spin here
        self._overall_progress.remove_task(self._overall_progress_task_id)
        self._spinner.update(text="Post processing...")

    def _complete_sub_task(self, task_id: TaskID):
        self._completed_tasks += 1
        self._download_progress.remove_task(task_id)
        self._overall_progress.update(
            self._overall_progress_task_id,
            advance=1,
            description=f"({self._completed_tasks} out of {self._total_tasks} files completed)",
        )

    def _advance_sub_task(self, task_id: TaskID, advance: float):
        self._download_progress.update(task_id, advance=advance)

    def progress(
        self,
        task_id: TaskID | None = None,
        advance: float | None = None,
        name: str | None = None,
        size: float | None = None,
        reset: bool | None = False,
        complete: bool | None = False,
    ) -> TaskID | None:
        try:
            if task_id is not None:
                if reset:
                    return self._reset_sub_task(task_id)
                elif complete:
                    return self._complete_sub_task(task_id)
                elif advance is not None:
                    return self._advance_sub_task(task_id, advance)
            elif name is not None and size is not None:
                return self._add_sub_task(name, size)
            elif complete:
                return self._complete_progress()
        except Exception as exc:
            # Liberal exception handling to avoid crashing downloads and uploads.
            logger.error(f"failed progress update: {exc}")
        raise NotImplementedError(
            "Unknown action to take with args: "
            + f"name={name} "
            + f"size={size} "
            + f"task_id={task_id} "
            + f"advance={advance} "
            + f"reset={reset} "
            + f"complete={complete} "
        )


async def stream_pty_shell_input(client: _Client, exec_id: str, finish_event: asyncio.Event):
    """
    Streams stdin to the given exec id until finish_event is triggered
    """

    async def _handle_input(data: bytes, message_index: int):
        await retry_transient_errors(
            client.stub.ContainerExecPutInput,
            api_pb2.ContainerExecPutInputRequest(
                exec_id=exec_id, input=api_pb2.RuntimeInputMessage(message=data, message_index=message_index)
            ),
            total_timeout=10,
        )

    async with stream_from_stdin(_handle_input, use_raw_terminal=True):
        await finish_event.wait()


async def put_pty_content(log: api_pb2.TaskLogs, stdout):
    if hasattr(stdout, "buffer"):
        # If we're not showing progress, there's no need to buffer lines,
        # because the progress spinner can't interfere with output.

        data = log.data.encode("utf-8")
        # Non-blocking terminals can fill the kernel buffer on output bursts, making flush() raise
        # BlockingIOError (EAGAIN) and appear frozen until a key is pressed (this happened e.g. when
        # printing large data from a pdb breakpoint). If stdout has a real fd, we await a
        # non-blocking fd write (write_to_fd) instead.
        fd = None
        try:
            if hasattr(stdout, "fileno"):
                fd = stdout.fileno()
        except Exception:
            fd = None

        if fd is not None:
            await write_to_fd(fd, data)
        else:
            # For streams without fileno(), use the normal write/flush path.
            written = 0
            n_retries = 0
            while written < len(data):
                try:
                    written += stdout.buffer.write(data[written:])
                    stdout.flush()
                except BlockingIOError:
                    if n_retries >= 5:
                        raise
                    n_retries += 1
                    await asyncio.sleep(0.1)
    else:
        # `stdout` isn't always buffered (e.g. %%capture in Jupyter notebooks redirects it to
        # io.StringIO).
        stdout.write(log.data)
        stdout.flush()


async def get_app_logs_loop(
    client: _Client,
    output_mgr: OutputManager,
    app_id: str | None = None,
    task_id: str | None = None,
    app_logs_url: str | None = None,
):
    last_log_batch_entry_id = ""

    pty_shell_stdout = None
    pty_shell_finish_event: asyncio.Event | None = None
    pty_shell_task_id: str | None = None
    pty_shell_input_task: asyncio.Task | None = None

    async def stop_pty_shell():
        nonlocal pty_shell_finish_event, pty_shell_input_task
        if pty_shell_finish_event:
            print("\r", end="")  # move cursor to beginning of line
            pty_shell_finish_event.set()
            pty_shell_finish_event = None

            if pty_shell_input_task:
                try:
                    await pty_shell_input_task
                except Exception as exc:
                    logger.exception(f"Exception in PTY shell input task: {exc}")
                finally:
                    pty_shell_input_task = None

    async def _put_log(log_batch: api_pb2.TaskLogsBatch, log: api_pb2.TaskLogs):
        if log.task_state:
            output_mgr.update_task_state(log_batch.task_id, log.task_state)
            if log.task_state == api_pb2.TASK_STATE_WORKER_ASSIGNED:
                # Close function's queueing progress bar (if it exists)
                output_mgr.update_queueing_progress(
                    function_id=log_batch.function_id, completed=1, total=1, description=None
                )
        elif log.task_progress.len or log.task_progress.pos:
            if log.task_progress.progress_type == api_pb2.FUNCTION_QUEUED:
                output_mgr.update_queueing_progress(
                    function_id=log_batch.function_id,
                    completed=log.task_progress.pos,
                    total=log.task_progress.len,
                    description=log.task_progress.description,
                )
            else:  # Ensure forward-compatible with new types.
                logger.debug(f"Received unrecognized progress type: {log.task_progress.progress_type}")
        elif log.data:
            if pty_shell_finish_event:
                await put_pty_content(log, pty_shell_stdout)
            else:
                await output_mgr.put_log_content(log)

    async def _get_logs():
        nonlocal last_log_batch_entry_id
        nonlocal pty_shell_stdout, pty_shell_finish_event, pty_shell_task_id, pty_shell_input_task

        request = api_pb2.AppGetLogsRequest(
            app_id=app_id or "",
            task_id=task_id or "",
            timeout=55,
            last_entry_id=last_log_batch_entry_id,
        )
        log_batch: api_pb2.TaskLogsBatch
        async for log_batch in client.stub.AppGetLogs.unary_stream(request):
            if log_batch.entry_id:
                # log_batch entry_id is empty for fd="server" messages from AppGetLogs
                last_log_batch_entry_id = log_batch.entry_id
            if log_batch.app_done:
                logger.debug("App logs are done")
                last_log_batch_entry_id = None
                break
            elif log_batch.image_id and not output_mgr._show_image_logs:
                # Ignore image logs while app is creating objects.
                # These logs are fetched through ImageJoinStreaming instead.
                # Logs from images built "dynamically" (after the app has started)
                # are printed through this loop.
                # TODO (akshat): have a better way of differentiating between
                # statically and dynamically built images.
                pass
            elif log_batch.pty_exec_id:
                # This corresponds to the `modal run -i` use case where a breakpoint
                # triggers and the task drops into an interactive PTY mode
                if pty_shell_finish_event:
                    print("ERROR: concurrent PTY shells are not supported.")
                else:
                    pty_shell_stdout = output_mgr._stdout
                    pty_shell_finish_event = asyncio.Event()
                    pty_shell_task_id = log_batch.task_id
                    output_mgr.disable()
                    pty_shell_input_task = asyncio.create_task(
                        stream_pty_shell_input(client, log_batch.pty_exec_id, pty_shell_finish_event)
                    )
            else:
                for log in log_batch.items:
                    await _put_log(log_batch, log)

            if log_batch.eof and log_batch.task_id == pty_shell_task_id:
                await stop_pty_shell()

        output_mgr.flush_lines()

    while True:
        try:
            await _get_logs()
        except (GRPCError, StreamTerminatedError, socket.gaierror, AttributeError) as exc:
            if isinstance(exc, GRPCError):
                if exc.status in RETRYABLE_GRPC_STATUS_CODES:
                    # Try again if we had a temporary connection drop,
                    # for example if computer went to sleep.
                    logger.debug("Log fetching timed out. Retrying ...")
                    continue
            elif isinstance(exc, StreamTerminatedError):
                logger.debug("Stream closed. Retrying ...")
                continue
            elif isinstance(exc, socket.gaierror):
                logger.debug("Lost connection. Retrying ...")
                continue
            elif isinstance(exc, AttributeError):
                if "_write_appdata" in str(exc):
                    # Happens after losing connection
                    # StreamTerminatedError are not properly raised in grpclib<=0.4.7
                    # fixed in https://github.com/vmagamedov/grpclib/issues/185
                    # TODO: update to newer version (>=0.4.8) once stable
                    logger.debug("Lost connection. Retrying ...")
                    continue
            raise

        if last_log_batch_entry_id is None:
            break

    await stop_pty_shell()

    logger.debug("Logging exited gracefully")



================================================
FILE: modal/_partial_function.py
================================================
# Copyright Modal Labs 2023
import enum
import inspect
import typing
from collections.abc import Coroutine, Iterable
from dataclasses import asdict, dataclass
from typing import (
    Any,
    Callable,
    Optional,
    Union,
)

import typing_extensions

from modal_proto import api_pb2

from ._functions import _Function
from ._utils.async_utils import synchronizer
from ._utils.deprecation import deprecation_warning
from ._utils.function_utils import callable_has_non_self_params
from .config import logger
from .exception import InvalidError

MAX_MAX_BATCH_SIZE = 1000
MAX_BATCH_WAIT_MS = 10 * 60 * 1000  # 10 minutes

if typing.TYPE_CHECKING:
    import modal.partial_function


class _PartialFunctionFlags(enum.IntFlag):
    # Lifecycle method flags
    ENTER_PRE_SNAPSHOT = 2
    ENTER_POST_SNAPSHOT = 4
    EXIT = 8
    # Interface flags
    CALLABLE_INTERFACE = 16
    WEB_INTERFACE = 32
    # Service decorator flags
    # It's, unclear if we need these, as we can also generally infer based on some params being set
    # In the current state where @modal.batched is used _instead_ of `@modal.method`, we need to give
    # `@modal.batched` two roles (exposing the callable interface, adding batching semantics).
    # But it's probably better to make `@modal.batched` and `@modal.method` stackable, or to move
    # `@modal.batched` to be a class-level decorator since it primarily governs service behavior.
    BATCHED = 64
    CONCURRENT = 128
    CLUSTERED = 256  # Experimental: Clustered functions

    @staticmethod
    def all() -> int:
        return ~_PartialFunctionFlags(0)

    @staticmethod
    def lifecycle_flags() -> int:
        return (
            _PartialFunctionFlags.ENTER_PRE_SNAPSHOT
            | _PartialFunctionFlags.ENTER_POST_SNAPSHOT
            | _PartialFunctionFlags.EXIT
        )

    @staticmethod
    def interface_flags() -> int:
        return _PartialFunctionFlags.CALLABLE_INTERFACE | _PartialFunctionFlags.WEB_INTERFACE


@dataclass
class _PartialFunctionParams:
    webhook_config: Optional[api_pb2.WebhookConfig] = None
    is_generator: Optional[bool] = None
    force_build: Optional[bool] = None
    batch_max_size: Optional[int] = None
    batch_wait_ms: Optional[int] = None
    cluster_size: Optional[int] = None
    max_concurrent_inputs: Optional[int] = None
    target_concurrent_inputs: Optional[int] = None
    build_timeout: Optional[int] = None
    rdma: Optional[bool] = None

    def update(self, other: "_PartialFunctionParams") -> None:
        """Update self with params set in other."""
        for key, val in asdict(other).items():
            if val is not None:
                if getattr(self, key, None) is not None:
                    raise InvalidError(f"Cannot set `{key}` twice.")
                setattr(self, key, val)


P = typing_extensions.ParamSpec("P")
ReturnType = typing_extensions.TypeVar("ReturnType", covariant=True)
OriginalReturnType = typing_extensions.TypeVar("OriginalReturnType", covariant=True)
NullaryFuncOrMethod = Union[Callable[[], Any], Callable[[Any], Any]]
NullaryMethod = Callable[[Any], Any]


class _PartialFunction(typing.Generic[P, ReturnType, OriginalReturnType]):
    """Object produced by a decorator in the `modal` namespace

    The object will eventually by consumed by an App decorator.
    """

    raw_f: Optional[Callable[P, ReturnType]]  # function or method
    user_cls: Optional[type] = None  # class
    flags: _PartialFunctionFlags
    params: _PartialFunctionParams
    registered: bool

    def __init__(
        self,
        obj: Union[Callable[P, ReturnType], type],
        flags: _PartialFunctionFlags,
        params: _PartialFunctionParams,
    ):
        if isinstance(obj, type):
            self.user_cls = obj
            self.raw_f = None
        else:
            self.raw_f = obj
            self.user_cls = None
        self.flags = flags
        self.params = params
        self.registered = False
        self.validate_flag_composition()

    def stack(self, flags: _PartialFunctionFlags, params: _PartialFunctionParams) -> typing_extensions.Self:
        """Implement decorator composition by combining the flags and params."""
        self.flags |= flags
        self.params.update(params)
        self.validate_flag_composition()
        return self

    def validate_flag_composition(self) -> None:
        """Validate decorator composition based on PartialFunctionFlags."""
        uses_interface_flags = self.flags & _PartialFunctionFlags.interface_flags()
        uses_lifecycle_flags = self.flags & _PartialFunctionFlags.lifecycle_flags()
        if uses_interface_flags and uses_lifecycle_flags:
            self.registered = True  # Hacky, avoid false-positive warning
            raise InvalidError("Interface decorators cannot be combined with lifecycle decorators.")

        has_web_interface = self.flags & _PartialFunctionFlags.WEB_INTERFACE
        has_callable_interface = self.flags & _PartialFunctionFlags.CALLABLE_INTERFACE
        if has_web_interface and has_callable_interface:
            self.registered = True  # Hacky, avoid false-positive warning
            raise InvalidError("Callable decorators cannot be combined with web interface decorators.")

    def validate_obj_compatibility(
        self, decorator_name: str, require_sync: bool = False, require_nullary: bool = False
    ) -> None:
        """Enforce compatibility with the wrapped object; called from individual decorator functions."""
        from .cls import _Cls  # Avoid circular import

        uses_lifecycle_flags = self.flags & _PartialFunctionFlags.lifecycle_flags()
        uses_interface_flags = self.flags & _PartialFunctionFlags.interface_flags()
        if self.user_cls is not None and (uses_lifecycle_flags or uses_interface_flags):
            self.registered = True  # Hacky, avoid false-positive warning
            raise InvalidError(
                f"Cannot apply `@modal.{decorator_name}` to a class. Hint: consider applying to a method instead."
            )

        wrapped_object = self.raw_f or self.user_cls
        if isinstance(wrapped_object, _Function):
            self.registered = True  # Hacky, avoid false-positive warning
            raise InvalidError(
                f"Cannot stack `@modal.{decorator_name}` on top of `@app.function`."
                " Hint: swap the order of the decorators."
            )
        elif isinstance(wrapped_object, _Cls):
            self.registered = True  # Hacky, avoid false-positive warning
            raise InvalidError(
                f"Cannot stack `@modal.{decorator_name}` on top of `@app.cls()`."
                " Hint: swap the order of the decorators."
            )

        # Run some assertions about a callable wrappee defined by the specific decorator used
        if self.raw_f is not None:
            if not callable(self.raw_f):
                self.registered = True  # Hacky, avoid false-positive warning
                raise InvalidError(f"The object wrapped by `@modal.{decorator_name}` must be callable.")

            if require_sync and inspect.iscoroutinefunction(self.raw_f):
                self.registered = True  # Hacky, avoid false-positive warning
                raise InvalidError(f"The `@modal.{decorator_name}` decorator can't be applied to an async function.")

            if require_nullary and callable_has_non_self_params(self.raw_f):
                self.registered = True  # Hacky, avoid false-positive warning
                raise InvalidError(f"Functions decorated by `@modal.{decorator_name}` can't have parameters.")

    def _get_raw_f(self) -> Callable[P, ReturnType]:
        assert self.raw_f is not None
        return self.raw_f

    def _is_web_endpoint(self) -> bool:
        if self.params.webhook_config is None:
            return False
        return self.params.webhook_config.type != api_pb2.WEBHOOK_TYPE_UNSPECIFIED

    def __get__(self, obj, objtype=None) -> _Function[P, ReturnType, OriginalReturnType]:
        # to type checkers, any @method or similar function on a modal class, would appear to be
        # of the type PartialFunction and this descriptor would be triggered when accessing it,
        #
        # However, modal classes are *actually* Cls instances (which isn't reflected in type checkers
        # due to Python's lack of type chekcing intersection types), so at runtime the Cls instance would
        # use its __getattr__ rather than this descriptor.
        assert self.raw_f is not None  # Should only be relevant in a method context
        k = self.raw_f.__name__
        if obj:  # accessing the method on an instance of a class, e.g. `MyClass().fun``
            if hasattr(obj, "_modal_functions"):
                # This happens inside "local" user methods when they refer to other methods,
                # e.g. Foo().parent_method.remote() calling self.other_method.remote()
                return getattr(obj, "_modal_functions")[k]
            else:
                # special edge case: referencing a method of an instance of an
                # unwrapped class (not using app.cls()) with @methods
                # not sure what would be useful here, but let's return a bound version of the underlying function,
                # since the class is just a vanilla class at this point
                # This wouldn't let the user access `.remote()` and `.local()` etc. on the function
                return self.raw_f.__get__(obj, objtype)

        else:  # accessing a method directly on the class, e.g. `MyClass.fun`
            # This happens mainly during serialization of the obj underlying class of a Cls
            # since we don't have the instance info here we just return the PartialFunction itself
            # to let it be bound to a variable and become a Function later on
            return self  # type: ignore  # this returns a PartialFunction in a special internal case

    def __del__(self):
        if self.registered is False:
            if self.raw_f is not None:
                name, object_type, suggestion = self.raw_f.__name__, "function", "@app.function or @app.cls"
            elif self.user_cls is not None:
                name, object_type, suggestion = self.user_cls.__name__, "class", "@app.cls"
            logger.warning(
                f"The `{name}` {object_type} was never registered with the App."
                f" Did you forget an {suggestion} decorator?"
            )


def _find_partial_methods_for_user_cls(user_cls: type[Any], flags: int) -> dict[str, _PartialFunction]:
    """Grabs all method on a user class, and returns partials. Includes legacy methods."""
    from .partial_function import PartialFunction  # obj type

    partial_functions: dict[str, _PartialFunction] = {}
    for parent_cls in reversed(user_cls.mro()):
        if parent_cls is not object:
            for k, v in parent_cls.__dict__.items():
                if isinstance(v, PartialFunction):  # type: ignore[reportArgumentType]   # synchronicity wrapper types
                    _partial_function: _PartialFunction = typing.cast(_PartialFunction, synchronizer._translate_in(v))
                    if _partial_function.flags & flags:
                        partial_functions[k] = _partial_function

    return partial_functions


def _find_callables_for_obj(user_obj: Any, flags: int) -> dict[str, Callable[..., Any]]:
    """Grabs all methods for an object, and binds them to the class"""
    user_cls: type = type(user_obj)
    return {
        k: pf.raw_f.__get__(user_obj)
        for k, pf in _find_partial_methods_for_user_cls(user_cls, flags).items()
        if pf.raw_f is not None  # Should be true for output of _find_partial_methods_for_user_cls, but hard to annotate
    }


class _MethodDecoratorType:
    @typing.overload
    def __call__(
        self,
        func: "modal.partial_function.PartialFunction[typing_extensions.Concatenate[Any, P], ReturnType, OriginalReturnType]",  # noqa
    ) -> "modal.partial_function.PartialFunction[P, ReturnType, OriginalReturnType]": ...

    @typing.overload
    def __call__(
        self, func: "Callable[typing_extensions.Concatenate[Any, P], Coroutine[Any, Any, ReturnType]]"
    ) -> "modal.partial_function.PartialFunction[P, ReturnType, Coroutine[Any, Any, ReturnType]]": ...

    @typing.overload
    def __call__(
        self, func: "Callable[typing_extensions.Concatenate[Any, P], ReturnType]"
    ) -> "modal.partial_function.PartialFunction[P, ReturnType, ReturnType]": ...

    def __call__(self, func): ...


# TODO(elias): fix support for coroutine type unwrapping for methods (static typing)
def _method(
    _warn_parentheses_missing=None,  # mdmd:line-hidden
    *,
    # Set this to True if it's a non-generator function returning
    # a [sync/async] generator object
    is_generator: Optional[bool] = None,
) -> _MethodDecoratorType:
    """Decorator for methods that should be transformed into a Modal Function registered against this class's App.

    **Usage:**

    ```python
    @app.cls(cpu=8)
    class MyCls:

        @modal.method()
        def f(self):
            ...
    ```
    """
    if _warn_parentheses_missing is not None:
        raise InvalidError(
            "Positional arguments are not allowed. Did you forget parentheses? Suggestion: `@modal.method()`."
        )

    def wrapper(obj: Union[Callable[..., Any], _PartialFunction]) -> _PartialFunction:
        flags = _PartialFunctionFlags.CALLABLE_INTERFACE

        nonlocal is_generator  # TODO(michael): we are likely to deprecate the explicit is_generator param
        if is_generator is None:
            callable = obj.raw_f if isinstance(obj, _PartialFunction) else obj
            is_generator = inspect.isgeneratorfunction(callable) or inspect.isasyncgenfunction(callable)
        params = _PartialFunctionParams(is_generator=is_generator)

        if isinstance(obj, _PartialFunction):
            pf = obj.stack(flags, params)
        else:
            pf = _PartialFunction(obj, flags, params)
        pf.validate_obj_compatibility("method")
        return pf

    # TODO(michael) verify that we still need the type: ignore
    return wrapper  # type: ignore  # synchronicity issue with obj vs unwrapped types and protocols


def _parse_custom_domains(custom_domains: Optional[Iterable[str]] = None) -> list[api_pb2.CustomDomainConfig]:
    assert not isinstance(custom_domains, str), "custom_domains must be `Iterable[str]` but is `str` instead."
    _custom_domains: list[api_pb2.CustomDomainConfig] = []
    if custom_domains is not None:
        for custom_domain in custom_domains:
            _custom_domains.append(api_pb2.CustomDomainConfig(name=custom_domain))

    return _custom_domains


def _fastapi_endpoint(
    _warn_parentheses_missing=None,  # mdmd:line-hidden
    *,
    method: str = "GET",  # REST method for the created endpoint.
    label: Optional[str] = None,  # Label for created endpoint. Final subdomain will be <workspace>--<label>.modal.run.
    custom_domains: Optional[Iterable[str]] = None,  # Custom fully-qualified domain name (FQDN) for the endpoint.
    docs: bool = False,  # Whether to enable interactive documentation for this endpoint at /docs.
    requires_proxy_auth: bool = False,  # Require Modal-Key and Modal-Secret HTTP Headers on requests.
) -> Callable[
    [Union[_PartialFunction[P, ReturnType, ReturnType], Callable[P, ReturnType]]],
    _PartialFunction[P, ReturnType, ReturnType],
]:
    """Convert a function into a basic web endpoint by wrapping it with a FastAPI App.

    Modal will internally use [FastAPI](https://fastapi.tiangolo.com/) to expose a
    simple, single request handler. If you are defining your own `FastAPI` application
    (e.g. if you want to define multiple routes), use `@modal.asgi_app` instead.

    The endpoint created with this decorator will automatically have
    [CORS](https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS) enabled
    and can leverage many of FastAPI's features.

    For more information on using Modal with popular web frameworks, see our
    [guide on web endpoints](https://modal.com/docs/guide/webhooks).

    *Added in v0.73.82*: This function replaces the deprecated `@web_endpoint` decorator.
    """
    if isinstance(_warn_parentheses_missing, str):
        # Probably passing the method string as a positional argument.
        raise InvalidError(
            f'Positional arguments are not allowed. Suggestion: `@modal.fastapi_endpoint(method="{method}")`.'
        )
    elif _warn_parentheses_missing is not None:
        raise InvalidError(
            "Positional arguments are not allowed. Did you forget parentheses? Suggestion: `@modal.fastapi_endpoint()`."
        )

    webhook_config = api_pb2.WebhookConfig(
        type=api_pb2.WEBHOOK_TYPE_FUNCTION,
        method=method,
        web_endpoint_docs=docs,
        requested_suffix=label or "",
        async_mode=api_pb2.WEBHOOK_ASYNC_MODE_AUTO,
        custom_domains=_parse_custom_domains(custom_domains),
        requires_proxy_auth=requires_proxy_auth,
    )

    flags = _PartialFunctionFlags.WEB_INTERFACE
    params = _PartialFunctionParams(webhook_config=webhook_config)

    def wrapper(
        obj: Union[_PartialFunction[P, ReturnType, ReturnType], Callable[P, ReturnType]],
    ) -> _PartialFunction[P, ReturnType, ReturnType]:
        if isinstance(obj, _PartialFunction):
            pf = obj.stack(flags, params)
        else:
            pf = _PartialFunction(obj, flags, params)
        pf.validate_obj_compatibility("fastapi_endpoint")
        return pf

    return wrapper


def _web_endpoint(
    _warn_parentheses_missing=None,  # mdmd:line-hidden
    *,
    method: str = "GET",  # REST method for the created endpoint.
    label: Optional[str] = None,  # Label for created endpoint. Final subdomain will be <workspace>--<label>.modal.run.
    docs: bool = False,  # Whether to enable interactive documentation for this endpoint at /docs.
    custom_domains: Optional[
        Iterable[str]
    ] = None,  # Create an endpoint using a custom domain fully-qualified domain name (FQDN).
    requires_proxy_auth: bool = False,  # Require Modal-Key and Modal-Secret HTTP Headers on requests.
) -> Callable[
    [Union[_PartialFunction[P, ReturnType, ReturnType], Callable[P, ReturnType]]],
    _PartialFunction[P, ReturnType, ReturnType],
]:
    """Register a basic web endpoint with this application.

    DEPRECATED: This decorator has been renamed to `@modal.fastapi_endpoint`.

    This is the simple way to create a web endpoint on Modal. The function
    behaves as a [FastAPI](https://fastapi.tiangolo.com/) handler and should
    return a response object to the caller.

    Endpoints created with `@modal.web_endpoint` are meant to be simple, single
    request handlers and automatically have
    [CORS](https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS) enabled.
    For more flexibility, use `@modal.asgi_app`.

    To learn how to use Modal with popular web frameworks, see the
    [guide on web endpoints](https://modal.com/docs/guide/webhooks).
    """
    if isinstance(_warn_parentheses_missing, str):
        # Probably passing the method string as a positional argument.
        raise InvalidError('Positional arguments are not allowed. Suggestion: `@modal.web_endpoint(method="GET")`.')
    elif _warn_parentheses_missing is not None:
        raise InvalidError(
            "Positional arguments are not allowed. Did you forget parentheses? Suggestion: `@modal.web_endpoint()`."
        )

    deprecation_warning(
        (2025, 3, 5), "The `@modal.web_endpoint` decorator has been renamed to `@modal.fastapi_endpoint`."
    )

    webhook_config = api_pb2.WebhookConfig(
        type=api_pb2.WEBHOOK_TYPE_FUNCTION,
        method=method,
        web_endpoint_docs=docs,
        requested_suffix=label or "",
        async_mode=api_pb2.WEBHOOK_ASYNC_MODE_AUTO,
        custom_domains=_parse_custom_domains(custom_domains),
        requires_proxy_auth=requires_proxy_auth,
    )

    flags = _PartialFunctionFlags.WEB_INTERFACE
    params = _PartialFunctionParams(webhook_config=webhook_config)

    def wrapper(
        obj: Union[_PartialFunction[P, ReturnType, ReturnType], Callable[P, ReturnType]],
    ) -> _PartialFunction[P, ReturnType, ReturnType]:
        if isinstance(obj, _PartialFunction):
            pf = obj.stack(flags, params)
        else:
            pf = _PartialFunction(obj, flags, params)
        pf.validate_obj_compatibility("web_endpoint")
        return pf

    return wrapper


def _asgi_app(
    _warn_parentheses_missing=None,  # mdmd:line-hidden
    *,
    label: Optional[str] = None,  # Label for created endpoint. Final subdomain will be <workspace>--<label>.modal.run.
    custom_domains: Optional[Iterable[str]] = None,  # Deploy this endpoint on a custom domain.
    requires_proxy_auth: bool = False,  # Require Modal-Key and Modal-Secret HTTP Headers on requests.
) -> Callable[[Union[_PartialFunction, NullaryFuncOrMethod]], _PartialFunction]:
    """Decorator for registering an ASGI app with a Modal function.

    Asynchronous Server Gateway Interface (ASGI) is a standard for Python
    synchronous and asynchronous apps, supported by all popular Python web
    libraries. This is an advanced decorator that gives full flexibility in
    defining one or more web endpoints on Modal.

    **Usage:**

    ```python
    from typing import Callable

    @app.function()
    @modal.asgi_app()
    def create_asgi() -> Callable:
        ...
    ```

    To learn how to use Modal with popular web frameworks, see the
    [guide on web endpoints](https://modal.com/docs/guide/webhooks).
    """
    if isinstance(_warn_parentheses_missing, str):
        raise InvalidError(f'Positional arguments are not allowed. Suggestion: `@modal.asgi_app(label="{label}")`.')
    elif _warn_parentheses_missing is not None:
        raise InvalidError(
            "Positional arguments are not allowed. Did you forget parentheses? Suggestion: `@modal.asgi_app()`."
        )

    webhook_config = api_pb2.WebhookConfig(
        type=api_pb2.WEBHOOK_TYPE_ASGI_APP,
        requested_suffix=label or "",
        async_mode=api_pb2.WEBHOOK_ASYNC_MODE_AUTO,
        custom_domains=_parse_custom_domains(custom_domains),
        requires_proxy_auth=requires_proxy_auth,
    )

    flags = _PartialFunctionFlags.WEB_INTERFACE
    params = _PartialFunctionParams(webhook_config=webhook_config)

    def wrapper(obj: Union[_PartialFunction, NullaryFuncOrMethod]) -> _PartialFunction:
        if isinstance(obj, _PartialFunction):
            pf = obj.stack(flags, params)
        else:
            pf = _PartialFunction(obj, flags, params)
        pf.validate_obj_compatibility("asgi_app", require_sync=True, require_nullary=True)
        return pf

    return wrapper


def _wsgi_app(
    _warn_parentheses_missing=None,  # mdmd:line-hidden
    *,
    label: Optional[str] = None,  # Label for created endpoint. Final subdomain will be <workspace>--<label>.modal.run.
    custom_domains: Optional[Iterable[str]] = None,  # Deploy this endpoint on a custom domain.
    requires_proxy_auth: bool = False,  # Require Modal-Key and Modal-Secret HTTP Headers on requests.
) -> Callable[[Union[_PartialFunction, NullaryFuncOrMethod]], _PartialFunction]:
    """Decorator for registering a WSGI app with a Modal function.

    Web Server Gateway Interface (WSGI) is a standard for synchronous Python web apps.
    It has been [succeeded by the ASGI interface](https://asgi.readthedocs.io/en/latest/introduction.html#wsgi-compatibility)
    which is compatible with ASGI and supports additional functionality such as web sockets.
    Modal supports ASGI via [`asgi_app`](https://modal.com/docs/reference/modal.asgi_app).

    **Usage:**

    ```python
    from typing import Callable

    @app.function()
    @modal.wsgi_app()
    def create_wsgi() -> Callable:
        ...
    ```

    To learn how to use this decorator with popular web frameworks, see the
    [guide on web endpoints](https://modal.com/docs/guide/webhooks).
    """
    if isinstance(_warn_parentheses_missing, str):
        raise InvalidError(f'Positional arguments are not allowed. Suggestion: `@modal.wsgi_app(label="{label}")`.')
    elif _warn_parentheses_missing is not None:
        raise InvalidError(
            "Positional arguments are not allowed. Did you forget parentheses? Suggestion: `@modal.wsgi_app()`."
        )

    webhook_config = api_pb2.WebhookConfig(
        type=api_pb2.WEBHOOK_TYPE_WSGI_APP,
        requested_suffix=label or "",
        async_mode=api_pb2.WEBHOOK_ASYNC_MODE_AUTO,
        custom_domains=_parse_custom_domains(custom_domains),
        requires_proxy_auth=requires_proxy_auth,
    )

    flags = _PartialFunctionFlags.WEB_INTERFACE
    params = _PartialFunctionParams(webhook_config=webhook_config)

    def wrapper(obj: Union[_PartialFunction, NullaryFuncOrMethod]) -> _PartialFunction:
        if isinstance(obj, _PartialFunction):
            pf = obj.stack(flags, params)
        else:
            pf = _PartialFunction(obj, flags, params)
        pf.validate_obj_compatibility("wsgi_app", require_sync=True, require_nullary=True)
        return pf

    return wrapper


def _web_server(
    port: int,
    *,
    startup_timeout: float = 5.0,  # Maximum number of seconds to wait for the web server to start.
    label: Optional[str] = None,  # Label for created endpoint. Final subdomain will be <workspace>--<label>.modal.run.
    custom_domains: Optional[Iterable[str]] = None,  # Deploy this endpoint on a custom domain.
    requires_proxy_auth: bool = False,  # Require Modal-Key and Modal-Secret HTTP Headers on requests.
) -> Callable[[Union[_PartialFunction, NullaryFuncOrMethod]], _PartialFunction]:
    """Decorator that registers an HTTP web server inside the container.

    This is similar to `@asgi_app` and `@wsgi_app`, but it allows you to expose a full HTTP server
    listening on a container port. This is useful for servers written in other languages like Rust,
    as well as integrating with non-ASGI frameworks like aiohttp and Tornado.

    **Usage:**

    ```python
    import subprocess

    @app.function()
    @modal.web_server(8000)
    def my_file_server():
        subprocess.Popen("python -m http.server -d / 8000", shell=True)
    ```

    The above example starts a simple file server, displaying the contents of the root directory.
    Here, requests to the web endpoint will go to external port 8000 on the container. The
    `http.server` module is included with Python, but you could run anything here.

    Internally, the web server is transparently converted into a web endpoint by Modal, so it has
    the same serverless autoscaling behavior as other web endpoints.

    For more info, see the [guide on web endpoints](https://modal.com/docs/guide/webhooks).
    """
    if not isinstance(port, int) or port < 1 or port > 65535:
        raise InvalidError("First argument of `@web_server` must be a local port, such as `@web_server(8000)`.")
    if startup_timeout <= 0:
        raise InvalidError("The `startup_timeout` argument of `@web_server` must be positive.")

    webhook_config = api_pb2.WebhookConfig(
        type=api_pb2.WEBHOOK_TYPE_WEB_SERVER,
        requested_suffix=label or "",
        async_mode=api_pb2.WEBHOOK_ASYNC_MODE_AUTO,
        custom_domains=_parse_custom_domains(custom_domains),
        web_server_port=port,
        web_server_startup_timeout=startup_timeout,
        requires_proxy_auth=requires_proxy_auth,
    )

    flags = _PartialFunctionFlags.WEB_INTERFACE
    params = _PartialFunctionParams(webhook_config=webhook_config)

    def wrapper(obj: Union[_PartialFunction, NullaryFuncOrMethod]) -> _PartialFunction:
        if isinstance(obj, _PartialFunction):
            pf = obj.stack(flags, params)
        else:
            pf = _PartialFunction(obj, flags, params)
        pf.validate_obj_compatibility("web_server", require_sync=True, require_nullary=True)
        return pf

    return wrapper


def _enter(
    _warn_parentheses_missing=None,  # mdmd:line-hidden
    *,
    snap: bool = False,
) -> Callable[[Union[_PartialFunction, NullaryMethod]], _PartialFunction]:
    """Decorator for methods which should be executed when a new container is started.

    See the [lifeycle function guide](https://modal.com/docs/guide/lifecycle-functions#enter) for more information."""
    if _warn_parentheses_missing is not None:
        raise InvalidError(
            "Positional arguments are not allowed. Did you forget parentheses? Suggestion: `@modal.enter()`."
        )

    flags = _PartialFunctionFlags.ENTER_PRE_SNAPSHOT if snap else _PartialFunctionFlags.ENTER_POST_SNAPSHOT
    params = _PartialFunctionParams()

    def wrapper(obj: Union[_PartialFunction, NullaryMethod]) -> _PartialFunction:
        # TODO: reject stacking once depreceate @modal.build
        if isinstance(obj, _PartialFunction):
            pf = obj.stack(flags, params)
        else:
            pf = _PartialFunction(obj, flags, params)
        pf.validate_obj_compatibility("enter")  # TODO require_nullary?
        return pf

    return wrapper


def _exit(_warn_parentheses_missing=None) -> Callable[[NullaryMethod], _PartialFunction]:
    """Decorator for methods which should be executed when a container is about to exit.

    See the [lifeycle function guide](https://modal.com/docs/guide/lifecycle-functions#exit) for more information."""
    if _warn_parentheses_missing is not None:
        raise InvalidError(
            "Positional arguments are not allowed. Did you forget parentheses? Suggestion: `@modal.exit()`."
        )

    flags = _PartialFunctionFlags.EXIT
    params = _PartialFunctionParams()

    def wrapper(obj: Union[_PartialFunction, NullaryMethod]) -> _PartialFunction:
        if isinstance(obj, _PartialFunction):
            pf = obj.stack(flags, params)
        else:
            pf = _PartialFunction(obj, flags, params)
        pf.validate_obj_compatibility("exit")  # TODO require_nullary?
        return pf

    return wrapper


def _batched(
    _warn_parentheses_missing=None,  # mdmd:line-hidden
    *,
    max_batch_size: int,
    wait_ms: int,
) -> Callable[
    [Union[_PartialFunction[P, ReturnType, ReturnType], Callable[P, ReturnType]]],
    _PartialFunction[P, ReturnType, ReturnType],
]:
    """Decorator for functions or class methods that should be batched.

    **Usage**

    ```python
    # Stack the decorator under `@app.function()` to enable dynamic batching
    @app.function()
    @modal.batched(max_batch_size=4, wait_ms=1000)
    async def batched_multiply(xs: list[int], ys: list[int]) -> list[int]:
        return [x * y for x, y in zip(xs, ys)]

    # call batched_multiply with individual inputs
    # batched_multiply.remote.aio(2, 100)

    # With `@app.cls()`, apply the decorator to a method (this may change in the future)
    @app.cls()
    class BatchedClass:
        @modal.batched(max_batch_size=4, wait_ms=1000)
        def batched_multiply(self, xs: list[int], ys: list[int]) -> list[int]:
            return [x * y for x, y in zip(xs, ys)]
    ```

    See the [dynamic batching guide](https://modal.com/docs/guide/dynamic-batching) for more information.
    """
    if _warn_parentheses_missing is not None:
        raise InvalidError(
            "Positional arguments are not allowed. Did you forget parentheses? Suggestion: `@modal.batched()`."
        )
    if max_batch_size < 1:
        raise InvalidError("max_batch_size must be a positive integer.")
    if max_batch_size > MAX_MAX_BATCH_SIZE:
        raise InvalidError(f"max_batch_size cannot be greater than {MAX_MAX_BATCH_SIZE}.")
    if wait_ms < 0:
        raise InvalidError("wait_ms must be a non-negative integer.")
    if wait_ms > MAX_BATCH_WAIT_MS:
        raise InvalidError(f"wait_ms cannot be greater than {MAX_BATCH_WAIT_MS}.")

    flags = _PartialFunctionFlags.CALLABLE_INTERFACE | _PartialFunctionFlags.BATCHED
    params = _PartialFunctionParams(batch_max_size=max_batch_size, batch_wait_ms=wait_ms)

    def wrapper(
        obj: Union[_PartialFunction[P, ReturnType, ReturnType], Callable[P, ReturnType]],
    ) -> _PartialFunction[P, ReturnType, ReturnType]:
        if isinstance(obj, _PartialFunction):
            pf = obj.stack(flags, params)
        else:
            pf = _PartialFunction(obj, flags, params)
        pf.validate_obj_compatibility("batched")
        return pf

    return wrapper


def _concurrent(
    _warn_parentheses_missing=None,  # mdmd:line-hidden
    *,
    max_inputs: int,  # Hard limit on each container's input concurrency
    target_inputs: Optional[int] = None,  # Input concurrency that Modal's autoscaler should target
) -> Callable[
    [Union[Callable[P, ReturnType], _PartialFunction[P, ReturnType, ReturnType]]],
    _PartialFunction[P, ReturnType, ReturnType],
]:
    """Decorator that allows individual containers to handle multiple inputs concurrently.

    The concurrency mechanism depends on whether the function is async or not:
    - Async functions will run inputs on a single thread as asyncio tasks.
    - Synchronous functions will use multi-threading. The code must be thread-safe.

    Input concurrency will be most useful for workflows that are IO-bound
    (e.g., making network requests) or when running an inference server that supports
    dynamic batching.

    When `target_inputs` is set, Modal's autoscaler will try to provision resources
    such that each container is running that many inputs concurrently, rather than
    autoscaling based on `max_inputs`. Containers may burst up to up to `max_inputs`
    if resources are insufficient to remain at the target concurrency, e.g. when the
    arrival rate of inputs increases. This can trade-off a small increase in average
    latency to avoid larger tail latencies from input queuing.

    **Examples:**
    ```python
    # Stack the decorator under `@app.function()` to enable input concurrency
    @app.function()
    @modal.concurrent(max_inputs=100)
    async def f(data):
        # Async function; will be scheduled as asyncio task
        ...

    # With `@app.cls()`, apply the decorator at the class level, not on individual methods
    @app.cls()
    @modal.concurrent(max_inputs=100, target_inputs=80)
    class C:
        @modal.method()
        def f(self, data):
            # Sync function; must be thread-safe
            ...

    ```

    *Added in v0.73.148:* This decorator replaces the `allow_concurrent_inputs` parameter
    in `@app.function()` and `@app.cls()`.

    """
    if _warn_parentheses_missing is not None:
        raise InvalidError(
            "Positional arguments are not allowed. Did you forget parentheses? Suggestion: `@modal.concurrent()`."
        )

    if target_inputs and target_inputs > max_inputs:
        raise InvalidError("`target_inputs` parameter cannot be greater than `max_inputs`.")

    flags = _PartialFunctionFlags.CONCURRENT
    params = _PartialFunctionParams(max_concurrent_inputs=max_inputs, target_concurrent_inputs=target_inputs)

    # Note: ideally we would have some way of declaring that this decorator cannot be used on an individual method.
    # I don't think there's any clear way for the wrapper function to know it's been passed "a method" rather than
    # a normal function. So we need to run that check in the `@app.cls` decorator, which is a little far removed.

    def wrapper(
        obj: Union[_PartialFunction[P, ReturnType, ReturnType], Callable[P, ReturnType]],
    ) -> _PartialFunction[P, ReturnType, ReturnType]:
        if isinstance(obj, _PartialFunction):
            pf = obj.stack(flags, params)
        else:
            pf = _PartialFunction(obj, flags, params)
        pf.validate_obj_compatibility("concurrent")
        return pf

    return wrapper


# NOTE: clustered is currently exposed through modal.experimental, not the top-level namespace
def _clustered(
    size: int, broadcast: bool = True, rdma: bool = False
) -> Callable[
    [Union[Callable[P, ReturnType], _PartialFunction[P, ReturnType, ReturnType]]],
    _PartialFunction[P, ReturnType, ReturnType],
]:
    """Provision clusters of colocated and networked containers for the Function.

    Parameters:
    size: int
        Number of containers spun up to handle each input.
    broadcast: bool = True
        If True, inputs will be sent simultaneously to each container. Otherwise,
        inputs will be sent only to the rank-0 container, which is responsible for
        delegating to the workers.
    """

    assert broadcast, "broadcast=False has not been implemented yet!"

    if size <= 0:
        raise ValueError("cluster size must be greater than 0")

    flags = _PartialFunctionFlags.CLUSTERED
    params = _PartialFunctionParams(cluster_size=size, rdma=rdma)

    def wrapper(
        obj: Union[_PartialFunction[P, ReturnType, ReturnType], Callable[P, ReturnType]],
    ) -> _PartialFunction[P, ReturnType, ReturnType]:
        if isinstance(obj, _PartialFunction):
            pf = obj.stack(flags, params)
        else:
            pf = _PartialFunction(obj, flags, params)
        pf.validate_obj_compatibility("clustered")
        return pf

    return wrapper



================================================
FILE: modal/_pty.py
================================================
# Copyright Modal Labs 2022
import contextlib
import os
import sys
from typing import Optional

from modal_proto import api_pb2


def get_winsz(fd=None) -> tuple[Optional[int], Optional[int]]:
    try:
        if fd is None:
            fd = sys.stdin.fileno()

        import fcntl
        import struct
        import termios

        return struct.unpack("hh", fcntl.ioctl(fd, termios.TIOCGWINSZ, "1234"))  # type: ignore
    except Exception:
        return None, None


def set_nonblocking(fd: int):
    import fcntl

    fl = fcntl.fcntl(fd, fcntl.F_GETFL)
    fcntl.fcntl(fd, fcntl.F_SETFL, fl | os.O_NONBLOCK)


@contextlib.contextmanager
def raw_terminal():
    import termios
    import tty

    fd = sys.stdin.fileno()
    old_settings = termios.tcgetattr(fd)

    try:
        tty.setraw(fd, termios.TCSADRAIN)
        yield
    finally:
        termios.tcsetattr(fd, termios.TCSADRAIN, old_settings)


def get_pty_info(shell: bool, no_terminate_on_idle_stdin: bool = False) -> api_pb2.PTYInfo:
    rows, cols = get_winsz()
    return api_pb2.PTYInfo(
        enabled=True,  # TODO(erikbern): deprecated
        winsz_rows=rows,
        winsz_cols=cols,
        env_term=os.environ.get("TERM"),
        env_colorterm=os.environ.get("COLORTERM"),
        env_term_program=os.environ.get("TERM_PROGRAM"),
        pty_type=api_pb2.PTYInfo.PTY_TYPE_SHELL if shell else api_pb2.PTYInfo.PTY_TYPE_FUNCTION,
        no_terminate_on_idle_stdin=no_terminate_on_idle_stdin,
    )



================================================
FILE: modal/_resolver.py
================================================
# Copyright Modal Labs 2023
import asyncio
import contextlib
import os
import tempfile
import typing
from asyncio import Future
from collections.abc import Hashable
from typing import TYPE_CHECKING, Optional

from modal._traceback import suppress_tb_frames
from modal_proto import api_pb2

from ._utils.async_utils import TaskContext
from .client import _Client

if TYPE_CHECKING:
    from rich.tree import Tree

    import modal._object


class StatusRow:
    def __init__(self, progress: "typing.Optional[Tree]"):
        self._spinner = None
        self._step_node = None
        if progress is not None:
            from ._output import OutputManager

            self._spinner = OutputManager.step_progress()
            self._step_node = progress.add(self._spinner)

    def message(self, message):
        if self._spinner is not None:
            self._spinner.update(text=message)

    def warning(self, warning: api_pb2.Warning):
        if self._step_node is not None:
            self._step_node.add(f"[yellow]:warning:[/yellow] {warning.message}")

    def finish(self, message):
        if self._step_node is not None and self._spinner is not None:
            from ._output import OutputManager

            self._spinner.update(text=message)
            self._step_node.label = OutputManager.substep_completed(message)


class Resolver:
    _local_uuid_to_future: dict[str, Future]
    _environment_name: Optional[str]
    _app_id: Optional[str]
    _deduplication_cache: dict[Hashable, Future]
    _client: _Client
    _build_start: float

    def __init__(
        self,
        client: _Client,
        *,
        environment_name: Optional[str] = None,
        app_id: Optional[str] = None,
    ):
        try:
            # TODO(michael) If we don't clean this up more thoroughly, it would probably
            # be good to have a single source of truth for "rich is installed" rather than
            # doing a try/catch everywhere we want to use it.
            from rich.tree import Tree

            from ._output import OutputManager

            tree = Tree(OutputManager.step_progress("Creating objects..."), guide_style="gray50")
        except ImportError:
            tree = None

        self._local_uuid_to_future = {}
        self._tree = tree
        self._client = client
        self._app_id = app_id
        self._environment_name = environment_name
        self._deduplication_cache = {}

        with tempfile.TemporaryFile() as temp_file:
            # Use file mtime to track build start time because we will later compare this baseline
            # to the mtime on mounted files, and want those measurements to have the same resolution.
            self._build_start = os.fstat(temp_file.fileno()).st_mtime

    @property
    def app_id(self) -> Optional[str]:
        return self._app_id

    @property
    def client(self):
        return self._client

    @property
    def environment_name(self):
        return self._environment_name

    @property
    def build_start(self) -> float:
        return self._build_start

    async def preload(self, obj, existing_object_id: Optional[str]):
        if obj._preload is not None:
            await obj._preload(obj, self, existing_object_id)

    async def load(self, obj: "modal._object._Object", existing_object_id: Optional[str] = None):
        if obj._is_hydrated and obj._is_another_app:
            # No need to reload this, it won't typically change
            if obj.local_uuid not in self._local_uuid_to_future:
                # a bit dumb - but we still need to store a reference to the object here
                # to be able to include all referenced objects when setting up the app
                fut: Future = Future()
                fut.set_result(obj)
                self._local_uuid_to_future[obj.local_uuid] = fut
            return obj

        deduplication_key: Optional[Hashable] = None
        if obj._deduplication_key:
            deduplication_key = await obj._deduplication_key()

        cached_future = self._local_uuid_to_future.get(obj.local_uuid)

        if not cached_future and deduplication_key is not None:
            # deduplication cache makes sure duplicate mounts are resolved only
            # once, even if they are different instances - as long as they have
            # the same content
            cached_future = self._deduplication_cache.get(deduplication_key)
            if cached_future:
                hydrated_object = await cached_future
                obj._hydrate(hydrated_object.object_id, self._client, hydrated_object._get_metadata())
                return obj

        if not cached_future:
            # don't run any awaits within this if-block to prevent race conditions
            async def loader():
                # Wait for all its dependencies
                # TODO(erikbern): do we need existing_object_id for those?
                await TaskContext.gather(*[self.load(dep) for dep in obj.deps()])

                # Load the object itself
                if not obj._load:
                    raise Exception(f"Object {obj} has no loader function")

                await obj._load(obj, self, existing_object_id)

                # Check that the id of functions didn't change
                # Persisted refs are ignored because their life cycle is managed independently.
                if (
                    not obj._is_another_app
                    and existing_object_id is not None
                    and existing_object_id.startswith("fu-")
                    and obj.object_id != existing_object_id
                ):
                    raise Exception(
                        f"Tried creating an object using existing id {existing_object_id} but it has id {obj.object_id}"
                    )

                return obj

            cached_future = asyncio.create_task(loader())
            self._local_uuid_to_future[obj.local_uuid] = cached_future
            if deduplication_key is not None:
                self._deduplication_cache[deduplication_key] = cached_future
        with suppress_tb_frames(2):
            # skip current frame + `loader()` closure frame from above
            return await cached_future

    def objects(self) -> list["modal._object._Object"]:
        unique_objects: dict[str, "modal._object._Object"] = {}
        for fut in self._local_uuid_to_future.values():
            if not fut.done():
                # this will raise an exception if not all loads have been awaited, but that *should* never happen
                raise RuntimeError(
                    "All loaded objects have not been resolved yet, can't get all objects for the resolver!"
                )
            obj = fut.result()
            unique_objects.setdefault(obj.object_id, obj)
        return list(unique_objects.values())

    @contextlib.contextmanager
    def display(self):
        # TODO(erikbern): get rid of this wrapper
        from .output import _get_output_manager

        if self._tree and (output_mgr := _get_output_manager()):
            with output_mgr.make_live(self._tree):
                yield
            self._tree.label = output_mgr.step_completed("Created objects.")
            output_mgr.print(self._tree)
        else:
            yield

    def add_status_row(self) -> StatusRow:
        return StatusRow(self._tree)



================================================
FILE: modal/_resources.py
================================================
# Copyright Modal Labs 2024
from typing import Optional, Union

from modal_proto import api_pb2

from .exception import InvalidError
from .gpu import GPU_T, parse_gpu_config


def convert_fn_config_to_resources_config(
    *,
    cpu: Optional[Union[float, tuple[float, float]]],
    memory: Optional[Union[int, tuple[int, int]]],
    gpu: GPU_T,
    ephemeral_disk: Optional[int],
    rdma: Optional[bool] = None,
) -> api_pb2.Resources:
    gpu_config = parse_gpu_config(gpu)
    if cpu and isinstance(cpu, tuple):
        if not cpu[0]:
            raise InvalidError("CPU request must be a positive number")
        elif not cpu[1]:
            raise InvalidError("CPU limit must be a positive number")
        milli_cpu = int(1000 * cpu[0])
        milli_cpu_max = int(1000 * cpu[1])
        if milli_cpu_max < milli_cpu:
            raise InvalidError(f"Cannot specify a CPU limit lower than request: {milli_cpu_max} < {milli_cpu}")
    elif cpu and isinstance(cpu, (float, int)):
        milli_cpu = int(1000 * cpu)
        milli_cpu_max = None
    else:
        milli_cpu = None
        milli_cpu_max = None

    if memory and isinstance(memory, int):
        memory_mb = memory
        memory_mb_max = 0  # no limit
    elif memory and isinstance(memory, tuple):
        memory_mb, memory_mb_max = memory
        if memory_mb_max < memory_mb:
            raise InvalidError(f"Cannot specify a memory limit lower than request: {memory_mb_max} < {memory_mb}")
    else:
        memory_mb = 0
        memory_mb_max = 0
    return api_pb2.Resources(
        milli_cpu=milli_cpu,
        milli_cpu_max=milli_cpu_max,
        gpu_config=gpu_config,
        memory_mb=memory_mb,
        memory_mb_max=memory_mb_max,
        ephemeral_disk_mb=ephemeral_disk,
        rdma=rdma or False,
    )



================================================
FILE: modal/_serialization.py
================================================
# Copyright Modal Labs 2022
import inspect
import io
import pickle
import typing
from inspect import Parameter
from typing import Any

from modal._traceback import extract_traceback
from modal.config import config

try:
    import cbor2  # type: ignore
except ImportError:  # pragma: no cover - optional dependency
    cbor2 = None

import google.protobuf.message

from modal._utils.async_utils import synchronizer
from modal_proto import api_pb2

from ._object import _Object
from ._type_manager import parameter_serde_registry, schema_registry
from ._vendor import cloudpickle
from .config import logger
from .exception import DeserializationError, ExecutionError, InvalidError, SerializationError
from .object import Object

if typing.TYPE_CHECKING:
    import modal.client

PICKLE_PROTOCOL = 4  # Support older Python versions.


class Pickler(cloudpickle.Pickler):
    def __init__(self, buf):
        super().__init__(buf, protocol=PICKLE_PROTOCOL)

    def persistent_id(self, obj):
        from modal.partial_function import PartialFunction

        if isinstance(obj, _Object):
            flag = "_o"
        elif isinstance(obj, Object):
            flag = "o"
        elif isinstance(obj, PartialFunction):
            # Special case for PartialObject since it's a synchronicity wrapped object
            # that's set on serialized classes.
            # The resulting pickled instance can't be deserialized without this in a
            # new process, since the original referenced synchronizer will have different
            # values for `._original_attr` etc.

            impl_object = synchronizer._translate_in(obj)
            attributes = impl_object.__dict__.copy()
            # ugly - we remove the `._wrapped_attr` attribute from the implementation instance
            # to avoid referencing and therefore pickling the wrapped instance despite having
            # translated it to the implementation type

            # it would be nice if we could avoid this by not having the wrapped instances
            # be directly linked from objects and instead having a lookup table in the Synchronizer:
            if synchronizer._wrapped_attr and synchronizer._wrapped_attr in attributes:
                attributes.pop(synchronizer._wrapped_attr)

            return ("sync", (impl_object.__class__, attributes))
        else:
            return
        if not obj.is_hydrated:
            raise InvalidError(f"Can't serialize object {obj} which hasn't been hydrated.")
        return (obj.object_id, flag, obj._get_metadata())


class Unpickler(pickle.Unpickler):
    def __init__(self, client, buf):
        self.client = client
        super().__init__(buf)

    def persistent_load(self, pid):
        if len(pid) == 2:
            # more general protocol
            obj_type, obj_data = pid
            if obj_type == "sync":  # synchronicity wrapped object
                # not actually a proto object in this case but the underlying object of a synchronicity object
                impl_class, attributes = obj_data
                impl_instance = impl_class.__new__(impl_class)
                impl_instance.__dict__.update(attributes)
                return synchronizer._translate_out(impl_instance)
            else:
                raise ExecutionError("Unknown serialization format")

        # old protocol, always a 3-tuple
        (object_id, flag, handle_proto) = pid
        if flag in ("o", "p", "h"):
            return Object._new_hydrated(object_id, self.client, handle_proto)
        elif flag in ("_o", "_p", "_h"):
            return _Object._new_hydrated(object_id, self.client, handle_proto)
        else:
            raise InvalidError("bad flag")


def serialize(obj: Any) -> bytes:
    """Serializes object and replaces all references to the client class by a placeholder."""
    buf = io.BytesIO()
    Pickler(buf).dump(obj)
    return buf.getvalue()


def deserialize(s: bytes, client) -> Any:
    """Deserializes object and replaces all client placeholders by self."""
    from ._runtime.execution_context import is_local  # Avoid circular import

    env = "local" if is_local() else "remote"
    try:
        return Unpickler(client, io.BytesIO(s)).load()
    except AttributeError as exc:
        # We use a different cloudpickle version pre- and post-3.11. Unfortunately cloudpickle
        # doesn't expose some kind of serialization version number, so we have to guess based
        # on the error message.
        if "Can't get attribute '_make_function'" in str(exc):
            raise DeserializationError(
                "Deserialization failed due to a version mismatch between local and remote environments. "
                "Try changing the Python version in your Modal image to match your local Python version. "
            ) from exc
        else:
            # On Python 3.10+, AttributeError has `.name` and `.obj` attributes for better custom reporting
            raise DeserializationError(
                f"Deserialization failed with an AttributeError, {exc}. This is probably because"
                " you have different versions of a library in your local and remote environments."
            ) from exc
    except ModuleNotFoundError as exc:
        raise DeserializationError(
            f"Deserialization failed because the '{exc.name}' module is not available in the {env} environment."
        ) from exc
    except Exception as exc:
        if env == "remote":
            # We currently don't always package the full traceback from errors in the remote entrypoint logic.
            # So try to include as much information as we can in the main error message.
            more = f": {type(exc)}({str(exc)})"
        else:
            # When running locally, we can just rely on standard exception chaining.
            more = " (see above for details)"
        raise DeserializationError(
            f"Encountered an error when deserializing an object in the {env} environment{more}."
        ) from exc


def _serialize_asgi(obj: Any) -> api_pb2.Asgi:
    def flatten_headers(obj):
        return [s for k, v in obj for s in (k, v)]

    if obj is None:
        return api_pb2.Asgi()

    msg_type = obj.get("type")

    if msg_type == "http":
        return api_pb2.Asgi(
            http=api_pb2.Asgi.Http(
                http_version=obj.get("http_version", "1.1"),
                method=obj["method"],
                scheme=obj.get("scheme", "http"),
                path=obj["path"],
                query_string=obj.get("query_string"),
                headers=flatten_headers(obj.get("headers", [])),
                client_host=obj["client"][0] if obj.get("client") else None,
                client_port=obj["client"][1] if obj.get("client") else None,
            )
        )
    elif msg_type == "http.request":
        return api_pb2.Asgi(
            http_request=api_pb2.Asgi.HttpRequest(
                body=obj.get("body"),
                more_body=obj.get("more_body"),
            )
        )
    elif msg_type == "http.response.start":
        return api_pb2.Asgi(
            http_response_start=api_pb2.Asgi.HttpResponseStart(
                status=obj["status"],
                headers=flatten_headers(obj.get("headers", [])),
                trailers=obj.get("trailers"),
            )
        )
    elif msg_type == "http.response.body":
        return api_pb2.Asgi(
            http_response_body=api_pb2.Asgi.HttpResponseBody(
                body=obj.get("body"),
                more_body=obj.get("more_body"),
            )
        )
    elif msg_type == "http.response.trailers":
        return api_pb2.Asgi(
            http_response_trailers=api_pb2.Asgi.HttpResponseTrailers(
                headers=flatten_headers(obj.get("headers", [])),
                more_trailers=obj.get("more_trailers"),
            )
        )
    elif msg_type == "http.disconnect":
        return api_pb2.Asgi(http_disconnect=api_pb2.Asgi.HttpDisconnect())

    elif msg_type == "websocket":
        return api_pb2.Asgi(
            websocket=api_pb2.Asgi.Websocket(
                http_version=obj.get("http_version", "1.1"),
                scheme=obj.get("scheme", "ws"),
                path=obj["path"],
                query_string=obj.get("query_string"),
                headers=flatten_headers(obj.get("headers", [])),
                client_host=obj["client"][0] if obj.get("client") else None,
                client_port=obj["client"][1] if obj.get("client") else None,
                subprotocols=obj.get("subprotocols"),
            )
        )
    elif msg_type == "websocket.connect":
        return api_pb2.Asgi(
            websocket_connect=api_pb2.Asgi.WebsocketConnect(),
        )
    elif msg_type == "websocket.accept":
        return api_pb2.Asgi(
            websocket_accept=api_pb2.Asgi.WebsocketAccept(
                subprotocol=obj.get("subprotocol"),
                headers=flatten_headers(obj.get("headers", [])),
            )
        )
    elif msg_type == "websocket.receive":
        return api_pb2.Asgi(
            websocket_receive=api_pb2.Asgi.WebsocketReceive(
                bytes=obj.get("bytes"),
                text=obj.get("text"),
            )
        )
    elif msg_type == "websocket.send":
        return api_pb2.Asgi(
            websocket_send=api_pb2.Asgi.WebsocketSend(
                bytes=obj.get("bytes"),
                text=obj.get("text"),
            )
        )
    elif msg_type == "websocket.disconnect":
        return api_pb2.Asgi(
            websocket_disconnect=api_pb2.Asgi.WebsocketDisconnect(
                code=obj.get("code"),
            )
        )
    elif msg_type == "websocket.close":
        return api_pb2.Asgi(
            websocket_close=api_pb2.Asgi.WebsocketClose(
                code=obj.get("code"),
                reason=obj.get("reason"),
            )
        )

    else:
        logger.debug("skipping serialization of unknown ASGI message type %r", msg_type)
        return api_pb2.Asgi()


def _deserialize_asgi(asgi: api_pb2.Asgi) -> Any:
    def unflatten_headers(obj):
        return list(zip(obj[::2], obj[1::2]))

    msg_type = asgi.WhichOneof("type")

    if msg_type == "http":
        return {
            "type": "http",
            "http_version": asgi.http.http_version,
            "method": asgi.http.method,
            "scheme": asgi.http.scheme,
            "path": asgi.http.path,
            "query_string": asgi.http.query_string,
            "headers": unflatten_headers(asgi.http.headers),
            **({"client": (asgi.http.client_host, asgi.http.client_port)} if asgi.http.HasField("client_host") else {}),
            "extensions": {
                "http.response.trailers": {},
            },
        }
    elif msg_type == "http_request":
        return {
            "type": "http.request",
            "body": asgi.http_request.body,
            "more_body": asgi.http_request.more_body,
        }
    elif msg_type == "http_response_start":
        return {
            "type": "http.response.start",
            "status": asgi.http_response_start.status,
            "headers": unflatten_headers(asgi.http_response_start.headers),
            "trailers": asgi.http_response_start.trailers,
        }
    elif msg_type == "http_response_body":
        return {
            "type": "http.response.body",
            "body": asgi.http_response_body.body,
            "more_body": asgi.http_response_body.more_body,
        }
    elif msg_type == "http_response_trailers":
        return {
            "type": "http.response.trailers",
            "headers": unflatten_headers(asgi.http_response_trailers.headers),
            "more_trailers": asgi.http_response_trailers.more_trailers,
        }
    elif msg_type == "http_disconnect":
        return {"type": "http.disconnect"}

    elif msg_type == "websocket":
        return {
            "type": "websocket",
            "http_version": asgi.websocket.http_version,
            "scheme": asgi.websocket.scheme,
            "path": asgi.websocket.path,
            "query_string": asgi.websocket.query_string,
            "headers": unflatten_headers(asgi.websocket.headers),
            **(
                {"client": (asgi.websocket.client_host, asgi.websocket.client_port)}
                if asgi.websocket.HasField("client_host")
                else {}
            ),
            "subprotocols": list(asgi.websocket.subprotocols),
        }
    elif msg_type == "websocket_connect":
        return {"type": "websocket.connect"}
    elif msg_type == "websocket_accept":
        return {
            "type": "websocket.accept",
            "subprotocol": asgi.websocket_accept.subprotocol if asgi.websocket_accept.HasField("subprotocol") else None,
            "headers": unflatten_headers(asgi.websocket_accept.headers),
        }
    elif msg_type == "websocket_receive":
        return {
            "type": "websocket.receive",
            "bytes": asgi.websocket_receive.bytes if asgi.websocket_receive.HasField("bytes") else None,
            "text": asgi.websocket_receive.text if asgi.websocket_receive.HasField("text") else None,
        }
    elif msg_type == "websocket_send":
        return {
            "type": "websocket.send",
            **({"bytes": asgi.websocket_send.bytes} if asgi.websocket_send.HasField("bytes") else {}),
            **({"text": asgi.websocket_send.text} if asgi.websocket_send.HasField("text") else {}),
        }
    elif msg_type == "websocket_disconnect":
        return {
            "type": "websocket.disconnect",
            "code": asgi.websocket_disconnect.code if asgi.websocket_disconnect.HasField("code") else 1005,
        }
    elif msg_type == "websocket_close":
        return {
            "type": "websocket.close",
            "code": asgi.websocket_close.code if asgi.websocket_close.HasField("code") else 1000,
            "reason": asgi.websocket_close.reason,
        }

    else:
        assert msg_type is None
        return None


def get_preferred_payload_format() -> "api_pb2.DataFormat.ValueType":
    payload_format = (config.get("payload_format") or "pickle").lower()
    data_format = api_pb2.DATA_FORMAT_CBOR if payload_format == "cbor" else api_pb2.DATA_FORMAT_PICKLE
    return data_format


def serialize_data_format(obj: Any, data_format: int) -> bytes:
    """Similar to serialize(), but supports other data formats."""
    if data_format == api_pb2.DATA_FORMAT_PICKLE:
        return serialize(obj)
    elif data_format == api_pb2.DATA_FORMAT_ASGI:
        return _serialize_asgi(obj).SerializeToString(deterministic=True)
    elif data_format == api_pb2.DATA_FORMAT_GENERATOR_DONE:
        assert isinstance(obj, api_pb2.GeneratorDone)
        return obj.SerializeToString(deterministic=True)
    elif data_format == api_pb2.DATA_FORMAT_CBOR:
        if cbor2 is None:
            raise InvalidError("CBOR support requires the 'cbor2' package to be installed.")
        try:
            return cbor2.dumps(obj)
        except cbor2.CBOREncodeTypeError:
            try:
                typename = f"{type(obj).__module__}.{type(obj).__name__}"
            except Exception:
                typename = str(type(obj))
            raise SerializationError(
                # TODO (elias): add documentation link for more information on this
                f"Can not serialize type {typename} as cbor. If you need to use a custom data type, "
                "try to serialize it yourself e.g. by using pickle.dumps(my_data)"
            )
    else:
        raise InvalidError(f"Unknown data format {data_format!r}")


def deserialize_data_format(s: bytes, data_format: int, client) -> Any:
    if data_format == api_pb2.DATA_FORMAT_PICKLE:
        return deserialize(s, client)
    elif data_format == api_pb2.DATA_FORMAT_ASGI:
        return _deserialize_asgi(api_pb2.Asgi.FromString(s))
    elif data_format == api_pb2.DATA_FORMAT_GENERATOR_DONE:
        return api_pb2.GeneratorDone.FromString(s)
    elif data_format == api_pb2.DATA_FORMAT_CBOR:
        if cbor2 is None:
            raise InvalidError("CBOR support requires the 'cbor2' package to be installed.")
        return cbor2.loads(s)
    else:
        raise InvalidError(f"Unknown data format {data_format!r}")


class ClsConstructorPickler(pickle.Pickler):
    def __init__(self, buf):
        super().__init__(buf, protocol=PICKLE_PROTOCOL)

    def persistent_id(self, obj):
        if isinstance(obj, (_Object, Object)):
            if not obj.object_id:
                raise InvalidError(f"Can't serialize object {obj} which hasn't been created.")
            return True


def check_valid_cls_constructor_arg(key, obj):
    # Basically pickle, but with support for modal objects
    buf = io.BytesIO()
    try:
        ClsConstructorPickler(buf).dump(obj)
        return True
    except (AttributeError, ValueError):
        raise ValueError(
            f"Only pickle-able types are allowed in remote class constructors: argument {key} of type {type(obj)}."
        )


def apply_defaults(
    python_params: typing.Mapping[str, Any], schema: typing.Sequence[api_pb2.ClassParameterSpec]
) -> dict[str, Any]:
    """Apply any declared defaults from the provided schema, if values aren't provided in python_params

    Conceptually similar to inspect.BoundArguments.apply_defaults.

    Note: Apply this before serializing parameters in order to get consistent parameter
        pools regardless if a value is explicitly provided or not.
    """
    result = {**python_params}
    for schema_param in schema:
        if schema_param.has_default and schema_param.name not in python_params:
            default_field_name = schema_param.WhichOneof("default_oneof")
            if default_field_name is None:
                raise InvalidError(f"{schema_param.name} declared as having a default, but has no default value")
            result[schema_param.name] = getattr(schema_param, default_field_name)
    return result


def encode_parameter_value(name: str, python_value: Any) -> api_pb2.ClassParameterValue:
    """Map to proto parameter representation using python runtime type information"""
    struct = parameter_serde_registry.encode(python_value)
    struct.name = name
    return struct


def serialize_proto_params(python_params: dict[str, Any]) -> bytes:
    proto_params: list[api_pb2.ClassParameterValue] = []
    for param_name, python_value in python_params.items():
        proto_params.append(encode_parameter_value(param_name, python_value))
    proto_bytes = api_pb2.ClassParameterSet(parameters=proto_params).SerializeToString(deterministic=True)
    return proto_bytes


def deserialize_proto_params(serialized_params: bytes) -> dict[str, Any]:
    proto_struct = api_pb2.ClassParameterSet.FromString(serialized_params)
    python_params = {}
    for param in proto_struct.parameters:
        python_params[param.name] = parameter_serde_registry.decode(param)

    return python_params


def validate_parameter_values(payload: dict[str, Any], schema: typing.Sequence[api_pb2.ClassParameterSpec]):
    """Ensure parameter payload conforms to the schema of a class

    Checks that:
    * All fields are specified (defaults are expected to already be applied on the payload)
    * No extra fields are specified
    * The type of each field is correct
    """
    for param_spec in schema:
        if param_spec.name not in payload:
            raise InvalidError(f"Missing required parameter: {param_spec.name}")
        python_value = payload[param_spec.name]
        if param_spec.HasField("full_type") and param_spec.full_type.base_type:
            type_enum_value = param_spec.full_type.base_type
        else:
            type_enum_value = param_spec.type  # backwards compatibility pre-full_type

        parameter_serde_registry.validate_value_for_enum_type(type_enum_value, python_value)

    schema_fields = {p.name for p in schema}
    # then check that no extra values are provided
    non_declared_fields = payload.keys() - schema_fields
    if non_declared_fields:
        raise InvalidError(
            f"The following parameter names were provided but are not defined class modal.parameters for the class: "
            f"{', '.join(non_declared_fields)}"
        )


def deserialize_params(serialized_params: bytes, function_def: api_pb2.Function, _client: "modal.client._Client"):
    if function_def.class_parameter_info.format in (
        api_pb2.ClassParameterInfo.PARAM_SERIALIZATION_FORMAT_UNSPECIFIED,
        api_pb2.ClassParameterInfo.PARAM_SERIALIZATION_FORMAT_PICKLE,
    ):
        # legacy serialization format - pickle of `(args, kwargs)` w/ support for modal object arguments
        try:
            param_args, param_kwargs = deserialize(serialized_params, _client)
        except DeserializationError as original_exc:
            # Fallback in case of proto -> pickle downgrades of a parameter serialization format
            # I.e. FunctionBindParams binding proto serialized params to a function defintion
            # that now assumes pickled data according to class_parameter_info
            param_args = ()
            try:
                param_kwargs = deserialize_proto_params(serialized_params)
            except Exception:
                raise original_exc

    elif function_def.class_parameter_info.format == api_pb2.ClassParameterInfo.PARAM_SERIALIZATION_FORMAT_PROTO:
        param_args = ()  # we use kwargs only for our implicit constructors
        try:
            param_kwargs = deserialize_proto_params(serialized_params)
        except google.protobuf.message.DecodeError as original_exc:
            # Fallback in case of pickle -> proto upgrades of a parameter serialization format
            # I.e. FunctionBindParams binding pickle serialized params to a function defintion
            # that now assumes proto data according to class_parameter_info
            try:
                param_args, param_kwargs = deserialize(serialized_params, _client)
            except Exception:
                raise original_exc

    else:
        raise ExecutionError(
            f"Unknown class parameter serialization format: {function_def.class_parameter_info.format}"
        )

    return param_args, param_kwargs


def _signature_parameter_to_spec(
    python_signature_parameter: inspect.Parameter, include_legacy_parameter_fields: bool = False
) -> api_pb2.ClassParameterSpec:
    """Returns proto representation of Parameter as returned by inspect.signature()

    Setting include_legacy_parameter_fields makes the output backwards compatible with
    pre v0.74 clients looking at class parameter specifications, and should not be used
    when registering *function* schemas.
    """
    declared_type = python_signature_parameter.annotation
    full_proto_type = schema_registry.get_proto_generic_type(declared_type)
    has_default = python_signature_parameter.default is not Parameter.empty

    field_spec = api_pb2.ClassParameterSpec(
        name=python_signature_parameter.name,
        full_type=full_proto_type,
        has_default=has_default,
    )
    if include_legacy_parameter_fields:
        # Specific to *class parameters*:
        # add the .{type}_default and `.type` values as required by legacy clients
        # looking at class parameter specs
        field_spec.type = field_spec.full_type.base_type

        if has_default:
            if full_proto_type.base_type == api_pb2.PARAM_TYPE_INT:
                field_spec.int_default = python_signature_parameter.default
            elif full_proto_type.base_type == api_pb2.PARAM_TYPE_STRING:
                field_spec.string_default = python_signature_parameter.default
            elif full_proto_type.base_type == api_pb2.PARAM_TYPE_BYTES:
                field_spec.bytes_default = python_signature_parameter.default
            elif full_proto_type.base_type == api_pb2.PARAM_TYPE_BOOL:
                field_spec.bool_default = python_signature_parameter.default

    return field_spec


def signature_to_parameter_specs(signature: inspect.Signature) -> list[api_pb2.ClassParameterSpec]:
    # only used for modal.parameter() specs, uses backwards compatible fields and types
    modal_parameters: list[api_pb2.ClassParameterSpec] = []
    for param in signature.parameters.values():
        field_spec = _signature_parameter_to_spec(param, include_legacy_parameter_fields=True)
        modal_parameters.append(field_spec)
    return modal_parameters


def get_callable_schema(
    callable: typing.Callable, *, is_web_endpoint: bool, ignore_first_argument: bool = False
) -> typing.Optional[api_pb2.FunctionSchema]:
    # ignore_first_argument can be used in case of unbound methods where we want to ignore the first (self) argument
    if is_web_endpoint:
        # we don't support schemas on web endpoints for now
        return None

    try:
        sig = inspect.signature(callable)
    except Exception as e:
        logger.debug(f"Error getting signature for function {callable}", exc_info=e)
        return None

    # TODO: treat no return value annotation as None return?
    return_type_proto = schema_registry.get_proto_generic_type(sig.return_annotation)
    arguments = []
    for i, p in enumerate(sig.parameters.values()):
        if i == 0 and ignore_first_argument:
            continue

        arguments.append(_signature_parameter_to_spec(p))

    return api_pb2.FunctionSchema(
        schema_type=api_pb2.FunctionSchema.FUNCTION_SCHEMA_V1,
        arguments=arguments,
        return_type=return_type_proto,
    )


def pickle_exception(exc: BaseException) -> bytes:
    try:
        return serialize(exc)
    except Exception as serialization_exc:
        # We can't always serialize exceptions.
        err = f"Failed to serialize exception {exc} of type {type(exc)}: {serialization_exc}"
        logger.info(err)
        return serialize(SerializationError(err))


def pickle_traceback(exc: BaseException, task_id: str) -> tuple[bytes, bytes]:
    serialized_tb, tb_line_cache = b"", b""

    try:
        tb_dict, line_cache = extract_traceback(exc, task_id)
        serialized_tb = serialize(tb_dict)
        tb_line_cache = serialize(line_cache)
    except Exception:
        logger.info("Failed to serialize exception traceback.")

    return serialized_tb, tb_line_cache



================================================
FILE: modal/_traceback.py
================================================
# Copyright Modal Labs 2022
"""Helper functions related to operating on exceptions, warnings, and traceback objects.

Functions related to *displaying* tracebacks should go in `modal/cli/_traceback.py`
so that Rich is not a dependency of the container Client.
"""

import re
import sys
import traceback
import typing
import warnings
from types import TracebackType
from typing import Any, Iterable, Optional

from modal.config import config, logger
from modal_proto import api_pb2

from ._vendor.tblib import Traceback as TBLibTraceback
from .exception import ServerWarning

TBDictType = dict[str, Any]
LineCacheType = dict[tuple[str, str], str]


def extract_traceback(exc: BaseException, task_id: str) -> tuple[TBDictType, LineCacheType]:
    """Given an exception, extract a serializable traceback (with task ID markers included),
    and a line cache that maps (filename, lineno) to line contents. The latter is used to show
    a helpful traceback to the user, even if they don't have packages installed locally that
    are referenced in the traceback."""

    tb = TBLibTraceback(exc.__traceback__)
    # Prefix traceback file paths with <task_id>. This lets us attribute which parts of
    # the traceback came from specific remote containers, while still fitting in the TracebackType
    # spec. Real paths can never start with <; we can use this to extract task_ids from filenames
    # at the client.
    cur = tb
    while cur is not None:
        file = cur.tb_frame.f_code.co_filename

        # Paths starting with < indicate that they're from a traceback from a remote
        # container. This means we've reached the end of the local traceback.
        if file.startswith("<"):
            break
        # We rely on this specific filename format when inferring where the exception was raised
        # in various other exception-related code
        cur.tb_frame.f_code.co_filename = f"<{task_id}>:{file}"
        cur = cur.tb_next

    tb_dict = tb.to_dict()

    line_cache = getattr(exc, "__line_cache__", {})

    for frame in traceback.extract_tb(exc.__traceback__):
        line_cache[(frame.filename, frame.lineno)] = frame.line

    return tb_dict, line_cache


def append_modal_tb(exc: BaseException, tb_dict: TBDictType, line_cache: LineCacheType) -> None:
    tb = TBLibTraceback.from_dict(tb_dict).as_traceback()

    # Filter out the prefix corresponding to internal Modal frames, and then make
    # the remote traceback from a Modal function the starting point of the current
    # exception's traceback.

    while tb is not None:
        if "/pkg/modal/" not in tb.tb_frame.f_code.co_filename:
            break
        tb = tb.tb_next

    exc.__traceback__ = tb

    setattr(exc, "__line_cache__", line_cache)


def reduce_traceback_to_user_code(tb: Optional[TracebackType], user_source: str) -> TracebackType:
    """Return a traceback that does not contain modal entrypoint or synchronicity frames."""

    # Step forward all the way through the traceback and drop any "Modal support" frames
    def skip_frame(filename: str) -> bool:
        return "/site-packages/synchronicity/" in filename or "modal/_utils/deprecation" in filename

    tb_root = tb
    while tb is not None:
        while tb.tb_next is not None:
            if skip_frame(tb.tb_next.tb_frame.f_code.co_filename):
                tb.tb_next = tb.tb_next.tb_next
            else:
                break
        tb = tb.tb_next
    tb = tb_root

    # Now step forward again until we get to first frame of user code
    if user_source.endswith(".py"):
        while tb is not None and tb.tb_frame.f_code.co_filename != user_source:
            tb = tb.tb_next
    else:
        while tb is not None and tb.tb_frame.f_code.co_name != "<module>":
            tb = tb.tb_next
    if tb is None:
        # In case we didn't find a frame that matched the user source, revert to the original root
        tb = tb_root

    return tb


def traceback_contains_remote_call(tb: Optional[TracebackType]) -> bool:
    """Inspect the traceback stack to determine whether an error was raised locally or remotely."""
    while tb is not None:
        if re.match(r"^<ta-[0-9A-Z]{26}>:", tb.tb_frame.f_code.co_filename):
            return True
        tb = tb.tb_next
    return False


def print_exception(exc: Optional[type[BaseException]], value: Optional[BaseException], tb: Optional[TracebackType]):
    """Add backwards compatibility for printing exceptions with "notes" for Python<3.11."""
    traceback.print_exception(exc, value, tb)
    if sys.version_info < (3, 11) and value is not None:  # type: ignore
        notes = getattr(value, "__notes__", [])
        print(*notes, sep="\n", file=sys.stderr)


def print_server_warnings(server_warnings: Iterable[api_pb2.Warning]):
    """Issue a warning originating from the server with empty metadata about local origin.

    When using the Modal CLI, these warnings should get caught and coerced into Rich panels.
    """
    for warning in server_warnings:
        warnings.warn_explicit(warning.message, ServerWarning, "<modal-server>", 0)


# for some reason, the traceback cleanup here can't be moved into a context manager :(
traceback_suppression_note = (
    "Internal Modal traceback frames are suppressed for readability. Use MODAL_TRACEBACK=1 to show a full traceback."
)


class suppress_tb_frames:
    def __init__(self, n: int):
        self.n = n

    def __enter__(self):
        pass

    def __exit__(
        self, exc_type: Optional[type[BaseException]], exc: Optional[BaseException], tb: Optional[TracebackType]
    ) -> typing.Literal[False]:
        # *base* exceptions like CancelledError, SystemExit etc. can come from random places,
        # so we don't suppress tracebacks for those
        is_base_exception = not isinstance(exc, Exception)
        if config.get("traceback") or exc_type is None or is_base_exception:
            return False

        # modify traceback on exception object
        try:
            final_tb = tb
            for _ in range(self.n):
                final_tb = final_tb.tb_next
        except AttributeError:
            logger.debug(f"Failed to suppress {self.n} traceback frames from {str(exc_type)} {str(exc)}")
            raise

        exc.with_traceback(final_tb)
        notes = getattr(exc, "__notes__", [])
        if traceback_suppression_note not in notes:
            # .add_note was added in Python 3.11
            notes.append(traceback_suppression_note)
        return False



================================================
FILE: modal/_tunnel.py
================================================
# Copyright Modal Labs 2023
"""Client for Modal relay servers, allowing users to expose TLS."""

from collections.abc import AsyncIterator
from dataclasses import dataclass
from typing import Optional

from grpclib import GRPCError, Status
from synchronicity.async_wrap import asynccontextmanager

from modal_proto import api_pb2

from ._utils.async_utils import synchronize_api
from .client import _Client
from .exception import InvalidError, RemoteError


@dataclass(frozen=True)
class Tunnel:
    """A port forwarded from within a running Modal container. Created by `modal.forward()`.

    **Important:** This is an experimental API which may change in the future.
    """

    host: str
    port: int
    unencrypted_host: str
    unencrypted_port: int

    @property
    def url(self) -> str:
        """Get the public HTTPS URL of the forwarded port."""
        value = f"https://{self.host}"
        if self.port != 443:
            value += f":{self.port}"
        return value

    @property
    def tls_socket(self) -> tuple[str, int]:
        """Get the public TLS socket as a (host, port) tuple."""
        return (self.host, self.port)

    @property
    def tcp_socket(self) -> tuple[str, int]:
        """Get the public TCP socket as a (host, port) tuple."""
        if not self.unencrypted_host:
            raise InvalidError(
                "This tunnel is not configured for unencrypted TCP. Please use `forward(..., unencrypted=True)`."
            )
        return (self.unencrypted_host, self.unencrypted_port)


@asynccontextmanager
async def _forward(port: int, *, unencrypted: bool = False, client: Optional[_Client] = None) -> AsyncIterator[Tunnel]:
    """Expose a port publicly from inside a running Modal container, with TLS.

    If `unencrypted` is set, this also exposes the TCP socket without encryption on a random port
    number. This can be used to SSH into a container (see example below). Note that it is on the public Internet, so
    make sure you are using a secure protocol over TCP.

    **Important:** This is an experimental API which may change in the future.

    **Usage:**

    ```python notest
    import modal
    from flask import Flask

    app = modal.App(image=modal.Image.debian_slim().pip_install("Flask"))
    flask_app = Flask(__name__)


    @flask_app.route("/")
    def hello_world():
        return "Hello, World!"


    @app.function()
    def run_app():
        # Start a web server inside the container at port 8000. `modal.forward(8000)` lets us
        # expose that port to the world at a random HTTPS URL.
        with modal.forward(8000) as tunnel:
            print("Server listening at", tunnel.url)
            flask_app.run("0.0.0.0", 8000)

        # When the context manager exits, the port is no longer exposed.
    ```

    **Raw TCP usage:**

    ```python
    import socket
    import threading

    import modal


    def run_echo_server(port: int):
        \"""Run a TCP echo server listening on the given port.\"""
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.bind(("0.0.0.0", port))
        sock.listen(1)

        while True:
            conn, addr = sock.accept()
            print("Connection from:", addr)

            # Start a new thread to handle the connection
            def handle(conn):
                with conn:
                    while True:
                        data = conn.recv(1024)
                        if not data:
                            break
                        conn.sendall(data)

            threading.Thread(target=handle, args=(conn,)).start()


    app = modal.App()


    @app.function()
    def tcp_tunnel():
        # This exposes port 8000 to public Internet traffic over TCP.
        with modal.forward(8000, unencrypted=True) as tunnel:
            # You can connect to this TCP socket from outside the container, for example, using `nc`:
            #  nc <HOST> <PORT>
            print("TCP tunnel listening at:", tunnel.tcp_socket)
            run_echo_server(8000)
    ```

    **SSH example:**
    This assumes you have a rsa keypair in `~/.ssh/id_rsa{.pub}`, this is a bare-bones example
    letting you SSH into a Modal container.

    ```python
    import subprocess
    import time

    import modal

    app = modal.App()
    image = (
        modal.Image.debian_slim()
        .apt_install("openssh-server")
        .run_commands("mkdir /run/sshd")
        .add_local_file("~/.ssh/id_rsa.pub", "/root/.ssh/authorized_keys", copy=True)
    )


    @app.function(image=image, timeout=3600)
    def some_function():
        subprocess.Popen(["/usr/sbin/sshd", "-D", "-e"])
        with modal.forward(port=22, unencrypted=True) as tunnel:
            hostname, port = tunnel.tcp_socket
            connection_cmd = f'ssh -p {port} root@{hostname}'
            print(f"ssh into container using: {connection_cmd}")
            time.sleep(3600)  # keep alive for 1 hour or until killed
    ```

    If you intend to use this more generally, a suggestion is to put the subprocess and port
    forwarding code in an `@enter` lifecycle method of an @app.cls, to only make a single
    ssh server and port for each container (and not one for each input to the function).
    """

    if not isinstance(port, int):
        raise InvalidError(f"The port argument should be an int, not {port!r}")
    if port < 1 or port > 65535:
        raise InvalidError(f"Invalid port number {port}")

    if not client:
        client = await _Client.from_env()

    if client.client_type != api_pb2.CLIENT_TYPE_CONTAINER:
        raise InvalidError("Forwarding ports only works inside a Modal container")

    try:
        response = await client.stub.TunnelStart(api_pb2.TunnelStartRequest(port=port, unencrypted=unencrypted))
    except GRPCError as exc:
        if exc.status == Status.ALREADY_EXISTS:
            raise InvalidError(f"Port {port} is already forwarded")
        elif exc.status == Status.UNAVAILABLE:
            raise RemoteError("Relay server is unavailable") from exc
        else:
            raise

    try:
        yield Tunnel(response.host, response.port, response.unencrypted_host, response.unencrypted_port)
    finally:
        await client.stub.TunnelStop(api_pb2.TunnelStopRequest(port=port))


forward = synchronize_api(_forward)



================================================
FILE: modal/_type_manager.py
================================================
# Copyright Modal Labs 2025
import typing
from typing import Any

import typing_extensions

from modal.exception import InvalidError
from modal_proto import api_pb2


class ParameterProtoSerde(typing.Protocol):
    def encode(self, value: Any) -> api_pb2.ClassParameterValue: ...

    def decode(self, proto_value: api_pb2.ClassParameterValue) -> Any: ...

    def validate(self, python_value: Any): ...


class ProtoParameterSerdeRegistry:
    _py_base_type_to_serde: dict[type, ParameterProtoSerde]
    _proto_type_to_serde: dict["api_pb2.ParameterType.ValueType", ParameterProtoSerde]

    def __init__(self):
        self._py_base_type_to_serde = {}
        self._proto_type_to_serde = {}

    def register_encoder(self, python_base_type: type) -> typing.Callable[[ParameterProtoSerde], ParameterProtoSerde]:
        def deco(ph: ParameterProtoSerde) -> ParameterProtoSerde:
            if python_base_type in self._py_base_type_to_serde:
                raise ValueError("Can't register the same encoder type twice")
            self._py_base_type_to_serde[python_base_type] = ph
            return ph

        return deco

    def register_decoder(
        self, enum_type_value: "api_pb2.ParameterType.ValueType"
    ) -> typing.Callable[[ParameterProtoSerde], ParameterProtoSerde]:
        def deco(ph: ParameterProtoSerde) -> ParameterProtoSerde:
            if enum_type_value in self._proto_type_to_serde:
                raise ValueError("Can't register the same decoder type twice")
            self._proto_type_to_serde[enum_type_value] = ph
            return ph

        return deco

    def encode(self, python_value: Any) -> api_pb2.ClassParameterValue:
        return self._get_encoder(type(python_value)).encode(python_value)

    def supports_type(self, declared_type: type) -> bool:
        try:
            self._get_encoder(declared_type)
            return True
        except InvalidError:
            return False

    def decode(self, param_value: api_pb2.ClassParameterValue) -> Any:
        return self._get_decoder(param_value.type).decode(param_value)

    def validate_parameter_type(self, declared_type: type):
        """Raises a helpful TypeError if the supplied type isn't supported by class parameters"""
        if not parameter_serde_registry.supports_type(declared_type):
            supported_types = self._py_base_type_to_serde.keys()
            supported_str = ", ".join(t.__name__ for t in supported_types)

            raise TypeError(
                f"{declared_type.__name__} is not a supported modal.parameter() type. Use one of: {supported_str}"
            )

    def validate_value_for_enum_type(self, enum_value: "api_pb2.ParameterType.ValueType", python_value: Any):
        serde = self._get_decoder(enum_value)  # use the schema's expected decoder
        serde.validate(python_value)

    def _get_encoder(self, python_base_type: type) -> ParameterProtoSerde:
        try:
            return self._py_base_type_to_serde[python_base_type]
        except KeyError:
            raise InvalidError(f"No class parameter encoder implemented for type `{python_base_type.__name__}`")

    def _get_decoder(self, enum_value: "api_pb2.ParameterType.ValueType") -> ParameterProtoSerde:
        try:
            return self._proto_type_to_serde[enum_value]
        except KeyError:
            try:
                enum_name = api_pb2.ParameterType.Name(enum_value)
            except ValueError:
                enum_name = str(enum_value)

            raise InvalidError(f"No class parameter decoder implemented for type {enum_name}.")


parameter_serde_registry = ProtoParameterSerdeRegistry()


@parameter_serde_registry.register_encoder(int)
@parameter_serde_registry.register_decoder(api_pb2.PARAM_TYPE_INT)
class IntParameter:
    @staticmethod
    def encode(value: Any) -> api_pb2.ClassParameterValue:
        return api_pb2.ClassParameterValue(type=api_pb2.PARAM_TYPE_INT, int_value=value)

    @staticmethod
    def decode(proto_value: api_pb2.ClassParameterValue) -> int:
        return proto_value.int_value

    @staticmethod
    def validate(python_value: Any):
        if not isinstance(python_value, int):
            raise TypeError(f"Expected int, got {type(python_value).__name__}")


@parameter_serde_registry.register_encoder(str)
@parameter_serde_registry.register_decoder(api_pb2.PARAM_TYPE_STRING)
class StringParameter:
    @staticmethod
    def encode(value: Any) -> api_pb2.ClassParameterValue:
        return api_pb2.ClassParameterValue(type=api_pb2.PARAM_TYPE_STRING, string_value=value)

    @staticmethod
    def decode(proto_value: api_pb2.ClassParameterValue) -> str:
        return proto_value.string_value

    @staticmethod
    def validate(python_value: Any):
        if not isinstance(python_value, str):
            raise TypeError(f"Expected str, got {type(python_value).__name__}")


@parameter_serde_registry.register_encoder(bytes)
@parameter_serde_registry.register_decoder(api_pb2.PARAM_TYPE_BYTES)
class BytesParameter:
    @staticmethod
    def encode(value: Any) -> api_pb2.ClassParameterValue:
        return api_pb2.ClassParameterValue(type=api_pb2.PARAM_TYPE_BYTES, bytes_value=value)

    @staticmethod
    def decode(proto_value: api_pb2.ClassParameterValue) -> bytes:
        return proto_value.bytes_value

    @staticmethod
    def validate(python_value: Any):
        if not isinstance(python_value, bytes):
            raise TypeError(f"Expected bytes, got {type(python_value).__name__}")


@parameter_serde_registry.register_encoder(bool)
@parameter_serde_registry.register_decoder(api_pb2.PARAM_TYPE_BOOL)
class BoolParameter:
    @staticmethod
    def encode(value: Any) -> api_pb2.ClassParameterValue:
        return api_pb2.ClassParameterValue(type=api_pb2.PARAM_TYPE_BOOL, bool_value=value)

    @staticmethod
    def decode(proto_value: api_pb2.ClassParameterValue) -> bool:
        return proto_value.bool_value

    @staticmethod
    def validate(python_value: Any):
        if not isinstance(python_value, bool):
            raise TypeError(f"Expected bool, got {type(python_value).__name__}")


SCHEMA_FACTORY_TYPE = typing.Callable[[type], api_pb2.GenericPayloadType]


class SchemaRegistry:
    _schema_factories: dict[type, SCHEMA_FACTORY_TYPE]

    def __init__(self):
        self._schema_factories = {}

    def add(self, python_base_type: type) -> typing.Callable[[SCHEMA_FACTORY_TYPE], SCHEMA_FACTORY_TYPE]:
        # decorator for schema factory functions for a base type
        def deco(factory_func: SCHEMA_FACTORY_TYPE) -> SCHEMA_FACTORY_TYPE:
            assert python_base_type not in self._schema_factories
            self._schema_factories[python_base_type] = factory_func
            return factory_func

        return deco

    def get(self, python_base_type: type) -> SCHEMA_FACTORY_TYPE:
        try:
            return self._schema_factories[python_base_type]
        except KeyError:
            return unknown_type_schema

    def get_proto_generic_type(self, declared_type: type):
        if origin := typing_extensions.get_origin(declared_type):
            base_type = origin
        else:
            base_type = declared_type

        return self.get(base_type)(declared_type)


schema_registry = SchemaRegistry()


@schema_registry.add(int)
def int_schema(full_python_type: type) -> api_pb2.GenericPayloadType:
    return api_pb2.GenericPayloadType(
        base_type=api_pb2.PARAM_TYPE_INT,
    )


@schema_registry.add(bytes)
def proto_type_def(declared_python_type: type) -> api_pb2.GenericPayloadType:
    return api_pb2.GenericPayloadType(
        base_type=api_pb2.PARAM_TYPE_BYTES,
    )


def unknown_type_schema(declared_python_type: type) -> api_pb2.GenericPayloadType:
    # TODO: add some metadata for unknown types to the type def?
    return api_pb2.GenericPayloadType(base_type=api_pb2.PARAM_TYPE_UNKNOWN)


@schema_registry.add(str)
def str_schema(full_python_type: type) -> api_pb2.GenericPayloadType:
    return api_pb2.GenericPayloadType(
        base_type=api_pb2.PARAM_TYPE_STRING,
    )


@schema_registry.add(bool)
def bool_schema(full_python_type: type) -> api_pb2.GenericPayloadType:
    return api_pb2.GenericPayloadType(
        base_type=api_pb2.PARAM_TYPE_BOOL,
    )


@schema_registry.add(type(None))
def none_type_schema(declared_python_type: type) -> api_pb2.GenericPayloadType:
    return api_pb2.GenericPayloadType(base_type=api_pb2.PARAM_TYPE_NONE)


@schema_registry.add(list)
def list_schema(full_python_type: type) -> api_pb2.GenericPayloadType:
    args = typing_extensions.get_args(full_python_type)

    return api_pb2.GenericPayloadType(
        base_type=api_pb2.PARAM_TYPE_LIST, sub_types=[schema_registry.get_proto_generic_type(arg) for arg in args]
    )


@schema_registry.add(dict)
def dict_schema(full_python_type: type) -> api_pb2.GenericPayloadType:
    args = typing_extensions.get_args(full_python_type)

    return api_pb2.GenericPayloadType(
        base_type=api_pb2.PARAM_TYPE_DICT,
        sub_types=[schema_registry.get_proto_generic_type(arg_type) for arg_type in args],
    )



================================================
FILE: modal/_watcher.py
================================================
# Copyright Modal Labs 2022
from collections import defaultdict
from collections.abc import AsyncGenerator
from pathlib import Path
from typing import Optional

from rich.tree import Tree
from watchfiles import Change, DefaultFilter, awatch

from modal.mount import _Mount

from .output import _get_output_manager

_TIMEOUT_SENTINEL = object()


class AppFilesFilter(DefaultFilter):
    def __init__(
        self,
        *,
        # A directory filter is used to only watch certain files within a directory.
        # Watching specific files is discouraged on Linux, so to watch a file we watch its
        # containing directory and then filter that directory's changes for relevant files.
        # https://github.com/notify-rs/notify/issues/394
        dir_filters: dict[Path, Optional[set[Path]]],
    ) -> None:
        self.dir_filters = dir_filters
        super().__init__()

    def __call__(self, change: Change, path: str) -> bool:
        p = Path(path).absolute()
        if p.name == ".DS_Store":
            return False
        # Vim creates this temporary file to see whether it can write
        # into a target directory.
        elif p.name == "4913":
            return False

        allowlists = set()

        for root, allowlist in self.dir_filters.items():
            # For every filter path that's a parent of the current path...
            if root in p.parents:
                # If allowlist is None, we're watching the directory and we have a match.
                if allowlist is None:
                    return super().__call__(change, path)

                # If not, it's specific files, and we could have a match.
                else:
                    allowlists |= allowlist

        if allowlists and p not in allowlists:
            return False

        return super().__call__(change, path)


async def _watch_paths(paths: set[Path], watch_filter: AppFilesFilter) -> AsyncGenerator[set[str], None]:
    try:
        async for changes in awatch(*paths, step=500, watch_filter=watch_filter):
            changed_paths = {stringpath for _, stringpath in changes}
            yield changed_paths
    except RuntimeError:
        # Thrown by watchfiles from Rust, when the generator is closed externally.
        pass


def _print_watched_paths(paths: set[Path]):
    msg = "ï¸ï¸âš¡ï¸ Serving... hit Ctrl-C to stop!"

    output_tree = Tree(msg, guide_style="gray50")

    for path in paths:
        output_tree.add(f"Watching {path}.")

    if output_mgr := _get_output_manager():
        output_mgr.print(output_tree)


def _watch_args_from_mounts(mounts: list[_Mount]) -> tuple[set[Path], AppFilesFilter]:
    paths = set()
    dir_filters: dict[Path, Optional[set[Path]]] = defaultdict(set)
    for mount in mounts:
        # TODO(elias): Make this part of the mount class instead, since it uses so much internals
        for entry in mount._entries:
            path, filter_file = entry.watch_entry()
            path = path.absolute().resolve()
            paths.add(path)
            if filter_file is None:
                dir_filters[path] = None
            elif dir_filters[path] is not None:
                dir_filters[path].add(filter_file.absolute().resolve())

    watch_filter = AppFilesFilter(dir_filters=dict(dir_filters))
    return paths, watch_filter


async def watch(mounts: list[_Mount]) -> AsyncGenerator[set[str], None]:
    paths, watch_filter = _watch_args_from_mounts(mounts)

    _print_watched_paths(paths)

    async for updated_paths in _watch_paths(paths, watch_filter):
        yield updated_paths



================================================
FILE: modal/app.py
================================================
# Copyright Modal Labs 2022
import inspect
import typing
from collections.abc import AsyncGenerator, Collection, Coroutine, Sequence
from pathlib import PurePosixPath
from textwrap import dedent
from typing import (
    Any,
    Callable,
    ClassVar,
    Optional,
    Union,
    overload,
)

import typing_extensions
from google.protobuf.message import Message
from synchronicity.async_wrap import asynccontextmanager

from modal_proto import api_pb2

from ._functions import _Function
from ._ipython import is_notebook
from ._object import _get_environment_name, _Object
from ._partial_function import (
    _find_partial_methods_for_user_cls,
    _PartialFunction,
    _PartialFunctionFlags,
)
from ._utils.async_utils import synchronize_api
from ._utils.deprecation import (
    deprecation_warning,
    warn_on_renamed_autoscaler_settings,
)
from ._utils.function_utils import FunctionInfo, is_global_object, is_method_fn
from ._utils.grpc_utils import retry_transient_errors
from ._utils.mount_utils import validate_volumes
from ._utils.name_utils import check_object_name, check_tag_dict
from .client import _Client
from .cloud_bucket_mount import _CloudBucketMount
from .cls import _Cls, parameter
from .config import logger
from .exception import ExecutionError, InvalidError
from .functions import Function
from .gpu import GPU_T
from .image import _Image
from .network_file_system import _NetworkFileSystem
from .partial_function import PartialFunction
from .proxy import _Proxy
from .retries import Retries
from .running_app import RunningApp
from .schedule import Schedule
from .scheduler_placement import SchedulerPlacement
from .secret import _Secret
from .volume import _Volume

_default_image: _Image = _Image.debian_slim()


class _LocalEntrypoint:
    _info: FunctionInfo
    _app: "_App"

    def __init__(self, info: FunctionInfo, app: "_App") -> None:
        self._info = info
        self._app = app

    def __call__(self, *args: Any, **kwargs: Any) -> Any:
        return self._info.raw_f(*args, **kwargs)

    @property
    def info(self) -> FunctionInfo:
        return self._info

    @property
    def app(self) -> "_App":
        return self._app


LocalEntrypoint = synchronize_api(_LocalEntrypoint)


def check_sequence(items: typing.Sequence[typing.Any], item_type: type[typing.Any], error_msg: str) -> None:
    if not isinstance(items, (list, tuple)):
        raise InvalidError(error_msg)
    if not all(isinstance(v, item_type) for v in items):
        raise InvalidError(error_msg)


CLS_T = typing.TypeVar("CLS_T", bound=type[Any])


P = typing_extensions.ParamSpec("P")
ReturnType = typing.TypeVar("ReturnType")
OriginalReturnType = typing.TypeVar("OriginalReturnType")


class _FunctionDecoratorType:
    @overload
    def __call__(
        self, func: PartialFunction[P, ReturnType, OriginalReturnType]
    ) -> Function[P, ReturnType, OriginalReturnType]: ...  # already wrapped by a modal decorator, e.g. web_endpoint

    @overload
    def __call__(
        self, func: Callable[P, Coroutine[Any, Any, ReturnType]]
    ) -> Function[P, ReturnType, Coroutine[Any, Any, ReturnType]]: ...  # decorated async function

    @overload
    def __call__(
        self, func: Callable[P, ReturnType]
    ) -> Function[P, ReturnType, ReturnType]: ...  # decorated non-async function

    def __call__(self, func): ...


class _App:
    """A Modal App is a group of functions and classes that are deployed together.

    The app serves at least three purposes:

    * A unit of deployment for functions and classes.
    * Syncing of identities of (primarily) functions and classes across processes
      (your local Python interpreter and every Modal container active in your application).
    * Manage log collection for everything that happens inside your code.

    **Registering functions with an app**

    The most common way to explicitly register an Object with an app is through the
    `@app.function()` decorator. It both registers the annotated function itself and
    other passed objects, like schedules and secrets, with the app:

    ```python
    import modal

    app = modal.App()

    @app.function(
        secrets=[modal.Secret.from_name("some_secret")],
        schedule=modal.Period(days=1),
    )
    def foo():
        pass
    ```

    In this example, the secret and schedule are registered with the app.
    """

    _all_apps: ClassVar[dict[Optional[str], list["_App"]]] = {}
    _container_app: ClassVar[Optional["_App"]] = None

    _name: Optional[str]
    _description: Optional[str]
    _tags: dict[str, str]

    _functions: dict[str, _Function]
    _classes: dict[str, _Cls]

    _image: Optional[_Image]
    _secrets: Sequence[_Secret]
    _volumes: dict[Union[str, PurePosixPath], _Volume]
    _web_endpoints: list[str]  # Used by the CLI
    _local_entrypoints: dict[str, _LocalEntrypoint]

    # Running apps only (container apps or running local)
    _app_id: Optional[str]  # Kept after app finishes
    _running_app: Optional[RunningApp]  # Various app info
    _client: Optional[_Client]

    _include_source_default: Optional[bool] = None

    def __init__(
        self,
        name: Optional[str] = None,
        *,
        tags: Optional[dict[str, str]] = None,  # Additional metadata to set on the App
        image: Optional[_Image] = None,  # Default Image for the App (otherwise default to `modal.Image.debian_slim()`)
        secrets: Sequence[_Secret] = [],  # Secrets to add for all Functions in the App
        volumes: dict[Union[str, PurePosixPath], _Volume] = {},  # Volume mounts to use for all Functions
        include_source: bool = True,  # Default configuration for adding Function source file(s) to the Modal container
    ) -> None:
        """Construct a new app, optionally with default image, mounts, secrets, or volumes.

        ```python notest
        image = modal.Image.debian_slim().pip_install(...)
        secret = modal.Secret.from_name("my-secret")
        volume = modal.Volume.from_name("my-data")
        app = modal.App(image=image, secrets=[secret], volumes={"/mnt/data": volume})
        ```
        """
        if name is not None and not isinstance(name, str):
            raise InvalidError("Invalid value for `name`: Must be string.")

        self._name = name
        self._description = name
        self._tags = check_tag_dict(tags or {})
        self._include_source_default = include_source

        check_sequence(secrets, _Secret, "`secrets=` has to be a list or tuple of `modal.Secret` objects")
        validate_volumes(volumes)

        if image is not None and not isinstance(image, _Image):
            raise InvalidError("`image=` has to be a `modal.Image` object")

        self._functions = {}
        self._classes = {}
        self._image = image
        self._secrets = secrets
        self._volumes = volumes
        self._local_entrypoints = {}
        self._web_endpoints = []

        self._app_id = None
        self._running_app = None  # Set inside container, OR during the time an app is running locally
        self._client = None

        # Register this app. This is used to look up the app in the container, when we can't get it from the function
        _App._all_apps.setdefault(self._name, []).append(self)

    @property
    def name(self) -> Optional[str]:
        """The user-provided name of the App."""
        return self._name

    @property
    def is_interactive(self) -> bool:
        """Whether the current app for the app is running in interactive mode."""
        # return self._name
        if self._running_app:
            return self._running_app.interactive
        else:
            return False

    @property
    def app_id(self) -> Optional[str]:
        """Return the app_id of a running or stopped app."""
        return self._app_id

    @property
    def description(self) -> Optional[str]:
        """The App's `name`, if available, or a fallback descriptive identifier."""
        return self._description

    @staticmethod
    async def lookup(
        name: str,
        *,
        client: Optional[_Client] = None,
        environment_name: Optional[str] = None,
        create_if_missing: bool = False,
    ) -> "_App":
        """Look up an App with a given name, creating a new App if necessary.

        Note that Apps created through this method will be in a deployed state,
        but they will not have any associated Functions or Classes. This method
        is mainly useful for creating an App to associate with a Sandbox:

        ```python
        app = modal.App.lookup("my-app", create_if_missing=True)
        modal.Sandbox.create("echo", "hi", app=app)
        ```
        """
        check_object_name(name, "App")

        if client is None:
            client = await _Client.from_env()

        environment_name = _get_environment_name(environment_name)

        request = api_pb2.AppGetOrCreateRequest(
            app_name=name,
            environment_name=environment_name,
            object_creation_type=(api_pb2.OBJECT_CREATION_TYPE_CREATE_IF_MISSING if create_if_missing else None),
        )

        response = await retry_transient_errors(client.stub.AppGetOrCreate, request)

        app = _App(name)
        app._app_id = response.app_id
        app._client = client
        app._running_app = RunningApp(response.app_id, interactive=False)
        return app

    def set_description(self, description: str):
        self._description = description

    def _validate_blueprint_value(self, key: str, value: Any):
        if not isinstance(value, _Object):
            raise InvalidError(f"App attribute `{key}` with value {value!r} is not a valid Modal object")

    @property
    def image(self) -> _Image:
        return self._image

    @image.setter
    def image(self, value):
        self._image = value

    def _uncreate_all_objects(self):
        # TODO(erikbern): this doesn't unhydrate objects that aren't tagged
        for obj in self._functions.values():
            obj._unhydrate()
        for obj in self._classes.values():
            obj._unhydrate()

    @asynccontextmanager
    async def _set_local_app(self, client: _Client, running_app: RunningApp) -> AsyncGenerator[None, None]:
        self._app_id = running_app.app_id
        self._running_app = running_app
        self._client = client
        try:
            yield
        finally:
            self._running_app = None
            self._client = None
            self._uncreate_all_objects()

    @asynccontextmanager
    async def run(
        self,
        *,
        client: Optional[_Client] = None,
        detach: bool = False,
        interactive: bool = False,
        environment_name: Optional[str] = None,
    ) -> AsyncGenerator["_App", None]:
        """Context manager that runs an ephemeral app on Modal.

        Use this as the main entry point for your Modal application. All calls
        to Modal Functions should be made within the scope of this context
        manager, and they will correspond to the current App.

        **Example**

        ```python notest
        with app.run():
            some_modal_function.remote()
        ```

        To enable output printing (i.e., to see App logs), use `modal.enable_output()`:

        ```python notest
        with modal.enable_output():
            with app.run():
                some_modal_function.remote()
        ```

        Note that you should not invoke this in global scope of a file where you have
        Modal Functions or Classes defined, since that would run the block when the Function
        or Cls is imported in your containers as well. If you want to run it as your entrypoint,
        consider protecting it:

        ```python
        if __name__ == "__main__":
            with app.run():
                some_modal_function.remote()
        ```

        You can then run your script with:

        ```shell
        python app_module.py
        ```

        """
        from .runner import _run_app  # Defer import of runner.py, which imports a lot from Rich

        async with _run_app(
            self, client=client, detach=detach, interactive=interactive, environment_name=environment_name
        ):
            yield self

    async def deploy(
        self,
        *,
        name: Optional[str] = None,  # Name for the deployment, overriding any set on the App
        environment_name: Optional[str] = None,  # Environment to deploy the App in
        tag: str = "",  # Optional metadata that is specific to this deployment
        client: Optional[_Client] = None,  # Alternate client to use for communication with the server
    ) -> typing_extensions.Self:
        """Deploy the App so that it is available persistently.

        Deployed Apps will be avaible for lookup or web-based invocations until they are stopped.
        Unlike with `App.run`, this method will return as soon as the deployment completes.

        This method is a programmatic alternative to the `modal deploy` CLI command.

        Examples:

        ```python notest
        app = App("my-app")
        app.deploy()
        ```

        To enable output printing (i.e., to see build logs), use `modal.enable_output()`:

        ```python notest
        app = App("my-app")
        with modal.enable_output():
            app.deploy()
        ```

        Unlike with `App.run`, Function logs will not stream back to the local client after the
        App is deployed.

        Note that you should not invoke this method in global scope, as that would redeploy
        the App every time the file is imported. If you want to write a programmatic deployment
        script, protect this call so that it only runs when the file is executed directly:

        ```python notest
        if __name__ == "__main__":
            with modal.enable_output():
                app.deploy()
        ```

        Then you can deploy your app with:

        ```shell
        python app_module.py
        ```

        """
        from .runner import _deploy_app  # Defer import of runner.py, which imports a lot from Rich

        if name is None and self._name is None:
            raise InvalidError(
                "You need to either supply a deployment name or have a name set on the app.\n"
                "\n"
                "Examples:\n"
                'app.deploy(name="some-name")\n\n'
                "or\n"
                'app = modal.App("some-name")'
            )
        result = await _deploy_app(self, name=name, environment_name=environment_name, tag=tag, client=client)
        self._app_id = result.app_id
        return self

    def _get_default_image(self):
        if self._image:
            return self._image
        else:
            return _default_image

    def _get_watch_mounts(self):
        if not self._running_app:
            raise ExecutionError("`_get_watch_mounts` requires a running app.")

        all_mounts = []
        for function in self.registered_functions.values():
            all_mounts.extend(function._serve_mounts)

        return [m for m in all_mounts if m.is_local()]

    def _add_function(self, function: _Function, is_web_endpoint: bool):
        if old_function := self._functions.get(function.tag, None):
            if old_function is function:
                return  # already added the same exact instance, ignore

            if not is_notebook():
                logger.warning(
                    f"Warning: function name '{function.tag}' collision!"
                    " Overriding existing function "
                    f"[{old_function._info.module_name}].{old_function._info.function_name}"
                    f" with new function [{function._info.module_name}].{function._info.function_name}"
                )
        if function.tag in self._classes:
            logger.warning(f"Warning: tag {function.tag} exists but is overridden by function")

        if self._running_app:
            # If this is inside a container, then objects can be defined after app initialization.
            # So we may have to initialize objects once they get bound to the app.
            if function.tag in self._running_app.function_ids:
                object_id: str = self._running_app.function_ids[function.tag]
                metadata: Message = self._running_app.object_handle_metadata[object_id]
                function._hydrate(object_id, self._client, metadata)

        self._functions[function.tag] = function
        if is_web_endpoint:
            self._web_endpoints.append(function.tag)

    def _add_class(self, tag: str, cls: _Cls):
        if self._running_app:
            # If this is inside a container, then objects can be defined after app initialization.
            # So we may have to initialize objects once they get bound to the app.
            if tag in self._running_app.class_ids:
                object_id: str = self._running_app.class_ids[tag]
                metadata: Message = self._running_app.object_handle_metadata[object_id]
                cls._hydrate(object_id, self._client, metadata)

        self._classes[tag] = cls

    def _init_container(self, client: _Client, running_app: RunningApp):
        self._app_id = running_app.app_id
        self._running_app = running_app
        self._client = client

        _App._container_app = self

        # Hydrate function objects
        for tag, object_id in running_app.function_ids.items():
            if tag in self._functions:
                obj = self._functions[tag]
                handle_metadata = running_app.object_handle_metadata[object_id]
                obj._hydrate(object_id, client, handle_metadata)

        # Hydrate class objects
        for tag, object_id in running_app.class_ids.items():
            if tag in self._classes:
                obj = self._classes[tag]
                handle_metadata = running_app.object_handle_metadata[object_id]
                obj._hydrate(object_id, client, handle_metadata)

    @property
    def registered_functions(self) -> dict[str, _Function]:
        """All modal.Function objects registered on the app.

        Note: this property is populated only during the build phase, and it is not
        expected to work when a deplyoed App has been retrieved via `modal.App.lookup`.
        """
        return self._functions

    @property
    def registered_classes(self) -> dict[str, _Cls]:
        """All modal.Cls objects registered on the app.

        Note: this property is populated only during the build phase, and it is not
        expected to work when a deplyoed App has been retrieved via `modal.App.lookup`.
        """
        return self._classes

    @property
    def registered_entrypoints(self) -> dict[str, _LocalEntrypoint]:
        """All local CLI entrypoints registered on the app.

        Note: this property is populated only during the build phase, and it is not
        expected to work when a deplyoed App has been retrieved via `modal.App.lookup`.
        """
        return self._local_entrypoints

    @property
    def registered_web_endpoints(self) -> list[str]:
        """Names of web endpoint (ie. webhook) functions registered on the app.

        Note: this property is populated only during the build phase, and it is not
        expected to work when a deplyoed App has been retrieved via `modal.App.lookup`.
        """
        return self._web_endpoints

    def local_entrypoint(
        self, _warn_parentheses_missing: Any = None, *, name: Optional[str] = None
    ) -> Callable[[Callable[..., Any]], _LocalEntrypoint]:
        """Decorate a function to be used as a CLI entrypoint for a Modal App.

        These functions can be used to define code that runs locally to set up the app,
        and act as an entrypoint to start Modal functions from. Note that regular
        Modal functions can also be used as CLI entrypoints, but unlike `local_entrypoint`,
        those functions are executed remotely directly.

        **Example**

        ```python
        @app.local_entrypoint()
        def main():
            some_modal_function.remote()
        ```

        You can call the function using `modal run` directly from the CLI:

        ```shell
        modal run app_module.py
        ```

        Note that an explicit [`app.run()`](https://modal.com/docs/reference/modal.App#run) is not needed, as an
        [app](https://modal.com/docs/guide/apps) is automatically created for you.

        **Multiple Entrypoints**

        If you have multiple `local_entrypoint` functions, you can qualify the name of your app and function:

        ```shell
        modal run app_module.py::app.some_other_function
        ```

        **Parsing Arguments**

        If your entrypoint function take arguments with primitive types, `modal run` automatically parses them as
        CLI options.
        For example, the following function can be called with `modal run app_module.py --foo 1 --bar "hello"`:

        ```python
        @app.local_entrypoint()
        def main(foo: int, bar: str):
            some_modal_function.call(foo, bar)
        ```

        Currently, `str`, `int`, `float`, `bool`, and `datetime.datetime` are supported.
        Use `modal run app_module.py --help` for more information on usage.

        """
        if _warn_parentheses_missing:
            raise InvalidError("Did you forget parentheses? Suggestion: `@app.local_entrypoint()`.")
        if name is not None and not isinstance(name, str):
            raise InvalidError("Invalid value for `name`: Must be string.")

        def wrapped(raw_f: Callable[..., Any]) -> _LocalEntrypoint:
            info = FunctionInfo(raw_f)
            tag = name if name is not None else raw_f.__qualname__
            if tag in self._local_entrypoints:
                # TODO: get rid of this limitation.
                raise InvalidError(f"Duplicate local entrypoint name: {tag}. Local entrypoint names must be unique.")
            entrypoint = self._local_entrypoints[tag] = _LocalEntrypoint(info, self)
            return entrypoint

        return wrapped

    @warn_on_renamed_autoscaler_settings
    def function(
        self,
        _warn_parentheses_missing=None,  # mdmd:line-hidden
        *,
        image: Optional[_Image] = None,  # The image to run as the container for the function
        schedule: Optional[Schedule] = None,  # An optional Modal Schedule for the function
        env: Optional[dict[str, Optional[str]]] = None,  # Environment variables to set in the container
        secrets: Optional[Collection[_Secret]] = None,  # Secrets to inject into the container as environment variables
        gpu: Union[
            GPU_T, list[GPU_T]
        ] = None,  # GPU request as string ("any", "T4", ...), object (`modal.GPU.A100()`, ...), or a list of either
        serialized: bool = False,  # Whether to send the function over using cloudpickle.
        network_file_systems: dict[
            Union[str, PurePosixPath], _NetworkFileSystem
        ] = {},  # Mountpoints for Modal NetworkFileSystems
        volumes: dict[
            Union[str, PurePosixPath], Union[_Volume, _CloudBucketMount]
        ] = {},  # Mount points for Modal Volumes & CloudBucketMounts
        # Specify, in fractional CPU cores, how many CPU cores to request.
        # Or, pass (request, limit) to additionally specify a hard limit in fractional CPU cores.
        # CPU throttling will prevent a container from exceeding its specified limit.
        cpu: Optional[Union[float, tuple[float, float]]] = None,
        # Specify, in MiB, a memory request which is the minimum memory required.
        # Or, pass (request, limit) to additionally specify a hard limit in MiB.
        memory: Optional[Union[int, tuple[int, int]]] = None,
        ephemeral_disk: Optional[int] = None,  # Specify, in MiB, the ephemeral disk size for the Function.
        min_containers: Optional[int] = None,  # Minimum number of containers to keep warm, even when Function is idle.
        max_containers: Optional[int] = None,  # Limit on the number of containers that can be concurrently running.
        buffer_containers: Optional[int] = None,  # Number of additional idle containers to maintain under active load.
        scaledown_window: Optional[int] = None,  # Max time (in seconds) a container can remain idle while scaling down.
        proxy: Optional[_Proxy] = None,  # Reference to a Modal Proxy to use in front of this function.
        retries: Optional[Union[int, Retries]] = None,  # Number of times to retry each input in case of failure.
        timeout: int = 300,  # Maximum execution time for inputs and startup time in seconds.
        startup_timeout: Optional[int] = None,  # Maximum startup time in seconds with higher precedence than `timeout`.
        name: Optional[str] = None,  # Sets the Modal name of the function within the app
        is_generator: Optional[
            bool
        ] = None,  # Set this to True if it's a non-generator function returning a [sync/async] generator object
        cloud: Optional[str] = None,  # Cloud provider to run the function on. Possible values are aws, gcp, oci, auto.
        region: Optional[Union[str, Sequence[str]]] = None,  # Region or regions to run the function on.
        enable_memory_snapshot: bool = False,  # Enable memory checkpointing for faster cold starts.
        block_network: bool = False,  # Whether to block network access
        restrict_modal_access: bool = False,  # Whether to allow this function access to other Modal resources
        # Maximum number of inputs a container should handle before shutting down.
        # With `max_inputs = 1`, containers will be single-use.
        max_inputs: Optional[int] = None,
        i6pn: Optional[bool] = None,  # Whether to enable IPv6 container networking within the region.
        # Whether the file or directory containing the Function's source should automatically be included
        # in the container. When unset, falls back to the App-level configuration, or is otherwise True by default.
        include_source: Optional[bool] = None,
        experimental_options: Optional[dict[str, Any]] = None,
        # Parameters below here are experimental. Use with caution!
        _experimental_scheduler_placement: Optional[
            SchedulerPlacement
        ] = None,  # Experimental controls over fine-grained scheduling (alpha).
        _experimental_proxy_ip: Optional[str] = None,  # IP address of proxy
        _experimental_custom_scaling_factor: Optional[float] = None,  # Custom scaling factor
        _experimental_restrict_output: bool = False,  # Don't use pickle for return values
        # Parameters below here are deprecated. Please update your code as suggested
        keep_warm: Optional[int] = None,  # Replaced with `min_containers`
        concurrency_limit: Optional[int] = None,  # Replaced with `max_containers`
        container_idle_timeout: Optional[int] = None,  # Replaced with `scaledown_window`
        allow_concurrent_inputs: Optional[int] = None,  # Replaced with the `@modal.concurrent` decorator
        _experimental_buffer_containers: Optional[int] = None,  # Now stable API with `buffer_containers`
    ) -> _FunctionDecoratorType:
        """Decorator to register a new Modal Function with this App."""
        if isinstance(_warn_parentheses_missing, _Image):
            # Handle edge case where maybe (?) some users passed image as a positional arg
            raise InvalidError("`image` needs to be a keyword argument: `@app.function(image=image)`.")
        if _warn_parentheses_missing:
            raise InvalidError("Did you forget parentheses? Suggestion: `@app.function()`.")

        if image is None:
            image = self._get_default_image()

        if allow_concurrent_inputs is not None:
            deprecation_warning(
                (2025, 4, 9),
                "The `allow_concurrent_inputs` parameter is deprecated."
                " Please use the `@modal.concurrent` decorator instead."
                "\n\nSee https://modal.com/docs/guide/modal-1-0-migration for more information.",
            )

        secrets = secrets or []
        if env:
            secrets = [*secrets, _Secret.from_dict(env)]
        secrets = [*self._secrets, *secrets]

        def wrapped(
            f: Union[_PartialFunction, Callable[..., Any], None],
        ) -> _Function:
            nonlocal is_generator, cloud, serialized

            # Check if the decorated object is a class
            if inspect.isclass(f):
                raise TypeError(
                    "The `@app.function` decorator cannot be used on a class. Please use `@app.cls` instead."
                )

            if isinstance(f, _PartialFunction):
                # typically for @function-wrapped @web_endpoint, @asgi_app, or @batched
                f.registered = True

                # but we don't support @app.function wrapping a method.
                if is_method_fn(f.raw_f.__qualname__):
                    raise InvalidError(
                        "The `@app.function` decorator cannot be used on class methods. "
                        "Swap with `@modal.method` or one of the web endpoint decorators. "
                        "Example: "
                        "\n\n"
                        "```python\n"
                        "@app.cls()\n"
                        "class MyClass:\n"
                        "    @modal.method()\n"
                        "    def f(self, x):\n"
                        "        ...\n"
                        "```\n"
                    )
                i6pn_enabled = i6pn or (f.flags & _PartialFunctionFlags.CLUSTERED)
                cluster_size = f.params.cluster_size  # Experimental: Clustered functions
                rdma = f.params.rdma

                info = FunctionInfo(f.raw_f, serialized=serialized, name_override=name)
                raw_f = f.raw_f
                webhook_config = f.params.webhook_config
                is_generator = f.params.is_generator
                batch_max_size = f.params.batch_max_size
                batch_wait_ms = f.params.batch_wait_ms
                if f.flags & _PartialFunctionFlags.CONCURRENT:
                    max_concurrent_inputs = f.params.max_concurrent_inputs
                    target_concurrent_inputs = f.params.target_concurrent_inputs
                else:
                    max_concurrent_inputs = allow_concurrent_inputs
                    target_concurrent_inputs = None
            else:
                if not is_global_object(f.__qualname__) and not serialized:
                    raise InvalidError(
                        dedent(
                            """
                            The `@app.function` decorator must apply to functions in global scope,
                            unless `serialized=True` is set.
                            If trying to apply additional decorators, they may need to use `functools.wraps`.
                            """
                        )
                    )

                if is_method_fn(f.__qualname__):
                    raise InvalidError(
                        dedent(
                            """
                            The `@app.function` decorator cannot be used on class methods.
                            Please use `@app.cls` with `@modal.method` instead. Example:

                            ```python
                            @app.cls()
                            class MyClass:
                                @modal.method()
                                def f(self, x):
                                    ...
                            ```
                            """
                        )
                    )

                info = FunctionInfo(f, serialized=serialized, name_override=name)
                raw_f = f
                webhook_config = None
                batch_max_size = None
                batch_wait_ms = None
                max_concurrent_inputs = allow_concurrent_inputs
                target_concurrent_inputs = None

                cluster_size = None  # Experimental: Clustered functions
                rdma = None
                i6pn_enabled = i6pn

            if is_generator is None:
                is_generator = inspect.isgeneratorfunction(raw_f) or inspect.isasyncgenfunction(raw_f)

            scheduler_placement: Optional[SchedulerPlacement] = _experimental_scheduler_placement
            if region:
                if scheduler_placement:
                    raise InvalidError("`region` and `_experimental_scheduler_placement` cannot be used together")
                scheduler_placement = SchedulerPlacement(region=region)

            function = _Function.from_local(
                info,
                app=self,
                image=image,
                secrets=secrets,
                schedule=schedule,
                is_generator=is_generator,
                gpu=gpu,
                network_file_systems=network_file_systems,
                volumes={**self._volumes, **volumes},
                cpu=cpu,
                memory=memory,
                ephemeral_disk=ephemeral_disk,
                proxy=proxy,
                retries=retries,
                min_containers=min_containers,
                max_containers=max_containers,
                buffer_containers=buffer_containers,
                scaledown_window=scaledown_window,
                max_concurrent_inputs=max_concurrent_inputs,
                target_concurrent_inputs=target_concurrent_inputs,
                batch_max_size=batch_max_size,
                batch_wait_ms=batch_wait_ms,
                timeout=timeout,
                startup_timeout=startup_timeout or timeout,
                cloud=cloud,
                webhook_config=webhook_config,
                enable_memory_snapshot=enable_memory_snapshot,
                block_network=block_network,
                restrict_modal_access=restrict_modal_access,
                max_inputs=max_inputs,
                scheduler_placement=scheduler_placement,
                i6pn_enabled=i6pn_enabled,
                cluster_size=cluster_size,  # Experimental: Clustered functions
                rdma=rdma,
                include_source=include_source if include_source is not None else self._include_source_default,
                experimental_options={k: str(v) for k, v in (experimental_options or {}).items()},
                _experimental_proxy_ip=_experimental_proxy_ip,
                restrict_output=_experimental_restrict_output,
            )

            self._add_function(function, webhook_config is not None)

            return function

        return wrapped

    @typing_extensions.dataclass_transform(field_specifiers=(parameter,), kw_only_default=True)
    @warn_on_renamed_autoscaler_settings
    def cls(
        self,
        _warn_parentheses_missing=None,  # mdmd:line-hidden
        *,
        image: Optional[_Image] = None,  # The image to run as the container for the function
        env: Optional[dict[str, Optional[str]]] = None,  # Environment variables to set in the container
        secrets: Optional[Collection[_Secret]] = None,  # Secrets to inject into the container as environment variables
        gpu: Union[
            GPU_T, list[GPU_T]
        ] = None,  # GPU request as string ("any", "T4", ...), object (`modal.GPU.A100()`, ...), or a list of either
        serialized: bool = False,  # Whether to send the function over using cloudpickle.
        network_file_systems: dict[
            Union[str, PurePosixPath], _NetworkFileSystem
        ] = {},  # Mountpoints for Modal NetworkFileSystems
        volumes: dict[
            Union[str, PurePosixPath], Union[_Volume, _CloudBucketMount]
        ] = {},  # Mount points for Modal Volumes & CloudBucketMounts
        # Specify, in fractional CPU cores, how many CPU cores to request.
        # Or, pass (request, limit) to additionally specify a hard limit in fractional CPU cores.
        # CPU throttling will prevent a container from exceeding its specified limit.
        cpu: Optional[Union[float, tuple[float, float]]] = None,
        # Specify, in MiB, a memory request which is the minimum memory required.
        # Or, pass (request, limit) to additionally specify a hard limit in MiB.
        memory: Optional[Union[int, tuple[int, int]]] = None,
        ephemeral_disk: Optional[int] = None,  # Specify, in MiB, the ephemeral disk size for the Function.
        min_containers: Optional[int] = None,  # Minimum number of containers to keep warm, even when Function is idle.
        max_containers: Optional[int] = None,  # Limit on the number of containers that can be concurrently running.
        buffer_containers: Optional[int] = None,  # Number of additional idle containers to maintain under active load.
        scaledown_window: Optional[int] = None,  # Max time (in seconds) a container can remain idle while scaling down.
        proxy: Optional[_Proxy] = None,  # Reference to a Modal Proxy to use in front of this function.
        retries: Optional[Union[int, Retries]] = None,  # Number of times to retry each input in case of failure.
        timeout: int = 300,  # Maximum execution time for inputs and startup time in seconds.
        startup_timeout: Optional[int] = None,  # Maximum startup time in seconds with higher precedence than `timeout`.
        cloud: Optional[str] = None,  # Cloud provider to run the function on. Possible values are aws, gcp, oci, auto.
        region: Optional[Union[str, Sequence[str]]] = None,  # Region or regions to run the function on.
        enable_memory_snapshot: bool = False,  # Enable memory checkpointing for faster cold starts.
        block_network: bool = False,  # Whether to block network access
        restrict_modal_access: bool = False,  # Whether to allow this class access to other Modal resources
        # Limits the number of inputs a container handles before shutting down.
        # Use `max_inputs = 1` for single-use containers.
        max_inputs: Optional[int] = None,
        i6pn: Optional[bool] = None,  # Whether to enable IPv6 container networking within the region.
        include_source: Optional[bool] = None,  # When `False`, don't automatically add the App source to the container.
        experimental_options: Optional[dict[str, Any]] = None,
        # Parameters below here are experimental. Use with caution!
        _experimental_scheduler_placement: Optional[
            SchedulerPlacement
        ] = None,  # Experimental controls over fine-grained scheduling (alpha).
        _experimental_proxy_ip: Optional[str] = None,  # IP address of proxy
        _experimental_custom_scaling_factor: Optional[float] = None,  # Custom scaling factor
        _experimental_restrict_output: bool = False,  # Don't use pickle for return values
        # Parameters below here are deprecated. Please update your code as suggested
        keep_warm: Optional[int] = None,  # Replaced with `min_containers`
        concurrency_limit: Optional[int] = None,  # Replaced with `max_containers`
        container_idle_timeout: Optional[int] = None,  # Replaced with `scaledown_window`
        allow_concurrent_inputs: Optional[int] = None,  # Replaced with the `@modal.concurrent` decorator
        _experimental_buffer_containers: Optional[int] = None,  # Now stable API with `buffer_containers`
    ) -> Callable[[Union[CLS_T, _PartialFunction]], CLS_T]:
        """
        Decorator to register a new Modal [Cls](https://modal.com/docs/reference/modal.Cls) with this App.
        """
        if _warn_parentheses_missing:
            raise InvalidError("Did you forget parentheses? Suggestion: `@app.cls()`.")

        scheduler_placement = _experimental_scheduler_placement
        if region:
            if scheduler_placement:
                raise InvalidError("`region` and `_experimental_scheduler_placement` cannot be used together")
            scheduler_placement = SchedulerPlacement(region=region)

        if allow_concurrent_inputs is not None:
            deprecation_warning(
                (2025, 4, 9),
                "The `allow_concurrent_inputs` parameter is deprecated."
                " Please use the `@modal.concurrent` decorator instead."
                "\n\nSee https://modal.com/docs/guide/modal-1-0-migration for more information.",
            )

        secrets = secrets or []
        if env:
            secrets = [*secrets, _Secret.from_dict(env)]

        def wrapper(wrapped_cls: Union[CLS_T, _PartialFunction]) -> CLS_T:
            # Check if the decorated object is a class
            if isinstance(wrapped_cls, _PartialFunction):
                wrapped_cls.registered = True
                user_cls = wrapped_cls.user_cls
                if wrapped_cls.flags & _PartialFunctionFlags.CONCURRENT:
                    max_concurrent_inputs = wrapped_cls.params.max_concurrent_inputs
                    target_concurrent_inputs = wrapped_cls.params.target_concurrent_inputs
                else:
                    max_concurrent_inputs = allow_concurrent_inputs
                    target_concurrent_inputs = None

                if wrapped_cls.flags & _PartialFunctionFlags.CLUSTERED:
                    cluster_size = wrapped_cls.params.cluster_size
                    rdma = wrapped_cls.params.rdma
                else:
                    cluster_size = None
                    rdma = None
            else:
                user_cls = wrapped_cls
                max_concurrent_inputs = allow_concurrent_inputs
                target_concurrent_inputs = None
                cluster_size = None
                rdma = None
            if not inspect.isclass(user_cls):
                raise TypeError("The @app.cls decorator must be used on a class.")

            interface_methods = _find_partial_methods_for_user_cls(user_cls, _PartialFunctionFlags.interface_flags())
            if cluster_size:
                if len(interface_methods) > 1:
                    raise InvalidError(f"Modal class {user_cls.__name__} cannot have multiple methods when clustered.")

            batch_functions = _find_partial_methods_for_user_cls(user_cls, _PartialFunctionFlags.BATCHED)
            if batch_functions:
                if len(batch_functions) > 1:
                    raise InvalidError(f"Modal class {user_cls.__name__} can only have one batched function.")
                if len(_find_partial_methods_for_user_cls(user_cls, _PartialFunctionFlags.interface_flags())) > 1:
                    raise InvalidError(
                        f"Modal class {user_cls.__name__} with a modal batched function cannot have other modal methods."  # noqa
                    )
                batch_function = next(iter(batch_functions.values()))
                batch_max_size = batch_function.params.batch_max_size
                batch_wait_ms = batch_function.params.batch_wait_ms
            else:
                batch_max_size = None
                batch_wait_ms = None

            if (
                _find_partial_methods_for_user_cls(user_cls, _PartialFunctionFlags.ENTER_PRE_SNAPSHOT)
                and not enable_memory_snapshot
            ):
                raise InvalidError("A class must have `enable_memory_snapshot=True` to use `snap=True` on its methods.")

            for method in _find_partial_methods_for_user_cls(user_cls, _PartialFunctionFlags.CONCURRENT).values():
                method.registered = True  # Avoid warning about not registering the method (hacky!)
                raise InvalidError(
                    "The `@modal.concurrent` decorator cannot be used on methods; decorate the class instead."
                )

            info = FunctionInfo(None, serialized=serialized, user_cls=user_cls)

            i6pn_enabled = i6pn or cluster_size is not None

            cls_func = _Function.from_local(
                info,
                app=self,
                image=image or self._get_default_image(),
                secrets=[*self._secrets, *secrets],
                gpu=gpu,
                network_file_systems=network_file_systems,
                volumes={**self._volumes, **volumes},
                cpu=cpu,
                memory=memory,
                ephemeral_disk=ephemeral_disk,
                min_containers=min_containers,
                max_containers=max_containers,
                buffer_containers=buffer_containers,
                scaledown_window=scaledown_window,
                proxy=proxy,
                retries=retries,
                max_concurrent_inputs=max_concurrent_inputs,
                target_concurrent_inputs=target_concurrent_inputs,
                batch_max_size=batch_max_size,
                batch_wait_ms=batch_wait_ms,
                timeout=timeout,
                startup_timeout=startup_timeout or timeout,
                cloud=cloud,
                enable_memory_snapshot=enable_memory_snapshot,
                block_network=block_network,
                restrict_modal_access=restrict_modal_access,
                max_inputs=max_inputs,
                scheduler_placement=scheduler_placement,
                i6pn_enabled=i6pn_enabled,
                cluster_size=cluster_size,
                rdma=rdma,
                include_source=include_source if include_source is not None else self._include_source_default,
                experimental_options={k: str(v) for k, v in (experimental_options or {}).items()},
                _experimental_proxy_ip=_experimental_proxy_ip,
                _experimental_custom_scaling_factor=_experimental_custom_scaling_factor,
                restrict_output=_experimental_restrict_output,
            )

            self._add_function(cls_func, is_web_endpoint=False)

            cls: _Cls = _Cls.from_local(user_cls, self, cls_func)

            tag: str = user_cls.__name__
            self._add_class(tag, cls)
            return cls  # type: ignore  # a _Cls instance "simulates" being the user provided class

        return wrapper

    def include(self, /, other_app: "_App", inherit_tags: bool = True) -> typing_extensions.Self:
        """Include another App's objects in this one.

        Useful for splitting up Modal Apps across different self-contained files.

        ```python
        app_a = modal.App("a")
        @app.function()
        def foo():
            ...

        app_b = modal.App("b")
        @app.function()
        def bar():
            ...

        app_a.include(app_b)

        @app_a.local_entrypoint()
        def main():
            # use function declared on the included app
            bar.remote()
        ```

        When `inherit_tags=True` any tags set on the other App will be inherited by this App
        (with this App's tags taking precedence in the case of conflicts).

        """
        for tag, function in other_app._functions.items():
            self._add_function(function, False)  # TODO(erikbern): webhook config?

        for tag, cls in other_app._classes.items():
            existing_cls = self._classes.get(tag)
            if existing_cls and existing_cls != cls:
                logger.warning(
                    f"Named app class {tag} with existing value {existing_cls} is being "
                    f"overwritten by a different class {cls}"
                )

            self._add_class(tag, cls)

        if inherit_tags:
            self._tags = {**other_app._tags, **self._tags}

        return self

    async def _logs(self, client: Optional[_Client] = None) -> AsyncGenerator[str, None]:
        """Stream logs from the app.

        This method is considered private and its interface may change - use at your own risk!
        """
        if not self._app_id:
            raise InvalidError("`app._logs` requires a running/stopped app.")

        client = client or self._client or await _Client.from_env()

        last_log_batch_entry_id: Optional[str] = None
        while True:
            request = api_pb2.AppGetLogsRequest(
                app_id=self._app_id,
                timeout=55,
                last_entry_id=last_log_batch_entry_id,
            )
            async for log_batch in client.stub.AppGetLogs.unary_stream(request):
                if log_batch.entry_id:
                    # log_batch entry_id is empty for fd="server" messages from AppGetLogs
                    last_log_batch_entry_id = log_batch.entry_id
                if log_batch.app_done:
                    return
                for log in log_batch.items:
                    if log.data:
                        yield log.data

    @classmethod
    def _get_container_app(cls) -> Optional["_App"]:
        """Returns the `App` running inside a container.

        This will return `None` outside of a Modal container."""
        return cls._container_app

    @classmethod
    def _reset_container_app(cls):
        """Only used for tests."""
        cls._container_app = None


App = synchronize_api(_App)



================================================
FILE: modal/call_graph.py
================================================
# Copyright Modal Labs 2022
from dataclasses import dataclass
from enum import IntEnum
from typing import Optional

from modal_proto import api_pb2


class InputStatus(IntEnum):
    """Enum representing status of a function input."""

    PENDING = 0
    SUCCESS = api_pb2.GenericResult.GENERIC_STATUS_SUCCESS
    FAILURE = api_pb2.GenericResult.GENERIC_STATUS_FAILURE
    INIT_FAILURE = api_pb2.GenericResult.GENERIC_STATUS_INIT_FAILURE
    TERMINATED = api_pb2.GenericResult.GENERIC_STATUS_TERMINATED
    TIMEOUT = api_pb2.GenericResult.GENERIC_STATUS_TIMEOUT

    @classmethod
    def _missing_(cls, value):
        return cls.PENDING


@dataclass
class InputInfo:
    """Simple data structure storing information about a function input."""

    input_id: str
    function_call_id: str
    task_id: str
    status: InputStatus
    function_name: str
    module_name: str
    children: list["InputInfo"]


def _reconstruct_call_graph(ser_graph: api_pb2.FunctionGetCallGraphResponse) -> list[InputInfo]:
    function_calls_by_id: dict[str, api_pb2.FunctionCallCallGraphInfo] = {}
    inputs_by_id: dict[str, api_pb2.InputCallGraphInfo] = {}

    for function_call in ser_graph.function_calls:
        function_calls_by_id[function_call.function_call_id] = function_call

    for input in ser_graph.inputs:
        inputs_by_id[input.input_id] = input

    input_info_by_id: dict[str, InputInfo] = {}
    result = []

    def _reconstruct(input_id: str) -> Optional[InputInfo]:
        if input_id in input_info_by_id:
            return input_info_by_id[input_id]

        # Input info can be missing, because input retention is limited.
        if input_id not in inputs_by_id:
            return None

        input = inputs_by_id[input_id]
        function_call = function_calls_by_id[input.function_call_id]
        input_info_by_id[input_id] = InputInfo(
            input_id,
            input.function_call_id,
            input.task_id,
            InputStatus(input.status),
            function_call.function_name,
            function_call.module_name,
            [],
        )

        if function_call.parent_input_id:
            # Find parent and append to list of children.
            parent = _reconstruct(function_call.parent_input_id)
            if parent:
                parent.children.append(input_info_by_id[input_id])
        else:
            # Top-level input.
            result.append(input_info_by_id[input_id])

        return input_info_by_id[input_id]

    for input_id in inputs_by_id.keys():
        _reconstruct(input_id)

    return result



================================================
FILE: modal/client.py
================================================
# Copyright Modal Labs 2022
import asyncio
import os
import platform
import sys
import urllib.parse
import warnings
from collections.abc import AsyncGenerator, AsyncIterator, Collection, Mapping
from typing import (
    Any,
    ClassVar,
    Generic,
    Optional,
    TypeVar,
    Union,
)

import grpclib.client
from google.protobuf import empty_pb2
from google.protobuf.message import Message
from grpclib import GRPCError, Status
from synchronicity.async_wrap import asynccontextmanager

from modal._utils.async_utils import synchronizer
from modal_proto import api_grpc, api_pb2, modal_api_grpc
from modal_version import __version__

from ._traceback import print_server_warnings, suppress_tb_frames
from ._utils import async_utils
from ._utils.async_utils import TaskContext, synchronize_api
from ._utils.auth_token_manager import _AuthTokenManager
from ._utils.grpc_utils import ConnectionManager, retry_transient_errors
from .config import _check_config, _is_remote, config, logger
from .exception import AuthError, ClientClosed, NotFoundError

HEARTBEAT_INTERVAL: float = config.get("heartbeat_interval")
HEARTBEAT_TIMEOUT: float = HEARTBEAT_INTERVAL + 0.1


def _get_metadata(client_type: int, credentials: Optional[tuple[str, str]], version: str) -> dict[str, str]:
    # This implements a simplified version of platform.platform() that's still machine-readable
    uname: platform.uname_result = platform.uname()
    if uname.system == "Darwin":
        system, release = "macOS", platform.mac_ver()[0]
    else:
        system, release = uname.system, uname.release
    platform_str = "-".join(s.replace("-", "_") for s in (system, release, uname.machine))

    # sys.version_info is structured unlike sys.version or platform.python_version()
    python_version = "%d.%d.%d" % (sys.version_info.major, sys.version_info.minor, sys.version_info.micro)

    metadata = {
        "x-modal-client-version": version,
        "x-modal-client-type": str(client_type),
        "x-modal-python-version": python_version,
        "x-modal-node": urllib.parse.quote(platform.node()),
        "x-modal-platform": urllib.parse.quote(platform_str),
    }
    if credentials and client_type == api_pb2.CLIENT_TYPE_CLIENT:
        token_id, token_secret = credentials
        metadata.update(
            {
                "x-modal-token-id": token_id,
                "x-modal-token-secret": token_secret,
            }
        )
    return metadata


ReturnType = TypeVar("ReturnType")
_Value = Union[str, bytes]
_MetadataLike = Union[Mapping[str, _Value], Collection[tuple[str, _Value]]]
RequestType = TypeVar("RequestType", bound=Message)
ResponseType = TypeVar("ResponseType", bound=Message)


class _Client:
    _client_from_env: ClassVar[Optional["_Client"]] = None
    _client_from_env_lock: ClassVar[Optional[asyncio.Lock]] = None
    _cancellation_context: TaskContext
    _cancellation_context_event_loop: asyncio.AbstractEventLoop = None
    _stub: Optional[api_grpc.ModalClientStub]
    _auth_token_manager: _AuthTokenManager = None
    _snapshotted: bool

    def __init__(
        self,
        server_url: str,
        client_type: int,
        credentials: Optional[tuple[str, str]],
        version: str = __version__,
    ):
        """mdmd:hidden
        The Modal client object is not intended to be instantiated directly by users.
        """
        self.server_url = server_url
        self.client_type = client_type
        self._credentials = credentials
        self.version = version
        self._closed = False
        self._stub: Optional[modal_api_grpc.ModalClientModal] = None
        self._auth_token_manager: Optional[_AuthTokenManager] = None
        self._snapshotted = False
        self._owner_pid = None

    def is_closed(self) -> bool:
        return self._closed

    @property
    def stub(self) -> modal_api_grpc.ModalClientModal:
        """mdmd:hidden
        The default stub. Stubs can safely be used across forks / client snapshots.

        This is useful if you want to make requests to the default Modal server in us-east, for example
        control plane requests.

        This is equivalent to client.get_stub(default_server_url), but it's cached, so it's a bit faster.
        """
        assert self._stub
        return self._stub

    async def get_stub(self, server_url: str) -> modal_api_grpc.ModalClientModal:
        """mdmd:hidden
        Get a stub for a specific server URL. Stubs can safely be used across forks / client snapshots.

        This is useful if you want to make requests to a regional Modal server, for example low-latency
        function calls in us-west.

        This function is O(n) where n is the number of RPCs in ModalClient.
        """
        return await modal_api_grpc.ModalClientModal._create(self, server_url)

    async def _open(self):
        self._closed = False
        assert self._stub is None
        metadata = _get_metadata(self.client_type, self._credentials, self.version)
        self._cancellation_context = TaskContext(grace=0.5)  # allow running rpcs to finish in 0.5s when closing client
        self._cancellation_context_event_loop = asyncio.get_running_loop()
        await self._cancellation_context.__aenter__()
        self._connection_manager = ConnectionManager(client=self, metadata=metadata)
        self._stub = await self.get_stub(self.server_url)
        self._auth_token_manager = _AuthTokenManager(self.stub)
        self._owner_pid = os.getpid()

    async def _close(self, prep_for_restore: bool = False):
        logger.debug(f"Client ({id(self)}): closing")
        self._closed = True
        if hasattr(self, "_cancellation_context"):
            await self._cancellation_context.__aexit__(None, None, None)  # wait for all rpcs to be finished/cancelled
        if self._connection_manager:
            self._connection_manager.close()

        if prep_for_restore:
            self._snapshotted = True

        # Remove cached client.
        self.set_env_client(None)

    async def hello(self):
        """Connect to server and retrieve version information; raise appropriate error for various failures."""
        logger.debug(f"Client ({id(self)}): Starting")
        resp = await retry_transient_errors(self.stub.ClientHello, empty_pb2.Empty())
        print_server_warnings(resp.server_warnings)

    async def __aenter__(self):
        await self._open()
        return self

    async def __aexit__(self, exc_type, exc, tb):
        await self._close()

    @classmethod
    @asynccontextmanager
    async def anonymous(cls, server_url: str) -> AsyncIterator["_Client"]:
        """mdmd:hidden
        Create a connection with no credentials; to be used for token creation.
        """
        logger.debug("Client: Starting client without authentication")
        client = cls(server_url, api_pb2.CLIENT_TYPE_CLIENT, credentials=None)
        try:
            await client._open()
            yield client
        finally:
            await client._close()

    @classmethod
    async def from_env(cls, _override_config=None) -> "_Client":
        """mdmd:hidden
        Singleton that is instantiated from the Modal config and reused on subsequent calls.
        """
        _check_config()

        if _override_config:
            # Only used for testing
            c = _override_config
        else:
            c = config

        credentials: Optional[tuple[str, str]]

        if cls._client_from_env_lock is None:
            cls._client_from_env_lock = asyncio.Lock()

        async with cls._client_from_env_lock:
            if cls._client_from_env:
                return cls._client_from_env

            token_id = c["token_id"]
            token_secret = c["token_secret"]
            if _is_remote():
                if token_id or token_secret:
                    warnings.warn(
                        "Modal tokens provided by MODAL_TOKEN_ID and MODAL_TOKEN_SECRET"
                        " (or through the config file) are ignored inside containers."
                    )
                client_type = api_pb2.CLIENT_TYPE_CONTAINER
                credentials = None
            elif token_id and token_secret:
                client_type = api_pb2.CLIENT_TYPE_CLIENT
                credentials = (token_id, token_secret)
            else:
                raise AuthError(
                    "Token missing. Could not authenticate client."
                    " If you have token credentials, see modal.com/docs/reference/modal.config for setup help."
                    " If you are a new user, register an account at modal.com, then run `modal token new`."
                )

            server_url = c["server_url"]
            client = _Client(server_url, client_type, credentials)
            await client._open()
            async_utils.on_shutdown(client._close())
            cls._client_from_env = client
            return client

    @classmethod
    async def from_credentials(cls, token_id: str, token_secret: str) -> "_Client":
        """
        Constructor based on token credentials; useful for managing Modal on behalf of third-party users.

        **Usage:**

        ```python notest
        client = modal.Client.from_credentials("my_token_id", "my_token_secret")

        modal.Sandbox.create("echo", "hi", client=client, app=app)
        ```
        """
        _check_config()
        server_url = config["server_url"]
        client_type = api_pb2.CLIENT_TYPE_CLIENT
        credentials = (token_id, token_secret)
        client = _Client(server_url, client_type, credentials)
        await client._open()
        async_utils.on_shutdown(client._close())
        return client

    @classmethod
    async def verify(cls, server_url: str, credentials: tuple[str, str]) -> None:
        """mdmd:hidden
        Check whether can the client can connect to this server with these credentials; raise if not.
        """
        async with cls(server_url, api_pb2.CLIENT_TYPE_CLIENT, credentials) as client:
            await client.hello()  # Will call ClientHello RPC and possibly raise AuthError or ConnectionError

    @classmethod
    def set_env_client(cls, client: Optional["_Client"]):
        """mdmd:hidden"""
        # Just used from tests.
        cls._client_from_env = client

    async def get_input_plane_metadata(self, input_plane_region: str) -> list[tuple[str, str]]:
        assert self._auth_token_manager, "Client must have an instance of auth token manager."
        token = await self._auth_token_manager.get_token()
        return [
            ("x-modal-input-plane-region", input_plane_region),
            ("x-modal-auth-token", token),
        ]

    async def _call_safely(self, coro, readable_method: str):
        """Runs coroutine wrapped in a task that's part of the client's task context

        * Raises ClientClosed in case the client is closed while the coroutine is executed
        * Logs warning if call is made outside of the event loop that the client is running in,
          and execute without the cancellation context in that case
        """

        if self.is_closed():
            coro.close()  # prevent "was never awaited"
            raise ClientClosed(id(self))

        current_event_loop = asyncio.get_running_loop()
        if current_event_loop == self._cancellation_context_event_loop:
            # make request cancellable if we are in the same event loop as the rpc context
            # this should usually be the case!
            try:
                request_task = self._cancellation_context.create_task(coro)
                request_task.set_name(readable_method)
                return await request_task
            except asyncio.CancelledError:
                if self.is_closed():
                    raise ClientClosed(id(self)) from None
                raise  # if the task is cancelled as part of synchronizer shutdown or similar, don't raise ClientClosed
        else:
            # this should be rare - mostly used in tests where rpc requests sometimes are triggered
            # outside of a client context/synchronicity loop
            logger.warning(f"RPC request to {readable_method} made outside of task context")
            return await coro

    async def _reset_on_pid_change(self):
        if self._owner_pid and self._owner_pid != os.getpid():
            # not calling .close() since that would also interact with stale resources
            # just reset the internal state
            self._connection_manager = None
            self._stub = None
            self._owner_pid = None

            self.set_env_client(None)
            # TODO(elias): reset _cancellation_context in case ?
            await self._open()

    async def _get_channel(self, server_url: str) -> grpclib.client.Channel:
        # Get a valid grpclib channel, reusing existing channels if possible.
        # This prevents usage of stale channels across forks of processes.
        await self._reset_on_pid_change()
        return await self._connection_manager.get_or_create_channel(server_url)

    @synchronizer.nowrap
    async def _call_unary(
        self,
        grpclib_method: grpclib.client.UnaryUnaryMethod[RequestType, ResponseType],
        request: Any,
        *,
        timeout: Optional[float] = None,
        metadata: Optional[_MetadataLike] = None,
    ) -> Any:
        coro = grpclib_method(request, timeout=timeout, metadata=metadata)
        return await self._call_safely(coro, grpclib_method.name)

    @synchronizer.nowrap
    async def _call_stream(
        self,
        grpclib_method: grpclib.client.UnaryStreamMethod[RequestType, ResponseType],
        request: Any,
        *,
        metadata: Optional[_MetadataLike],
    ) -> AsyncGenerator[Any, None]:
        stream_context = grpclib_method.open(metadata=metadata)
        stream = await self._call_safely(stream_context.__aenter__(), f"{grpclib_method.name}.open")
        try:
            await self._call_safely(stream.send_message(request, end=True), f"{grpclib_method.name}.send_message")
            while 1:
                try:
                    yield await self._call_safely(stream.__anext__(), f"{grpclib_method.name}.recv")
                except StopAsyncIteration:
                    break
        except BaseException as exc:
            did_handle_exception = await stream_context.__aexit__(type(exc), exc, exc.__traceback__)
            if not did_handle_exception:
                raise
        else:
            await stream_context.__aexit__(None, None, None)


Client = synchronize_api(_Client)


class grpc_error_converter:
    def __enter__(self):
        pass

    def __exit__(self, exc_type, exc, traceback) -> bool:
        # skip all internal frames from grpclib
        use_full_traceback = config.get("traceback")
        with suppress_tb_frames(1):
            if isinstance(exc, GRPCError):
                if exc.status == Status.NOT_FOUND:
                    if use_full_traceback:
                        raise NotFoundError(exc.message)
                    else:
                        raise NotFoundError(exc.message) from None  # from None to skip the grpc-internal cause

                if not use_full_traceback:
                    # just include the frame in grpclib that actually raises the GRPCError
                    tb = exc.__traceback__
                    while tb.tb_next:
                        tb = tb.tb_next
                    exc.with_traceback(tb)
                    raise exc from None  # from None to skip the grpc-internal cause
                raise exc

        return False


class UnaryUnaryWrapper(Generic[RequestType, ResponseType]):
    # Calls a grpclib.UnaryUnaryMethod using a specific Client instance, respecting
    # if that client is closed etc. and possibly introducing Modal-specific retry logic
    wrapped_method: grpclib.client.UnaryUnaryMethod[RequestType, ResponseType]
    client: _Client

    def __init__(
        self,
        wrapped_method: grpclib.client.UnaryUnaryMethod[RequestType, ResponseType],
        client: _Client,
        server_url: str,
    ):
        self.wrapped_method = wrapped_method
        self.client = client
        self.server_url = server_url

    @property
    def name(self) -> str:
        return self.wrapped_method.name

    async def __call__(
        self,
        req: RequestType,
        *,
        timeout: Optional[float] = None,
        metadata: Optional[_MetadataLike] = None,
    ) -> ResponseType:
        if self.client._snapshotted:
            logger.debug(f"refreshing client after snapshot for {self.name.rsplit('/', 1)[1]}")
            self.client = await _Client.from_env()

        # Note: We override the grpclib method's channel (see grpclib's code [1]). I think this is fine
        # since grpclib's code doesn't seem to change very much, but we could also recreate the
        # grpclib stub if we aren't comfortable with this. The downside is then we need to cache
        # the grpclib stub so the rest of our code becomes a bit more complicated.
        #
        # We need to override the channel because after the process is forked or the client is
        # snapshotted, the existing channel may be stale / unusable.
        #
        # [1]: https://github.com/vmagamedov/grpclib/blob/62f968a4c84e3f64e6966097574ff0a59969ea9b/grpclib/client.py#L844
        self.wrapped_method.channel = await self.client._get_channel(self.server_url)
        with suppress_tb_frames(1), grpc_error_converter():
            return await self.client._call_unary(self.wrapped_method, req, timeout=timeout, metadata=metadata)


class UnaryStreamWrapper(Generic[RequestType, ResponseType]):
    wrapped_method: grpclib.client.UnaryStreamMethod[RequestType, ResponseType]

    def __init__(
        self,
        wrapped_method: grpclib.client.UnaryStreamMethod[RequestType, ResponseType],
        client: _Client,
        server_url: str,
    ):
        self.wrapped_method = wrapped_method
        self.client = client
        self.server_url = server_url

    @property
    def name(self) -> str:
        return self.wrapped_method.name

    async def unary_stream(
        self,
        request,
        metadata: Optional[Any] = None,
    ):
        if self.client._snapshotted:
            logger.debug(f"refreshing client after snapshot for {self.name.rsplit('/', 1)[1]}")
            self.client = await _Client.from_env()
        self.wrapped_method.channel = await self.client._get_channel(self.server_url)
        async for response in self.client._call_stream(self.wrapped_method, request, metadata=metadata):
            yield response



================================================
FILE: modal/cloud_bucket_mount.py
================================================
# Copyright Modal Labs 2022
from dataclasses import dataclass
from typing import Optional, Sequence
from urllib.parse import urlparse

from modal_proto import api_pb2

from ._utils.async_utils import synchronize_api
from .config import logger
from .secret import _Secret


@dataclass
class _CloudBucketMount:
    """Mounts a cloud bucket to your container. Currently supports AWS S3 buckets.

    S3 buckets are mounted using [AWS S3 Mountpoint](https://github.com/awslabs/mountpoint-s3).
    S3 mounts are optimized for reading large files sequentially. It does not support every file operation; consult
    [the AWS S3 Mountpoint documentation](https://github.com/awslabs/mountpoint-s3/blob/main/doc/SEMANTICS.md)
    for more information.

    **AWS S3 Usage**

    ```python
    import subprocess

    app = modal.App()
    secret = modal.Secret.from_name(
        "aws-secret",
        required_keys=["AWS_ACCESS_KEY_ID", "AWS_SECRET_ACCESS_KEY"]
        # Note: providing AWS_REGION can help when automatic detection of the bucket region fails.
    )

    @app.function(
        volumes={
            "/my-mount": modal.CloudBucketMount(
                bucket_name="s3-bucket-name",
                secret=secret,
                read_only=True
            )
        }
    )
    def f():
        subprocess.run(["ls", "/my-mount"], check=True)
    ```

    **Cloudflare R2 Usage**

    Cloudflare R2 is [S3-compatible](https://developers.cloudflare.com/r2/api/s3/api/) so its setup looks
    very similar to S3. But additionally the `bucket_endpoint_url` argument must be passed.

    ```python
    import subprocess

    app = modal.App()
    secret = modal.Secret.from_name(
        "r2-secret",
        required_keys=["AWS_ACCESS_KEY_ID", "AWS_SECRET_ACCESS_KEY"]
    )

    @app.function(
        volumes={
            "/my-mount": modal.CloudBucketMount(
                bucket_name="my-r2-bucket",
                bucket_endpoint_url="https://<ACCOUNT ID>.r2.cloudflarestorage.com",
                secret=secret,
                read_only=True
            )
        }
    )
    def f():
        subprocess.run(["ls", "/my-mount"], check=True)
    ```

    **Google GCS Usage**

    Google Cloud Storage (GCS) is [S3-compatible](https://cloud.google.com/storage/docs/interoperability).
    GCS Buckets also require a secret with Google-specific key names (see below) populated with
    a [HMAC key](https://cloud.google.com/storage/docs/authentication/managing-hmackeys#create).

    ```python
    import subprocess

    app = modal.App()
    gcp_hmac_secret = modal.Secret.from_name(
        "gcp-secret",
        required_keys=["GOOGLE_ACCESS_KEY_ID", "GOOGLE_ACCESS_KEY_SECRET"]
    )

    @app.function(
        volumes={
            "/my-mount": modal.CloudBucketMount(
                bucket_name="my-gcs-bucket",
                bucket_endpoint_url="https://storage.googleapis.com",
                secret=gcp_hmac_secret,
            )
        }
    )
    def f():
        subprocess.run(["ls", "/my-mount"], check=True)
    ```
    """

    bucket_name: str
    # Endpoint URL is used to support Cloudflare R2 and Google Cloud Platform GCS.
    bucket_endpoint_url: Optional[str] = None

    key_prefix: Optional[str] = None

    # Credentials used to access a cloud bucket.
    # If the bucket is private, the secret **must** contain AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY.
    # If the bucket is publicly accessible, the secret is unnecessary and can be omitted.
    secret: Optional[_Secret] = None

    # Role ARN used for using OIDC authentication to access a cloud bucket.
    oidc_auth_role_arn: Optional[str] = None

    read_only: bool = False
    requester_pays: bool = False


def cloud_bucket_mounts_to_proto(mounts: Sequence[tuple[str, _CloudBucketMount]]) -> list[api_pb2.CloudBucketMount]:
    """Helper function to convert `CloudBucketMount` to a list of protobufs that can be passed to the server."""
    cloud_bucket_mounts: list[api_pb2.CloudBucketMount] = []

    for path, mount in mounts:
        # crude mapping from mount arguments to type.
        if mount.bucket_endpoint_url:
            parse_result = urlparse(mount.bucket_endpoint_url)
            if parse_result.hostname.endswith("r2.cloudflarestorage.com"):
                bucket_type = api_pb2.CloudBucketMount.BucketType.R2
            elif parse_result.hostname.endswith("storage.googleapis.com"):
                bucket_type = api_pb2.CloudBucketMount.BucketType.GCP
            else:
                logger.warning(
                    "CloudBucketMount received unrecognized bucket endpoint URL. "
                    "Assuming AWS S3 configuration as fallback."
                )
                bucket_type = api_pb2.CloudBucketMount.BucketType.S3
        else:
            # just assume S3; this is backwards and forwards compatible.
            bucket_type = api_pb2.CloudBucketMount.BucketType.S3

        if mount.requester_pays and not mount.secret:
            raise ValueError("Credentials required in order to use Requester Pays.")

        if mount.key_prefix and not mount.key_prefix.endswith("/"):
            raise ValueError("key_prefix will be prefixed to all object paths, so it must end in a '/'")
        else:
            key_prefix = mount.key_prefix

        cloud_bucket_mount = api_pb2.CloudBucketMount(
            bucket_name=mount.bucket_name,
            bucket_endpoint_url=mount.bucket_endpoint_url,
            mount_path=path,
            credentials_secret_id=mount.secret.object_id if mount.secret else "",
            read_only=mount.read_only,
            bucket_type=bucket_type,
            requester_pays=mount.requester_pays,
            key_prefix=key_prefix,
            oidc_auth_role_arn=mount.oidc_auth_role_arn,
        )
        cloud_bucket_mounts.append(cloud_bucket_mount)

    return cloud_bucket_mounts


CloudBucketMount = synchronize_api(_CloudBucketMount)



================================================
FILE: modal/cls.py
================================================
# Copyright Modal Labs 2022
import dataclasses
import inspect
import typing
from collections.abc import Collection
from pathlib import PurePosixPath
from typing import Any, Callable, Optional, Sequence, TypeVar, Union

from google.protobuf.message import Message
from grpclib import GRPCError, Status

from modal_proto import api_pb2

from ._functions import _Function, _parse_retries
from ._object import _Object, live_method
from ._partial_function import (
    _find_callables_for_obj,
    _find_partial_methods_for_user_cls,
    _PartialFunction,
    _PartialFunctionFlags,
)
from ._resolver import Resolver
from ._resources import convert_fn_config_to_resources_config
from ._serialization import check_valid_cls_constructor_arg
from ._traceback import print_server_warnings
from ._type_manager import parameter_serde_registry
from ._utils.async_utils import synchronize_api, synchronizer
from ._utils.deprecation import (
    deprecation_warning,
    warn_if_passing_namespace,
    warn_on_renamed_autoscaler_settings,
)
from ._utils.grpc_utils import retry_transient_errors
from ._utils.mount_utils import validate_volumes
from .cloud_bucket_mount import _CloudBucketMount
from .config import config
from .exception import ExecutionError, InvalidError, NotFoundError
from .gpu import GPU_T
from .retries import Retries
from .scheduler_placement import SchedulerPlacement
from .secret import _Secret
from .volume import _Volume

T = TypeVar("T")


if typing.TYPE_CHECKING:
    import modal.app


def _use_annotation_parameters(user_cls: type) -> bool:
    has_parameters = any(is_parameter(cls_member) for cls_member in user_cls.__dict__.values())
    has_explicit_constructor = user_cls.__init__ != object.__init__
    return has_parameters and not has_explicit_constructor


def _get_class_constructor_signature(user_cls: type) -> inspect.Signature:
    if not _use_annotation_parameters(user_cls):
        return inspect.signature(user_cls)
    else:
        constructor_parameters = []
        for name, annotation_value in typing.get_type_hints(user_cls).items():
            if hasattr(user_cls, name):
                parameter_spec = getattr(user_cls, name)
                if is_parameter(parameter_spec):
                    maybe_default = {}
                    if not isinstance(parameter_spec.default, _NO_DEFAULT):
                        maybe_default["default"] = parameter_spec.default

                    param = inspect.Parameter(
                        name=name,
                        annotation=annotation_value,
                        kind=inspect.Parameter.POSITIONAL_OR_KEYWORD,
                        **maybe_default,
                    )
                    constructor_parameters.append(param)

        return inspect.Signature(constructor_parameters)


@dataclasses.dataclass()
class _ServiceOptions:
    # Note that default values should always be "untruthy" so we can detect when they are not set
    secrets: Collection[_Secret] = ()
    validated_volumes: typing.Sequence[tuple[str, _Volume]] = ()
    resources: Optional[api_pb2.Resources] = None
    retry_policy: Optional[api_pb2.FunctionRetryPolicy] = None
    max_containers: Optional[int] = None
    buffer_containers: Optional[int] = None
    scaledown_window: Optional[int] = None
    timeout_secs: Optional[int] = None
    max_concurrent_inputs: Optional[int] = None
    target_concurrent_inputs: Optional[int] = None
    batch_max_size: Optional[int] = None
    batch_wait_ms: Optional[int] = None
    scheduler_placement: Optional[api_pb2.SchedulerPlacement] = None
    cloud: Optional[str] = None
    cloud_bucket_mounts: typing.Sequence[tuple[str, _CloudBucketMount]] = ()

    def merge_options(self, new_options: "_ServiceOptions") -> "_ServiceOptions":
        """Implement protobuf-like MergeFrom semantics for this dataclass.

        This mostly exists to support "stacking" of `.with_options()` calls.
        """
        # Don't use dataclasses.asdict() because it does a deepcopy(), which chokes on a hydrated object
        new_options_dict = {k.name: getattr(new_options, k.name) for k in dataclasses.fields(new_options)}

        # Resources needs special merge handling because individual fields are parameters in the public API
        merged_resources = api_pb2.Resources()
        if self.resources:
            merged_resources.MergeFrom(self.resources)
        if new_resources := new_options_dict.pop("resources"):
            merged_resources.MergeFrom(new_resources)
        self.resources = merged_resources

        for key, value in new_options_dict.items():
            if value:  # Only overwrite data when the value was set in the new options
                setattr(self, key, value)


def _bind_instance_method(cls: "_Cls", service_function: _Function, method_name: str):
    """Binds an "instance service function" to a specific method using metadata for that method

    This "dummy" _Function gets no unique object_id and isn't backend-backed at all, since all
    it does it forward invocations to the underlying instance_service_function with the specified method
    """
    assert service_function._obj

    def hydrate_from_instance_service_function(new_function: _Function):
        assert service_function.is_hydrated
        assert cls.is_hydrated
        # After 0.67 is minimum required version, we should be able to use method metadata directly
        # from the service_function instead (see _Cls._hydrate_metadata), but for now we use the Cls
        # since it can take the data from the cls metadata OR function metadata depending on source
        method_metadata = cls._method_metadata[method_name]
        new_function._hydrate(service_function.object_id, service_function.client, method_metadata)

    async def _load(fun: "_Function", resolver: Resolver, existing_object_id: Optional[str]):
        # there is currently no actual loading logic executed to create each method on
        # the *parametrized* instance of a class - it uses the parameter-bound service-function
        # for the instance. This load method just makes sure to set all attributes after the
        # `service_function` has been loaded (it's in the `_deps`)
        hydrate_from_instance_service_function(fun)

    def _deps():
        unhydrated_deps = []
        # without this check, the common service_function will be reloaded by all methods
        # TODO(elias): Investigate if we can fix this multi-loader in the resolver - feels like a bug?
        if not cls.is_hydrated:
            unhydrated_deps.append(cls)
        if not service_function.is_hydrated:
            unhydrated_deps.append(service_function)
        return unhydrated_deps

    rep = f"Method({cls._name}.{method_name})"

    fun = _Function._from_loader(
        _load,
        rep,
        deps=_deps,
        hydrate_lazily=True,
    )
    if service_function.is_hydrated:
        # Eager hydration (skip load) if the instance service function is already loaded
        hydrate_from_instance_service_function(fun)

    if cls._is_local():
        partial_function = cls._method_partials[method_name]
        from modal._utils.function_utils import FunctionInfo

        fun._info = FunctionInfo(
            # ugly - needed for .local()  TODO (elias): Clean up!
            partial_function.raw_f,
            user_cls=cls._user_cls,
            serialized=True,  # service_function.info.is_serialized(),
        )

    fun._obj = service_function._obj
    fun._is_method = True
    fun._app = service_function._app
    fun._spec = service_function._spec
    return fun


class _Obj:
    """An instance of a `Cls`, i.e. `Cls("foo", 42)` returns an `Obj`.

    All this class does is to return `Function` objects."""

    _cls: "_Cls"  # parent
    _functions: dict[str, _Function]
    _has_entered: bool
    _user_cls_instance: Optional[Any] = None
    _args: tuple[Any, ...]
    _kwargs: dict[str, Any]

    _instance_service_function: Optional[_Function] = None  # this gets set lazily
    _options: Optional[_ServiceOptions]

    def __init__(
        self,
        cls: "_Cls",
        user_cls: Optional[type],  # this would be None in case of lookups
        options: Optional[_ServiceOptions],
        args,
        kwargs,
    ):
        for i, arg in enumerate(args):
            check_valid_cls_constructor_arg(i + 1, arg)
        for key, kwarg in kwargs.items():
            check_valid_cls_constructor_arg(key, kwarg)
        self._cls = cls

        # Used for construction local object lazily
        self._has_entered = False
        self._user_cls = user_cls

        # used for lazy construction in case of explicit constructors
        self._args = args
        self._kwargs = kwargs
        self._options = options

    def _cached_service_function(self) -> "modal.functions._Function":
        # Returns a service function for this _Obj, serving all its methods
        # In case of methods without parameters or options, this is simply proxying to the class service function
        if not self._instance_service_function:
            assert self._cls._class_service_function
            self._instance_service_function = self._cls._class_service_function._bind_parameters(
                self, self._options, self._args, self._kwargs
            )
        return self._instance_service_function

    def _get_parameter_values(self) -> dict[str, Any]:
        # binds args and kwargs according to the class constructor signature
        # (implicit by parameters or explicit)
        # can only be called where the local definition exists
        sig = _get_class_constructor_signature(self._user_cls)
        bound_vars = sig.bind(*self._args, **self._kwargs)
        bound_vars.apply_defaults()
        return bound_vars.arguments

    def _new_user_cls_instance(self):
        if not _use_annotation_parameters(self._user_cls):
            # TODO(elias): deprecate this code path eventually
            user_cls_instance = self._user_cls(*self._args, **self._kwargs)
        else:
            # ignore constructor (assumes there is no custom constructor,
            # which is guaranteed by _use_annotation_parameters)
            # set the attributes on the class corresponding to annotations
            # with = parameter() specifications
            param_values = self._get_parameter_values()
            user_cls_instance = self._user_cls.__new__(self._user_cls)  # new instance without running __init__
            user_cls_instance.__dict__.update(param_values)

        # TODO: always use Obj instances instead of making modifications to user cls
        # TODO: OR (if simpler for now) replace all the PartialFunctions on the user cls
        #   with getattr(self, method_name)

        # user cls instances are only created locally, so we have all partial functions available
        instance_methods = {}
        for method_name in _find_partial_methods_for_user_cls(self._user_cls, _PartialFunctionFlags.interface_flags()):
            instance_methods[method_name] = getattr(self, method_name)

        user_cls_instance._modal_functions = instance_methods
        return user_cls_instance

    async def update_autoscaler(
        self,
        *,
        min_containers: Optional[int] = None,
        max_containers: Optional[int] = None,
        scaledown_window: Optional[int] = None,
        buffer_containers: Optional[int] = None,
    ) -> None:
        """Override the current autoscaler behavior for this Cls instance.

        Unspecified parameters will retain their current value, i.e. either the static value
        from the function decorator, or an override value from a previous call to this method.

        Subsequent deployments of the App containing this Cls will reset the autoscaler back to
        its static configuration.

        Note: When calling this method on a Cls that is defined locally, static type checkers will
        issue an error, because the object will appear to have the user-defined type.

        Examples:

        ```python notest
        Model = modal.Cls.from_name("my-app", "Model")
        model = Model()  # This method is called on an *instance* of the class

        # Always have at least 2 containers running, with an extra buffer when the Function is active
        model.update_autoscaler(min_containers=2, buffer_containers=1)

        # Limit this Function to avoid spinning up more than 5 containers
        f.update_autoscaler(max_containers=5)
        ```

        """
        return await self._cached_service_function().update_autoscaler(
            min_containers=min_containers,
            max_containers=max_containers,
            scaledown_window=scaledown_window,
            buffer_containers=buffer_containers,
        )

    async def keep_warm(self, warm_pool_size: int) -> None:
        """mdmd:hidden
        Set the warm pool size for the class containers

        DEPRECATED: Please adapt your code to use the more general `update_autoscaler` method instead:

        ```python notest
        Model = modal.Cls.from_name("my-app", "Model")
        model = Model()  # This method is called on an *instance* of the class

        # Old pattern (deprecated)
        model.keep_warm(2)

        # New pattern
        model.update_autoscaler(min_containers=2)
        ```

        """
        deprecation_warning(
            (2025, 5, 5),
            "The .keep_warm() method has been deprecated in favor of the more general "
            ".update_autoscaler(min_containers=...) method.",
            show_source=True,
        )
        await self._cached_service_function().update_autoscaler(min_containers=warm_pool_size)

    def _cached_user_cls_instance(self):
        """Get or construct the local object

        Used for .local() calls and getting attributes of classes"""
        if not self._user_cls_instance:
            self._user_cls_instance = self._new_user_cls_instance()  # Instantiate object

        return self._user_cls_instance

    def _enter(self):
        assert self._user_cls
        if not self._has_entered:
            user_cls_instance = self._cached_user_cls_instance()
            if hasattr(user_cls_instance, "__enter__"):
                user_cls_instance.__enter__()

            for method_flag in (
                _PartialFunctionFlags.ENTER_PRE_SNAPSHOT,
                _PartialFunctionFlags.ENTER_POST_SNAPSHOT,
            ):
                for enter_method in _find_callables_for_obj(user_cls_instance, method_flag).values():
                    enter_method()

            self._has_entered = True

    @property
    def _entered(self) -> bool:
        # needed because _aenter is nowrap
        return self._has_entered

    @_entered.setter
    def _entered(self, val: bool):
        self._has_entered = val

    @synchronizer.nowrap
    async def _aenter(self):
        if not self._entered:  # use the property to get at the impl class
            user_cls_instance = self._cached_user_cls_instance()
            if hasattr(user_cls_instance, "__aenter__"):
                await user_cls_instance.__aenter__()
            elif hasattr(user_cls_instance, "__enter__"):
                user_cls_instance.__enter__()
        self._has_entered = True

    def __getattr__(self, k):
        # This is a bit messy and branchy because:
        # * Support .remote() on both hydrated (local or remote classes) or unhydrated classes (remote classes only)
        # * Support .local() on both hydrated and unhydrated classes (assuming local access to code)
        # * Support attribute access (when local cls is available)

        # The returned _Function objects need to be lazily loaded (including loading the Cls and/or service function)
        # since we can't assume the class is already loaded when this gets called, e.g.
        # CLs.from_name(...)().my_func.remote().

        def _get_maybe_method() -> Optional["_Function"]:
            """Gets _Function object for method - either for a local or a hydrated remote class

            * If class is neither local or hydrated - raise exception (should never happen)
            * If attribute isn't a method - return None
            """
            if self._cls._is_local():
                if k not in self._cls._method_partials:
                    return None
            elif self._cls.is_hydrated:
                if k not in self._cls._method_metadata:
                    return None
            else:
                raise ExecutionError(
                    "Class is neither hydrated or local - this is probably a bug in the Modal client. Contact support"
                )

            return _bind_instance_method(self._cls, self._cached_service_function(), k)

        if self._cls.is_hydrated or self._cls._is_local():
            # Class is hydrated or local so we know which methods exist
            if maybe_method := _get_maybe_method():
                return maybe_method
            elif self._cls._is_local():
                # We have the local definition, and the attribute isn't a method
                # so we instantiate if we don't have an instance, and try to get the attribute
                user_cls_instance = self._cached_user_cls_instance()
                return getattr(user_cls_instance, k)
            else:
                # This is the case for a *hydrated* class without the local definition, i.e. a lookup
                # where the attribute isn't a registered method of the class
                raise NotFoundError(
                    f"Class has no method `{k}` and attributes (or undecorated methods) can't be accessed for"
                    f" remote classes (`Cls.from_name` instances)"
                )

        # Not hydrated Cls, and we don't have the class - typically a Cls.from_name that
        # has not yet been loaded. So use a special loader that loads it lazily:
        async def method_loader(fun, resolver: Resolver, existing_object_id):
            await resolver.load(self._cls)  # load class so we get info about methods
            method_function = _get_maybe_method()
            if method_function is None:
                raise NotFoundError(
                    f"Class has no method {k}, and attributes can't be accessed for `Cls.from_name` instances"
                )
            await resolver.load(method_function)  # get the appropriate method handle (lazy)
            fun._hydrate_from_other(method_function)

        # The reason we don't *always* use this lazy loader is because it precludes attribute access
        # on local classes.
        return _Function._from_loader(
            method_loader,
            rep=f"Method({self._cls._name}.{k})",
            deps=lambda: [],  # TODO: use cls as dep instead of loading inside method_loader?
            hydrate_lazily=True,
        )


Obj = synchronize_api(_Obj)


class _Cls(_Object, type_prefix="cs"):
    """
    Cls adds method pooling and [lifecycle hook](https://modal.com/docs/guide/lifecycle-functions) behavior
    to [modal.Function](https://modal.com/docs/reference/modal.Function).

    Generally, you will not construct a Cls directly.
    Instead, use the [`@app.cls()`](https://modal.com/docs/reference/modal.App#cls) decorator on the App object.
    """

    _class_service_function: Optional[_Function]  # The _Function (read "service") serving *all* methods of the class
    _options: _ServiceOptions

    _app: Optional["modal.app._App"] = None  # not set for lookups
    _name: Optional[str]
    # Only set for hydrated classes:
    _method_metadata: Optional[dict[str, api_pb2.FunctionHandleMetadata]] = None

    # These are only set where source is locally available:
    # TODO: wrap these in a single optional/property for consistency
    _user_cls: Optional[type] = None
    _method_partials: Optional[dict[str, _PartialFunction]] = None
    _callables: dict[str, Callable[..., Any]]

    def _initialize_from_empty(self):
        self._user_cls = None
        self._class_service_function = None
        self._options = _ServiceOptions()
        self._callables = {}
        self._name = None

    def _initialize_from_other(self, other: "_Cls"):
        super()._initialize_from_other(other)
        self._app = other._app
        self._user_cls = other._user_cls
        self._class_service_function = other._class_service_function
        self._method_partials = other._method_partials
        self._options = other._options
        self._callables = other._callables
        self._name = other._name
        self._method_metadata = other._method_metadata

    def _get_partial_functions(self) -> dict[str, _PartialFunction]:
        if not self._user_cls:
            raise AttributeError("You can only get the partial functions of a local Cls instance")
        return _find_partial_methods_for_user_cls(self._user_cls, _PartialFunctionFlags.all())

    def _get_app(self) -> "modal.app._App":
        assert self._app is not None
        return self._app

    def _get_user_cls(self) -> type:
        assert self._user_cls is not None
        return self._user_cls

    def _get_name(self) -> str:
        assert self._name is not None
        return self._name

    def _get_class_service_function(self) -> _Function:
        assert self._class_service_function is not None
        return self._class_service_function

    def _get_method_names(self) -> Collection[str]:
        # returns method names for a *local* class only for now (used by cli)
        return self._method_partials.keys()

    @live_method
    async def _experimental_get_flash_urls(self) -> Optional[list[str]]:
        """URL of the flash service for the class."""
        return await self._get_class_service_function()._experimental_get_flash_urls()

    def _hydrate_metadata(self, metadata: Message):
        assert isinstance(metadata, api_pb2.ClassHandleMetadata)
        class_service_function = self._get_class_service_function()
        assert class_service_function.is_hydrated

        if class_service_function._method_handle_metadata and len(class_service_function._method_handle_metadata):
            # If we have the metadata on the class service function
            # This should be the case for any loaded class (remote or local) as of v0.67
            method_metadata = class_service_function._method_handle_metadata
        else:
            # Method metadata stored on the backend Cls object - pre 0.67 lookups
            # Can be removed when v0.67 is least supported version (all metadata is on the function)
            method_metadata = {}
            for method in metadata.methods:
                method_metadata[method.function_name] = method.function_handle_metadata
        self._method_metadata = method_metadata

    @staticmethod
    def validate_construction_mechanism(user_cls):
        """mdmd:hidden"""
        params = {k: v for k, v in user_cls.__dict__.items() if is_parameter(v)}
        has_custom_constructor = user_cls.__init__ != object.__init__
        if params and has_custom_constructor:
            raise InvalidError(
                "A class can't have both a custom __init__ constructor "
                "and dataclass-style modal.parameter() annotations"
            )
        elif has_custom_constructor:
            deprecation_warning(
                (2025, 4, 15),
                f"""
{user_cls} uses a non-default constructor (__init__) method.
Custom constructors will not be supported in a a future version of Modal.

To parameterize classes, use dataclass-style modal.parameter() declarations instead,
e.g.:\n

class {user_cls.__name__}:
    model_name: str = modal.parameter()

More information on class parameterization can be found here: https://modal.com/docs/guide/parametrized-functions
""",
            )
        annotations = user_cls.__dict__.get("__annotations__", {})  # compatible with older pythons
        missing_annotations = params.keys() - annotations.keys()
        if missing_annotations:
            raise InvalidError("All modal.parameter() specifications need to be type-annotated")

        annotated_params = {k: t for k, t in annotations.items() if k in params}
        for k, t in annotated_params.items():
            try:
                parameter_serde_registry.validate_parameter_type(t)
            except TypeError as exc:
                raise InvalidError(f"Class parameter '{k}': {exc}")

    @staticmethod
    def from_local(user_cls, app: "modal.app._App", class_service_function: _Function) -> "_Cls":
        """mdmd:hidden"""
        # validate signature
        _Cls.validate_construction_mechanism(user_cls)

        method_partials: dict[str, _PartialFunction] = _find_partial_methods_for_user_cls(
            user_cls, _PartialFunctionFlags.interface_flags()
        )

        for method_name, partial_function in method_partials.items():
            if partial_function.params.webhook_config is not None:
                full_name = f"{user_cls.__name__}.{method_name}"
                app._web_endpoints.append(full_name)
            partial_function.registered = True

        # Disable the warning that lifecycle methods are not wrapped
        for partial_function in _find_partial_methods_for_user_cls(
            user_cls, ~_PartialFunctionFlags.interface_flags()
        ).values():
            partial_function.registered = True

        # Get all callables
        callables: dict[str, Callable] = {
            k: pf.raw_f
            for k, pf in _find_partial_methods_for_user_cls(user_cls, _PartialFunctionFlags.all()).items()
            if pf.raw_f is not None  # Should be true for _find_partial_methods output, but hard to annotate
        }

        def _deps() -> list[_Function]:
            return [class_service_function]

        async def _load(self: "_Cls", resolver: Resolver, existing_object_id: Optional[str]):
            req = api_pb2.ClassCreateRequest(
                app_id=resolver.app_id, existing_class_id=existing_object_id, only_class_function=True
            )
            resp = await resolver.client.stub.ClassCreate(req)
            self._hydrate(resp.class_id, resolver.client, resp.handle_metadata)

        rep = f"Cls({user_cls.__name__})"
        cls: _Cls = _Cls._from_loader(_load, rep, deps=_deps)
        cls._app = app
        cls._user_cls = user_cls
        cls._class_service_function = class_service_function
        cls._method_partials = method_partials
        cls._callables = callables
        cls._name = user_cls.__name__
        return cls

    @classmethod
    def from_name(
        cls: type["_Cls"],
        app_name: str,
        name: str,
        *,
        namespace: Any = None,  # mdmd:line-hidden
        environment_name: Optional[str] = None,
    ) -> "_Cls":
        """Reference a Cls from a deployed App by its name.

        This is a lazy method that defers hydrating the local
        object with metadata from Modal servers until the first
        time it is actually used.

        ```python
        Model = modal.Cls.from_name("other-app", "Model")
        ```
        """
        warn_if_passing_namespace(namespace, "modal.Cls.from_name")
        _environment_name = environment_name or config.get("environment")

        async def _load_remote(self: _Cls, resolver: Resolver, existing_object_id: Optional[str]):
            request = api_pb2.ClassGetRequest(
                app_name=app_name,
                object_tag=name,
                environment_name=_environment_name,
                only_class_function=True,
            )
            try:
                response = await retry_transient_errors(resolver.client.stub.ClassGet, request)
            except NotFoundError as exc:
                env_context = f" (in the '{environment_name}' environment)" if environment_name else ""
                raise NotFoundError(
                    f"Lookup failed for Cls '{name}' from the '{app_name}' app{env_context}: {exc}."
                ) from None
            except GRPCError as exc:
                if exc.status == Status.FAILED_PRECONDITION:
                    raise InvalidError(exc.message) from None
                else:
                    raise

            print_server_warnings(response.server_warnings)
            await resolver.load(self._class_service_function)
            self._hydrate(response.class_id, resolver.client, response.handle_metadata)

        environment_rep = f", environment_name={environment_name!r}" if environment_name else ""
        rep = f"Cls.from_name({app_name!r}, {name!r}{environment_rep})"
        cls = cls._from_loader(_load_remote, rep, is_another_app=True, hydrate_lazily=True)

        class_service_name = f"{name}.*"  # special name of the base service function for the class
        cls._class_service_function = _Function._from_name(
            app_name,
            class_service_name,
            namespace=namespace,
            environment_name=_environment_name,
        )
        cls._name = name
        return cls

    @warn_on_renamed_autoscaler_settings
    def with_options(
        self: "_Cls",
        *,
        cpu: Optional[Union[float, tuple[float, float]]] = None,
        memory: Optional[Union[int, tuple[int, int]]] = None,
        gpu: GPU_T = None,
        env: Optional[dict[str, Optional[str]]] = None,
        secrets: Optional[Collection[_Secret]] = None,
        volumes: dict[Union[str, PurePosixPath], Union[_Volume, _CloudBucketMount]] = {},
        retries: Optional[Union[int, Retries]] = None,
        max_containers: Optional[int] = None,  # Limit on the number of containers that can be concurrently running.
        buffer_containers: Optional[int] = None,  # Additional containers to scale up while Function is active.
        scaledown_window: Optional[int] = None,  # Max amount of time a container can remain idle before scaling down.
        timeout: Optional[int] = None,
        region: Optional[Union[str, Sequence[str]]] = None,  # Region or regions to run the function on.
        cloud: Optional[str] = None,  # Cloud provider to run the function on. Possible values are aws, gcp, oci, auto.
        # The following parameters are deprecated
        concurrency_limit: Optional[int] = None,  # Now called `max_containers`
        container_idle_timeout: Optional[int] = None,  # Now called `scaledown_window`
        allow_concurrent_inputs: Optional[int] = None,  # See `.with_concurrency`
    ) -> "_Cls":
        """Override the static Function configuration at runtime.

        This method will return a new instance of the cls that will autoscale independently of the
        original instance. Note that options cannot be "unset" with this method (i.e., if a GPU
        is configured in the `@app.cls()` decorator, passing `gpu=None` here will not create a
        CPU-only instance).

        **Usage:**

        You can use this method after looking up the Cls from a deployed App or if you have a
        direct reference to a Cls from another Function or local entrypoint on its App:

        ```python notest
        Model = modal.Cls.from_name("my_app", "Model")
        ModelUsingGPU = Model.with_options(gpu="A100")
        ModelUsingGPU().generate.remote(input_prompt)  # Run with an A100 GPU
        ```

        The method can be called multiple times to "stack" updates:

        ```python notest
        Model.with_options(gpu="A100").with_options(scaledown_window=300)  # Use an A100 with slow scaledown
        ```

        Note that container arguments (i.e. `volumes` and `secrets`) passed in subsequent calls
        will not be merged.
        """
        retry_policy = _parse_retries(retries, f"Class {self.__name__}" if self._user_cls else "")
        if gpu or cpu or memory:
            resources = convert_fn_config_to_resources_config(cpu=cpu, memory=memory, gpu=gpu, ephemeral_disk=None)
        else:
            resources = None

        scheduler_placement = SchedulerPlacement(region=region).proto if region else None

        if allow_concurrent_inputs is not None:
            deprecation_warning(
                (2025, 5, 9),
                "The `allow_concurrent_inputs` argument is deprecated;"
                " please use the `.with_concurrency` method instead.",
            )

        async def _load_from_base(new_cls, resolver, existing_object_id):
            # this is a bit confusing, the cls will always have the same metadata
            # since it has the same *class* service function (i.e. "template")
            # But the (instance) service function for each Obj will be different
            # since it will rebind to whatever `_options` have been assigned on
            # the particular Cls parent
            if not self.is_hydrated:
                # this should only happen for Cls.from_name instances
                # other classes should already be hydrated!
                await resolver.load(self)

            new_cls._initialize_from_other(self)

        def _deps():
            return []

        cls = _Cls._from_loader(_load_from_base, rep=f"{self._name}.with_options(...)", is_another_app=True, deps=_deps)
        cls._initialize_from_other(self)

        # Validate volumes
        validated_volumes = validate_volumes(volumes)
        cloud_bucket_mounts = [(k, v) for k, v in validated_volumes if isinstance(v, _CloudBucketMount)]
        validated_volumes_no_cloud_buckets = [(k, v) for k, v in validated_volumes if isinstance(v, _Volume)]

        secrets = secrets or []
        if env:
            secrets = [*secrets, _Secret.from_dict(env)]

        new_options = _ServiceOptions(
            secrets=secrets,
            validated_volumes=validated_volumes_no_cloud_buckets,
            cloud_bucket_mounts=cloud_bucket_mounts,
            resources=resources,
            retry_policy=retry_policy,
            max_containers=max_containers,
            buffer_containers=buffer_containers,
            scaledown_window=scaledown_window,
            timeout_secs=timeout,
            scheduler_placement=scheduler_placement,
            cloud=cloud,
            # Note: set both for backwards / forwards compatibility
            # But going forward `.with_concurrency` is the preferred method with distinct parameterization
            max_concurrent_inputs=allow_concurrent_inputs,
            target_concurrent_inputs=allow_concurrent_inputs,
        )

        cls._options.merge_options(new_options)
        return cls

    def with_concurrency(self: "_Cls", *, max_inputs: int, target_inputs: Optional[int] = None) -> "_Cls":
        """Create an instance of the Cls with input concurrency enabled or overridden with new values.

        **Usage:**

        ```python notest
        Model = modal.Cls.from_name("my_app", "Model")
        ModelUsingGPU = Model.with_options(gpu="A100").with_concurrency(max_inputs=100)
        ModelUsingGPU().generate.remote(42)  # will run on an A100 GPU with input concurrency enabled
        ```
        """

        async def _load_from_base(new_cls, resolver, existing_object_id):
            if not self.is_hydrated:
                await resolver.load(self)
            new_cls._initialize_from_other(self)

        def _deps():
            return []

        cls = _Cls._from_loader(
            _load_from_base, rep=f"{self._name}.with_concurrency(...)", is_another_app=True, deps=_deps
        )
        cls._initialize_from_other(self)

        concurrency_options = _ServiceOptions(max_concurrent_inputs=max_inputs, target_concurrent_inputs=target_inputs)
        cls._options.merge_options(concurrency_options)
        return cls

    def with_batching(self: "_Cls", *, max_batch_size: int, wait_ms: int) -> "_Cls":
        """Create an instance of the Cls with dynamic batching enabled or overridden with new values.

        **Usage:**

        ```python notest
        Model = modal.Cls.from_name("my_app", "Model")
        ModelUsingGPU = Model.with_options(gpu="A100").with_batching(max_batch_size=100, batch_wait_ms=1000)
        ModelUsingGPU().generate.remote(42)  # will run on an A100 GPU with input concurrency enabled
        ```
        """

        async def _load_from_base(new_cls, resolver, existing_object_id):
            if not self.is_hydrated:
                await resolver.load(self)
            new_cls._initialize_from_other(self)

        def _deps():
            return []

        cls = _Cls._from_loader(
            _load_from_base, rep=f"{self._name}.with_concurrency(...)", is_another_app=True, deps=_deps
        )
        cls._initialize_from_other(self)

        batching_options = _ServiceOptions(batch_max_size=max_batch_size, batch_wait_ms=wait_ms)
        cls._options.merge_options(batching_options)
        return cls

    @synchronizer.no_input_translation
    def __call__(self, *args, **kwargs) -> _Obj:
        """This acts as the class constructor."""
        return _Obj(
            self,
            self._user_cls,
            self._options,
            args,
            kwargs,
        )

    def __getattr__(self, k):
        # TODO: remove this method - access to attributes on classes (not instances) should be discouraged
        if not self._is_local() or k in self._method_partials:
            # if not local (== k *could* be a method) or it is local and we know k is a method
            deprecation_warning(
                (2025, 1, 13),
                "Calling a method on an uninstantiated class will soon be deprecated; "
                "update your code to instantiate the class first, i.e.:\n"
                f"{self._name}().{k} instead of {self._name}.{k}",
            )
            return getattr(self(), k)
        # non-method attribute access on local class - arguably shouldn't be used either:
        return getattr(self._user_cls, k)

    def _is_local(self) -> bool:
        return self._user_cls is not None


Cls = synchronize_api(_Cls)


@synchronize_api
async def _get_constructor_args(cls: _Cls) -> typing.Sequence[api_pb2.ClassParameterSpec]:
    # for internal use only - defined separately to not clutter Cls namespace
    await cls.hydrate()
    service_function = cls._get_class_service_function()
    metadata = service_function._metadata
    assert metadata
    if metadata.class_parameter_info.format != metadata.class_parameter_info.PARAM_SERIALIZATION_FORMAT_PROTO:
        raise InvalidError("Can only get constructor args for strictly parameterized classes")
    return metadata.class_parameter_info.schema


@synchronize_api
async def _get_method_schemas(cls: _Cls) -> dict[str, api_pb2.FunctionSchema]:
    # for internal use only - defined separately to not clutter Cls namespace
    await cls.hydrate()
    assert cls._method_metadata
    return {
        method_name: method_metadata.function_schema for method_name, method_metadata in cls._method_metadata.items()
    }


class _NO_DEFAULT:
    def __repr__(self):
        return "modal.cls._NO_DEFAULT()"


_no_default = _NO_DEFAULT()


class _Parameter:
    default: Any
    init: bool

    def __init__(self, default: Any, init: bool):
        self.default = default
        self.init = init

    def __get__(self, obj, obj_type=None) -> Any:
        if obj:
            if self.default is _no_default:
                raise AttributeError("field has no default value and no specified value")
            return self.default
        return self


def is_parameter(p: Any) -> bool:
    return isinstance(p, _Parameter) and p.init


def parameter(*, default: Any = _no_default, init: bool = True) -> Any:
    """Used to specify options for modal.cls parameters, similar to dataclass.field for dataclasses
    ```
    class A:
        a: str = modal.parameter()

    ```

    If `init=False` is specified, the field is not considered a parameter for the
    Modal class and not used in the synthesized constructor. This can be used to
    optionally annotate the type of a field that's used internally, for example values
    being set by @enter lifecycle methods, without breaking type checkers, but it has
    no runtime effect on the class.
    """
    # has to return Any to be assignable to any annotation (https://github.com/microsoft/pyright/issues/5102)
    return _Parameter(default=default, init=init)



================================================
FILE: modal/config.py
================================================
# Copyright Modal Labs 2022
r"""Modal intentionally keeps configurability to a minimum.

The main configuration options are the API tokens: the token id and the token secret.
These can be configured in two ways:

1. By running the `modal token set` command.
   This writes the tokens to `.modal.toml` file in your home directory.
2. By setting the environment variables `MODAL_TOKEN_ID` and `MODAL_TOKEN_SECRET`.
   This takes precedence over the previous method.

.modal.toml
---------------

The `.modal.toml` file is generally stored in your home directory.
It should look like this::

```toml
[default]
token_id = "ak-12345..."
token_secret = "as-12345..."
```

You can create this file manually, or you can run the `modal token set ...`
command (see below).

Setting tokens using the CLI
----------------------------

You can set a token by running the command::

```
modal token set \
  --token-id <token id> \
  --token-secret <token secret>
```

This will write the token id and secret to `.modal.toml`.

If the token id or secret is provided as the string `-` (a single dash),
then it will be read in a secret way from stdin instead.

Other configuration options
---------------------------

Other possible configuration options are:

* `loglevel` (in the .toml file) / `MODAL_LOGLEVEL` (as an env var).
  Defaults to `WARNING`. Set this to `DEBUG` to see internal messages.
* `logs_timeout` (in the .toml file) / `MODAL_LOGS_TIMEOUT` (as an env var).
  Defaults to 10.
  Number of seconds to wait for logs to drain when closing the session,
  before giving up.
* `force_build` (in the .toml file) / `MODAL_FORCE_BUILD` (as an env var).
  Defaults to False.
  When set, ignores the Image cache and builds all Image layers. Note that this
  will break the cache for all images based on the rebuilt layers, so other images
  may rebuild on subsequent runs / deploys even if the config is reverted.
* `ignore_cache` (in the .toml file) / `MODAL_IGNORE_CACHE` (as an env var).
  Defaults to False.
  When set, ignores the Image cache and builds all Image layers. Unlike `force_build`,
  this will not overwrite the cache for other images that have the same recipe.
  Subsequent runs that do not use this option will pull the *previous* Image from
  the cache, if one exists. It can be useful for testing an App's robustness to
  Image rebuilds without clobbering Images used by other Apps.
* `traceback` (in the .toml file) / `MODAL_TRACEBACK` (as an env var).
  Defaults to False. Enables printing full tracebacks on unexpected CLI
  errors, which can be useful for debugging client issues.
* `log_pattern` (in the .toml file) / `MODAL_LOG_PATTERN` (as an env var).
  Defaults to `"[modal-client] %(asctime)s %(message)s"`
  The log formatting pattern that will be used by the modal client itself.
  See https://docs.python.org/3/library/logging.html#logrecord-attributes for available
  log attributes.

Meta-configuration
------------------

Some "meta-options" are set using environment variables only:

* `MODAL_CONFIG_PATH` lets you override the location of the .toml file,
  by default `~/.modal.toml`.
* `MODAL_PROFILE` lets you use multiple sections in the .toml file
  and switch between them. It defaults to "default".
"""

import logging
import os
import typing
import warnings
from typing import Any, Callable, Optional

from google.protobuf.empty_pb2 import Empty

from modal_proto import api_pb2

from ._utils.logger import configure_logger
from .exception import InvalidError, NotFoundError

DEFAULT_SERVER_URL = "https://api.modal.com"


# Locate config file and read it

user_config_path: str = os.environ.get("MODAL_CONFIG_PATH") or os.path.expanduser("~/.modal.toml")


def _is_remote() -> bool:
    # We want to prevent read/write on a modal config file in the container
    # environment, both because that doesn't make sense and might cause weird
    # behavior, and because we want to keep the `toml` dependency out of the
    # container runtime.
    return os.environ.get("MODAL_IS_REMOTE") == "1"


def _read_user_config():
    config_data = {}
    if not _is_remote() and os.path.exists(user_config_path):
        # Defer toml import so we don't need it in the container runtime environment
        import toml

        try:
            with open(user_config_path) as f:
                config_data = toml.load(f)
        except Exception as exc:
            config_problem = str(exc)
        else:
            if not all(isinstance(e, dict) for e in config_data.values()):
                config_problem = "TOML file must contain table sections for each profile."
            else:
                config_problem = ""
        if config_problem:
            message = f"\nError when reading the modal configuration from `{user_config_path}`.\n\n{config_problem}"
            raise InvalidError(message)
    return config_data


_user_config = _read_user_config()


async def _lookup_workspace(server_url: str, token_id: str, token_secret: str) -> api_pb2.WorkspaceNameLookupResponse:
    from .client import _Client

    credentials = (token_id, token_secret)
    async with _Client(server_url, api_pb2.CLIENT_TYPE_CLIENT, credentials) as client:
        return await client.stub.WorkspaceNameLookup(Empty(), timeout=3)


def config_profiles():
    """List the available modal profiles in the .modal.toml file."""
    return _user_config.keys()


def _config_active_profile() -> str:
    for key, values in _user_config.items():
        if values.get("active", False) is True:
            return key
    else:
        return "default"


def config_set_active_profile(profile: str) -> None:
    """Set the user's active modal profile by writing it to the `.modal.toml` file."""
    if profile not in _user_config:
        raise NotFoundError(f"No profile named '{profile}' found in {user_config_path}")

    for profile_data in _user_config.values():
        profile_data.pop("active", None)

    _user_config[profile]["active"] = True  # type: ignore
    _write_user_config(_user_config)


def _check_config() -> None:
    num_profiles = len(_user_config)
    num_active = sum(v.get("active", False) for v in _user_config.values())
    if num_active > 1:
        raise InvalidError(
            "More than one Modal profile is active. "
            "Please fix with `modal profile activate` or by editing your Modal config file "
            f"({user_config_path})."
        )
    elif num_profiles > 1 and num_active == 0 and _profile == "default":
        # TODO: We should get rid of the `_profile = "default"` concept entirely now
        raise InvalidError(
            "No Modal profile is active.\n\n"
            "Please fix by running `modal profile activate` or by editing your Modal config file "
            f"({user_config_path})."
        )


_profile = os.environ.get("MODAL_PROFILE") or _config_active_profile()

# Define settings


def _to_boolean(x: object) -> bool:
    return str(x).lower() not in {"", "0", "false"}


def _check_value(options: list[str]) -> Callable[[str], str]:
    def checker(x: str) -> str:
        if x not in options:
            raise ValueError(f"Must be one of {options}.")
        return x

    return checker


class _Setting(typing.NamedTuple):
    default: typing.Any = None
    transform: typing.Callable[[str], typing.Any] = lambda x: x  # noqa: E731


_SETTINGS = {
    "loglevel": _Setting("WARNING", lambda s: s.upper()),
    "log_format": _Setting("STRING", lambda s: s.upper()),
    "log_pattern": _Setting(),  # optional override of the formatting pattern
    "server_url": _Setting(DEFAULT_SERVER_URL),
    "token_id": _Setting(),
    "token_secret": _Setting(),
    "task_id": _Setting(),
    "serve_timeout": _Setting(transform=float),
    "sync_entrypoint": _Setting(),
    "logs_timeout": _Setting(10, float),
    "image_id": _Setting(),
    "heartbeat_interval": _Setting(15, float),
    "function_runtime": _Setting(),
    "function_runtime_debug": _Setting(False, transform=_to_boolean),  # For internal debugging use.
    "runtime_perf_record": _Setting(False, transform=_to_boolean),  # For internal debugging use.
    "environment": _Setting(),
    "default_cloud": _Setting(None, transform=lambda x: x if x else None),
    "worker_id": _Setting(),  # For internal debugging use.
    "restore_state_path": _Setting("/__modal/restore-state.json"),
    "force_build": _Setting(False, transform=_to_boolean),
    "ignore_cache": _Setting(False, transform=_to_boolean),
    "traceback": _Setting(False, transform=_to_boolean),
    "image_builder_version": _Setting(),
    "strict_parameters": _Setting(False, transform=_to_boolean),  # For internal/experimental use
    "snapshot_debug": _Setting(False, transform=_to_boolean),
    "cuda_checkpoint_path": _Setting("/__modal/.bin/cuda-checkpoint"),  # Used for snapshotting GPU memory.
    "build_validation": _Setting("error", transform=_check_value(["error", "warn", "ignore"])),
    # Payload format for function inputs/outputs: 'pickle' (default) or 'cbor'
    "payload_format": _Setting(
        "pickle",
        transform=lambda s: _check_value(["pickle", "cbor"])(s.lower()),
    ),
}


class Config:
    """Singleton that holds configuration used by Modal internally."""

    def __init__(self):
        pass

    def get(self, key, profile=None, use_env=True):
        """Looks up a configuration value.

        Will check (in decreasing order of priority):
        1. Any environment variable of the form MODAL_FOO_BAR (when use_env is True)
        2. Settings in the user's .toml configuration file
        3. The default value of the setting
        """
        if profile is None:
            profile = _profile
        s = _SETTINGS[key]
        env_var_key = "MODAL_" + key.upper()

        def transform(val: str) -> Any:
            try:
                return s.transform(val)
            except Exception as e:
                raise InvalidError(f"Invalid value for {key} config ({val!r}): {e}")

        if use_env and env_var_key in os.environ:
            return transform(os.environ[env_var_key])
        elif profile in _user_config and key in _user_config[profile]:
            return transform(_user_config[profile][key])
        else:
            return s.default

    def override_locally(self, key: str, value: str):
        # Override setting in this process by overriding environment variable for the setting
        #
        # Does NOT write back to settings file etc.
        try:
            self.get(key)
            os.environ["MODAL_" + key.upper()] = value
        except KeyError:
            # Override env vars not available in config, e.g. NVIDIA_VISIBLE_DEVICES.
            # This is used for restoring env vars from a memory snapshot.
            os.environ[key.upper()] = value

    def __getitem__(self, key):
        return self.get(key)

    def __repr__(self):
        return repr(self.to_dict())

    def to_dict(self):
        return {key: self.get(key) for key in sorted(_SETTINGS)}


config = Config()

# Logging

logger = logging.getLogger("modal-client")
configure_logger(logger, config["loglevel"], config["log_format"])

# Utils to write config


def _store_user_config(
    new_settings: dict[str, Any],
    profile: Optional[str] = None,
    active_profile: Optional[str] = None,
):
    """Internal method, used by the CLI to set tokens."""
    if profile is None:
        profile = _profile
    user_config = _read_user_config()
    user_config.setdefault(profile, {}).update(**new_settings)
    if active_profile is not None:
        for prof_name, prof_config in user_config.items():
            if prof_name == active_profile:
                prof_config["active"] = True
            else:
                prof_config.pop("active", None)
    _write_user_config(user_config)


def _write_user_config(user_config):
    if _is_remote():
        raise InvalidError("Can't update config file in remote environment.")

    # Defer toml import so we don't need it in the container runtime environment
    import toml

    with open(user_config_path, "w") as f:
        toml.dump(user_config, f)


# Make sure all deprecation warnings are shown
# See https://docs.python.org/3/library/warnings.html#overriding-the-default-filter
warnings.filterwarnings(
    "default",
    category=DeprecationWarning,
    module="modal",
)



================================================
FILE: modal/container_process.py
================================================
# Copyright Modal Labs 2024
import asyncio
import platform
import time
from typing import Generic, Optional, TypeVar

from modal_proto import api_pb2

from ._utils.async_utils import TaskContext, synchronize_api
from ._utils.grpc_utils import retry_transient_errors
from ._utils.shell_utils import stream_from_stdin, write_to_fd
from .client import _Client
from .config import logger
from .exception import InteractiveTimeoutError, InvalidError
from .io_streams import _StreamReader, _StreamWriter
from .stream_type import StreamType

T = TypeVar("T", str, bytes)


class _ContainerProcess(Generic[T]):
    _process_id: Optional[str] = None
    _stdout: _StreamReader[T]
    _stderr: _StreamReader[T]
    _stdin: _StreamWriter
    _exec_deadline: Optional[float] = None
    _text: bool
    _by_line: bool
    _returncode: Optional[int] = None

    def __init__(
        self,
        process_id: str,
        client: _Client,
        stdout: StreamType = StreamType.PIPE,
        stderr: StreamType = StreamType.PIPE,
        exec_deadline: Optional[float] = None,
        text: bool = True,
        by_line: bool = False,
    ) -> None:
        self._process_id = process_id
        self._client = client
        self._exec_deadline = exec_deadline
        self._text = text
        self._by_line = by_line
        self._stdout = _StreamReader[T](
            api_pb2.FILE_DESCRIPTOR_STDOUT,
            process_id,
            "container_process",
            self._client,
            stream_type=stdout,
            text=text,
            by_line=by_line,
            deadline=exec_deadline,
        )
        self._stderr = _StreamReader[T](
            api_pb2.FILE_DESCRIPTOR_STDERR,
            process_id,
            "container_process",
            self._client,
            stream_type=stderr,
            text=text,
            by_line=by_line,
            deadline=exec_deadline,
        )
        self._stdin = _StreamWriter(process_id, "container_process", self._client)

    def __repr__(self) -> str:
        return f"ContainerProcess(process_id={self._process_id!r})"

    @property
    def stdout(self) -> _StreamReader[T]:
        """StreamReader for the container process's stdout stream."""
        return self._stdout

    @property
    def stderr(self) -> _StreamReader[T]:
        """StreamReader for the container process's stderr stream."""
        return self._stderr

    @property
    def stdin(self) -> _StreamWriter:
        """StreamWriter for the container process's stdin stream."""
        return self._stdin

    @property
    def returncode(self) -> int:
        if self._returncode is None:
            raise InvalidError(
                "You must call wait() before accessing the returncode. "
                "To poll for the status of a running process, use poll() instead."
            )
        return self._returncode

    async def poll(self) -> Optional[int]:
        """Check if the container process has finished running.

        Returns `None` if the process is still running, else returns the exit code.
        """
        if self._returncode is not None:
            return self._returncode
        if self._exec_deadline and time.monotonic() >= self._exec_deadline:
            # TODO(matt): In the future, it would be nice to raise a ContainerExecTimeoutError to make it
            # clear to the user that their sandbox terminated due to a timeout
            self._returncode = -1
            return self._returncode

        req = api_pb2.ContainerExecWaitRequest(exec_id=self._process_id, timeout=0)
        resp: api_pb2.ContainerExecWaitResponse = await retry_transient_errors(self._client.stub.ContainerExecWait, req)

        if resp.completed:
            self._returncode = resp.exit_code
            return self._returncode

        return None

    async def _wait_for_completion(self) -> int:
        while True:
            req = api_pb2.ContainerExecWaitRequest(exec_id=self._process_id, timeout=10)
            resp: api_pb2.ContainerExecWaitResponse = await retry_transient_errors(
                self._client.stub.ContainerExecWait, req
            )
            if resp.completed:
                return resp.exit_code

    async def wait(self) -> int:
        """Wait for the container process to finish running. Returns the exit code."""
        if self._returncode is not None:
            return self._returncode

        try:
            timeout = None
            if self._exec_deadline:
                timeout = self._exec_deadline - time.monotonic()
                if timeout <= 0:
                    raise TimeoutError()
            self._returncode = await asyncio.wait_for(self._wait_for_completion(), timeout=timeout)
        except (asyncio.TimeoutError, TimeoutError):
            self._returncode = -1
        logger.debug(f"ContainerProcess {self._process_id} wait completed with returncode {self._returncode}")
        return self._returncode

    async def attach(self):
        """mdmd:hidden"""
        if platform.system() == "Windows":
            print("interactive exec is not currently supported on Windows.")
            return

        from ._output import make_console

        console = make_console()

        connecting_status = console.status("Connecting...")
        connecting_status.start()
        on_connect = asyncio.Event()

        async def _write_to_fd_loop(stream: _StreamReader):
            # Don't skip empty messages so we can detect when the process has booted.
            async for chunk in stream._get_logs(skip_empty_messages=False):
                if chunk is None:
                    break

                if not on_connect.is_set():
                    connecting_status.stop()
                    on_connect.set()

                await write_to_fd(stream.file_descriptor, chunk)

        async def _handle_input(data: bytes, message_index: int):
            self.stdin.write(data)
            await self.stdin.drain()

        async with TaskContext() as tc:
            stdout_task = tc.create_task(_write_to_fd_loop(self.stdout))
            stderr_task = tc.create_task(_write_to_fd_loop(self.stderr))

            try:
                # time out if we can't connect to the server fast enough
                await asyncio.wait_for(on_connect.wait(), timeout=60)

                async with stream_from_stdin(_handle_input, use_raw_terminal=True):
                    await stdout_task
                    await stderr_task

                # TODO: this doesn't work right now.
                # if exit_status != 0:
                #     raise ExecutionError(f"Process exited with status code {exit_status}")

            except (asyncio.TimeoutError, TimeoutError):
                connecting_status.stop()
                stdout_task.cancel()
                stderr_task.cancel()
                raise InteractiveTimeoutError("Failed to establish connection to container. Please try again.")


ContainerProcess = synchronize_api(_ContainerProcess)



================================================
FILE: modal/dict.py
================================================
# Copyright Modal Labs 2022
from collections.abc import AsyncIterator, Mapping
from dataclasses import dataclass
from datetime import datetime
from typing import Any, Optional, Union

from google.protobuf.message import Message
from grpclib import GRPCError, Status
from synchronicity import classproperty
from synchronicity.async_wrap import asynccontextmanager

from modal_proto import api_pb2

from ._object import (
    EPHEMERAL_OBJECT_HEARTBEAT_SLEEP,
    _get_environment_name,
    _Object,
    live_method,
    live_method_gen,
)
from ._resolver import Resolver
from ._serialization import deserialize, serialize
from ._utils.async_utils import TaskContext, synchronize_api
from ._utils.deprecation import deprecation_warning, warn_if_passing_namespace
from ._utils.grpc_utils import retry_transient_errors
from ._utils.name_utils import check_object_name
from ._utils.time_utils import as_timestamp, timestamp_to_localized_dt
from .client import _Client
from .config import logger
from .exception import AlreadyExistsError, InvalidError, NotFoundError, RequestSizeError


class _NoDefaultSentinel:
    def __repr__(self) -> str:
        return "..."


_NO_DEFAULT = _NoDefaultSentinel()


def _serialize_dict(data):
    return [api_pb2.DictEntry(key=serialize(k), value=serialize(v)) for k, v in data.items()]


@dataclass
class DictInfo:
    """Information about a Dict object."""

    # This dataclass should be limited to information that is unchanging over the lifetime of the Dict,
    # since it is transmitted from the server when the object is hydrated and could be stale when accessed.

    name: Optional[str]
    created_at: datetime
    created_by: Optional[str]


class _DictManager:
    """Namespace with methods for managing named Dict objects."""

    @staticmethod
    async def create(
        name: str,  # Name to use for the new Dict
        *,
        allow_existing: bool = False,  # If True, no-op when the Dict already exists
        environment_name: Optional[str] = None,  # Uses active environment if not specified
        client: Optional[_Client] = None,  # Optional client with Modal credentials
    ) -> None:
        """Create a new Dict object.

        **Examples:**

        ```python notest
        modal.Dict.objects.create("my-dict")
        ```

        Dicts will be created in the active environment, or another one can be specified:

        ```python notest
        modal.Dict.objects.create("my-dict", environment_name="dev")
        ```

        By default, an error will be raised if the Dict already exists, but passing
        `allow_existing=True` will make the creation attempt a no-op in this case.

        ```python notest
        modal.Dict.objects.create("my-dict", allow_existing=True)
        ```

        Note that this method does not return a local instance of the Dict. You can use
        `modal.Dict.from_name` to perform a lookup after creation.

        Added in v1.1.2.

        """
        check_object_name(name, "Dict")
        client = await _Client.from_env() if client is None else client
        object_creation_type = (
            api_pb2.OBJECT_CREATION_TYPE_CREATE_IF_MISSING
            if allow_existing
            else api_pb2.OBJECT_CREATION_TYPE_CREATE_FAIL_IF_EXISTS
        )
        req = api_pb2.DictGetOrCreateRequest(
            deployment_name=name,
            environment_name=_get_environment_name(environment_name),
            object_creation_type=object_creation_type,
        )
        try:
            await retry_transient_errors(client.stub.DictGetOrCreate, req)
        except GRPCError as exc:
            if exc.status == Status.ALREADY_EXISTS and not allow_existing:
                raise AlreadyExistsError(exc.message)
            else:
                raise

    @staticmethod
    async def list(
        *,
        max_objects: Optional[int] = None,  # Limit results to this size
        created_before: Optional[Union[datetime, str]] = None,  # Limit based on creation date
        environment_name: str = "",  # Uses active environment if not specified
        client: Optional[_Client] = None,  # Optional client with Modal credentials
    ) -> list["_Dict"]:
        """Return a list of hydrated Dict objects.

        **Examples:**

        ```python
        dicts = modal.Dict.objects.list()
        print([d.name for d in dicts])
        ```

        Dicts will be retreived from the active environment, or another one can be specified:

        ```python notest
        dev_dicts = modal.Dict.objects.list(environment_name="dev")
        ```

        By default, all named Dict are returned, newest to oldest. It's also possible to limit the
        number of results and to filter by creation date:

        ```python
        dicts = modal.Dict.objects.list(max_objects=10, created_before="2025-01-01")
        ```

        Added in v1.1.2.

        """
        client = await _Client.from_env() if client is None else client
        if max_objects is not None and max_objects < 0:
            raise InvalidError("max_objects cannot be negative")

        items: list[api_pb2.DictListResponse.DictInfo] = []

        async def retrieve_page(created_before: float) -> bool:
            max_page_size = 100 if max_objects is None else min(100, max_objects - len(items))
            pagination = api_pb2.ListPagination(max_objects=max_page_size, created_before=created_before)
            req = api_pb2.DictListRequest(
                environment_name=_get_environment_name(environment_name), pagination=pagination
            )
            resp = await retry_transient_errors(client.stub.DictList, req)
            items.extend(resp.dicts)
            finished = (len(resp.dicts) < max_page_size) or (max_objects is not None and len(items) >= max_objects)
            return finished

        finished = await retrieve_page(as_timestamp(created_before))
        while True:
            if finished:
                break
            finished = await retrieve_page(items[-1].metadata.creation_info.created_at)

        dicts = [
            _Dict._new_hydrated(
                item.dict_id,
                client,
                item.metadata,
                is_another_app=True,
                rep=_Dict._repr(item.name, environment_name),
            )
            for item in items
        ]
        return dicts[:max_objects] if max_objects is not None else dicts

    @staticmethod
    async def delete(
        name: str,  # Name of the Dict to delete
        *,
        allow_missing: bool = False,  # If True, don't raise an error if the Dict doesn't exist
        environment_name: Optional[str] = None,  # Uses active environment if not specified
        client: Optional[_Client] = None,  # Optional client with Modal credentials
    ):
        """Delete a named Dict.

        Warning: This deletes an *entire Dict*, not just a specific key.
        Deletion is irreversible and will affect any Apps currently using the Dict.

        **Examples:**

        ```python notest
        await modal.Dict.objects.delete("my-dict")
        ```

        Dicts will be deleted from the active environment, or another one can be specified:

        ```python notest
        await modal.Dict.objects.delete("my-dict", environment_name="dev")
        ```

        Added in v1.1.2.

        """
        try:
            obj = await _Dict.from_name(name, environment_name=environment_name).hydrate(client)
        except NotFoundError:
            if not allow_missing:
                raise
        else:
            req = api_pb2.DictDeleteRequest(dict_id=obj.object_id)
            await retry_transient_errors(obj._client.stub.DictDelete, req)


DictManager = synchronize_api(_DictManager)


class _Dict(_Object, type_prefix="di"):
    """Distributed dictionary for storage in Modal apps.

    Dict contents can be essentially any object so long as they can be serialized by
    `cloudpickle`. This includes other Modal objects. If writing and reading in different
    environments (eg., writing locally and reading remotely), it's necessary to have the
    library defining the data type installed, with compatible versions, on both sides.
    Additionally, cloudpickle serialization is not guaranteed to be deterministic, so it is
    generally recommended to use primitive types for keys.

    **Lifetime of a Dict and its items**

    An individual Dict entry will expire after 7 days of inactivity (no reads or writes). The
    Dict entries are written to durable storage.

    Legacy Dicts (created before 2025-05-20) will still have entries expire 30 days after being
    last added. Additionally, contents are stored in memory on the Modal server and could be lost
    due to unexpected server restarts. Eventually, these Dicts will be fully sunset.

    **Usage**

    ```python
    from modal import Dict

    my_dict = Dict.from_name("my-persisted_dict", create_if_missing=True)

    my_dict["some key"] = "some value"
    my_dict[123] = 456

    assert my_dict["some key"] == "some value"
    assert my_dict[123] == 456
    ```

    The `Dict` class offers a few methods for operations that are usually accomplished
    in Python with operators, such as `Dict.put` and `Dict.contains`. The advantage of
    these methods is that they can be safely called in an asynchronous context by using
    the `.aio` suffix on the method, whereas their operator-based analogues will always
    run synchronously and block the event loop.

    For more examples, see the [guide](https://modal.com/docs/guide/dicts-and-queues#modal-dicts).
    """

    _name: Optional[str] = None
    _metadata: Optional[api_pb2.DictMetadata] = None

    def __init__(self, data={}):
        """mdmd:hidden"""
        raise RuntimeError(
            "`Dict(...)` constructor is not allowed. Please use `Dict.from_name` or `Dict.ephemeral` instead"
        )

    @classproperty
    def objects(cls) -> _DictManager:
        return _DictManager

    @property
    def name(self) -> Optional[str]:
        return self._name

    def _hydrate_metadata(self, metadata: Optional[Message]):
        if metadata:
            assert isinstance(metadata, api_pb2.DictMetadata)
            self._metadata = metadata
            self._name = metadata.name

    def _get_metadata(self) -> api_pb2.DictMetadata:
        assert self._metadata
        return self._metadata

    @classmethod
    @asynccontextmanager
    async def ephemeral(
        cls: type["_Dict"],
        data: Optional[dict] = None,  # DEPRECATED
        client: Optional[_Client] = None,
        environment_name: Optional[str] = None,
        _heartbeat_sleep: float = EPHEMERAL_OBJECT_HEARTBEAT_SLEEP,  # mdmd:line-hidden
    ) -> AsyncIterator["_Dict"]:
        """Creates a new ephemeral Dict within a context manager:

        Usage:
        ```python
        from modal import Dict

        with Dict.ephemeral() as d:
            d["foo"] = "bar"
        ```

        ```python notest
        async with Dict.ephemeral() as d:
            await d.put.aio("foo", "bar")
        ```
        """
        if client is None:
            client = await _Client.from_env()
        if data:
            deprecation_warning(
                (2025, 5, 6),
                "Passing data to `modal.Dict.ephemeral` is deprecated and will stop working in a future release.",
            )
        serialized = _serialize_dict(data if data is not None else {})
        request = api_pb2.DictGetOrCreateRequest(
            object_creation_type=api_pb2.OBJECT_CREATION_TYPE_EPHEMERAL,
            environment_name=_get_environment_name(environment_name),
            data=serialized,
        )
        response = await retry_transient_errors(client.stub.DictGetOrCreate, request, total_timeout=10.0)
        async with TaskContext() as tc:
            request = api_pb2.DictHeartbeatRequest(dict_id=response.dict_id)
            tc.infinite_loop(lambda: client.stub.DictHeartbeat(request), sleep=_heartbeat_sleep)
            yield cls._new_hydrated(
                response.dict_id,
                client,
                response.metadata,
                is_another_app=True,
                rep="modal.Dict.ephemeral()",
            )

    @staticmethod
    def from_name(
        name: str,
        data: Optional[dict] = None,  # DEPRECATED, mdmd:line-hidden
        *,
        namespace=None,  # mdmd:line-hidden
        environment_name: Optional[str] = None,
        create_if_missing: bool = False,
    ) -> "_Dict":
        """Reference a named Dict, creating if necessary.

        This is a lazy method that defers hydrating the local
        object with metadata from Modal servers until the first
        time it is actually used.

        ```python
        d = modal.Dict.from_name("my-dict", create_if_missing=True)
        d[123] = 456
        ```
        """
        check_object_name(name, "Dict")
        warn_if_passing_namespace(namespace, "modal.Dict.from_name")

        if data:
            deprecation_warning(
                (2025, 5, 6),
                "Passing data to `modal.Dict.from_name` is deprecated and will stop working in a future release.",
            )

        async def _load(self: _Dict, resolver: Resolver, existing_object_id: Optional[str]):
            serialized = _serialize_dict(data if data is not None else {})
            req = api_pb2.DictGetOrCreateRequest(
                deployment_name=name,
                environment_name=_get_environment_name(environment_name, resolver),
                object_creation_type=(api_pb2.OBJECT_CREATION_TYPE_CREATE_IF_MISSING if create_if_missing else None),
                data=serialized,
            )
            response = await resolver.client.stub.DictGetOrCreate(req)
            logger.debug(f"Created dict with id {response.dict_id}")
            self._hydrate(response.dict_id, resolver.client, response.metadata)

        rep = _Dict._repr(name, environment_name)
        return _Dict._from_loader(_load, rep, is_another_app=True, hydrate_lazily=True, name=name)

    @staticmethod
    async def delete(
        name: str,
        *,
        client: Optional[_Client] = None,
        environment_name: Optional[str] = None,
    ):
        """mdmd:hidden
        Delete a named Dict object.

        Warning: This deletes an *entire Dict*, not just a specific key.
        Deletion is irreversible and will affect any Apps currently using the Dict.

        DEPRECATED: This method is deprecated; we recommend using `modal.Dict.objects.delete` instead.
        """
        deprecation_warning(
            (2025, 8, 6), "`modal.Dict.delete` is deprecated; we recommend using `modal.Dict.objects.delete` instead."
        )
        await _Dict.objects.delete(name, environment_name=environment_name, client=client)

    @live_method
    async def info(self) -> DictInfo:
        """Return information about the Dict object."""
        metadata = self._get_metadata()
        creation_info = metadata.creation_info
        return DictInfo(
            name=metadata.name or None,
            created_at=timestamp_to_localized_dt(creation_info.created_at),
            created_by=creation_info.created_by or None,
        )

    @live_method
    async def clear(self) -> None:
        """Remove all items from the Dict."""
        req = api_pb2.DictClearRequest(dict_id=self.object_id)
        await retry_transient_errors(self._client.stub.DictClear, req)

    @live_method
    async def get(self, key: Any, default: Optional[Any] = None) -> Any:
        """Get the value associated with a key.

        Returns `default` if key does not exist.
        """
        req = api_pb2.DictGetRequest(dict_id=self.object_id, key=serialize(key))
        resp = await retry_transient_errors(self._client.stub.DictGet, req)
        if not resp.found:
            return default
        return deserialize(resp.value, self._client)

    @live_method
    async def contains(self, key: Any) -> bool:
        """Return if a key is present."""
        req = api_pb2.DictContainsRequest(dict_id=self.object_id, key=serialize(key))
        resp = await retry_transient_errors(self._client.stub.DictContains, req)
        return resp.found

    @live_method
    async def len(self) -> int:
        """Return the length of the Dict.

        Note: This is an expensive operation and will return at most 100,000.
        """
        req = api_pb2.DictLenRequest(dict_id=self.object_id)
        resp = await retry_transient_errors(self._client.stub.DictLen, req)
        return resp.len

    @live_method
    async def __getitem__(self, key: Any) -> Any:
        """Get the value associated with a key.

        Note: this function will block the event loop when called in an async context.
        """
        NOT_FOUND = object()
        value = await self.get(key, NOT_FOUND)
        if value is NOT_FOUND:
            raise KeyError(f"{key} not in dict {self.object_id}")

        return value

    @live_method
    async def update(self, other: Optional[Mapping] = None, /, **kwargs) -> None:
        """Update the Dict with additional items."""
        # Support the Python dict.update API
        # https://docs.python.org/3/library/stdtypes.html#dict.update
        contents = {}
        if other:
            contents.update({k: other[k] for k in other.keys()})
        if kwargs:
            contents.update(kwargs)
        serialized = _serialize_dict(contents)
        req = api_pb2.DictUpdateRequest(dict_id=self.object_id, updates=serialized)
        try:
            await retry_transient_errors(self._client.stub.DictUpdate, req)
        except GRPCError as exc:
            if "status = '413'" in exc.message:
                raise RequestSizeError("Dict.update request is too large") from exc
            else:
                raise exc

    @live_method
    async def put(self, key: Any, value: Any, *, skip_if_exists: bool = False) -> bool:
        """Add a specific key-value pair to the Dict.

        Returns True if the key-value pair was added and False if it wasn't because the key already existed and
        `skip_if_exists` was set.
        """
        updates = {key: value}
        serialized = _serialize_dict(updates)
        req = api_pb2.DictUpdateRequest(dict_id=self.object_id, updates=serialized, if_not_exists=skip_if_exists)
        try:
            resp = await retry_transient_errors(self._client.stub.DictUpdate, req)
            return resp.created
        except GRPCError as exc:
            if "status = '413'" in exc.message:
                raise RequestSizeError("Dict.put request is too large") from exc
            else:
                raise exc

    @live_method
    async def __setitem__(self, key: Any, value: Any) -> None:
        """Set a specific key-value pair to the Dict.

        Note: this function will block the event loop when called in an async context.
        """
        return await self.put(key, value)

    @live_method
    async def pop(self, key: Any, default: Any = _NO_DEFAULT) -> Any:
        """Remove a key from the Dict, returning the value if it exists.

        If key is not found, return default if provided, otherwise raise KeyError.
        """
        req = api_pb2.DictPopRequest(dict_id=self.object_id, key=serialize(key))
        resp = await retry_transient_errors(self._client.stub.DictPop, req)
        if not resp.found:
            if default is not _NO_DEFAULT:
                return default
            raise KeyError(f"{key} not in dict {self.object_id}")
        return deserialize(resp.value, self._client)

    @live_method
    async def __delitem__(self, key: Any) -> Any:
        """Delete a key from the Dict.

        Note: this function will block the event loop when called in an async context.
        """
        return await self.pop(key)

    @live_method
    async def __contains__(self, key: Any) -> bool:
        """Return if a key is present.

        Note: this function will block the event loop when called in an async context.
        """
        return await self.contains(key)

    @live_method_gen
    async def keys(self) -> AsyncIterator[Any]:
        """Return an iterator over the keys in this Dict.

        Note that (unlike with Python dicts) the return value is a simple iterator,
        and results are unordered.
        """
        req = api_pb2.DictContentsRequest(dict_id=self.object_id, keys=True)
        async for resp in self._client.stub.DictContents.unary_stream(req):
            yield deserialize(resp.key, self._client)

    @live_method_gen
    async def values(self) -> AsyncIterator[Any]:
        """Return an iterator over the values in this Dict.

        Note that (unlike with Python dicts) the return value is a simple iterator,
        and results are unordered.
        """
        req = api_pb2.DictContentsRequest(dict_id=self.object_id, values=True)
        async for resp in self._client.stub.DictContents.unary_stream(req):
            yield deserialize(resp.value, self._client)

    @live_method_gen
    async def items(self) -> AsyncIterator[tuple[Any, Any]]:
        """Return an iterator over the (key, value) tuples in this Dict.

        Note that (unlike with Python dicts) the return value is a simple iterator,
        and results are unordered.
        """
        req = api_pb2.DictContentsRequest(dict_id=self.object_id, keys=True, values=True)
        async for resp in self._client.stub.DictContents.unary_stream(req):
            yield (deserialize(resp.key, self._client), deserialize(resp.value, self._client))


Dict = synchronize_api(_Dict)



================================================
FILE: modal/environments.py
================================================
# Copyright Modal Labs 2023
from dataclasses import dataclass
from typing import Optional

from google.protobuf.empty_pb2 import Empty
from google.protobuf.message import Message
from google.protobuf.wrappers_pb2 import StringValue

from modal_proto import api_pb2

from ._object import _Object
from ._resolver import Resolver
from ._utils.async_utils import synchronize_api, synchronizer
from ._utils.grpc_utils import retry_transient_errors
from ._utils.name_utils import check_object_name
from .client import _Client
from .config import config, logger


@dataclass(frozen=True)
class EnvironmentSettings:
    image_builder_version: str  # Ideally would be typed with ImageBuilderVersion literal
    webhook_suffix: str


class _Environment(_Object, type_prefix="en"):
    _settings: EnvironmentSettings

    def __init__(self):
        """mdmd:hidden"""
        raise RuntimeError("`Environment(...)` constructor is not allowed. Please use `Environment.from_name` instead.")

    # TODO(michael) Keeping this private for now until we decide what else should be in it
    # And what the rules should be about updates / mutability
    # @property
    # def settings(self) -> EnvironmentSettings:
    #     return self._settings

    def _hydrate_metadata(self, metadata: Message):
        # Overridden concrete implementation of base class method
        assert metadata and isinstance(metadata, api_pb2.EnvironmentMetadata)
        # TODO(michael) should probably expose the `name` from the metadata
        # as the way to discover the name of the "default" environment

        # Is there a simpler way to go Message -> Dataclass?
        self._settings = EnvironmentSettings(
            image_builder_version=metadata.settings.image_builder_version,
            webhook_suffix=metadata.settings.webhook_suffix,
        )

    @staticmethod
    def from_name(
        name: str,
        *,
        create_if_missing: bool = False,
    ):
        if name:
            # Allow null names for the case where we want to look up the "default" environment,
            # which is defined by the server. It feels messy to have "from_name" without a name, though?
            # We're adding this mostly for internal use right now. We could consider an environment-only
            # alternate constructor, like `Environment.get_default`, rather than exposing "unnamed"
            # environments as part of public API when we make this class more useful.
            check_object_name(name, "Environment")

        async def _load(self: _Environment, resolver: Resolver, existing_object_id: Optional[str]):
            request = api_pb2.EnvironmentGetOrCreateRequest(
                deployment_name=name,
                object_creation_type=(
                    api_pb2.OBJECT_CREATION_TYPE_CREATE_IF_MISSING
                    if create_if_missing
                    else api_pb2.OBJECT_CREATION_TYPE_UNSPECIFIED
                ),
            )
            response = await retry_transient_errors(resolver.client.stub.EnvironmentGetOrCreate, request)
            logger.debug(f"Created environment with id {response.environment_id}")
            self._hydrate(response.environment_id, resolver.client, response.metadata)

        # TODO environment name (and id?) in the repr? (We should make reprs consistently more useful)
        return _Environment._from_loader(_load, "Environment()", is_another_app=True, hydrate_lazily=True)


Environment = synchronize_api(_Environment)


# Needs to be after definition; synchronicity interferes with forward references?
ENVIRONMENT_CACHE: dict[str, _Environment] = {}


async def _get_environment_cached(name: str, client: _Client) -> _Environment:
    if name in ENVIRONMENT_CACHE:
        return ENVIRONMENT_CACHE[name]
    environment = await _Environment.from_name(name).hydrate(client)
    ENVIRONMENT_CACHE[name] = environment
    return environment


@synchronizer.create_blocking
async def delete_environment(name: str, client: Optional[_Client] = None):
    if client is None:
        client = await _Client.from_env()
    await client.stub.EnvironmentDelete(api_pb2.EnvironmentDeleteRequest(name=name))


@synchronizer.create_blocking
async def update_environment(
    current_name: str,
    *,
    new_name: Optional[str] = None,
    new_web_suffix: Optional[str] = None,
    client: Optional[_Client] = None,
):
    new_name_pb2 = None
    new_web_suffix_pb2 = None
    if new_name is not None:
        if len(new_name) < 1:
            raise ValueError("The new environment name cannot be empty")

        new_name_pb2 = StringValue(value=new_name)

    if new_web_suffix is not None:
        new_web_suffix_pb2 = StringValue(value=new_web_suffix)

    update_payload = api_pb2.EnvironmentUpdateRequest(
        current_name=current_name, name=new_name_pb2, web_suffix=new_web_suffix_pb2
    )
    if client is None:
        client = await _Client.from_env()
    await client.stub.EnvironmentUpdate(update_payload)


@synchronizer.create_blocking
async def create_environment(name: str, client: Optional[_Client] = None):
    if client is None:
        client = await _Client.from_env()
    await client.stub.EnvironmentCreate(api_pb2.EnvironmentCreateRequest(name=name))


@synchronizer.create_blocking
async def list_environments(client: Optional[_Client] = None) -> list[api_pb2.EnvironmentListItem]:
    if client is None:
        client = await _Client.from_env()
    resp = await client.stub.EnvironmentList(Empty())
    return list(resp.items)


def ensure_env(environment_name: Optional[str] = None) -> str:
    """Override config environment with environment from environment_name

    This is necessary since a cli command that runs Modal code, without explicit
    environment specification wouldn't pick up the environment specified in a
    command line flag otherwise, e.g. when doing `modal run --env=foo`
    """
    if environment_name is not None:
        config.override_locally("environment", environment_name)

    return config.get("environment")



================================================
FILE: modal/exception.py
================================================
# Copyright Modal Labs 2022
import random
import signal

import synchronicity.exceptions

UserCodeException = synchronicity.exceptions.UserCodeException  # Deprecated type used for return_exception wrapping


class Error(Exception):
    """
    Base class for all Modal errors. See [`modal.exception`](https://modal.com/docs/reference/modal.exception)
    for the specialized error classes.

    **Usage**

    ```python notest
    import modal

    try:
        ...
    except modal.Error:
        # Catch any exception raised by Modal's systems.
        print("Responding to error...")
    ```
    """


class AlreadyExistsError(Error):
    """Raised when a resource creation conflicts with an existing resource."""


class RemoteError(Error):
    """Raised when an error occurs on the Modal server."""


class TimeoutError(Error):
    """Base class for Modal timeouts."""


class SandboxTimeoutError(TimeoutError):
    """Raised when a Sandbox exceeds its execution duration limit and times out."""


class SandboxTerminatedError(Error):
    """Raised when a Sandbox is terminated for an internal reason."""


class FunctionTimeoutError(TimeoutError):
    """Raised when a Function exceeds its execution duration limit and times out."""


class MountUploadTimeoutError(TimeoutError):
    """Raised when a Mount upload times out."""


class VolumeUploadTimeoutError(TimeoutError):
    """Raised when a Volume upload times out."""


class InteractiveTimeoutError(TimeoutError):
    """Raised when interactive frontends time out while trying to connect to a container."""


class OutputExpiredError(TimeoutError):
    """Raised when the Output exceeds expiration and times out."""


class AuthError(Error):
    """Raised when a client has missing or invalid authentication."""


class ConnectionError(Error):
    """Raised when an issue occurs while connecting to the Modal servers."""


class InvalidError(Error):
    """Raised when user does something invalid."""


class VersionError(Error):
    """Raised when the current client version of Modal is unsupported."""


class NotFoundError(Error):
    """Raised when a requested resource was not found."""


class ExecutionError(Error):
    """Raised when something unexpected happened during runtime."""


class DeserializationError(Error):
    """Raised to provide more context when an error is encountered during deserialization."""


class SerializationError(Error):
    """Raised to provide more context when an error is encountered during serialization."""


class RequestSizeError(Error):
    """Raised when an operation produces a gRPC request that is rejected by the server for being too large."""


class DeprecationError(UserWarning):
    """UserWarning category emitted when a deprecated Modal feature or API is used."""

    # Overloading it to evade the default filter, which excludes __main__.


class PendingDeprecationError(UserWarning):
    """Soon to be deprecated feature. Only used intermittently because of multi-repo concerns."""


class ServerWarning(UserWarning):
    """Warning originating from the Modal server and re-issued in client code."""


class InternalFailure(Error):
    """
    Retriable internal error.
    """


class _CliUserExecutionError(Exception):
    """mdmd:hidden
    Private wrapper for exceptions during when importing or running Apps from the CLI.

    This intentionally does not inherit from `modal.exception.Error` because it
    is a private type that should never bubble up to users. Exceptions raised in
    the CLI at this stage will have tracebacks printed.
    """

    def __init__(self, user_source: str):
        # `user_source` should be the filepath for the user code that is the source of the exception.
        # This is used by our exception handler to show the traceback starting from that point.
        self.user_source = user_source


def _simulate_preemption_interrupt(signum, frame):
    signal.alarm(30)  # simulate a SIGKILL after 30s
    raise KeyboardInterrupt("Simulated preemption interrupt from modal-client!")


def simulate_preemption(wait_seconds: int, jitter_seconds: int = 0):
    """
    Utility for simulating a preemption interrupt after `wait_seconds` seconds.
    The first interrupt is the SIGINT signal. After 30 seconds, a second
    interrupt will trigger.

    This second interrupt simulates SIGKILL, and should not be caught.
    Optionally add between zero and `jitter_seconds` seconds of additional waiting before first interrupt.

    **Usage:**

    ```python notest
    import time
    from modal.exception import simulate_preemption

    simulate_preemption(3)

    try:
        time.sleep(4)
    except KeyboardInterrupt:
        print("got preempted") # Handle interrupt
        raise
    ```

    See https://modal.com/docs/guide/preemption for more details on preemption
    handling.
    """
    if wait_seconds <= 0:
        raise ValueError("Time to wait must be greater than 0")
    signal.signal(signal.SIGALRM, _simulate_preemption_interrupt)
    jitter = random.randrange(0, jitter_seconds) if jitter_seconds else 0
    signal.alarm(wait_seconds + jitter)


class InputCancellation(BaseException):
    """Raised when the current input is cancelled by the task

    Intentionally a BaseException instead of an Exception, so it won't get
    caught by unspecified user exception clauses that might be used for retries and
    other control flow.
    """


class ModuleNotMountable(Exception):
    pass


class ClientClosed(Error):
    pass


class FilesystemExecutionError(Error):
    """Raised when an unknown error is thrown during a container filesystem operation."""



================================================
FILE: modal/file_io.py
================================================
# Copyright Modal Labs 2024
import asyncio
import enum
import io
from dataclasses import dataclass
from typing import TYPE_CHECKING, AsyncIterator, Generic, Optional, Sequence, TypeVar, Union, cast

if TYPE_CHECKING:
    import _typeshed

import json

from grpclib.exceptions import GRPCError, StreamTerminatedError

from modal._utils.async_utils import TaskContext
from modal._utils.grpc_utils import retry_transient_errors
from modal.exception import ClientClosed
from modal_proto import api_pb2

from ._utils.async_utils import synchronize_api
from ._utils.grpc_utils import RETRYABLE_GRPC_STATUS_CODES
from .client import _Client
from .exception import FilesystemExecutionError, InvalidError

WRITE_CHUNK_SIZE = 16 * 1024 * 1024  # 16 MiB
WRITE_FILE_SIZE_LIMIT = 1024 * 1024 * 1024  # 1 GiB
READ_FILE_SIZE_LIMIT = 100 * 1024 * 1024  # 100 MiB

ERROR_MAPPING = {
    api_pb2.SystemErrorCode.SYSTEM_ERROR_CODE_UNSPECIFIED: FilesystemExecutionError,
    api_pb2.SystemErrorCode.SYSTEM_ERROR_CODE_PERM: PermissionError,
    api_pb2.SystemErrorCode.SYSTEM_ERROR_CODE_NOENT: FileNotFoundError,
    api_pb2.SystemErrorCode.SYSTEM_ERROR_CODE_IO: IOError,
    api_pb2.SystemErrorCode.SYSTEM_ERROR_CODE_NXIO: IOError,
    api_pb2.SystemErrorCode.SYSTEM_ERROR_CODE_NOMEM: MemoryError,
    api_pb2.SystemErrorCode.SYSTEM_ERROR_CODE_ACCES: PermissionError,
    api_pb2.SystemErrorCode.SYSTEM_ERROR_CODE_EXIST: FileExistsError,
    api_pb2.SystemErrorCode.SYSTEM_ERROR_CODE_NOTDIR: NotADirectoryError,
    api_pb2.SystemErrorCode.SYSTEM_ERROR_CODE_ISDIR: IsADirectoryError,
    api_pb2.SystemErrorCode.SYSTEM_ERROR_CODE_INVAL: OSError,
    api_pb2.SystemErrorCode.SYSTEM_ERROR_CODE_MFILE: OSError,
    api_pb2.SystemErrorCode.SYSTEM_ERROR_CODE_FBIG: OSError,
    api_pb2.SystemErrorCode.SYSTEM_ERROR_CODE_NOSPC: OSError,
}

T = TypeVar("T", str, bytes)


async def _delete_bytes(file: "_FileIO", start: Optional[int] = None, end: Optional[int] = None) -> None:
    """Delete a range of bytes from the file.

    `start` and `end` are byte offsets. `start` is inclusive, `end` is exclusive.
    If either is None, the start or end of the file is used, respectively.
    """
    assert file._file_descriptor is not None
    file._check_closed()
    if start is not None and end is not None:
        if start >= end:
            raise ValueError("start must be less than end")
    resp = await retry_transient_errors(
        file._client.stub.ContainerFilesystemExec,
        api_pb2.ContainerFilesystemExecRequest(
            file_delete_bytes_request=api_pb2.ContainerFileDeleteBytesRequest(
                file_descriptor=file._file_descriptor,
                start_inclusive=start,
                end_exclusive=end,
            ),
            task_id=file._task_id,
        ),
    )
    await file._wait(resp.exec_id)


async def _replace_bytes(file: "_FileIO", data: bytes, start: Optional[int] = None, end: Optional[int] = None) -> None:
    """Replace a range of bytes in the file with new data. The length of the data does not
    have to be the same as the length of the range being replaced.

    `start` and `end` are byte offsets. `start` is inclusive, `end` is exclusive.
    If either is None, the start or end of the file is used, respectively.
    """
    assert file._file_descriptor is not None
    file._check_closed()
    if start is not None and end is not None:
        if start >= end:
            raise InvalidError("start must be less than end")
    if len(data) > WRITE_CHUNK_SIZE:
        raise InvalidError("Write request payload exceeds 16 MiB limit")
    resp = await retry_transient_errors(
        file._client.stub.ContainerFilesystemExec,
        api_pb2.ContainerFilesystemExecRequest(
            file_write_replace_bytes_request=api_pb2.ContainerFileWriteReplaceBytesRequest(
                file_descriptor=file._file_descriptor,
                data=data,
                start_inclusive=start,
                end_exclusive=end,
            ),
            task_id=file._task_id,
        ),
    )
    await file._wait(resp.exec_id)


class FileWatchEventType(enum.Enum):
    Unknown = "Unknown"
    Access = "Access"
    Create = "Create"
    Modify = "Modify"
    Remove = "Remove"


@dataclass
class FileWatchEvent:
    paths: list[str]
    type: FileWatchEventType


# The FileIO class is designed to mimic Python's io.FileIO
# See https://github.com/python/cpython/blob/main/Lib/_pyio.py#L1459
class _FileIO(Generic[T]):
    """[Alpha] FileIO handle, used in the Sandbox filesystem API.

    The API is designed to mimic Python's io.FileIO.

    Currently this API is in Alpha and is subject to change. File I/O operations
    may be limited in size to 100 MiB, and the throughput of requests is
    restricted in the current implementation. For our recommendations on large file transfers
    see the Sandbox [filesystem access guide](https://modal.com/docs/guide/sandbox-files).

    **Usage**

    ```python notest
    import modal

    app = modal.App.lookup("my-app", create_if_missing=True)

    sb = modal.Sandbox.create(app=app)
    f = sb.open("/tmp/foo.txt", "w")
    f.write("hello")
    f.close()
    ```
    """

    _binary = False
    _readable = False
    _writable = False
    _appended = False
    _closed = True

    _task_id: str = ""
    _file_descriptor: str = ""
    _client: _Client
    _watch_output_buffer: list[Union[Optional[bytes], Exception]] = []

    def __init__(self, client: _Client, task_id: str) -> None:
        self._client = client
        self._task_id = task_id
        self._watch_output_buffer = []

    def _validate_mode(self, mode: str) -> None:
        if not any(char in mode for char in "rwax"):
            raise ValueError(f"Invalid file mode: {mode}")

        self._readable = "r" in mode or "+" in mode
        self._writable = "w" in mode or "a" in mode or "x" in mode or "+" in mode
        self._appended = "a" in mode
        self._binary = "b" in mode

        valid_chars = set("rwaxb+")
        if any(char not in valid_chars for char in mode):
            raise ValueError(f"Invalid file mode: {mode}")

        mode_count = sum(1 for c in mode if c in "rwax")
        if mode_count > 1:
            raise ValueError("must have exactly one of create/read/write/append mode")

        seen_chars = set()
        for char in mode:
            if char in seen_chars:
                raise ValueError(f"Invalid file mode: {mode}")
            seen_chars.add(char)

    async def _consume_output(self, exec_id: str) -> AsyncIterator[Union[Optional[bytes], Exception]]:
        req = api_pb2.ContainerFilesystemExecGetOutputRequest(
            exec_id=exec_id,
            timeout=55,
        )
        async for batch in self._client.stub.ContainerFilesystemExecGetOutput.unary_stream(req):
            if batch.eof:
                yield None
                break
            if batch.HasField("error"):
                error_class = ERROR_MAPPING.get(batch.error.error_code, FilesystemExecutionError)
                yield error_class(batch.error.error_message)
            for message in batch.output:
                yield message

    async def _consume_watch_output(self, exec_id: str) -> None:
        completed = False
        retries_remaining = 10
        while not completed:
            try:
                iterator = self._consume_output(exec_id)
                async for message in iterator:
                    self._watch_output_buffer.append(message)
                    if message is None:
                        completed = True
                        break

            except (GRPCError, StreamTerminatedError, ClientClosed) as exc:
                if retries_remaining > 0:
                    retries_remaining -= 1
                    if isinstance(exc, GRPCError):
                        if exc.status in RETRYABLE_GRPC_STATUS_CODES:
                            await asyncio.sleep(1.0)
                            continue
                    elif isinstance(exc, StreamTerminatedError):
                        continue
                    elif isinstance(exc, ClientClosed):
                        # If the client was closed, the user has triggered a cleanup.
                        break
                raise exc

    async def _parse_watch_output(self, event: bytes) -> Optional[FileWatchEvent]:
        try:
            event_json = json.loads(event.decode())
            return FileWatchEvent(type=FileWatchEventType(event_json["event_type"]), paths=event_json["paths"])
        except (json.JSONDecodeError, KeyError, ValueError):
            # skip invalid events
            return None

    async def _wait(self, exec_id: str) -> bytes:
        # The logic here is similar to how output is read from `exec`
        output = b""
        completed = False
        retries_remaining = 10
        while not completed:
            try:
                async for data in self._consume_output(exec_id):
                    if data is None:
                        completed = True
                        break
                    if isinstance(data, Exception):
                        raise data
                    output += data
            except (GRPCError, StreamTerminatedError) as exc:
                if retries_remaining > 0:
                    retries_remaining -= 1
                    if isinstance(exc, GRPCError):
                        if exc.status in RETRYABLE_GRPC_STATUS_CODES:
                            await asyncio.sleep(1.0)
                            continue
                    elif isinstance(exc, StreamTerminatedError):
                        continue
                raise
        return output

    def _validate_type(self, data: Union[bytes, str]) -> None:
        if self._binary and isinstance(data, str):
            raise TypeError("Expected bytes when in binary mode")
        if not self._binary and isinstance(data, bytes):
            raise TypeError("Expected str when in text mode")

    async def _open_file(self, path: str, mode: str) -> None:
        resp = await retry_transient_errors(
            self._client.stub.ContainerFilesystemExec,
            api_pb2.ContainerFilesystemExecRequest(
                file_open_request=api_pb2.ContainerFileOpenRequest(path=path, mode=mode),
                task_id=self._task_id,
            ),
        )
        if not resp.HasField("file_descriptor"):
            raise FilesystemExecutionError("Failed to open file")
        self._file_descriptor = resp.file_descriptor
        await self._wait(resp.exec_id)

    @classmethod
    async def create(
        cls, path: str, mode: Union["_typeshed.OpenTextMode", "_typeshed.OpenBinaryMode"], client: _Client, task_id: str
    ) -> "_FileIO":
        """Create a new FileIO handle."""
        self = _FileIO(client, task_id)
        self._validate_mode(mode)
        await self._open_file(path, mode)
        self._closed = False
        return self

    async def _make_read_request(self, n: Optional[int]) -> bytes:
        resp = await retry_transient_errors(
            self._client.stub.ContainerFilesystemExec,
            api_pb2.ContainerFilesystemExecRequest(
                file_read_request=api_pb2.ContainerFileReadRequest(file_descriptor=self._file_descriptor, n=n),
                task_id=self._task_id,
            ),
        )
        return await self._wait(resp.exec_id)

    async def read(self, n: Optional[int] = None) -> T:
        """Read n bytes from the current position, or the entire remaining file if n is None."""
        self._check_closed()
        self._check_readable()
        if n is not None and n > READ_FILE_SIZE_LIMIT:
            raise ValueError("Read request payload exceeds 100 MiB limit")
        output = await self._make_read_request(n)
        if self._binary:
            return cast(T, output)
        return cast(T, output.decode("utf-8"))

    async def readline(self) -> T:
        """Read a single line from the current position."""
        self._check_closed()
        self._check_readable()
        resp = await retry_transient_errors(
            self._client.stub.ContainerFilesystemExec,
            api_pb2.ContainerFilesystemExecRequest(
                file_read_line_request=api_pb2.ContainerFileReadLineRequest(file_descriptor=self._file_descriptor),
                task_id=self._task_id,
            ),
        )
        output = await self._wait(resp.exec_id)
        if self._binary:
            return cast(T, output)
        return cast(T, output.decode("utf-8"))

    async def readlines(self) -> Sequence[T]:
        """Read all lines from the current position."""
        self._check_closed()
        self._check_readable()
        output = await self._make_read_request(None)
        if self._binary:
            lines_bytes = output.split(b"\n")
            return_bytes = [line + b"\n" for line in lines_bytes[:-1]] + ([lines_bytes[-1]] if lines_bytes[-1] else [])
            return cast(Sequence[T], return_bytes)
        else:
            lines = output.decode("utf-8").split("\n")
            return_strs = [line + "\n" for line in lines[:-1]] + ([lines[-1]] if lines[-1] else [])
            return cast(Sequence[T], return_strs)

    async def write(self, data: Union[bytes, str]) -> None:
        """Write data to the current position.

        Writes may not appear until the entire buffer is flushed, which
        can be done manually with `flush()` or automatically when the file is
        closed.
        """
        self._check_closed()
        self._check_writable()
        self._validate_type(data)
        if isinstance(data, str):
            data = data.encode("utf-8")
        if len(data) > WRITE_FILE_SIZE_LIMIT:
            raise ValueError("Write request payload exceeds 1 GiB limit")
        for i in range(0, len(data), WRITE_CHUNK_SIZE):
            chunk = data[i : i + WRITE_CHUNK_SIZE]
            resp = await retry_transient_errors(
                self._client.stub.ContainerFilesystemExec,
                api_pb2.ContainerFilesystemExecRequest(
                    file_write_request=api_pb2.ContainerFileWriteRequest(
                        file_descriptor=self._file_descriptor,
                        data=chunk,
                    ),
                    task_id=self._task_id,
                ),
            )
            await self._wait(resp.exec_id)

    async def flush(self) -> None:
        """Flush the buffer to disk."""
        self._check_closed()
        self._check_writable()
        resp = await retry_transient_errors(
            self._client.stub.ContainerFilesystemExec,
            api_pb2.ContainerFilesystemExecRequest(
                file_flush_request=api_pb2.ContainerFileFlushRequest(file_descriptor=self._file_descriptor),
                task_id=self._task_id,
            ),
        )
        await self._wait(resp.exec_id)

    def _get_whence(self, whence: int):
        if whence == 0:
            return api_pb2.SeekWhence.SEEK_SET
        elif whence == 1:
            return api_pb2.SeekWhence.SEEK_CUR
        elif whence == 2:
            return api_pb2.SeekWhence.SEEK_END
        else:
            raise ValueError(f"Invalid whence value: {whence}")

    async def seek(self, offset: int, whence: int = 0) -> None:
        """Move to a new position in the file.

        `whence` defaults to 0 (absolute file positioning); other values are 1
        (relative to the current position) and 2 (relative to the file's end).
        """
        self._check_closed()
        resp = await retry_transient_errors(
            self._client.stub.ContainerFilesystemExec,
            api_pb2.ContainerFilesystemExecRequest(
                file_seek_request=api_pb2.ContainerFileSeekRequest(
                    file_descriptor=self._file_descriptor,
                    offset=offset,
                    whence=self._get_whence(whence),
                ),
                task_id=self._task_id,
            ),
        )
        await self._wait(resp.exec_id)

    @classmethod
    async def ls(cls, path: str, client: _Client, task_id: str) -> list[str]:
        """List the contents of the provided directory."""
        self = _FileIO(client, task_id)
        resp = await retry_transient_errors(
            self._client.stub.ContainerFilesystemExec,
            api_pb2.ContainerFilesystemExecRequest(
                file_ls_request=api_pb2.ContainerFileLsRequest(path=path),
                task_id=task_id,
            ),
        )
        output = await self._wait(resp.exec_id)
        try:
            return json.loads(output.decode("utf-8"))["paths"]
        except json.JSONDecodeError:
            raise FilesystemExecutionError("failed to parse list output")

    @classmethod
    async def mkdir(cls, path: str, client: _Client, task_id: str, parents: bool = False) -> None:
        """Create a new directory."""
        self = _FileIO(client, task_id)
        resp = await retry_transient_errors(
            self._client.stub.ContainerFilesystemExec,
            api_pb2.ContainerFilesystemExecRequest(
                file_mkdir_request=api_pb2.ContainerFileMkdirRequest(path=path, make_parents=parents),
                task_id=self._task_id,
            ),
        )
        await self._wait(resp.exec_id)

    @classmethod
    async def rm(cls, path: str, client: _Client, task_id: str, recursive: bool = False) -> None:
        """Remove a file or directory in the Sandbox."""
        self = _FileIO(client, task_id)
        resp = await retry_transient_errors(
            self._client.stub.ContainerFilesystemExec,
            api_pb2.ContainerFilesystemExecRequest(
                file_rm_request=api_pb2.ContainerFileRmRequest(path=path, recursive=recursive),
                task_id=self._task_id,
            ),
        )
        await self._wait(resp.exec_id)

    @classmethod
    async def watch(
        cls,
        path: str,
        client: _Client,
        task_id: str,
        filter: Optional[list[FileWatchEventType]] = None,
        recursive: bool = False,
        timeout: Optional[int] = None,
    ) -> AsyncIterator[FileWatchEvent]:
        self = _FileIO(client, task_id)
        resp = await retry_transient_errors(
            self._client.stub.ContainerFilesystemExec,
            api_pb2.ContainerFilesystemExecRequest(
                file_watch_request=api_pb2.ContainerFileWatchRequest(
                    path=path,
                    recursive=recursive,
                    timeout_secs=timeout,
                ),
                task_id=self._task_id,
            ),
        )
        async with TaskContext() as tc:
            tc.create_task(self._consume_watch_output(resp.exec_id))

            buffer = b""
            while True:
                if len(self._watch_output_buffer) > 0:
                    item = self._watch_output_buffer.pop(0)
                    if item is None:
                        break
                    if isinstance(item, Exception):
                        raise item
                    buffer += item
                    # a single event may be split across multiple messages
                    # the end of an event is marked by two newlines
                    if buffer.endswith(b"\n\n"):
                        try:
                            event_json = json.loads(buffer.strip().decode())
                            event = FileWatchEvent(
                                type=FileWatchEventType(event_json["event_type"]),
                                paths=event_json["paths"],
                            )
                            if not filter or event.type in filter:
                                yield event
                        except (json.JSONDecodeError, KeyError, ValueError):
                            # skip invalid events
                            pass
                        buffer = b""
                else:
                    await asyncio.sleep(0.1)

    async def _close(self) -> None:
        # Buffer is flushed by the runner on close
        resp = await retry_transient_errors(
            self._client.stub.ContainerFilesystemExec,
            api_pb2.ContainerFilesystemExecRequest(
                file_close_request=api_pb2.ContainerFileCloseRequest(file_descriptor=self._file_descriptor),
                task_id=self._task_id,
            ),
        )
        self._closed = True
        await self._wait(resp.exec_id)

    async def close(self) -> None:
        """Flush the buffer and close the file."""
        await self._close()

    # also validated in the runner, but checked in the client to catch errors early
    def _check_writable(self) -> None:
        if not self._writable:
            raise io.UnsupportedOperation("not writeable")

    # also validated in the runner, but checked in the client to catch errors early
    def _check_readable(self) -> None:
        if not self._readable:
            raise io.UnsupportedOperation("not readable")

    # also validated in the runner, but checked in the client to catch errors early
    def _check_closed(self) -> None:
        if self._closed:
            raise ValueError("I/O operation on closed file")

    async def __aenter__(self) -> "_FileIO":
        return self

    async def __aexit__(self, exc_type, exc_value, traceback) -> None:
        await self._close()


delete_bytes = synchronize_api(_delete_bytes)
replace_bytes = synchronize_api(_replace_bytes)
FileIO = synchronize_api(_FileIO)



================================================
FILE: modal/file_pattern_matcher.py
================================================
# Copyright Modal Labs 2024
"""Pattern matching library ported from https://github.com/moby/patternmatcher.

This is the same pattern-matching logic used by Docker, except it is written in
Python rather than Go. Also, the original Go library has a couple deprecated
functions that we don't implement in this port.

The main way to use this library is by constructing a `FilePatternMatcher` object,
then asking it whether file paths match any of its patterns.
"""

import os
from abc import abstractmethod
from functools import cached_property
from pathlib import Path
from typing import Callable, Optional, Sequence, Union

from ._utils.pattern_utils import Pattern


class _AbstractPatternMatcher:
    _custom_repr: Optional[str] = None

    def __invert__(self) -> "_AbstractPatternMatcher":
        """Invert the filter. Returns a function that returns True if the path does not match any of the patterns.

        Usage:
        ```python
        from pathlib import Path
        from modal import FilePatternMatcher

        inverted_matcher = ~FilePatternMatcher("**/*.py")

        assert not inverted_matcher(Path("foo.py"))
        ```
        """
        return _CustomPatternMatcher(lambda path: not self(path))

    def _with_repr(self, custom_repr) -> "_AbstractPatternMatcher":
        # use to give an instance of a matcher a custom name - useful for visualizing default values in signatures
        self._custom_repr = custom_repr
        return self

    def __repr__(self) -> str:
        if self._custom_repr:
            return self._custom_repr

        return super().__repr__()

    @abstractmethod
    def can_prune_directories(self) -> bool:
        """
        Returns True if this pattern matcher allows safe early directory pruning.

        Directory pruning is safe when matching directories can be skipped entirely
        without missing any files that should be included.

        An example where pruning is not safe is for inverted patterns, like "!**/*.py".
        """
        ...

    @abstractmethod
    def __call__(self, path: Path) -> bool: ...


class _CustomPatternMatcher(_AbstractPatternMatcher):
    def __init__(self, predicate: Callable[[Path], bool]):
        self._predicate = predicate

    def can_prune_directories(self) -> bool:
        """
        Custom pattern matchers (like negated matchers) cannot safely prune directories.

        Since these are arbitrary predicates, we cannot determine if a directory
        can be safely skipped without evaluating all files within it.
        """
        return False

    def __call__(self, path: Path) -> bool:
        return self._predicate(path)


class FilePatternMatcher(_AbstractPatternMatcher):
    """
    Allows matching file Path objects against a list of patterns.

    **Usage:**
    ```python
    from pathlib import Path
    from modal import FilePatternMatcher

    matcher = FilePatternMatcher("*.py")

    assert matcher(Path("foo.py"))

    # You can also negate the matcher.
    negated_matcher = ~matcher

    assert not negated_matcher(Path("foo.py"))
    ```
    """

    _file_path: Optional[Union[str, Path]]
    _pattern_strings: Optional[Sequence[str]]

    def _parse_patterns(self, patterns: Sequence[str]) -> list[Pattern]:
        parsed_patterns = []
        for pattern in list(patterns):
            pattern = pattern.strip().strip(os.path.sep)
            if not pattern:
                continue
            pattern = os.path.normpath(pattern)
            new_pattern = Pattern()
            if pattern[0] == "!":
                if len(pattern) == 1:
                    raise ValueError('Illegal exclusion pattern: "!"')
                new_pattern.exclusion = True
                pattern = pattern[1:]
            # In Python, we can proceed without explicit syntax checking
            new_pattern.cleaned_pattern = pattern
            new_pattern.dirs = pattern.split(os.path.sep)
            parsed_patterns.append(new_pattern)
        return parsed_patterns

    def __init__(self, *pattern: str) -> None:
        """Initialize a new FilePatternMatcher instance.

        Args:
            pattern (str): One or more pattern strings.

        Raises:
            ValueError: If an illegal exclusion pattern is provided.
        """
        self._pattern_strings = pattern
        self._file_path = None

    @classmethod
    def from_file(cls, file_path: Union[str, Path]) -> "FilePatternMatcher":
        """Initialize a new FilePatternMatcher instance from a file.

        The patterns in the file will be read lazily when the matcher is first used.

        Args:
            file_path (Path): The path to the file containing patterns.

        **Usage:**
        ```python
        from modal import FilePatternMatcher

        matcher = FilePatternMatcher.from_file("/path/to/ignorefile")
        ```

        """
        instance = cls.__new__(cls)
        instance._file_path = file_path
        instance._pattern_strings = None
        return instance

    def _matches(self, file_path: str) -> bool:
        """Check if the file path or any of its parent directories match the patterns.

        This is equivalent to `MatchesOrParentMatches()` in the original Go
        library. The reason is that `Matches()` in the original library is
        deprecated due to buggy behavior.
        """
        matched = False
        file_path = os.path.normpath(file_path)
        if file_path == ".":
            # Don't let them exclude everything; kind of silly.
            return False
        parent_path = os.path.dirname(file_path)
        if parent_path == "":
            parent_path = "."
        parent_path_dirs = parent_path.split(os.path.sep)

        for pattern in self.patterns:
            # Skip evaluation based on current match status and pattern exclusion
            if pattern.exclusion != matched:
                continue

            match = pattern.match(file_path)

            if not match and parent_path != ".":
                # Check if the pattern matches any of the parent directories
                for i in range(len(parent_path_dirs)):
                    dir_path = os.path.sep.join(parent_path_dirs[: i + 1])
                    if pattern.match(dir_path):
                        match = True
                        break

            if match:
                matched = not pattern.exclusion

        return matched

    @cached_property
    def patterns(self) -> list[Pattern]:
        """Get the patterns, loading from file if necessary."""
        if self._file_path is not None:
            # Lazy load from file
            pattern_strings = Path(self._file_path).read_text("utf8").splitlines()
        else:
            # Use patterns provided in __init__
            pattern_strings = list(self._pattern_strings)

        return self._parse_patterns(pattern_strings)

    def can_prune_directories(self) -> bool:
        """
        Returns True if this pattern matcher allows safe early directory pruning.

        Directory pruning is safe when matching directories can be skipped entirely
        without missing any files that should be included. This is for example not
        safe when we have inverted/negated ignore patterns (e.g. "!**/*.py").
        """
        return not any(pattern.exclusion for pattern in self.patterns)

    def __call__(self, file_path: Path) -> bool:
        return self._matches(str(file_path))


# _with_repr allows us to use this matcher as a default value in a function signature
#  and get a nice repr in the docs and auto-generated type stubs:
NON_PYTHON_FILES = (~FilePatternMatcher("**/*.py"))._with_repr(f"{__name__}.NON_PYTHON_FILES")
_NOTHING = (~FilePatternMatcher())._with_repr(f"{__name__}._NOTHING")  # match everything = ignore nothing


def _ignore_fn(ignore: Union[Sequence[str], Callable[[Path], bool]]) -> Callable[[Path], bool]:
    # if a callable is passed, return it
    # otherwise, treat input as a sequence of patterns and return a callable pattern matcher for those
    if callable(ignore):
        return ignore

    return FilePatternMatcher(*ignore)



================================================
FILE: modal/functions.py
================================================
# Copyright Modal Labs 2025
from ._functions import _Function, _FunctionCall, _gather
from ._utils.async_utils import synchronize_api

Function = synchronize_api(_Function, target_module=__name__)
FunctionCall = synchronize_api(_FunctionCall, target_module=__name__)
gather = synchronize_api(_gather, target_module=__name__)



================================================
FILE: modal/gpu.py
================================================
# Copyright Modal Labs 2022
from typing import Union

from modal_proto import api_pb2

from ._utils.deprecation import deprecation_warning
from .exception import InvalidError


class _GPUConfig:
    gpu_type: str
    count: int

    def __init__(self, gpu_type: str, count: int):
        name = self.__class__.__name__
        str_value = gpu_type
        if count > 1:
            str_value += f":{count}"
        deprecation_warning((2025, 2, 7), f'`gpu={name}(...)` is deprecated. Use `gpu="{str_value}"` instead.')
        self.gpu_type = gpu_type
        self.count = count

    def _to_proto(self) -> api_pb2.GPUConfig:
        """Convert this GPU config to an internal protobuf representation."""
        return api_pb2.GPUConfig(
            gpu_type=self.gpu_type,
            count=self.count,
        )


class T4(_GPUConfig):
    """
    [NVIDIA T4 Tensor Core](https://www.nvidia.com/en-us/data-center/tesla-t4/) GPU class.

    A low-cost data center GPU based on the Turing architecture, providing 16GB of GPU memory.
    """

    def __init__(
        self,
        count: int = 1,  # Number of GPUs per container. Defaults to 1.
    ):
        super().__init__("T4", count)

    def __repr__(self):
        return f"GPU(T4, count={self.count})"


class L4(_GPUConfig):
    """
    [NVIDIA L4 Tensor Core](https://www.nvidia.com/en-us/data-center/l4/) GPU class.

    A mid-tier data center GPU based on the Ada Lovelace architecture, providing 24GB of GPU memory.
    Includes RTX (ray tracing) support.
    """

    def __init__(
        self,
        count: int = 1,  # Number of GPUs per container. Defaults to 1.
    ):
        super().__init__("L4", count)

    def __repr__(self):
        return f"GPU(L4, count={self.count})"


class A100(_GPUConfig):
    """
    [NVIDIA A100 Tensor Core](https://www.nvidia.com/en-us/data-center/a100/) GPU class.

    The flagship data center GPU of the Ampere architecture. Available in 40GB and 80GB GPU memory configurations.
    """

    def __init__(
        self,
        *,
        count: int = 1,  # Number of GPUs per container. Defaults to 1.
        size: Union[str, None] = None,  # Select GB configuration of GPU device: "40GB" or "80GB". Defaults to "40GB".
    ):
        if size == "40GB" or not size:
            super().__init__("A100-40GB", count)
        elif size == "80GB":
            super().__init__("A100-80GB", count)
        else:
            raise ValueError(f"size='{size}' is invalid. A100s can only have memory values of 40GB or 80GB.")

    def __repr__(self):
        return f"GPU({self.gpu_type}, count={self.count})"


class A10G(_GPUConfig):
    """
    [NVIDIA A10G Tensor Core](https://www.nvidia.com/en-us/data-center/products/a10-gpu/) GPU class.

    A mid-tier data center GPU based on the Ampere architecture, providing 24 GB of memory.
    A10G GPUs deliver up to 3.3x better ML training performance, 3x better ML inference performance,
    and 3x better graphics performance, in comparison to NVIDIA T4 GPUs.
    """

    def __init__(
        self,
        *,
        # Number of GPUs per container. Defaults to 1.
        # Useful if you have very large models that don't fit on a single GPU.
        count: int = 1,
    ):
        super().__init__("A10G", count)

    def __repr__(self):
        return f"GPU(A10G, count={self.count})"


class H100(_GPUConfig):
    """
    [NVIDIA H100 Tensor Core](https://www.nvidia.com/en-us/data-center/h100/) GPU class.

    The flagship data center GPU of the Hopper architecture.
    Enhanced support for FP8 precision and a Transformer Engine that provides up to 4X faster training
    over the prior generation for GPT-3 (175B) models.
    """

    def __init__(
        self,
        *,
        # Number of GPUs per container. Defaults to 1.
        # Useful if you have very large models that don't fit on a single GPU.
        count: int = 1,
    ):
        super().__init__("H100", count)

    def __repr__(self):
        return f"GPU(H100, count={self.count})"


class L40S(_GPUConfig):
    """
    [NVIDIA L40S](https://www.nvidia.com/en-us/data-center/l40s/) GPU class.

    The L40S is a data center GPU for the Ada Lovelace architecture. It has 48 GB of on-chip
    GDDR6 RAM and enhanced support for FP8 precision.
    """

    def __init__(
        self,
        *,
        # Number of GPUs per container. Defaults to 1.
        # Useful if you have very large models that don't fit on a single GPU.
        count: int = 1,
    ):
        super().__init__("L40S", count)

    def __repr__(self):
        return f"GPU(L40S, count={self.count})"


class Any(_GPUConfig):
    """Selects any one of the GPU classes available within Modal, according to availability."""

    def __init__(self, *, count: int = 1):
        super().__init__("ANY", count)

    def __repr__(self):
        return f"GPU(Any, count={self.count})"


__doc__ = """
**GPU configuration shortcodes**

You can pass a wide range of `str` values for the `gpu` parameter of
[`@app.function`](https://modal.com/docs/reference/modal.App#function).

For instance:
- `gpu="H100"` will attach 1 H100 GPU to each container
- `gpu="L40S"` will attach 1 L40S GPU to each container
- `gpu="T4:4"` will attach 4 T4 GPUs to each container

You can see a list of Modal GPU options in the
[GPU docs](https://modal.com/docs/guide/gpu).

**Example**

```python
@app.function(gpu="A100-80GB:4")
def my_gpu_function():
    ... # This will have 4 A100-80GB with each container
```

**Deprecation notes**

An older deprecated way to configure GPU is also still supported,
but will be removed in future versions of Modal. Examples:

- `gpu=modal.gpu.H100()` will attach 1 H100 GPU to each container
- `gpu=modal.gpu.T4(count=4)` will attach 4 T4 GPUs to each container
- `gpu=modal.gpu.A100()` will attach 1 A100-40GB GPUs to each container
- `gpu=modal.gpu.A100(size="80GB")` will attach 1 A100-80GB GPUs to each container
"""

GPU_T = Union[None, str, _GPUConfig]


def parse_gpu_config(value: GPU_T) -> api_pb2.GPUConfig:
    if isinstance(value, _GPUConfig):
        return value._to_proto()
    elif isinstance(value, str):
        count = 1
        if ":" in value:
            value, count_str = value.split(":", 1)
            try:
                count = int(count_str)
            except ValueError:
                raise InvalidError(f"Invalid GPU count: {count_str}. Value must be an integer.")
        gpu_type = value.upper()
        return api_pb2.GPUConfig(
            gpu_type=gpu_type,
            count=count,
        )
    elif value is None:
        return api_pb2.GPUConfig()
    else:
        raise InvalidError(
            f"Invalid GPU config: {value}. Value must be a string or `None` (or a deprecated `modal.gpu` object)"
        )



================================================
FILE: modal/io_streams.py
================================================
# Copyright Modal Labs 2022
import asyncio
import time
from collections.abc import AsyncGenerator, AsyncIterator
from typing import (
    TYPE_CHECKING,
    Generic,
    Literal,
    Optional,
    TypeVar,
    Union,
    cast,
)

from grpclib import Status
from grpclib.exceptions import GRPCError, StreamTerminatedError

from modal.exception import ClientClosed, InvalidError
from modal_proto import api_pb2

from ._utils.async_utils import synchronize_api
from ._utils.grpc_utils import RETRYABLE_GRPC_STATUS_CODES, retry_transient_errors
from .client import _Client
from .config import logger
from .stream_type import StreamType

if TYPE_CHECKING:
    pass


async def _sandbox_logs_iterator(
    sandbox_id: str, file_descriptor: "api_pb2.FileDescriptor.ValueType", last_entry_id: str, client: _Client
) -> AsyncGenerator[tuple[Optional[bytes], str], None]:
    req = api_pb2.SandboxGetLogsRequest(
        sandbox_id=sandbox_id,
        file_descriptor=file_descriptor,
        timeout=55,
        last_entry_id=last_entry_id,
    )
    async for log_batch in client.stub.SandboxGetLogs.unary_stream(req):
        last_entry_id = log_batch.entry_id

        for message in log_batch.items:
            yield (message.data.encode("utf-8"), last_entry_id)
        if log_batch.eof:
            yield (None, last_entry_id)
            break


async def _container_process_logs_iterator(
    process_id: str,
    file_descriptor: "api_pb2.FileDescriptor.ValueType",
    client: _Client,
    last_index: int,
    deadline: Optional[float] = None,
) -> AsyncGenerator[tuple[Optional[bytes], int], None]:
    req = api_pb2.ContainerExecGetOutputRequest(
        exec_id=process_id,
        timeout=55,
        file_descriptor=file_descriptor,
        get_raw_bytes=True,
        last_batch_index=last_index,
    )

    stream = client.stub.ContainerExecGetOutput.unary_stream(req)
    while True:
        # Check deadline before attempting to receive the next batch
        try:
            remaining = (deadline - time.monotonic()) if deadline else None
            batch = await asyncio.wait_for(stream.__anext__(), timeout=remaining)
        except asyncio.TimeoutError:
            yield None, -1
            break
        except StopAsyncIteration:
            break
        if batch.HasField("exit_code"):
            yield None, batch.batch_index
            break
        for item in batch.items:
            yield item.message_bytes, batch.batch_index


T = TypeVar("T", str, bytes)


class _StreamReader(Generic[T]):
    """Retrieve logs from a stream (`stdout` or `stderr`).

    As an asynchronous iterable, the object supports the `for` and `async for`
    statements. Just loop over the object to read in chunks.

    **Usage**

    ```python fixture:running_app
    from modal import Sandbox

    sandbox = Sandbox.create(
        "bash",
        "-c",
        "for i in $(seq 1 10); do echo foo; sleep 0.1; done",
        app=running_app,
    )
    for message in sandbox.stdout:
        print(f"Message: {message}")
    ```
    """

    _stream: Optional[AsyncGenerator[Optional[bytes], None]]

    def __init__(
        self,
        file_descriptor: "api_pb2.FileDescriptor.ValueType",
        object_id: str,
        object_type: Literal["sandbox", "container_process"],
        client: _Client,
        stream_type: StreamType = StreamType.PIPE,
        text: bool = True,
        by_line: bool = False,
        deadline: Optional[float] = None,
    ) -> None:
        """mdmd:hidden"""
        self._file_descriptor = file_descriptor
        self._object_type = object_type
        self._object_id = object_id
        self._client = client
        self._stream = None
        self._last_entry_id: str = ""
        self._line_buffer = b""
        self._deadline = deadline

        # Sandbox logs are streamed to the client as strings, so StreamReaders reading
        # them must have text mode enabled.
        if object_type == "sandbox" and not text:
            raise ValueError("Sandbox streams must have text mode enabled.")

        # line-buffering is only supported when text=True
        if by_line and not text:
            raise ValueError("line-buffering is only supported when text=True")

        self._text = text
        self._by_line = by_line

        # Whether the reader received an EOF. Once EOF is True, it returns
        # an empty string for any subsequent reads (including async for)
        self.eof = False

        if not isinstance(stream_type, StreamType):
            raise TypeError(f"stream_type must be of type StreamType, got {type(stream_type)}")

        # We only support piping sandbox logs because they're meant to be durable logs stored
        # on the user's application.
        if object_type == "sandbox" and stream_type != StreamType.PIPE:
            raise ValueError("Sandbox streams must be piped.")
        self._stream_type = stream_type

        if self._object_type == "container_process":
            # Container process streams need to be consumed as they are produced,
            # otherwise the process will block. Use a buffer to store the stream
            # until the client consumes it.
            self._container_process_buffer: list[Optional[bytes]] = []
            self._consume_container_process_task = asyncio.create_task(self._consume_container_process_stream())

    @property
    def file_descriptor(self) -> int:
        """Possible values are `1` for stdout and `2` for stderr."""
        return self._file_descriptor

    async def read(self) -> T:
        """Fetch the entire contents of the stream until EOF.

        **Usage**

        ```python fixture:running_app
        from modal import Sandbox

        sandbox = Sandbox.create("echo", "hello", app=running_app)
        sandbox.wait()

        print(sandbox.stdout.read())
        ```
        """
        data_str = ""
        data_bytes = b""
        logger.debug(f"{self._object_id} StreamReader fd={self._file_descriptor} read starting")
        async for message in self._get_logs():
            if message is None:
                break
            if self._text:
                data_str += message.decode("utf-8")
            else:
                data_bytes += message

        logger.debug(f"{self._object_id} StreamReader fd={self._file_descriptor} read completed after EOF")
        if self._text:
            return cast(T, data_str)
        else:
            return cast(T, data_bytes)

    async def _consume_container_process_stream(self):
        """Consume the container process stream and store messages in the buffer."""
        if self._stream_type == StreamType.DEVNULL:
            return

        completed = False
        retries_remaining = 10
        last_index = 0
        while not completed:
            if self._deadline and time.monotonic() >= self._deadline:
                break
            try:
                iterator = _container_process_logs_iterator(
                    self._object_id, self._file_descriptor, self._client, last_index, self._deadline
                )
                async for message, batch_index in iterator:
                    if self._stream_type == StreamType.STDOUT and message:
                        print(message.decode("utf-8"), end="")
                    elif self._stream_type == StreamType.PIPE:
                        self._container_process_buffer.append(message)

                    if message is None:
                        completed = True
                        break
                    else:
                        last_index = batch_index

            except (GRPCError, StreamTerminatedError, ClientClosed) as exc:
                if retries_remaining > 0:
                    retries_remaining -= 1
                    if isinstance(exc, GRPCError):
                        if exc.status in RETRYABLE_GRPC_STATUS_CODES:
                            await asyncio.sleep(1.0)
                            continue
                    elif isinstance(exc, StreamTerminatedError):
                        continue
                    elif isinstance(exc, ClientClosed):
                        # If the client was closed, the user has triggered a cleanup.
                        break
                logger.error(f"{self._object_id} stream read failure while consuming process output: {exc}")
                raise exc

    async def _stream_container_process(self) -> AsyncGenerator[tuple[Optional[bytes], str], None]:
        """Streams the container process buffer to the reader."""
        entry_id = 0
        if self._last_entry_id:
            entry_id = int(self._last_entry_id) + 1

        while True:
            if entry_id >= len(self._container_process_buffer):
                await asyncio.sleep(0.1)
                continue

            item = self._container_process_buffer[entry_id]

            yield (item, str(entry_id))
            if item is None:
                break

            entry_id += 1

    async def _get_logs(self, skip_empty_messages: bool = True) -> AsyncGenerator[Optional[bytes], None]:
        """Streams sandbox or process logs from the server to the reader.

        Logs returned by this method may contain partial or multiple lines at a time.

        When the stream receives an EOF, it yields None. Once an EOF is received,
        subsequent invocations will not yield logs.
        """
        if self._stream_type != StreamType.PIPE:
            raise InvalidError("Logs can only be retrieved using the PIPE stream type.")

        if self.eof:
            yield None
            return

        completed = False

        retries_remaining = 10
        while not completed:
            try:
                if self._object_type == "sandbox":
                    iterator = _sandbox_logs_iterator(
                        self._object_id, self._file_descriptor, self._last_entry_id, self._client
                    )
                else:
                    iterator = self._stream_container_process()

                async for message, entry_id in iterator:
                    self._last_entry_id = entry_id
                    # Empty messages are sent when the process boots up. Don't yield them unless
                    # we're using the empty message to signal process liveness.
                    if skip_empty_messages and message == b"":
                        continue

                    if message is None:
                        completed = True
                        self.eof = True
                    yield message

            except (GRPCError, StreamTerminatedError) as exc:
                if retries_remaining > 0:
                    retries_remaining -= 1
                    if isinstance(exc, GRPCError):
                        if exc.status in RETRYABLE_GRPC_STATUS_CODES:
                            await asyncio.sleep(1.0)
                            continue
                    elif isinstance(exc, StreamTerminatedError):
                        continue
                raise

    async def _get_logs_by_line(self) -> AsyncGenerator[Optional[bytes], None]:
        """Process logs from the server and yield complete lines only."""
        async for message in self._get_logs():
            if message is None:
                if self._line_buffer:
                    yield self._line_buffer
                    self._line_buffer = b""
                yield None
            else:
                assert isinstance(message, bytes)
                self._line_buffer += message
                while b"\n" in self._line_buffer:
                    line, self._line_buffer = self._line_buffer.split(b"\n", 1)
                    yield line + b"\n"

    def _ensure_stream(self) -> AsyncGenerator[Optional[bytes], None]:
        if not self._stream:
            if self._by_line:
                self._stream = self._get_logs_by_line()
            else:
                self._stream = self._get_logs()
        return self._stream

    def __aiter__(self) -> AsyncIterator[T]:
        """mdmd:hidden"""
        self._ensure_stream()
        return self

    async def __anext__(self) -> T:
        """mdmd:hidden"""
        stream = self._ensure_stream()

        value = await stream.__anext__()

        # The stream yields None if it receives an EOF batch.
        if value is None:
            raise StopAsyncIteration

        if self._text:
            return cast(T, value.decode("utf-8"))
        else:
            return cast(T, value)

    async def aclose(self):
        """mdmd:hidden"""
        if self._stream:
            await self._stream.aclose()


MAX_BUFFER_SIZE = 2 * 1024 * 1024


class _StreamWriter:
    """Provides an interface to buffer and write logs to a sandbox or container process stream (`stdin`)."""

    def __init__(self, object_id: str, object_type: Literal["sandbox", "container_process"], client: _Client) -> None:
        """mdmd:hidden"""
        self._index = 1
        self._object_id = object_id
        self._object_type = object_type
        self._client = client
        self._is_closed = False
        self._buffer = bytearray()

    def _get_next_index(self) -> int:
        index = self._index
        self._index += 1
        return index

    def write(self, data: Union[bytes, bytearray, memoryview, str]) -> None:
        """Write data to the stream but does not send it immediately.

        This is non-blocking and queues the data to an internal buffer. Must be
        used along with the `drain()` method, which flushes the buffer.

        **Usage**

        ```python fixture:running_app
        from modal import Sandbox

        sandbox = Sandbox.create(
            "bash",
            "-c",
            "while read line; do echo $line; done",
            app=running_app,
        )
        sandbox.stdin.write(b"foo\\n")
        sandbox.stdin.write(b"bar\\n")
        sandbox.stdin.write_eof()

        sandbox.stdin.drain()
        sandbox.wait()
        ```
        """
        if self._is_closed:
            raise ValueError("Stdin is closed. Cannot write to it.")
        if isinstance(data, (bytes, bytearray, memoryview, str)):
            if isinstance(data, str):
                data = data.encode("utf-8")
            if len(self._buffer) + len(data) > MAX_BUFFER_SIZE:
                raise BufferError("Buffer size exceed limit. Call drain to clear the buffer.")
            self._buffer.extend(data)
        else:
            raise TypeError(f"data argument must be a bytes-like object, not {type(data).__name__}")

    def write_eof(self) -> None:
        """Close the write end of the stream after the buffered data is drained.

        If the process was blocked on input, it will become unblocked after
        `write_eof()`. This method needs to be used along with the `drain()`
        method, which flushes the EOF to the process.
        """
        self._is_closed = True

    async def drain(self) -> None:
        """Flush the write buffer and send data to the running process.

        This is a flow control method that blocks until data is sent. It returns
        when it is appropriate to continue writing data to the stream.

        **Usage**

        ```python notest
        writer.write(data)
        writer.drain()
        ```

        Async usage:
        ```python notest
        writer.write(data)  # not a blocking operation
        await writer.drain.aio()
        ```
        """
        data = bytes(self._buffer)
        self._buffer.clear()
        index = self._get_next_index()

        try:
            if self._object_type == "sandbox":
                await retry_transient_errors(
                    self._client.stub.SandboxStdinWrite,
                    api_pb2.SandboxStdinWriteRequest(
                        sandbox_id=self._object_id, index=index, eof=self._is_closed, input=data
                    ),
                )
            else:
                await retry_transient_errors(
                    self._client.stub.ContainerExecPutInput,
                    api_pb2.ContainerExecPutInputRequest(
                        exec_id=self._object_id,
                        input=api_pb2.RuntimeInputMessage(message=data, message_index=index, eof=self._is_closed),
                    ),
                )
        except GRPCError as exc:
            if exc.status == Status.FAILED_PRECONDITION:
                raise ValueError(exc.message)
            else:
                raise exc


StreamReader = synchronize_api(_StreamReader)
StreamWriter = synchronize_api(_StreamWriter)



================================================
FILE: modal/mount.py
================================================
# Copyright Modal Labs 2022
import abc
import asyncio
import concurrent.futures
import dataclasses
import os
import re
import time
import typing
import warnings
from collections.abc import AsyncGenerator, Generator
from pathlib import Path, PurePosixPath
from typing import Callable, Optional, Sequence, Union

from google.protobuf.message import Message
from grpclib import GRPCError

import modal.exception
import modal.file_pattern_matcher
from modal_proto import api_pb2
from modal_version import __version__

from ._object import _get_environment_name, _Object
from ._resolver import Resolver
from ._utils.async_utils import TaskContext, aclosing, async_map, synchronize_api
from ._utils.blob_utils import FileUploadSpec, blob_upload_file, get_file_upload_spec_from_path
from ._utils.grpc_utils import retry_transient_errors
from ._utils.name_utils import check_object_name
from ._utils.package_utils import get_module_mount_info
from .client import _Client
from .config import config, logger
from .exception import ExecutionError, InvalidError
from .file_pattern_matcher import FilePatternMatcher

ROOT_DIR: PurePosixPath = PurePosixPath("/root")
MOUNT_PUT_FILE_CLIENT_TIMEOUT = 10 * 60  # 10 min max for transferring files

# Supported releases and versions for python-build-standalone.
#
# These can be updated safely, but changes will trigger a rebuild for all images
# that rely on `add_python()` in their constructor.
PYTHON_STANDALONE_VERSIONS: dict[str, tuple[str, str]] = {
    "3.9": ("20230826", "3.9.18"),
    "3.10": ("20230826", "3.10.13"),
    "3.11": ("20230826", "3.11.5"),
    "3.12": ("20240107", "3.12.1"),
    "3.13": ("20241008", "3.13.0"),
}

MOUNT_DEPRECATION_MESSAGE_PATTERN = """modal.Mount usage will soon be deprecated.

Use {replacement} instead, which is functionally and performance-wise equivalent.

See https://modal.com/docs/guide/modal-1-0-migration for more details.
"""


def client_mount_name() -> str:
    """Get the deployed name of the client package mount."""
    # Strip any annotations (i.e. `+{git_hash}`) becuase the + is not valid in a mount name
    if m := re.match(r"^([\d\.\w]+)", __version__):
        version = m.group(1)
    else:
        raise ExecutionError(f"Modal client has improperly formatted version: {__version__!r}")
    return f"modal-client-mount-{version}"


def python_standalone_mount_name(version: str) -> str:
    """Get the deployed name of the python-build-standalone mount."""
    if "-" in version:  # default to glibc
        version, libc = version.split("-")
    else:
        libc = "gnu"
    if version not in PYTHON_STANDALONE_VERSIONS:
        raise modal.exception.InvalidError(
            f"Unsupported standalone python version: {version!r}, supported values are "
            f"{list(PYTHON_STANDALONE_VERSIONS)}"
        )
    if libc != "gnu":
        raise modal.exception.InvalidError(f"Unsupported libc identifier: {libc}")
    release, full_version = PYTHON_STANDALONE_VERSIONS[version]
    return f"python-build-standalone.{release}.{full_version}-{libc}"


class _MountEntry(metaclass=abc.ABCMeta):
    @abc.abstractmethod
    def description(self) -> str: ...

    @abc.abstractmethod
    def get_files_to_upload(self) -> typing.Iterator[tuple[Path, str]]: ...

    @abc.abstractmethod
    def watch_entry(self) -> tuple[Path, Path]: ...

    @abc.abstractmethod
    def top_level_paths(self) -> list[tuple[Path, PurePosixPath]]: ...


def _select_files(entries: list[_MountEntry]) -> list[tuple[Path, PurePosixPath]]:
    # TODO: make this async
    all_files: set[tuple[Path, PurePosixPath]] = set()
    for entry in entries:
        all_files |= set(entry.get_files_to_upload())
    return list(all_files)


@dataclasses.dataclass
class _MountFile(_MountEntry):
    local_file: Path
    remote_path: PurePosixPath

    def description(self) -> str:
        return str(self.local_file)

    def get_files_to_upload(self):
        local_file = self.local_file.resolve()
        if not local_file.exists():
            msg = f"local file {local_file} does not exist"
            raise FileNotFoundError(msg)

        rel_filename = self.remote_path
        yield local_file, rel_filename

    def watch_entry(self):
        safe_path = self.local_file.expanduser().absolute()
        return safe_path.parent, safe_path

    def top_level_paths(self) -> list[tuple[Path, PurePosixPath]]:
        return [(self.local_file, self.remote_path)]


@dataclasses.dataclass
class _MountDir(_MountEntry):
    local_dir: Path
    remote_path: PurePosixPath
    ignore: Union[Callable[[Path], bool], modal.file_pattern_matcher._AbstractPatternMatcher]
    recursive: bool

    def description(self):
        return str(self.local_dir.expanduser().absolute())

    def _walk_and_prune(self, top_dir: Path) -> Generator[str, None, None]:
        """Walk directories and prune ignored directories early."""
        for root, dirs, files in os.walk(top_dir, topdown=True):
            # with topdown=True, os.walk allows modifying the dirs list in-place, and will only
            # recurse into dirs that are not ignored.
            dirs[:] = [d for d in dirs if not self.ignore(Path(os.path.join(root, d)).relative_to(top_dir))]
            for file in files:
                yield os.path.join(root, file)

    def _walk_all(self, top_dir: Path) -> Generator[str, None, None]:
        """Walk all directories without early pruning - safe for complex/inverted ignore patterns."""
        for root, _, files in os.walk(top_dir):
            for file in files:
                yield os.path.join(root, file)

    def get_files_to_upload(self):
        # we can't use .resolve() eagerly here since that could end up "renaming" symlinked files
        # see test_mount_directory_with_symlinked_file
        local_dir = self.local_dir.expanduser().absolute()

        if not local_dir.exists():
            msg = f"local dir {local_dir} does not exist"
            raise FileNotFoundError(msg)

        if not local_dir.is_dir():
            msg = f"local dir {local_dir} is not a directory"
            raise NotADirectoryError(msg)

        if self.recursive:
            if (
                isinstance(self.ignore, modal.file_pattern_matcher._AbstractPatternMatcher)
                and self.ignore.can_prune_directories()
            ):
                gen = self._walk_and_prune(local_dir)
            else:
                gen = self._walk_all(local_dir)
        else:
            gen = (dir_entry.path for dir_entry in os.scandir(local_dir) if dir_entry.is_file())

        for local_filename in gen:
            local_path = Path(local_filename)
            rel_local_path = local_path.relative_to(local_dir)
            if not self.ignore(rel_local_path):
                mount_path = self.remote_path / rel_local_path.as_posix()
                yield local_path.resolve(), mount_path

    def watch_entry(self):
        return self.local_dir.resolve().expanduser(), None

    def top_level_paths(self) -> list[tuple[Path, PurePosixPath]]:
        return [(self.local_dir, self.remote_path)]


def module_mount_condition(module_base: Path):
    SKIP_BYTECODE = True  # hard coded for now
    SKIP_DOT_PREFIXED = True

    def condition(f: str):
        path = Path(f)
        if SKIP_BYTECODE and path.suffix == ".pyc":
            return False

        # Check parent dir names to see if file should be included,
        # but ignore dir names above root of mounted module:
        # /a/.venv/site-packages/mymod/foo.py should be included by default
        # /a/my_mod/.config/foo.py should *not* be included by default
        while path != module_base and path != path.parent:
            if SKIP_BYTECODE and path.name == "__pycache__":
                return False

            if SKIP_DOT_PREFIXED and path.name.startswith("."):
                return False

            path = path.parent

        return True

    return condition


def module_mount_ignore_condition(module_base: Path):
    return lambda f: not module_mount_condition(module_base)(str(f))


@dataclasses.dataclass
class _MountedPythonModule(_MountEntry):
    # the purpose of this is to keep printable information about which Python package
    # was mounted. Functionality wise it's the same as mounting a dir or a file with
    # the Module

    module_name: str
    remote_dir: Union[PurePosixPath, str] = ROOT_DIR.as_posix()  # cast needed here for type stub generation...
    ignore: Optional[Callable[[Path], bool]] = None

    def description(self) -> str:
        return f"PythonPackage:{self.module_name}"

    def _proxy_entries(self) -> list[_MountEntry]:
        mount_infos = get_module_mount_info(self.module_name)
        entries = []
        for mount_info in mount_infos:
            is_package, base_path = mount_info
            if is_package:
                remote_dir = PurePosixPath(self.remote_dir, *self.module_name.split("."))
                entries.append(
                    _MountDir(
                        base_path,
                        remote_path=remote_dir,
                        ignore=self.ignore or module_mount_ignore_condition(base_path),
                        recursive=True,
                    )
                )
            else:
                path_segments = self.module_name.split(".")[:-1]
                remote_path = PurePosixPath(self.remote_dir, *path_segments, Path(base_path).name)
                entries.append(
                    _MountFile(
                        local_file=Path(base_path),
                        remote_path=remote_path,
                    )
                )
        return entries

    def get_files_to_upload(self) -> typing.Iterator[tuple[Path, str]]:
        for entry in self._proxy_entries():
            yield from entry.get_files_to_upload()

    def watch_entry(self) -> tuple[Path, Path]:
        for entry in self._proxy_entries():
            # TODO: fix watch for mounts of multi-path packages
            return entry.watch_entry()

    def top_level_paths(self) -> list[tuple[Path, PurePosixPath]]:
        paths = []
        for sub in self._proxy_entries():
            paths.extend(sub.top_level_paths())
        return paths


class NonLocalMountError(Exception):
    # used internally to signal an error when trying to access entries on a non-local mount definition
    pass


class _Mount(_Object, type_prefix="mo"):
    """
    **Deprecated**: Mounts should not be used explicitly anymore, use `Image.add_local_*` commands instead.

    Create a mount for a local directory or file that can be attached
    to one or more Modal functions.

    **Usage**

    ```python notest
    import modal
    import os
    app = modal.App()

    @app.function(mounts=[modal.Mount.from_local_dir("~/foo", remote_path="/root/foo")])
    def f():
        # `/root/foo` has the contents of `~/foo`.
        print(os.listdir("/root/foo/"))
    ```

    Modal syncs the contents of the local directory every time the app runs, but uses the hash of
    the file's contents to skip uploading files that have been uploaded before.
    """

    _entries: Optional[list[_MountEntry]] = None
    _deployment_name: Optional[str] = None
    _namespace: Optional[int] = None
    _environment_name: Optional[str] = None
    _allow_overwrite: bool = False
    _content_checksum_sha256_hex: Optional[str] = None

    @staticmethod
    def _new(entries: list[_MountEntry] = []) -> "_Mount":
        rep = f"Mount({entries})"

        async def mount_content_deduplication_key():
            try:
                included_files = await asyncio.get_event_loop().run_in_executor(None, _select_files, entries)
            except NonLocalMountError:
                return None
            return (_Mount._type_prefix, "local", frozenset(included_files))

        obj = _Mount._from_loader(_Mount._load_mount, rep, deduplication_key=mount_content_deduplication_key)
        obj._entries = entries
        obj._is_local = True
        return obj

    def _extend(self, entry: _MountEntry) -> "_Mount":
        return _Mount._new(self._entries + [entry])

    @property
    def entries(self):
        """mdmd:hidden"""
        if self._entries is None:
            raise NonLocalMountError()
        return self._entries

    def _hydrate_metadata(self, handle_metadata: Optional[Message]):
        assert isinstance(handle_metadata, api_pb2.MountHandleMetadata)
        self._content_checksum_sha256_hex = handle_metadata.content_checksum_sha256_hex

    def _top_level_paths(self) -> list[tuple[Path, PurePosixPath]]:
        # Returns [(local_absolute_path, remote_path), ...] for all top level entries in the Mount
        # Used to determine if a package mount is installed in a sys directory or not
        res: list[tuple[Path, PurePosixPath]] = []
        for entry in self.entries:
            res.extend(entry.top_level_paths())
        return res

    def is_local(self) -> bool:
        """mdmd:hidden"""
        # TODO(erikbern): since any remote ref bypasses the constructor,
        # we can't rely on it to be set. Let's clean this up later.
        return getattr(self, "_is_local", False)

    @staticmethod
    def _add_local_dir(
        local_path: Path,
        remote_path: PurePosixPath,
        ignore: Callable[[Path], bool] = modal.file_pattern_matcher._NOTHING,
    ):
        return _Mount._new()._extend(
            _MountDir(
                local_dir=local_path,
                remote_path=remote_path,
                ignore=ignore,
                recursive=True,
            ),
        )

    def add_local_dir(
        self,
        local_path: Union[str, Path],
        *,
        # Where the directory is placed within in the mount
        remote_path: Union[str, PurePosixPath, None] = None,
        # Predicate filter function for file selection, which should accept a filepath and return `True` for inclusion.
        # Defaults to including all files.
        condition: Optional[Callable[[str], bool]] = None,
        # add files from subdirectories as well
        recursive: bool = True,
    ) -> "_Mount":
        """
        Add a local directory to the `Mount` object.
        """
        local_path = Path(local_path)
        if remote_path is None:
            remote_path = local_path.name
        remote_path = PurePosixPath("/", remote_path)
        if condition is None:

            def include_all(path):
                return True

            condition = include_all

        def converted_condition(path: Path) -> bool:
            return not condition(str(path))

        return self._extend(
            _MountDir(
                local_dir=local_path,
                ignore=converted_condition,
                remote_path=remote_path,
                recursive=recursive,
            ),
        )

    @staticmethod
    def _from_local_dir(
        local_path: Union[str, Path],
        *,
        # Where the directory is placed within in the mount
        remote_path: Union[str, PurePosixPath, None] = None,
        # Predicate filter function for file selection, which should accept a filepath and return `True` for inclusion.
        # Defaults to including all files.
        condition: Optional[Callable[[str], bool]] = None,
        # add files from subdirectories as well
        recursive: bool = True,
    ) -> "_Mount":
        return _Mount._new().add_local_dir(
            local_path, remote_path=remote_path, condition=condition, recursive=recursive
        )

    def add_local_file(
        self,
        local_path: Union[str, Path],
        remote_path: Union[str, PurePosixPath, None] = None,
    ) -> "_Mount":
        """
        Add a local file to the `Mount` object.
        """
        local_path = Path(local_path)
        if remote_path is None:
            remote_path = local_path.name
        remote_path = PurePosixPath("/", remote_path)
        return self._extend(
            _MountFile(
                local_file=local_path,
                remote_path=PurePosixPath(remote_path),
            ),
        )

    @staticmethod
    def _from_local_file(local_path: Union[str, Path], remote_path: Union[str, PurePosixPath, None] = None) -> "_Mount":
        return _Mount._new().add_local_file(local_path, remote_path=remote_path)

    @staticmethod
    def _description(entries: list[_MountEntry]) -> str:
        local_contents = [e.description() for e in entries]
        return ", ".join(local_contents)

    @staticmethod
    async def _get_files(entries: list[_MountEntry]) -> AsyncGenerator[FileUploadSpec, None]:
        loop = asyncio.get_event_loop()
        with concurrent.futures.ThreadPoolExecutor() as exe:
            all_files = await loop.run_in_executor(exe, _select_files, entries)

            futs = []
            for local_filename, remote_filename in all_files:
                logger.debug(f"Mounting {local_filename} as {remote_filename}")
                futs.append(loop.run_in_executor(exe, get_file_upload_spec_from_path, local_filename, remote_filename))

            logger.debug(f"Computing checksums for {len(futs)} files using {exe._max_workers} worker threads")
            for fut in asyncio.as_completed(futs):
                try:
                    yield await fut
                except FileNotFoundError as exc:
                    # Can happen with temporary files (e.g. emacs will write temp files and delete them quickly)
                    logger.info(f"Ignoring file not found: {exc}")

    async def _load_mount(
        self: "_Mount",
        resolver: Resolver,
        existing_object_id: Optional[str],
    ):
        t0 = time.monotonic()

        # Asynchronously list and checksum files with a thread pool, then upload them concurrently.
        n_seen, n_finished = 0, 0
        total_uploads, total_bytes = 0, 0
        accounted_hashes: set[str] = set()
        message_label = _Mount._description(self._entries)
        blob_upload_concurrency = asyncio.Semaphore(16)  # Limit uploads of large files.
        status_row = resolver.add_status_row()

        async def _put_file(file_spec: FileUploadSpec) -> api_pb2.MountFile:
            nonlocal n_seen, n_finished, total_uploads, total_bytes
            n_seen += 1
            status_row.message(f"Creating mount {message_label}: Uploaded {n_finished}/{n_seen} files")

            remote_filename = file_spec.mount_filename
            mount_file = api_pb2.MountFile(
                filename=remote_filename,
                sha256_hex=file_spec.sha256_hex,
                mode=file_spec.mode,
            )

            if file_spec.sha256_hex in accounted_hashes:
                n_finished += 1
                return mount_file

            # Try to catch cases where user modified their local files (e.g. changed git branches)
            # between triggering a build and Modal actually uploading the file
            if config.get("build_validation") != "ignore" and file_spec.source_is_path:
                mtime = os.stat(file_spec.source_description).st_mtime
                if mtime > resolver.build_start:
                    msg = f"{file_spec.source_description} was modified during build process."
                    if config.get("build_validation") == "error":
                        raise modal.exception.ExecutionError(msg)
                    elif config.get("build_validation") == "warn":
                        warnings.warn(msg)

            request = api_pb2.MountPutFileRequest(sha256_hex=file_spec.sha256_hex)
            accounted_hashes.add(file_spec.sha256_hex)
            response = await retry_transient_errors(resolver.client.stub.MountPutFile, request, base_delay=1)

            if response.exists:
                n_finished += 1
                return mount_file

            total_uploads += 1
            total_bytes += file_spec.size

            if file_spec.use_blob:
                logger.debug(f"Creating blob file for {file_spec.source_description} ({file_spec.size} bytes)")
                async with blob_upload_concurrency:
                    with file_spec.source() as fp:
                        blob_id = await blob_upload_file(
                            fp, resolver.client.stub, sha256_hex=file_spec.sha256_hex, md5_hex=file_spec.md5_hex
                        )
                logger.debug(f"Uploading blob file {file_spec.source_description} as {remote_filename}")
                request2 = api_pb2.MountPutFileRequest(data_blob_id=blob_id, sha256_hex=file_spec.sha256_hex)
            else:
                logger.debug(
                    f"Uploading file {file_spec.source_description} to {remote_filename} ({file_spec.size} bytes)"
                )
                request2 = api_pb2.MountPutFileRequest(data=file_spec.content, sha256_hex=file_spec.sha256_hex)

            start_time = time.monotonic()
            while time.monotonic() - start_time < MOUNT_PUT_FILE_CLIENT_TIMEOUT:
                response = await retry_transient_errors(resolver.client.stub.MountPutFile, request2, base_delay=1)
                if response.exists:
                    n_finished += 1
                    return mount_file

            raise modal.exception.MountUploadTimeoutError(f"Mounting of {file_spec.source_description} timed out")

        # Upload files, or check if they already exist.
        n_concurrent_uploads = 512
        files: list[api_pb2.MountFile] = []
        async with aclosing(
            async_map(_Mount._get_files(self._entries), _put_file, concurrency=n_concurrent_uploads)
        ) as stream:
            async for file in stream:
                files.append(file)

        if not files:
            logger.warning(f"Mount of '{message_label}' is empty.")

        # Build the mount.
        status_row.message(f"Creating mount {message_label}: Finalizing index of {len(files)} files")
        if self._deployment_name:
            creation_type = (
                api_pb2.OBJECT_CREATION_TYPE_CREATE_IF_MISSING
                if self._allow_overwrite
                else api_pb2.OBJECT_CREATION_TYPE_CREATE_FAIL_IF_EXISTS
            )
            req = api_pb2.MountGetOrCreateRequest(
                deployment_name=self._deployment_name,
                namespace=self._namespace,
                environment_name=self._environment_name,
                object_creation_type=creation_type,
                files=files,
            )
        elif resolver.app_id is not None:
            req = api_pb2.MountGetOrCreateRequest(
                object_creation_type=api_pb2.OBJECT_CREATION_TYPE_ANONYMOUS_OWNED_BY_APP,
                files=files,
                app_id=resolver.app_id,
            )
        else:
            req = api_pb2.MountGetOrCreateRequest(
                object_creation_type=api_pb2.OBJECT_CREATION_TYPE_EPHEMERAL,
                files=files,
                environment_name=resolver.environment_name,
            )

        resp = await retry_transient_errors(resolver.client.stub.MountGetOrCreate, req, base_delay=1)
        status_row.finish(f"Created mount {message_label}")

        logger.debug(f"Uploaded {total_uploads} new files and {total_bytes} bytes in {time.monotonic() - t0}s")
        self._hydrate(resp.mount_id, resolver.client, resp.handle_metadata)

    @staticmethod
    def _from_local_python_packages(
        *module_names: str,
        remote_dir: Union[str, PurePosixPath] = ROOT_DIR.as_posix(),
        # Predicate filter function for file selection, which should accept a filepath and return `True` for inclusion.
        # Defaults to including all files.
        condition: Optional[Callable[[str], bool]] = None,
        ignore: Optional[Union[Sequence[str], Callable[[Path], bool]]] = None,
    ) -> "_Mount":
        if condition is not None:
            if ignore is not None:
                raise InvalidError("Cannot specify both `ignore` and `condition`")

            def converted_condition(path: Path) -> bool:
                return not condition(str(path))

            ignore = converted_condition
        elif isinstance(ignore, list):
            ignore = FilePatternMatcher(*ignore)

        mount = _Mount._new()
        for module_name in module_names:
            mount = mount._extend(_MountedPythonModule(module_name, remote_dir, ignore))
        return mount

    @staticmethod
    def from_name(
        name: str,
        *,
        namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,
        environment_name: Optional[str] = None,
    ) -> "_Mount":
        """mdmd:hidden"""

        async def _load(provider: _Mount, resolver: Resolver, existing_object_id: Optional[str]):
            req = api_pb2.MountGetOrCreateRequest(
                deployment_name=name,
                namespace=namespace,
                environment_name=_get_environment_name(environment_name, resolver),
            )
            response = await resolver.client.stub.MountGetOrCreate(req)
            provider._hydrate(response.mount_id, resolver.client, response.handle_metadata)

        return _Mount._from_loader(_load, "Mount()", hydrate_lazily=True)

    async def _deploy(
        self: "_Mount",
        deployment_name: Optional[str] = None,
        namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,
        *,
        environment_name: Optional[str] = None,
        allow_overwrite: bool = False,
        client: Optional[_Client] = None,
    ) -> None:
        check_object_name(deployment_name, "Mount")
        environment_name = _get_environment_name(environment_name, resolver=None)
        self._deployment_name = deployment_name
        self._namespace = namespace
        self._environment_name = environment_name
        self._allow_overwrite = allow_overwrite
        if client is None:
            client = await _Client.from_env()
        resolver = Resolver(client=client, environment_name=environment_name)
        await resolver.load(self)

    def _get_metadata(self) -> api_pb2.MountHandleMetadata:
        if self._content_checksum_sha256_hex is None:
            raise ValueError("Trying to access checksum of unhydrated mount")

        return api_pb2.MountHandleMetadata(content_checksum_sha256_hex=self._content_checksum_sha256_hex)


Mount = synchronize_api(_Mount)


def _create_client_mount():
    # TODO(erikbern): make this a static method on the Mount class
    import synchronicity

    import modal

    # Get the base_path because it also contains `modal_proto`.
    modal_parent_dir, _ = os.path.split(modal.__path__[0])
    client_mount = _Mount._new()

    for pkg_name in MODAL_PACKAGES:
        package_base_path = Path(modal_parent_dir) / pkg_name
        client_mount = client_mount.add_local_dir(
            package_base_path,
            remote_path=f"/pkg/{pkg_name}",
            condition=module_mount_condition(package_base_path),
            recursive=True,
        )

    # Mount synchronicity, so version changes don't trigger image rebuilds for users.
    synchronicity_base_path = Path(synchronicity.__path__[0])
    client_mount = client_mount.add_local_dir(
        synchronicity_base_path,
        remote_path="/pkg/synchronicity",
        condition=module_mount_condition(synchronicity_base_path),
        recursive=True,
    )
    return client_mount


create_client_mount = synchronize_api(_create_client_mount)


def _get_client_mount():
    # TODO(erikbern): make this a static method on the Mount class
    if config["sync_entrypoint"]:
        return _create_client_mount()
    else:
        return _Mount.from_name(client_mount_name(), namespace=api_pb2.DEPLOYMENT_NAMESPACE_GLOBAL)


MODAL_PACKAGES = ["modal", "modal_proto", "modal_version"]


def _is_modal_path(remote_path: PurePosixPath):
    path_prefix = remote_path.parts[:3]
    remote_python_paths = [("/", "root"), ("/", "pkg")]
    for base in remote_python_paths:
        is_modal_path = path_prefix in [base + (mod,) for mod in MODAL_PACKAGES] or path_prefix == base + (
            "synchronicity",
        )
        if is_modal_path:
            return True
    return False


REMOTE_PACKAGES_PATH = "/__modal/deps"
REMOTE_SITECUSTOMIZE_PATH = "/pkg/sitecustomize.py"

SITECUSTOMIZE_CONTENT = f"""
# This file is automatically generated by Modal.
# It ensures that Modal's python dependencies are available in the Python PATH,
# while prioritizing user-installed packages.
import sys; sys.path.append('{REMOTE_PACKAGES_PATH}')
""".strip()


async def _create_single_client_dependency_mount(
    client: _Client,
    builder_version: str,
    python_version: str,
    arch: str,
    platform: str,
    uv_python_platform: str,
    check_if_exists: bool = True,
    allow_overwrite: bool = False,
    dry_run: bool = False,
):
    import tempfile

    profile_environment = config.get("environment")
    abi_tag = "cp" + python_version.replace(".", "")
    mount_name = f"{builder_version}-{abi_tag}-{platform}-{arch}"

    if check_if_exists:
        try:
            await Mount.from_name(mount_name, namespace=api_pb2.DEPLOYMENT_NAMESPACE_GLOBAL).hydrate.aio(client)
            print(f"âž– Found existing mount {mount_name} in global namespace.")
            return
        except modal.exception.NotFoundError:
            pass

    with tempfile.TemporaryDirectory(ignore_cleanup_errors=True) as tmpd:
        print(f"ðŸ“¦ Building {mount_name}.")
        requirements = os.path.join(os.path.dirname(__file__), f"builder/{builder_version}.txt")
        cmd = " ".join(
            [
                "uv",
                "pip",
                "install",
                "--strict",
                "--no-deps",
                "--no-cache",
                "-r",
                requirements,
                "--compile-bytecode",
                "--target",
                tmpd,
                "--python-platform",
                uv_python_platform,
                "--python-version",
                python_version,
            ]
        )
        proc = await asyncio.create_subprocess_shell(
            cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
        )
        await proc.wait()
        if proc.returncode:
            stdout, stderr = await proc.communicate()
            print(stdout.decode("utf-8"))
            print(stderr.decode("utf-8"))
            raise RuntimeError(f"Subprocess failed with {proc.returncode}")

        print(f"ðŸŒ Downloaded and unpacked {mount_name} packages to {tmpd}.")

        python_mount = Mount._from_local_dir(tmpd, remote_path=REMOTE_PACKAGES_PATH)

        with tempfile.NamedTemporaryFile() as sitecustomize:
            sitecustomize.write(
                SITECUSTOMIZE_CONTENT.encode("utf-8"),
            )
            sitecustomize.flush()

            python_mount = python_mount.add_local_file(
                sitecustomize.name,
                remote_path=REMOTE_SITECUSTOMIZE_PATH,
            )

            if not dry_run:
                try:
                    await python_mount._deploy.aio(
                        mount_name,
                        api_pb2.DEPLOYMENT_NAMESPACE_GLOBAL,
                        environment_name=profile_environment,
                        allow_overwrite=allow_overwrite,
                        client=client,
                    )
                    print(f"âœ… Deployed mount {mount_name} to global namespace.")
                except GRPCError as e:
                    print(f"âš ï¸ Mount creation failed with {e.status}: {e.message}")
            else:
                print(f"Dry run - skipping deployment of mount {mount_name}")


async def _create_client_dependency_mounts(
    client=None,
    python_versions: list[str] = list(PYTHON_STANDALONE_VERSIONS),
    builder_versions: list[str] = ["2025.06"],  # Reenable "PREVIEW" during testing
    check_if_exists=True,
    dry_run=False,
):
    arch = "x86_64"
    platform_tags = [
        ("manylinux_2_17", f"{arch}-manylinux_2_17"),  # glibc >= 2.17
        ("musllinux_1_2", f"{arch}-unknown-linux-musl"),  # musl >= 1.2
    ]
    coros = []
    for python_version in python_versions:
        for builder_version in builder_versions:
            for platform, uv_python_platform in platform_tags:
                coros.append(
                    _create_single_client_dependency_mount(
                        client,
                        builder_version,
                        python_version,
                        arch,
                        platform,
                        uv_python_platform,
                        # This check_if_exists / allow_overwrite parameterization is very awkward
                        # Also it doesn't provide a hook for overwriting a non-preview version, which
                        # in theory we may need to do at some point (hopefully not, but...)
                        check_if_exists=check_if_exists and builder_version != "PREVIEW",
                        allow_overwrite=builder_version == "PREVIEW",
                        dry_run=dry_run,
                    )
                )
    await TaskContext.gather(*coros)


create_client_dependency_mounts = synchronize_api(_create_client_dependency_mounts)



================================================
FILE: modal/network_file_system.py
================================================
# Copyright Modal Labs 2023
import functools
import os
import time
from collections.abc import AsyncIterator
from pathlib import Path, PurePosixPath
from typing import Any, BinaryIO, Callable, Optional, Union

from synchronicity.async_wrap import asynccontextmanager

import modal
from modal_proto import api_pb2

from ._object import (
    EPHEMERAL_OBJECT_HEARTBEAT_SLEEP,
    _get_environment_name,
    _Object,
    live_method,
    live_method_gen,
)
from ._resolver import Resolver
from ._utils.async_utils import TaskContext, aclosing, async_map, sync_or_async_iter, synchronize_api
from ._utils.blob_utils import LARGE_FILE_LIMIT, blob_iter, blob_upload_file
from ._utils.deprecation import warn_if_passing_namespace
from ._utils.grpc_utils import retry_transient_errors
from ._utils.hash_utils import get_sha256_hex
from ._utils.name_utils import check_object_name
from .client import _Client
from .exception import InvalidError
from .volume import FileEntry

NETWORK_FILE_SYSTEM_PUT_FILE_CLIENT_TIMEOUT = (
    10 * 60
)  # 10 min max for transferring files (does not include upload time to s3)


def network_file_system_mount_protos(
    validated_network_file_systems: list[tuple[str, "_NetworkFileSystem"]],
) -> list[api_pb2.SharedVolumeMount]:
    network_file_system_mounts = []
    # Relies on dicts being ordered (true as of Python 3.6).
    for path, volume in validated_network_file_systems:
        network_file_system_mounts.append(
            api_pb2.SharedVolumeMount(
                mount_path=path,
                shared_volume_id=volume.object_id,
            )
        )
    return network_file_system_mounts


class _NetworkFileSystem(_Object, type_prefix="sv"):
    """A shared, writable file system accessible by one or more Modal functions.

    By attaching this file system as a mount to one or more functions, they can
    share and persist data with each other.

    **Note: `NetworkFileSystem` has been deprecated and will be removed.**

    **Usage**

    ```python
    import modal

    nfs = modal.NetworkFileSystem.from_name("my-nfs", create_if_missing=True)
    app = modal.App()

    @app.function(network_file_systems={"/root/foo": nfs})
    def f():
        pass

    @app.function(network_file_systems={"/root/goo": nfs})
    def g():
        pass
    ```

    Also see the CLI methods for accessing network file systems:

    ```
    modal nfs --help
    ```

    A `NetworkFileSystem` can also be useful for some local scripting scenarios, e.g.:

    ```python notest
    nfs = modal.NetworkFileSystem.from_name("my-network-file-system")
    for chunk in nfs.read_file("my_db_dump.csv"):
        ...
    ```
    """

    @staticmethod
    def from_name(
        name: str,
        *,
        namespace=None,  # mdmd:line-hidden
        environment_name: Optional[str] = None,
        create_if_missing: bool = False,
    ) -> "_NetworkFileSystem":
        """Reference a NetworkFileSystem by its name, creating if necessary.

        This is a lazy method that defers hydrating the local object with
        metadata from Modal servers until the first time it is actually
        used.

        ```python notest
        nfs = NetworkFileSystem.from_name("my-nfs", create_if_missing=True)

        @app.function(network_file_systems={"/data": nfs})
        def f():
            pass
        ```
        """
        check_object_name(name, "NetworkFileSystem")
        warn_if_passing_namespace(namespace, "modal.NetworkFileSystem.from_name")

        async def _load(self: _NetworkFileSystem, resolver: Resolver, existing_object_id: Optional[str]):
            req = api_pb2.SharedVolumeGetOrCreateRequest(
                deployment_name=name,
                environment_name=_get_environment_name(environment_name, resolver),
                object_creation_type=(api_pb2.OBJECT_CREATION_TYPE_CREATE_IF_MISSING if create_if_missing else None),
            )
            try:
                response = await resolver.client.stub.SharedVolumeGetOrCreate(req)
                self._hydrate(response.shared_volume_id, resolver.client, None)
            except modal.exception.NotFoundError as exc:
                if exc.args[0] == "App has wrong entity vo":
                    raise InvalidError(
                        f"Attempted to mount: `{name}` as a NetworkFileSystem " + "which already exists as a Volume"
                    )
                raise

        return _NetworkFileSystem._from_loader(_load, "NetworkFileSystem()", hydrate_lazily=True)

    @classmethod
    @asynccontextmanager
    async def ephemeral(
        cls: type["_NetworkFileSystem"],
        client: Optional[_Client] = None,
        environment_name: Optional[str] = None,
        _heartbeat_sleep: float = EPHEMERAL_OBJECT_HEARTBEAT_SLEEP,  # mdmd:line-hidden
    ) -> AsyncIterator["_NetworkFileSystem"]:
        """Creates a new ephemeral network filesystem within a context manager:

        Usage:
        ```python
        with modal.NetworkFileSystem.ephemeral() as nfs:
            assert nfs.listdir("/") == []
        ```

        ```python notest
        async with modal.NetworkFileSystem.ephemeral() as nfs:
            assert await nfs.listdir("/") == []
        ```
        """
        if client is None:
            client = await _Client.from_env()
        request = api_pb2.SharedVolumeGetOrCreateRequest(
            object_creation_type=api_pb2.OBJECT_CREATION_TYPE_EPHEMERAL,
            environment_name=_get_environment_name(environment_name),
        )
        response = await client.stub.SharedVolumeGetOrCreate(request)
        async with TaskContext() as tc:
            request = api_pb2.SharedVolumeHeartbeatRequest(shared_volume_id=response.shared_volume_id)
            tc.infinite_loop(lambda: client.stub.SharedVolumeHeartbeat(request), sleep=_heartbeat_sleep)
            yield cls._new_hydrated(
                response.shared_volume_id,
                client,
                None,
                is_another_app=True,
                rep="modal.NetworkFileSystem.ephemeral()",
            )

    @staticmethod
    async def create_deployed(
        deployment_name: str,
        namespace=None,  # mdmd:line-hidden
        client: Optional[_Client] = None,
        environment_name: Optional[str] = None,
    ) -> str:
        """mdmd:hidden"""
        check_object_name(deployment_name, "NetworkFileSystem")
        warn_if_passing_namespace(namespace, "modal.NetworkFileSystem.create_deployed")
        if client is None:
            client = await _Client.from_env()
        request = api_pb2.SharedVolumeGetOrCreateRequest(
            deployment_name=deployment_name,
            environment_name=_get_environment_name(environment_name),
            object_creation_type=api_pb2.OBJECT_CREATION_TYPE_CREATE_FAIL_IF_EXISTS,
        )
        resp = await retry_transient_errors(client.stub.SharedVolumeGetOrCreate, request)
        return resp.shared_volume_id

    @staticmethod
    async def delete(name: str, client: Optional[_Client] = None, environment_name: Optional[str] = None):
        obj = await _NetworkFileSystem.from_name(name, environment_name=environment_name).hydrate(client)
        req = api_pb2.SharedVolumeDeleteRequest(shared_volume_id=obj.object_id)
        await retry_transient_errors(obj._client.stub.SharedVolumeDelete, req)

    @live_method
    async def write_file(self, remote_path: str, fp: BinaryIO, progress_cb: Optional[Callable[..., Any]] = None) -> int:
        """Write from a file object to a path on the network file system, atomically.

        Will create any needed parent directories automatically.

        If remote_path ends with `/` it's assumed to be a directory and the
        file will be uploaded with its current name to that directory.
        """
        progress_cb = progress_cb or (lambda *_, **__: None)

        sha_hash = get_sha256_hex(fp)
        fp.seek(0, os.SEEK_END)
        data_size = fp.tell()
        fp.seek(0)
        if data_size > LARGE_FILE_LIMIT:
            progress_task_id = progress_cb(name=remote_path, size=data_size)
            blob_id = await blob_upload_file(
                fp,
                self._client.stub,
                progress_report_cb=functools.partial(progress_cb, progress_task_id),
                sha256_hex=sha_hash,
            )
            req = api_pb2.SharedVolumePutFileRequest(
                shared_volume_id=self.object_id,
                path=remote_path,
                data_blob_id=blob_id,
                sha256_hex=sha_hash,
                resumable=True,
            )
        else:
            data = fp.read()
            req = api_pb2.SharedVolumePutFileRequest(
                shared_volume_id=self.object_id, path=remote_path, data=data, resumable=True
            )

        t0 = time.monotonic()
        while time.monotonic() - t0 < NETWORK_FILE_SYSTEM_PUT_FILE_CLIENT_TIMEOUT:
            response = await retry_transient_errors(self._client.stub.SharedVolumePutFile, req)
            if response.exists:
                break
        else:
            raise modal.exception.TimeoutError(f"Uploading of {remote_path} timed out")

        return data_size  # might be better if this is returned from the server

    @live_method_gen
    async def read_file(self, path: str) -> AsyncIterator[bytes]:
        """Read a file from the network file system"""
        req = api_pb2.SharedVolumeGetFileRequest(shared_volume_id=self.object_id, path=path)
        try:
            response = await retry_transient_errors(self._client.stub.SharedVolumeGetFile, req)
        except modal.exception.NotFoundError as exc:
            raise FileNotFoundError(exc.args[0])

        if response.WhichOneof("data_oneof") == "data":
            yield response.data
        else:
            async for data in blob_iter(response.data_blob_id, self._client.stub):
                yield data

    @live_method_gen
    async def iterdir(self, path: str) -> AsyncIterator[FileEntry]:
        """Iterate over all files in a directory in the network file system.

        * Passing a directory path lists all files in the directory (names are relative to the directory)
        * Passing a file path returns a list containing only that file's listing description
        * Passing a glob path (including at least one * or ** sequence) returns all files matching
        that glob path (using absolute paths)
        """
        req = api_pb2.SharedVolumeListFilesRequest(shared_volume_id=self.object_id, path=path)
        async for batch in self._client.stub.SharedVolumeListFilesStream.unary_stream(req):
            for entry in batch.entries:
                yield FileEntry._from_proto(entry)

    @live_method
    async def add_local_file(
        self,
        local_path: Union[Path, str],
        remote_path: Optional[Union[str, PurePosixPath, None]] = None,
        progress_cb: Optional[Callable[..., Any]] = None,
    ):
        local_path = Path(local_path)
        if remote_path is None:
            remote_path = PurePosixPath("/", local_path.name).as_posix()
        else:
            remote_path = PurePosixPath(remote_path).as_posix()

        with local_path.open("rb") as local_file:
            return await self.write_file(remote_path, local_file, progress_cb=progress_cb)

    @live_method
    async def add_local_dir(
        self,
        local_path: Union[Path, str],
        remote_path: Optional[Union[str, PurePosixPath, None]] = None,
        progress_cb: Optional[Callable[..., Any]] = None,
    ):
        _local_path = Path(local_path)
        if remote_path is None:
            remote_path = PurePosixPath("/", _local_path.name).as_posix()
        else:
            remote_path = PurePosixPath(remote_path).as_posix()

        assert _local_path.is_dir()

        def gen_transfers():
            for subpath in _local_path.rglob("*"):
                if subpath.is_dir():
                    continue
                relpath_str = subpath.relative_to(_local_path).as_posix()
                yield subpath, PurePosixPath(remote_path, relpath_str)

        async def _add_local_file(paths: tuple[Path, PurePosixPath]) -> int:
            return await self.add_local_file(paths[0], paths[1], progress_cb)

        async with aclosing(async_map(sync_or_async_iter(gen_transfers()), _add_local_file, concurrency=20)) as stream:
            async for _ in stream:  # consume/execute the map
                pass

    @live_method
    async def listdir(self, path: str) -> list[FileEntry]:
        """List all files in a directory in the network file system.

        * Passing a directory path lists all files in the directory (names are relative to the directory)
        * Passing a file path returns a list containing only that file's listing description
        * Passing a glob path (including at least one * or ** sequence) returns all files matching
        that glob path (using absolute paths)
        """
        return [entry async for entry in self.iterdir(path)]

    @live_method
    async def remove_file(self, path: str, recursive=False):
        """Remove a file in a network file system."""
        req = api_pb2.SharedVolumeRemoveFileRequest(shared_volume_id=self.object_id, path=path, recursive=recursive)
        try:
            await retry_transient_errors(self._client.stub.SharedVolumeRemoveFile, req)
        except modal.exception.NotFoundError as exc:
            raise FileNotFoundError(exc.args[0])


NetworkFileSystem = synchronize_api(_NetworkFileSystem)



================================================
FILE: modal/object.py
================================================
# Copyright Modal Labs 2025
from ._object import _Object
from ._utils.async_utils import synchronize_api

Object = synchronize_api(_Object, target_module=__name__)



================================================
FILE: modal/output.py
================================================
# Copyright Modal Labs 2024
"""Interface to Modal's OutputManager functionality.

These functions live here so that Modal library code can import them without
transitively importing Rich, as we do in global scope in _output.py. This allows
us to avoid importing Rich for client code that runs in the container environment.

"""

import contextlib
from collections.abc import Generator
from typing import TYPE_CHECKING, Optional

if TYPE_CHECKING:
    from ._output import OutputManager


OUTPUT_ENABLED = False


@contextlib.contextmanager
def enable_output(show_progress: bool = True) -> Generator[None, None, None]:
    """Context manager that enable output when using the Python SDK.

    This will print to stdout and stderr things such as
    1. Logs from running functions
    2. Status of creating objects
    3. Map progress

    Example:
    ```python
    app = modal.App()
    with modal.enable_output():
        with app.run():
            ...
    ```
    """
    from ._output import OutputManager

    # Toggle the output flag from within this function so that we can
    # call _get_output_manager from within the library and only import
    # the _output module if output is explicitly enabled. That prevents
    # us from trying to import rich inside a container environment where
    # it might not be installed. This is sort of hacky and I would prefer
    # a more thorough refactor where the OutputManager is fully useable
    # without rich installed, but that's a larger project.
    global OUTPUT_ENABLED

    try:
        with OutputManager.enable_output(show_progress):
            OUTPUT_ENABLED = True
            yield
    finally:
        OUTPUT_ENABLED = False


def _get_output_manager() -> Optional["OutputManager"]:
    """Interface to the OutputManager that returns None when output is not enabled."""
    if OUTPUT_ENABLED:
        from ._output import OutputManager

        return OutputManager.get()
    else:
        return None



================================================
FILE: modal/partial_function.py
================================================
# Copyright Modal Labs 2025

from ._partial_function import (
    _asgi_app,
    _batched,
    _concurrent,
    _enter,
    _exit,
    _fastapi_endpoint,
    _method,
    _PartialFunction,
    _web_endpoint,
    _web_server,
    _wsgi_app,
)
from ._utils.async_utils import synchronize_api

# The only reason these are wrapped is to get translated type stubs, they
# don't actually run any async code as of 2025-02-04:
PartialFunction = synchronize_api(_PartialFunction, target_module=__name__)
method = synchronize_api(_method, target_module=__name__)
web_endpoint = synchronize_api(_web_endpoint, target_module=__name__)
fastapi_endpoint = synchronize_api(_fastapi_endpoint, target_module=__name__)
asgi_app = synchronize_api(_asgi_app, target_module=__name__)
wsgi_app = synchronize_api(_wsgi_app, target_module=__name__)
web_server = synchronize_api(_web_server, target_module=__name__)
enter = synchronize_api(_enter, target_module=__name__)
exit = synchronize_api(_exit, target_module=__name__)
batched = synchronize_api(_batched, target_module=__name__)
concurrent = synchronize_api(_concurrent, target_module=__name__)



================================================
FILE: modal/proxy.py
================================================
# Copyright Modal Labs 2024
from typing import Optional

from modal_proto import api_pb2

from ._object import _get_environment_name, _Object
from ._resolver import Resolver
from ._utils.async_utils import synchronize_api


class _Proxy(_Object, type_prefix="pr"):
    """Proxy objects give your Modal containers a static outbound IP address.

    This can be used for connecting to a remote address with network whitelist, for example
    a database. See [the guide](https://modal.com/docs/guide/proxy-ips) for more information.
    """

    @staticmethod
    def from_name(
        name: str,
        *,
        environment_name: Optional[str] = None,
    ) -> "_Proxy":
        """Reference a Proxy by its name.

        In contrast to most other Modal objects, new Proxy objects must be
        provisioned via the Dashboard and cannot be created on the fly from code.

        """

        async def _load(self: _Proxy, resolver: Resolver, existing_object_id: Optional[str]):
            req = api_pb2.ProxyGetRequest(
                name=name,
                environment_name=_get_environment_name(environment_name, resolver),
            )
            response: api_pb2.ProxyGetResponse = await resolver.client.stub.ProxyGet(req)
            self._hydrate(response.proxy.proxy_id, resolver.client, None)

        rep = _Proxy._repr(name, environment_name)
        return _Proxy._from_loader(_load, rep, is_another_app=True)


Proxy = synchronize_api(_Proxy, target_module=__name__)



================================================
FILE: modal/py.typed
================================================
[Empty file]


================================================
FILE: modal/queue.py
================================================
# Copyright Modal Labs 2022
import queue  # The system library
import time
import warnings
from collections.abc import AsyncGenerator, AsyncIterator
from dataclasses import dataclass
from datetime import datetime
from typing import Any, Optional, Union

from google.protobuf.message import Message
from grpclib import GRPCError, Status
from synchronicity import classproperty
from synchronicity.async_wrap import asynccontextmanager

from modal_proto import api_pb2

from ._object import (
    EPHEMERAL_OBJECT_HEARTBEAT_SLEEP,
    _get_environment_name,
    _Object,
    live_method,
    live_method_gen,
)
from ._resolver import Resolver
from ._serialization import deserialize, serialize
from ._utils.async_utils import TaskContext, synchronize_api, warn_if_generator_is_not_consumed
from ._utils.deprecation import deprecation_warning, warn_if_passing_namespace
from ._utils.grpc_utils import retry_transient_errors
from ._utils.name_utils import check_object_name
from ._utils.time_utils import as_timestamp, timestamp_to_localized_dt
from .client import _Client
from .exception import AlreadyExistsError, InvalidError, NotFoundError, RequestSizeError


@dataclass
class QueueInfo:
    """Information about the Queue object."""

    # This dataclass should be limited to information that is unchanging over the lifetime of the Queue,
    # since it is transmitted from the server when the object is hydrated and could be stale when accessed.

    name: Optional[str]
    created_at: datetime
    created_by: Optional[str]


class _QueueManager:
    """Namespace with methods for managing named Queue objects."""

    @staticmethod
    async def create(
        name: str,  # Name to use for the new Queue
        *,
        allow_existing: bool = False,  # If True, no-op when the Queue already exists
        environment_name: Optional[str] = None,  # Uses active environment if not specified
        client: Optional[_Client] = None,  # Optional client with Modal credentials
    ) -> None:
        """Create a new Queue object.

        **Examples:**

        ```python notest
        modal.Queue.objects.create("my-queue")
        ```

        Queues will be created in the active environment, or another one can be specified:

        ```python notest
        modal.Queue.objects.create("my-queue", environment_name="dev")
        ```

        By default, an error will be raised if the Queue already exists, but passing
        `allow_existing=True` will make the creation attempt a no-op in this case.

        ```python notest
        modal.Queue.objects.create("my-queue", allow_existing=True)
        ```

        Note that this method does not return a local instance of the Queue. You can use
        `modal.Queue.from_name` to perform a lookup after creation.

        Added in v1.1.2.

        """
        check_object_name(name, "Queue")
        client = await _Client.from_env() if client is None else client
        object_creation_type = (
            api_pb2.OBJECT_CREATION_TYPE_CREATE_IF_MISSING
            if allow_existing
            else api_pb2.OBJECT_CREATION_TYPE_CREATE_FAIL_IF_EXISTS
        )
        req = api_pb2.QueueGetOrCreateRequest(
            deployment_name=name,
            environment_name=_get_environment_name(environment_name),
            object_creation_type=object_creation_type,
        )
        try:
            await retry_transient_errors(client.stub.QueueGetOrCreate, req)
        except GRPCError as exc:
            if exc.status == Status.ALREADY_EXISTS and not allow_existing:
                raise AlreadyExistsError(exc.message)
            else:
                raise

    @staticmethod
    async def list(
        *,
        max_objects: Optional[int] = None,  # Limit results to this size
        created_before: Optional[Union[datetime, str]] = None,  # Limit based on creation date
        environment_name: str = "",  # Uses active environment if not specified
        client: Optional[_Client] = None,  # Optional client with Modal credentials
    ) -> list["_Queue"]:
        """Return a list of hydrated Queue objects.

        **Examples:**

        ```python
        queues = modal.Queue.objects.list()
        print([q.name for q in queues])
        ```

        Queues will be retreived from the active environment, or another one can be specified:

        ```python notest
        dev_queues = modal.Queue.objects.list(environment_name="dev")
        ```

        By default, all named Queues are returned, newest to oldest. It's also possible to limit the
        number of results and to filter by creation date:

        ```python
        queues = modal.Queue.objects.list(max_objects=10, created_before="2025-01-01")
        ```

        Added in v1.1.2.

        """
        client = await _Client.from_env() if client is None else client
        if max_objects is not None and max_objects < 0:
            raise InvalidError("max_objects cannot be negative")

        items: list[api_pb2.QueueListResponse.QueueInfo] = []

        async def retrieve_page(created_before: float) -> bool:
            max_page_size = 100 if max_objects is None else min(100, max_objects - len(items))
            pagination = api_pb2.ListPagination(max_objects=max_page_size, created_before=created_before)
            req = api_pb2.QueueListRequest(
                environment_name=_get_environment_name(environment_name), pagination=pagination
            )
            resp = await retry_transient_errors(client.stub.QueueList, req)
            items.extend(resp.queues)
            finished = (len(resp.queues) < max_page_size) or (max_objects is not None and len(items) >= max_objects)
            return finished

        finished = await retrieve_page(as_timestamp(created_before))
        while True:
            if finished:
                break
            finished = await retrieve_page(items[-1].metadata.creation_info.created_at)

        queues = [
            _Queue._new_hydrated(
                item.queue_id,
                client,
                item.metadata,
                is_another_app=True,
                rep=_Queue._repr(item.name, environment_name),
            )
            for item in items
        ]
        return queues[:max_objects] if max_objects is not None else queues

    @staticmethod
    async def delete(
        name: str,  # Name of the Queue to delete
        *,
        allow_missing: bool = False,  # If True, don't raise an error if the Queue doesn't exist
        environment_name: Optional[str] = None,  # Uses active environment if not specified
        client: Optional[_Client] = None,  # Optional client with Modal credentials
    ):
        """Delete a named Queue.

        Warning: This deletes an *entire Queue*, not just a specific entry or partition.
        Deletion is irreversible and will affect any Apps currently using the Queue.

        **Examples:**

        ```python notest
        await modal.Queue.objects.delete("my-queue")
        ```

        Queues will be deleted from the active environment, or another one can be specified:

        ```python notest
        await modal.Queue.objects.delete("my-queue", environment_name="dev")
        ```

        Added in v1.1.2.

        """
        try:
            obj = await _Queue.from_name(name, environment_name=environment_name).hydrate(client)
        except NotFoundError:
            if not allow_missing:
                raise
        else:
            req = api_pb2.QueueDeleteRequest(queue_id=obj.object_id)
            await retry_transient_errors(obj._client.stub.QueueDelete, req)


QueueManager = synchronize_api(_QueueManager)


class _Queue(_Object, type_prefix="qu"):
    """Distributed, FIFO queue for data flow in Modal apps.

    The queue can contain any object serializable by `cloudpickle`, including Modal objects.

    By default, the `Queue` object acts as a single FIFO queue which supports puts and gets (blocking and non-blocking).

    **Usage**

    ```python
    from modal import Queue

    # Create an ephemeral queue which is anonymous and garbage collected
    with Queue.ephemeral() as my_queue:
        # Putting values
        my_queue.put("some value")
        my_queue.put(123)

        # Getting values
        assert my_queue.get() == "some value"
        assert my_queue.get() == 123

        # Using partitions
        my_queue.put(0)
        my_queue.put(1, partition="foo")
        my_queue.put(2, partition="bar")

        # Default and "foo" partition are ignored by the get operation.
        assert my_queue.get(partition="bar") == 2

        # Set custom 10s expiration time on "foo" partition.
        my_queue.put(3, partition="foo", partition_ttl=10)

        # (beta feature) Iterate through items in place (read immutably)
        my_queue.put(1)
        assert [v for v in my_queue.iterate()] == [0, 1]

    # You can also create persistent queues that can be used across apps
    queue = Queue.from_name("my-persisted-queue", create_if_missing=True)
    queue.put(42)
    assert queue.get() == 42
    ```

    For more examples, see the [guide](https://modal.com/docs/guide/dicts-and-queues#modal-queues).

    **Queue partitions (beta)**

    Specifying partition keys gives access to other independent FIFO partitions within the same `Queue` object.
    Across any two partitions, puts and gets are completely independent.
    For example, a put in one partition does not affect a get in any other partition.

    When no partition key is specified (by default), puts and gets will operate on a default partition.
    This default partition is also isolated from all other partitions.
    Please see the Usage section below for an example using partitions.

    **Lifetime of a queue and its partitions**

    By default, each partition is cleared 24 hours after the last `put` operation.
    A lower TTL can be specified by the `partition_ttl` argument in the `put` or `put_many` methods.
    Each partition's expiry is handled independently.

    As such, `Queue`s are best used for communication between active functions and not relied on for persistent storage.

    On app completion or after stopping an app any associated `Queue` objects are cleaned up.
    All its partitions will be cleared.

    **Limits**

    A single `Queue` can contain up to 100,000 partitions, each with up to 5,000 items. Each item can be up to 1 MiB.

    Partition keys must be non-empty and must not exceed 64 bytes.
    """

    _metadata: Optional[api_pb2.QueueMetadata] = None

    def __init__(self):
        """mdmd:hidden"""
        raise RuntimeError("Queue() is not allowed. Please use `Queue.from_name(...)` or `Queue.ephemeral()` instead.")

    @classproperty
    def objects(cls) -> _QueueManager:
        return _QueueManager

    @property
    def name(self) -> Optional[str]:
        return self._name

    def _hydrate_metadata(self, metadata: Optional[Message]):
        if metadata:
            assert isinstance(metadata, api_pb2.QueueMetadata)
            self._metadata = metadata
            self._name = metadata.name

    def _get_metadata(self) -> api_pb2.QueueMetadata:
        assert self._metadata
        return self._metadata

    @staticmethod
    def validate_partition_key(partition: Optional[str]) -> bytes:
        if partition is not None:
            partition_key = partition.encode("utf-8")
            if len(partition_key) == 0 or len(partition_key) > 64:
                raise InvalidError("Queue partition key must be between 1 and 64 characters.")
        else:
            partition_key = b""

        return partition_key

    @classmethod
    @asynccontextmanager
    async def ephemeral(
        cls: type["_Queue"],
        client: Optional[_Client] = None,
        environment_name: Optional[str] = None,
        _heartbeat_sleep: float = EPHEMERAL_OBJECT_HEARTBEAT_SLEEP,  # mdmd:line-hidden
    ) -> AsyncIterator["_Queue"]:
        """Creates a new ephemeral queue within a context manager:

        Usage:
        ```python
        from modal import Queue

        with Queue.ephemeral() as q:
            q.put(123)
        ```

        ```python notest
        async with Queue.ephemeral() as q:
            await q.put.aio(123)
        ```
        """
        if client is None:
            client = await _Client.from_env()
        request = api_pb2.QueueGetOrCreateRequest(
            object_creation_type=api_pb2.OBJECT_CREATION_TYPE_EPHEMERAL,
            environment_name=_get_environment_name(environment_name),
        )
        response = await client.stub.QueueGetOrCreate(request)
        async with TaskContext() as tc:
            request = api_pb2.QueueHeartbeatRequest(queue_id=response.queue_id)
            tc.infinite_loop(lambda: client.stub.QueueHeartbeat(request), sleep=_heartbeat_sleep)
            yield cls._new_hydrated(response.queue_id, client, response.metadata, is_another_app=True)

    @staticmethod
    def from_name(
        name: str,
        *,
        namespace=None,  # mdmd:line-hidden
        environment_name: Optional[str] = None,
        create_if_missing: bool = False,
    ) -> "_Queue":
        """Reference a named Queue, creating if necessary.

        This is a lazy method the defers hydrating the local
        object with metadata from Modal servers until the first
        time it is actually used.

        ```python
        q = modal.Queue.from_name("my-queue", create_if_missing=True)
        q.put(123)
        ```
        """
        check_object_name(name, "Queue")
        warn_if_passing_namespace(namespace, "modal.Queue.from_name")

        async def _load(self: _Queue, resolver: Resolver, existing_object_id: Optional[str]):
            req = api_pb2.QueueGetOrCreateRequest(
                deployment_name=name,
                environment_name=_get_environment_name(environment_name, resolver),
                object_creation_type=(api_pb2.OBJECT_CREATION_TYPE_CREATE_IF_MISSING if create_if_missing else None),
            )
            response = await resolver.client.stub.QueueGetOrCreate(req)
            self._hydrate(response.queue_id, resolver.client, response.metadata)

        rep = _Queue._repr(name, environment_name)
        return _Queue._from_loader(_load, rep, is_another_app=True, hydrate_lazily=True, name=name)

    @staticmethod
    async def delete(name: str, *, client: Optional[_Client] = None, environment_name: Optional[str] = None):
        """mdmd:hidden
        Delete a named Queue.

        Warning: This deletes an *entire Queue*, not just a specific entry or partition.
        Deletion is irreversible and will affect any Apps currently using the Queue.

        DEPRECATED: This method is deprecated; we recommend using `modal.Queue.objects.delete` instead.

        """
        deprecation_warning(
            (2025, 8, 6),
            "`modal.Queue.delete` is deprecated; we recommend using `modal.Queue.objects.delete` instead.",
        )
        await _Queue.objects.delete(name, environment_name=environment_name, client=client)

    @live_method
    async def info(self) -> QueueInfo:
        """Return information about the Queue object."""
        metadata = self._get_metadata()
        creation_info = metadata.creation_info
        return QueueInfo(
            name=metadata.name or None,
            created_at=timestamp_to_localized_dt(creation_info.created_at),
            created_by=creation_info.created_by or None,
        )

    async def _get_nonblocking(self, partition: Optional[str], n_values: int) -> list[Any]:
        request = api_pb2.QueueGetRequest(
            queue_id=self.object_id,
            partition_key=self.validate_partition_key(partition),
            timeout=0,
            n_values=n_values,
        )

        response = await retry_transient_errors(self._client.stub.QueueGet, request)
        if response.values:
            return [deserialize(value, self._client) for value in response.values]
        else:
            return []

    async def _get_blocking(self, partition: Optional[str], timeout: Optional[float], n_values: int) -> list[Any]:
        if timeout is not None:
            deadline = time.time() + timeout
        else:
            deadline = None

        while True:
            request_timeout = 50.0  # We prevent longer ones in order to keep the connection alive

            if deadline is not None:
                request_timeout = min(request_timeout, deadline - time.time())

            request = api_pb2.QueueGetRequest(
                queue_id=self.object_id,
                partition_key=self.validate_partition_key(partition),
                timeout=request_timeout,
                n_values=n_values,
            )

            response = await retry_transient_errors(self._client.stub.QueueGet, request)

            if response.values:
                return [deserialize(value, self._client) for value in response.values]

            if deadline is not None and time.time() > deadline:
                break

        raise queue.Empty()

    @live_method
    async def clear(self, *, partition: Optional[str] = None, all: bool = False) -> None:
        """Clear the contents of a single partition or all partitions."""
        if partition and all:
            raise InvalidError("Partition must be null when requesting to clear all.")
        request = api_pb2.QueueClearRequest(
            queue_id=self.object_id,
            partition_key=self.validate_partition_key(partition),
            all_partitions=all,
        )
        await retry_transient_errors(self._client.stub.QueueClear, request)

    @live_method
    async def get(
        self, block: bool = True, timeout: Optional[float] = None, *, partition: Optional[str] = None
    ) -> Optional[Any]:
        """Remove and return the next object in the queue.

        If `block` is `True` (the default) and the queue is empty, `get` will wait indefinitely for
        an object, or until `timeout` if specified. Raises a native `queue.Empty` exception
        if the `timeout` is reached.

        If `block` is `False`, `get` returns `None` immediately if the queue is empty. The `timeout` is
        ignored in this case.
        """

        if block:
            values = await self._get_blocking(partition, timeout, 1)
        else:
            if timeout is not None:
                warnings.warn("Timeout is ignored for non-blocking get.")
            values = await self._get_nonblocking(partition, 1)

        if values:
            return values[0]
        else:
            return None

    @live_method
    async def get_many(
        self, n_values: int, block: bool = True, timeout: Optional[float] = None, *, partition: Optional[str] = None
    ) -> list[Any]:
        """Remove and return up to `n_values` objects from the queue.

        If there are fewer than `n_values` items in the queue, return all of them.

        If `block` is `True` (the default) and the queue is empty, `get` will wait indefinitely for
        at least 1 object to be present, or until `timeout` if specified. Raises the stdlib's `queue.Empty`
        exception if the `timeout` is reached.

        If `block` is `False`, `get` returns `None` immediately if the queue is empty. The `timeout` is
        ignored in this case.
        """

        if block:
            return await self._get_blocking(partition, timeout, n_values)
        else:
            if timeout is not None:
                warnings.warn("Timeout is ignored for non-blocking get.")
            return await self._get_nonblocking(partition, n_values)

    @live_method
    async def put(
        self,
        v: Any,
        block: bool = True,
        timeout: Optional[float] = None,
        *,
        partition: Optional[str] = None,
        partition_ttl: int = 24 * 3600,  # After 24 hours of no activity, this partition will be deletd.
    ) -> None:
        """Add an object to the end of the queue.

        If `block` is `True` and the queue is full, this method will retry indefinitely or
        until `timeout` if specified. Raises the stdlib's `queue.Full` exception if the `timeout` is reached.
        If blocking it is not recommended to omit the `timeout`, as the operation could wait indefinitely.

        If `block` is `False`, this method raises `queue.Full` immediately if the queue is full. The `timeout` is
        ignored in this case."""
        await self.put_many([v], block, timeout, partition=partition, partition_ttl=partition_ttl)

    @live_method
    async def put_many(
        self,
        vs: list[Any],
        block: bool = True,
        timeout: Optional[float] = None,
        *,
        partition: Optional[str] = None,
        partition_ttl: int = 24 * 3600,  # After 24 hours of no activity, this partition will be deletd.
    ) -> None:
        """Add several objects to the end of the queue.

        If `block` is `True` and the queue is full, this method will retry indefinitely or
        until `timeout` if specified. Raises the stdlib's `queue.Full` exception if the `timeout` is reached.
        If blocking it is not recommended to omit the `timeout`, as the operation could wait indefinitely.

        If `block` is `False`, this method raises `queue.Full` immediately if the queue is full. The `timeout` is
        ignored in this case.
        """
        if block:
            await self._put_many_blocking(partition, partition_ttl, vs, timeout)
        else:
            if timeout is not None:
                warnings.warn("`timeout` argument is ignored for non-blocking put.")
            await self._put_many_nonblocking(partition, partition_ttl, vs)

    async def _put_many_blocking(
        self, partition: Optional[str], partition_ttl: int, vs: list[Any], timeout: Optional[float] = None
    ):
        vs_encoded = [serialize(v) for v in vs]

        request = api_pb2.QueuePutRequest(
            queue_id=self.object_id,
            partition_key=self.validate_partition_key(partition),
            values=vs_encoded,
            partition_ttl_seconds=partition_ttl,
        )
        try:
            await retry_transient_errors(
                self._client.stub.QueuePut,
                request,
                # A full queue will return this status.
                additional_status_codes=[Status.RESOURCE_EXHAUSTED],
                max_delay=30.0,
                max_retries=None,
                total_timeout=timeout,
            )
        except GRPCError as exc:
            if exc.status == Status.RESOURCE_EXHAUSTED:
                raise queue.Full(str(exc))
            elif "status = '413'" in exc.message:
                method = "put_many" if len(vs) > 1 else "put"
                raise RequestSizeError(f"Queue.{method} request is too large") from exc
            else:
                raise exc

    async def _put_many_nonblocking(self, partition: Optional[str], partition_ttl: int, vs: list[Any]):
        vs_encoded = [serialize(v) for v in vs]
        request = api_pb2.QueuePutRequest(
            queue_id=self.object_id,
            partition_key=self.validate_partition_key(partition),
            values=vs_encoded,
            partition_ttl_seconds=partition_ttl,
        )
        try:
            await retry_transient_errors(self._client.stub.QueuePut, request)
        except GRPCError as exc:
            if exc.status == Status.RESOURCE_EXHAUSTED:
                raise queue.Full(exc.message)
            elif "status = '413'" in exc.message:
                method = "put_many" if len(vs) > 1 else "put"
                raise RequestSizeError(f"Queue.{method} request is too large") from exc
            else:
                raise exc

    @live_method
    async def len(self, *, partition: Optional[str] = None, total: bool = False) -> int:
        """Return the number of objects in the queue partition."""
        if partition and total:
            raise InvalidError("Partition must be null when requesting total length.")
        request = api_pb2.QueueLenRequest(
            queue_id=self.object_id,
            partition_key=self.validate_partition_key(partition),
            total=total,
        )
        response = await retry_transient_errors(self._client.stub.QueueLen, request)
        return response.len

    @warn_if_generator_is_not_consumed()
    @live_method_gen
    async def iterate(
        self, *, partition: Optional[str] = None, item_poll_timeout: float = 0.0
    ) -> AsyncGenerator[Any, None]:
        """(Beta feature) Iterate through items in the queue without mutation.

        Specify `item_poll_timeout` to control how long the iterator should wait for the next time before giving up.
        """
        last_entry_id: Optional[str] = None
        validated_partition_key = self.validate_partition_key(partition)
        fetch_deadline = time.time() + item_poll_timeout

        MAX_POLL_DURATION = 30.0
        while True:
            poll_duration = max(0.0, min(MAX_POLL_DURATION, fetch_deadline - time.time()))
            request = api_pb2.QueueNextItemsRequest(
                queue_id=self.object_id,
                partition_key=validated_partition_key,
                last_entry_id=last_entry_id,
                item_poll_timeout=poll_duration,
            )

            response: api_pb2.QueueNextItemsResponse = await retry_transient_errors(
                self._client.stub.QueueNextItems, request
            )
            if response.items:
                for item in response.items:
                    yield deserialize(item.value, self._client)
                    last_entry_id = item.entry_id
                fetch_deadline = time.time() + item_poll_timeout
            elif time.time() > fetch_deadline:
                break


Queue = synchronize_api(_Queue)



================================================
FILE: modal/retries.py
================================================
# Copyright Modal Labs 2022
from datetime import timedelta
from typing import Union

from modal_proto import api_pb2

from .exception import InvalidError

MIN_INPUT_RETRY_DELAY_MS = 1000
MAX_INPUT_RETRY_DELAY_MS = 24 * 60 * 60 * 1000


class Retries:
    """Adds a retry policy to a Modal function.

    **Usage**

    ```python
    import modal
    app = modal.App()

    # Basic configuration.
    # This sets a policy of max 4 retries with 1-second delay between failures.
    @app.function(retries=4)
    def f():
        pass


    # Fixed-interval retries with 3-second delay between failures.
    @app.function(
        retries=modal.Retries(
            max_retries=2,
            backoff_coefficient=1.0,
            initial_delay=3.0,
        )
    )
    def g():
        pass


    # Exponential backoff, with retry delay doubling after each failure.
    @app.function(
        retries=modal.Retries(
            max_retries=4,
            backoff_coefficient=2.0,
            initial_delay=1.0,
        )
    )
    def h():
        pass
    ```
    """

    def __init__(
        self,
        *,
        # The maximum number of retries that can be made in the presence of failures.
        max_retries: int,
        # Coefficent controlling how much the retry delay increases each retry attempt.
        # A backoff coefficient of 1.0 creates fixed-delay where the delay period always equals the initial delay.
        backoff_coefficient: float = 2.0,
        # Number of seconds that must elapse before the first retry occurs.
        initial_delay: float = 1.0,
        # Maximum length of retry delay in seconds, preventing the delay from growing infinitely.
        max_delay: float = 60.0,
    ):
        """
        Construct a new retries policy, supporting exponential and fixed-interval delays via a backoff coefficient.
        """
        if max_retries < 0:
            raise InvalidError(f"Invalid retries number: {max_retries}. Function retries must be non-negative.")

        if max_retries > 10:
            raise InvalidError(f"Invalid retries number: {max_retries}. Retries must be between 0 and 10.")

        if max_delay < 1.0:
            raise InvalidError(f"Invalid max_delay: {max_delay}. max_delay must be at least 1 second.")

        # TODO(Jonathon): Right now we can only support a maximum delay of 60 seconds
        # b/c tasks can finish as early as after MIN_CONTAINER_IDLE_TIMEOUT seconds
        if max_delay > 60:
            raise InvalidError(f"Invalid max_delay argument: {max_delay}. Must be between 1-60 seconds.")

        if initial_delay < 0.0:
            raise InvalidError(f"Invalid initial_delay argument: {initial_delay}. Delay must be positive.")

        # initial_delay should be bounded by max_delay, but this is an extra defensive check.
        if initial_delay > 60:
            raise InvalidError(f"Invalid initial_delay argument: {initial_delay}. Must be between 0-60 seconds.")

        if not 1.0 <= backoff_coefficient <= 10.0:
            raise InvalidError(
                f"Invalid backoff_coefficient: {backoff_coefficient}. "
                "Coefficient must be between 1.0 (fixed-interval backoff) and 10.0"
            )

        self.max_retries = max_retries
        self.backoff_coefficient = backoff_coefficient
        self.initial_delay = timedelta(seconds=initial_delay)
        self.max_delay = timedelta(seconds=max_delay)

    def _to_proto(self) -> api_pb2.FunctionRetryPolicy:
        """Convert this retries policy to an internal protobuf representation."""
        return api_pb2.FunctionRetryPolicy(
            retries=self.max_retries,
            backoff_coefficient=self.backoff_coefficient,
            initial_delay_ms=self.initial_delay // timedelta(milliseconds=1),
            max_delay_ms=self.max_delay // timedelta(milliseconds=1),
        )


class RetryManager:
    """
    Helper class to apply the specified retry policy.
    """

    def __init__(self, retry_policy: api_pb2.FunctionRetryPolicy):
        self.retry_policy = retry_policy
        self.retry_count = 0

    def get_delay_ms(self) -> Union[float, None]:
        """
        Returns the delay in milliseconds before the next retry, or None
        if the maximum number of retries has been reached.
        """
        self.retry_count += 1

        if self.retry_count > self.retry_policy.retries:
            return None

        return self._retry_delay_ms(self.retry_count, self.retry_policy)

    @staticmethod
    def _retry_delay_ms(attempt_count: int, retry_policy: api_pb2.FunctionRetryPolicy) -> float:
        """
        Computes the amount of time to sleep before retrying based on the backend_coefficient and initial_delay_ms args.
        """
        if attempt_count < 1:
            raise ValueError(f"Cannot compute retry delay. attempt_count must be at least 1, but was {attempt_count}")
        delay_ms = retry_policy.initial_delay_ms * (retry_policy.backoff_coefficient ** (attempt_count - 1))
        if delay_ms < MIN_INPUT_RETRY_DELAY_MS:
            return MIN_INPUT_RETRY_DELAY_MS
        if delay_ms > MAX_INPUT_RETRY_DELAY_MS:
            return MAX_INPUT_RETRY_DELAY_MS
        return delay_ms



================================================
FILE: modal/runner.py
================================================
# Copyright Modal Labs 2025
"""Internal module for building and running Apps."""
# Note: While this is mostly internal code, the `modal.runner.deploy_app` function was
# the only way to programmatically deploy Apps for some time, so users have reached into here.
# We may eventually deprecate it from the public API, but for now we should keep that in mind.

import asyncio
import dataclasses
import os
import time
import typing
from collections.abc import AsyncGenerator
from contextlib import nullcontext
from multiprocessing.synchronize import Event
from typing import TYPE_CHECKING, Any, Optional, TypeVar

from grpclib import GRPCError, Status
from synchronicity.async_wrap import asynccontextmanager

import modal._runtime.execution_context
import modal_proto.api_pb2
from modal_proto import api_pb2

from ._functions import _Function
from ._object import _get_environment_name, _Object
from ._pty import get_pty_info
from ._resolver import Resolver
from ._traceback import print_server_warnings, traceback_contains_remote_call
from ._utils.async_utils import TaskContext, gather_cancel_on_exc, synchronize_api
from ._utils.deprecation import warn_if_passing_namespace
from ._utils.git_utils import get_git_commit_info
from ._utils.grpc_utils import retry_transient_errors
from ._utils.name_utils import check_object_name, is_valid_tag
from .client import HEARTBEAT_INTERVAL, HEARTBEAT_TIMEOUT, _Client
from .cls import _Cls
from .config import config, logger
from .environments import _get_environment_cached
from .exception import InteractiveTimeoutError, InvalidError, RemoteError, _CliUserExecutionError
from .output import _get_output_manager, enable_output
from .running_app import RunningApp, running_app_from_layout
from .sandbox import _Sandbox
from .secret import _Secret
from .stream_type import StreamType

if TYPE_CHECKING:
    from .app import _App
else:
    _App = TypeVar("_App")


V = TypeVar("V")


async def _heartbeat(client: _Client, app_id: str) -> None:
    request = api_pb2.AppHeartbeatRequest(app_id=app_id)
    # TODO(erikbern): we should capture exceptions here
    # * if request fails: destroy the client
    # * if server says the app is gone: print a helpful warning about detaching
    await retry_transient_errors(client.stub.AppHeartbeat, request, attempt_timeout=HEARTBEAT_TIMEOUT)


async def _init_local_app_existing(client: _Client, existing_app_id: str, environment_name: str) -> RunningApp:
    # Get all the objects first
    obj_req = api_pb2.AppGetLayoutRequest(app_id=existing_app_id)
    obj_resp, _ = await gather_cancel_on_exc(
        retry_transient_errors(client.stub.AppGetLayout, obj_req),
        # Cache the environment associated with the app now as we will use it later
        _get_environment_cached(environment_name, client),
    )
    app_page_url = f"https://modal.com/apps/{existing_app_id}"  # TODO (elias): this should come from the backend
    return running_app_from_layout(
        existing_app_id,
        obj_resp.app_layout,
        app_page_url=app_page_url,
    )


async def _init_local_app_new(
    client: _Client,
    description: str,
    app_state: int,  # ValueType
    environment_name: str = "",
    interactive: bool = False,
) -> RunningApp:
    app_req = api_pb2.AppCreateRequest(
        description=description,
        environment_name=environment_name,
        app_state=app_state,  # type: ignore
    )
    app_resp, _ = await gather_cancel_on_exc(  # TODO: use TaskGroup?
        retry_transient_errors(client.stub.AppCreate, app_req),
        # Cache the environment associated with the app now as we will use it later
        _get_environment_cached(environment_name, client),
    )
    logger.debug(f"Created new app with id {app_resp.app_id}")
    return RunningApp(
        app_resp.app_id,
        app_page_url=app_resp.app_page_url,
        app_logs_url=app_resp.app_logs_url,
        interactive=interactive,
    )


async def _init_local_app_from_name(
    client: _Client,
    name: str,
    environment_name: str = "",
) -> RunningApp:
    # Look up any existing deployment
    app_req = api_pb2.AppGetByDeploymentNameRequest(
        name=name,
        environment_name=environment_name,
    )
    app_resp = await retry_transient_errors(client.stub.AppGetByDeploymentName, app_req)
    existing_app_id = app_resp.app_id or None

    # Grab the app
    if existing_app_id is not None:
        return await _init_local_app_existing(client, existing_app_id, environment_name)
    else:
        return await _init_local_app_new(
            client, name, api_pb2.APP_STATE_INITIALIZING, environment_name=environment_name
        )


async def _create_all_objects(
    client: _Client,
    running_app: RunningApp,
    functions: dict[str, _Function],
    classes: dict[str, _Cls],
    environment_name: str,
) -> None:
    """Create objects that have been defined but not created on the server."""
    indexed_objects: dict[str, _Object] = {**functions, **classes}
    resolver = Resolver(
        client,
        environment_name=environment_name,
        app_id=running_app.app_id,
    )
    with resolver.display():
        # Get current objects, and reset all objects
        tag_to_object_id = {**running_app.function_ids, **running_app.class_ids}
        running_app.function_ids = {}
        running_app.class_ids = {}

        # Assign all objects
        for tag, obj in indexed_objects.items():
            # Reset object_id in case the app runs twice
            # TODO(erikbern): clean up the interface
            obj._unhydrate()

        # Preload all functions to make sure they have ids assigned before they are loaded.
        # This is important to make sure any enclosed function handle references in serialized
        # functions have ids assigned to them when the function is serialized.
        # Note: when handles/objs are merged, all objects will need to get ids pre-assigned
        # like this in order to be referrable within serialized functions
        async def _preload(tag, obj):
            existing_object_id = tag_to_object_id.get(tag)
            # Note: preload only currently implemented for Functions, returns None otherwise
            # this is to ensure that directly referenced functions from the global scope has
            # ids associated with them when they are serialized into other functions
            await resolver.preload(obj, existing_object_id)
            if obj.is_hydrated:
                tag_to_object_id[tag] = obj.object_id

        await TaskContext.gather(*(_preload(tag, obj) for tag, obj in indexed_objects.items()))

        async def _load(tag, obj):
            existing_object_id = tag_to_object_id.get(tag)
            await resolver.load(obj, existing_object_id)
            if _Function._is_id_type(obj.object_id):
                running_app.function_ids[tag] = obj.object_id
            elif _Cls._is_id_type(obj.object_id):
                running_app.class_ids[tag] = obj.object_id
            else:
                raise RuntimeError(f"Unexpected object {obj.object_id}")

        await TaskContext.gather(*(_load(tag, obj) for tag, obj in indexed_objects.items()))


async def _publish_app(
    client: _Client,
    running_app: RunningApp,
    app_state: int,  # api_pb2.AppState.value
    functions: dict[str, _Function],
    classes: dict[str, _Cls],
    name: str = "",
    tags: dict[str, str] = {},  # Additional App metadata
    deployment_tag: str = "",  # Only relevant for deployments
    commit_info: Optional[api_pb2.CommitInfo] = None,  # Git commit information
) -> tuple[str, list[api_pb2.Warning]]:
    """Wrapper for AppPublish RPC."""

    definition_ids = {obj.object_id: obj._get_metadata().definition_id for obj in functions.values()}  # type: ignore

    request = api_pb2.AppPublishRequest(
        app_id=running_app.app_id,
        name=name,
        tags=tags,
        deployment_tag=deployment_tag,
        commit_info=commit_info,
        app_state=app_state,  # type: ignore  : should be a api_pb2.AppState.value
        function_ids=running_app.function_ids,
        class_ids=running_app.class_ids,
        definition_ids=definition_ids,
    )

    try:
        response = await retry_transient_errors(client.stub.AppPublish, request)
    except GRPCError as exc:
        if exc.status == Status.INVALID_ARGUMENT or exc.status == Status.FAILED_PRECONDITION:
            raise InvalidError(exc.message)
        raise

    print_server_warnings(response.server_warnings)
    return response.url, response.server_warnings


async def _disconnect(
    client: _Client,
    app_id: str,
    reason: "modal_proto.api_pb2.AppDisconnectReason.ValueType",
    exc_str: str = "",
) -> None:
    """Tell the server the client has disconnected for this app. Terminates all running tasks
    for ephemeral apps."""

    if exc_str:
        exc_str = exc_str[:1000]  # Truncate to 1000 chars

    logger.debug("Sending app disconnect/stop request")
    req_disconnect = api_pb2.AppClientDisconnectRequest(app_id=app_id, reason=reason, exception=exc_str)
    await retry_transient_errors(client.stub.AppClientDisconnect, req_disconnect)
    logger.debug("App disconnected")


async def _status_based_disconnect(client: _Client, app_id: str, exc_info: Optional[BaseException] = None):
    """Disconnect local session of a running app, sending relevant metadata

    exc_info: Exception if an exception caused the disconnect
    """
    if isinstance(exc_info, (KeyboardInterrupt, asyncio.CancelledError)):
        reason = api_pb2.APP_DISCONNECT_REASON_KEYBOARD_INTERRUPT
    elif exc_info is not None:
        if traceback_contains_remote_call(exc_info.__traceback__):
            reason = api_pb2.APP_DISCONNECT_REASON_REMOTE_EXCEPTION
        else:
            reason = api_pb2.APP_DISCONNECT_REASON_LOCAL_EXCEPTION
    else:
        reason = api_pb2.APP_DISCONNECT_REASON_ENTRYPOINT_COMPLETED
    if isinstance(exc_info, _CliUserExecutionError):
        exc_str = repr(exc_info.__cause__)
    elif exc_info:
        exc_str = repr(exc_info)
    else:
        exc_str = ""

    await _disconnect(client, app_id, reason, exc_str)


@asynccontextmanager
async def _run_app(
    app: _App,
    *,
    client: Optional[_Client] = None,
    detach: bool = False,
    environment_name: Optional[str] = None,
    interactive: bool = False,
) -> AsyncGenerator[_App, None]:
    """mdmd:hidden"""
    if environment_name is None:
        environment_name = typing.cast(str, config.get("environment"))

    if modal._runtime.execution_context._is_currently_importing:
        raise InvalidError("Can not run an app in global scope within a container")

    if app._running_app:
        raise InvalidError(
            "App is already running and can't be started again.\n"
            "You should not use `app.run` or `run_app` within a Modal `local_entrypoint`"
        )

    if app.description is None:
        import __main__

        if "__file__" in dir(__main__):
            app.set_description(os.path.basename(__main__.__file__))
        else:
            # Interactive mode does not have __file__.
            # https://docs.python.org/3/library/__main__.html#import-main
            app.set_description(__main__.__name__)

    if client is None:
        client = await _Client.from_env()

    app_state = api_pb2.APP_STATE_DETACHED if detach else api_pb2.APP_STATE_EPHEMERAL

    output_mgr = _get_output_manager()
    if interactive and output_mgr is None:
        msg = "Interactive mode requires output to be enabled. (Use the the `modal.enable_output()` context manager.)"
        raise InvalidError(msg)

    running_app: RunningApp = await _init_local_app_new(
        client,
        app.description or "",
        environment_name=environment_name or "",
        app_state=app_state,
        interactive=interactive,
    )

    logs_timeout = config["logs_timeout"]
    async with app._set_local_app(client, running_app), TaskContext(grace=logs_timeout) as tc:
        # Start heartbeats loop to keep the client alive
        # we don't log heartbeat exceptions in detached mode
        # as losing the local connection will not affect the running app
        def heartbeat():
            return _heartbeat(client, running_app.app_id)

        heartbeat_loop = tc.infinite_loop(heartbeat, sleep=HEARTBEAT_INTERVAL, log_exception=not detach)
        logs_loop: Optional[asyncio.Task] = None

        if output_mgr is not None:
            # Defer import so this module is rich-safe
            # TODO(michael): The get_app_logs_loop function is itself rich-safe aside from accepting an OutputManager
            # as an argument, so with some refactoring we could avoid the need for this deferred import.
            from modal._output import get_app_logs_loop

            with output_mgr.make_live(output_mgr.step_progress("Initializing...")):
                initialized_msg = (
                    f"Initialized. [grey70]View run at [underline]{running_app.app_page_url}[/underline][/grey70]"
                )
                output_mgr.print(output_mgr.step_completed(initialized_msg))
                output_mgr.update_app_page_url(running_app.app_page_url or "ERROR:NO_APP_PAGE")

            # Start logs loop

            logs_loop = tc.create_task(
                get_app_logs_loop(client, output_mgr, app_id=running_app.app_id, app_logs_url=running_app.app_logs_url)
            )

        try:
            # Create all members
            await _create_all_objects(client, running_app, app._functions, app._classes, environment_name)

            # Publish the app
            await _publish_app(client, running_app, app_state, app._functions, app._classes, tags=app._tags)
        except asyncio.CancelledError as e:
            # this typically happens on sigint/ctrl-C during setup (the KeyboardInterrupt happens in the main thread)
            if output_mgr := _get_output_manager():
                output_mgr.print("Aborting app initialization...\n")

            await _status_based_disconnect(client, running_app.app_id, e)
            raise
        except BaseException as e:
            await _status_based_disconnect(client, running_app.app_id, e)
            raise

        try:
            # Show logs from dynamically created images.
            # TODO: better way to do this
            if output_mgr := _get_output_manager():
                output_mgr.enable_image_logs()

            # Yield to context
            if output_mgr := _get_output_manager():
                # Don't show status spinner in interactive mode to avoid interfering with breakpoints
                spinner_ctx = nullcontext() if interactive else output_mgr.show_status_spinner()
                with spinner_ctx:
                    yield app
            else:
                yield app
            # successful completion!
            heartbeat_loop.cancel()
            await _status_based_disconnect(client, running_app.app_id, exc_info=None)
        except KeyboardInterrupt as e:
            # this happens only if sigint comes in during the yield block above
            if detach:
                if output_mgr := _get_output_manager():
                    output_mgr.print(output_mgr.step_completed("Shutting down Modal client."))
                    output_mgr.print(
                        "The detached app keeps running. You can track its progress at: "
                        f"[magenta]{running_app.app_page_url}[/magenta]"
                        ""
                    )
                if logs_loop:
                    logs_loop.cancel()
                await _status_based_disconnect(client, running_app.app_id, e)
            else:
                if output_mgr := _get_output_manager():
                    output_mgr.print(
                        "Disconnecting from Modal - This will terminate your Modal app in a few seconds.\n"
                    )
                await _status_based_disconnect(client, running_app.app_id, e)
                if logs_loop:
                    try:
                        await asyncio.wait_for(logs_loop, timeout=logs_timeout)
                    except asyncio.TimeoutError:
                        logger.warning("Timed out waiting for final app logs.")

                if output_mgr:
                    output_mgr.print(
                        output_mgr.step_completed(
                            "App aborted. "
                            f"[grey70]View run at [underline]{running_app.app_page_url}[/underline][/grey70]"
                        )
                    )
            return
        except BaseException as e:
            logger.info("Exception during app run")
            await _status_based_disconnect(client, running_app.app_id, e)
            raise

        # wait for logs gracefully, even though the task context would do the same
        # this allows us to log a more specific warning in case the app doesn't
        # provide all logs before exit
        if logs_loop:
            try:
                await asyncio.wait_for(logs_loop, timeout=logs_timeout)
            except asyncio.TimeoutError:
                logger.warning("Timed out waiting for final app logs.")

    if output_mgr := _get_output_manager():
        output_mgr.print(
            output_mgr.step_completed(
                f"App completed. [grey70]View run at [underline]{running_app.app_page_url}[/underline][/grey70]"
            )
        )


async def _serve_update(
    app: _App,
    existing_app_id: str,
    is_ready: Event,
    environment_name: str,
) -> None:
    """mdmd:hidden"""
    # Used by child process to reinitialize a served app
    client = await _Client.from_env()
    try:
        running_app: RunningApp = await _init_local_app_existing(client, existing_app_id, environment_name)

        # Create objects
        await _create_all_objects(
            client,
            running_app,
            app._functions,
            app._classes,
            environment_name,
        )

        # Publish the updated app
        await _publish_app(
            client,
            running_app,
            app_state=api_pb2.APP_STATE_UNSPECIFIED,
            functions=app._functions,
            classes=app._classes,
            tags=app._tags,
        )

        # Communicate to the parent process
        is_ready.set()
    except asyncio.exceptions.CancelledError:
        # Stopped by parent process
        pass


@dataclasses.dataclass(frozen=True)
class DeployResult:
    """Dataclass representing the result of deploying an app."""

    app_id: str
    app_page_url: str
    app_logs_url: str
    warnings: list[str]


async def _deploy_app(
    app: _App,
    name: Optional[str] = None,
    namespace: Any = None,  # mdmd:line-hidden
    client: Optional[_Client] = None,
    environment_name: Optional[str] = None,
    tag: str = "",
) -> DeployResult:
    """Internal function for deploying an App.

    Users should prefer the `modal deploy` CLI or the `App.deploy` method.
    """
    if environment_name is None:
        environment_name = typing.cast(str, config.get("environment"))

    warn_if_passing_namespace(namespace, "modal.runner.deploy_app")

    name = name or app.name or ""
    if not name:
        raise InvalidError(
            "You need to either supply a deployment name or have a name set on the app.\n"
            "\n"
            "Examples:\n"
            'modal.runner.deploy_app(app, name="some_name")\n\n'
            "or\n"
            'app = modal.App("some-name")'
        )
    else:
        check_object_name(name, "App")

    if tag and not is_valid_tag(tag, max_length=50):
        raise InvalidError(
            f"Deployment tag {tag!r} is invalid."
            "\n\nTags may only contain alphanumeric characters, dashes, periods, and underscores, "
            "and must be 50 characters or less"
        )

    if client is None:
        client = await _Client.from_env()

    t0 = time.time()

    # Get git information to track deployment history
    commit_info_task = asyncio.create_task(get_git_commit_info())

    running_app: RunningApp = await _init_local_app_from_name(client, name, environment_name=environment_name)

    async with TaskContext(0) as tc:
        # Start heartbeats loop to keep the client alive
        def heartbeat():
            return _heartbeat(client, running_app.app_id)

        tc.infinite_loop(heartbeat, sleep=HEARTBEAT_INTERVAL)

        try:
            # Create all members
            await _create_all_objects(
                client,
                running_app,
                app._functions,
                app._classes,
                environment_name=environment_name,
            )

            commit_info = None
            try:
                commit_info = await commit_info_task
            except Exception as e:
                logger.debug("Failed to get git commit info", exc_info=e)

            app_url, warnings = await _publish_app(
                client,
                running_app,
                app_state=api_pb2.APP_STATE_DEPLOYED,
                functions=app._functions,
                classes=app._classes,
                name=name,
                tags=app._tags,
                deployment_tag=tag,
                commit_info=commit_info,
            )
        except Exception as e:
            # Note that AppClientDisconnect only stops the app if it's still initializing, and is a no-op otherwise.
            await _disconnect(client, running_app.app_id, reason=api_pb2.APP_DISCONNECT_REASON_DEPLOYMENT_EXCEPTION)
            raise e

    if output_mgr := _get_output_manager():
        t = time.time() - t0
        output_mgr.print(output_mgr.step_completed(f"App deployed in {t:.3f}s! ðŸŽ‰"))
        output_mgr.print(f"\nView Deployment: [magenta]{app_url}[/magenta]")
    return DeployResult(
        app_id=running_app.app_id,
        app_page_url=running_app.app_page_url,
        app_logs_url=running_app.app_logs_url,  # type: ignore
        warnings=[warning.message for warning in warnings],
    )


async def _interactive_shell(
    _app: _App, cmds: list[str], environment_name: str = "", pty: bool = True, **kwargs: Any
) -> None:
    """Run an interactive shell (like `bash`) within the image for this app.

    This is useful for online debugging and interactive exploration of the
    contents of this image. If `cmd` is optionally provided, it will be run
    instead of the default shell inside this image.

    **Example**

    ```python
    import modal

    app = modal.App(image=modal.Image.debian_slim().apt_install("vim"))
    ```

    You can now run this using

    ```
    modal shell script.py --cmd /bin/bash
    ```

    When calling programmatically, `kwargs` are passed to `Sandbox.create()`.
    """

    client = await _Client.from_env()
    async with _run_app(_app, client=client, environment_name=environment_name):
        sandbox_cmds = cmds if len(cmds) > 0 else ["/bin/bash"]
        sandbox_env = {
            "MODAL_TOKEN_ID": config["token_id"],
            "MODAL_TOKEN_SECRET": config["token_secret"],
            "MODAL_ENVIRONMENT": _get_environment_name(),
        }
        secrets = kwargs.pop("secrets", []) + [_Secret.from_dict(sandbox_env)]

        with enable_output():  # show any image build logs
            sandbox = await _Sandbox._create(
                "sleep",
                "100000",
                app=_app,
                secrets=secrets,
                **kwargs,
            )

        try:
            if pty:
                container_process = await sandbox._exec(
                    *sandbox_cmds, pty_info=get_pty_info(shell=True) if pty else None
                )
                await container_process.attach()
            else:
                container_process = await sandbox._exec(
                    *sandbox_cmds, stdout=StreamType.STDOUT, stderr=StreamType.STDOUT
                )
                await container_process.wait()
        except InteractiveTimeoutError:
            # Check on status of Sandbox. It may have crashed, causing connection failure.
            req = api_pb2.SandboxWaitRequest(sandbox_id=sandbox._object_id, timeout=0)
            resp = await retry_transient_errors(sandbox._client.stub.SandboxWait, req)
            if resp.result.exception:
                raise RemoteError(resp.result.exception)
            else:
                raise


run_app = synchronize_api(_run_app)
serve_update = synchronize_api(_serve_update)
deploy_app = synchronize_api(_deploy_app)
interactive_shell = synchronize_api(_interactive_shell)



================================================
FILE: modal/running_app.py
================================================
# Copyright Modal Labs 2024
from dataclasses import dataclass, field
from typing import Optional

from google.protobuf.message import Message

from modal._utils.grpc_utils import get_proto_oneof
from modal_proto import api_pb2


@dataclass
class RunningApp:
    app_id: str
    app_page_url: Optional[str] = None
    app_logs_url: Optional[str] = None
    function_ids: dict[str, str] = field(default_factory=dict)
    class_ids: dict[str, str] = field(default_factory=dict)
    object_handle_metadata: dict[str, Optional[Message]] = field(default_factory=dict)
    interactive: bool = False


def running_app_from_layout(
    app_id: str,
    app_layout: api_pb2.AppLayout,
    app_page_url: Optional[str] = None,
) -> RunningApp:
    object_handle_metadata = {}
    for obj in app_layout.objects:
        handle_metadata: Optional[Message] = get_proto_oneof(obj, "handle_metadata_oneof")
        object_handle_metadata[obj.object_id] = handle_metadata

    return RunningApp(
        app_id,
        function_ids=dict(app_layout.function_ids),
        class_ids=dict(app_layout.class_ids),
        object_handle_metadata=object_handle_metadata,
        app_page_url=app_page_url,
    )



================================================
FILE: modal/sandbox.py
================================================
# Copyright Modal Labs 2022
import asyncio
import json
import os
import time
from collections.abc import AsyncGenerator, Collection, Sequence
from dataclasses import dataclass
from typing import TYPE_CHECKING, Any, AsyncIterator, Literal, Optional, Union, overload

from ._pty import get_pty_info
from .config import config, logger

if TYPE_CHECKING:
    import _typeshed

from google.protobuf.message import Message
from grpclib import GRPCError, Status

from modal._tunnel import Tunnel
from modal.cloud_bucket_mount import _CloudBucketMount, cloud_bucket_mounts_to_proto
from modal.mount import _Mount
from modal.volume import _Volume
from modal_proto import api_pb2

from ._object import _get_environment_name, _Object
from ._resolver import Resolver
from ._resources import convert_fn_config_to_resources_config
from ._utils.async_utils import TaskContext, synchronize_api
from ._utils.deprecation import deprecation_warning
from ._utils.grpc_utils import retry_transient_errors
from ._utils.mount_utils import validate_network_file_systems, validate_volumes
from ._utils.name_utils import is_valid_object_name
from .client import _Client
from .container_process import _ContainerProcess
from .exception import AlreadyExistsError, ExecutionError, InvalidError, SandboxTerminatedError, SandboxTimeoutError
from .file_io import FileWatchEvent, FileWatchEventType, _FileIO
from .gpu import GPU_T
from .image import _Image
from .io_streams import StreamReader, StreamWriter, _StreamReader, _StreamWriter
from .network_file_system import _NetworkFileSystem, network_file_system_mount_protos
from .proxy import _Proxy
from .scheduler_placement import SchedulerPlacement
from .secret import _Secret
from .snapshot import _SandboxSnapshot
from .stream_type import StreamType

_default_image: _Image = _Image.debian_slim()


# The maximum number of bytes that can be passed to an exec on Linux.
# Though this is technically a 'server side' limit, it is unlikely to change.
# getconf ARG_MAX will show this value on a host.
#
# By probing in production, the limit is 131072 bytes (2**17).
# We need some bytes of overhead for the rest of the command line besides the args,
# e.g. 'runsc exec ...'. So we use 2**16 as the limit.
ARG_MAX_BYTES = 2**16

# This buffer extends the user-supplied timeout on ContainerExec-related RPCs. This was introduced to
# give any in-flight status codes/IO data more time to reach the client before the stream is closed.
CONTAINER_EXEC_TIMEOUT_BUFFER = 5


if TYPE_CHECKING:
    import modal.app


def _validate_exec_args(args: Sequence[str]) -> None:
    # Entrypoint args must be strings.
    if not all(isinstance(arg, str) for arg in args):
        raise InvalidError("All entrypoint arguments must be strings")
    # Avoid "[Errno 7] Argument list too long" errors.
    total_arg_len = sum(len(arg) for arg in args)
    if total_arg_len > ARG_MAX_BYTES:
        raise InvalidError(
            f"Total length of CMD arguments must be less than {ARG_MAX_BYTES} bytes (ARG_MAX). "
            f"Got {total_arg_len} bytes."
        )


def _warn_if_invalid_name(name: str) -> None:
    if not is_valid_object_name(name):
        deprecation_warning(
            (2025, 9, 3),
            f"Sandbox name '{name}' will be considered invalid in a future release."
            "\n\nNames may contain only alphanumeric characters, dashes, periods, and underscores,"
            " must be shorter than 64 characters, and cannot conflict with App ID strings.",
        )


class DefaultSandboxNameOverride(str):
    """A singleton class that represents the default sandbox name override.

    It is used to indicate that the sandbox name should not be overridden.
    """

    def __repr__(self) -> str:
        # NOTE: this must match the instance var name below in order for type stubs to work ðŸ˜¬
        return "_DEFAULT_SANDBOX_NAME_OVERRIDE"


_DEFAULT_SANDBOX_NAME_OVERRIDE = DefaultSandboxNameOverride()


@dataclass(frozen=True)
class SandboxConnectCredentials:
    """Simple data structure storing credentials for making HTTP connections to a sandbox."""

    url: str
    token: str


class _Sandbox(_Object, type_prefix="sb"):
    """A `Sandbox` object lets you interact with a running sandbox. This API is similar to Python's
    [asyncio.subprocess.Process](https://docs.python.org/3/library/asyncio-subprocess.html#asyncio.subprocess.Process).

    Refer to the [guide](https://modal.com/docs/guide/sandbox) on how to spawn and use sandboxes.
    """

    _result: Optional[api_pb2.GenericResult]
    _stdout: _StreamReader[str]
    _stderr: _StreamReader[str]
    _stdin: _StreamWriter
    _task_id: Optional[str] = None
    _tunnels: Optional[dict[int, Tunnel]] = None
    _enable_snapshot: bool = False

    @staticmethod
    def _default_pty_info() -> api_pb2.PTYInfo:
        return get_pty_info(shell=True, no_terminate_on_idle_stdin=True)

    @staticmethod
    def _new(
        args: Sequence[str],
        image: _Image,
        secrets: Collection[_Secret],
        name: Optional[str] = None,
        timeout: int = 300,
        idle_timeout: Optional[int] = None,
        workdir: Optional[str] = None,
        gpu: GPU_T = None,
        cloud: Optional[str] = None,
        region: Optional[Union[str, Sequence[str]]] = None,
        cpu: Optional[float] = None,
        memory: Optional[Union[int, tuple[int, int]]] = None,
        mounts: Sequence[_Mount] = (),
        network_file_systems: dict[Union[str, os.PathLike], _NetworkFileSystem] = {},
        block_network: bool = False,
        cidr_allowlist: Optional[Sequence[str]] = None,
        volumes: dict[Union[str, os.PathLike], Union[_Volume, _CloudBucketMount]] = {},
        pty: bool = False,
        pty_info: Optional[api_pb2.PTYInfo] = None,  # deprecated
        encrypted_ports: Sequence[int] = [],
        h2_ports: Sequence[int] = [],
        unencrypted_ports: Sequence[int] = [],
        proxy: Optional[_Proxy] = None,
        experimental_options: Optional[dict[str, bool]] = None,
        _experimental_scheduler_placement: Optional[SchedulerPlacement] = None,
        enable_snapshot: bool = False,
        verbose: bool = False,
    ) -> "_Sandbox":
        """mdmd:hidden"""

        validated_network_file_systems = validate_network_file_systems(network_file_systems)

        scheduler_placement: Optional[SchedulerPlacement] = _experimental_scheduler_placement
        if region:
            if scheduler_placement:
                raise InvalidError("`region` and `_experimental_scheduler_placement` cannot be used together")
            scheduler_placement = SchedulerPlacement(region=region)

        if isinstance(gpu, list):
            raise InvalidError(
                "Sandboxes do not support configuring a list of GPUs. "
                "Specify a single GPU configuration, e.g. gpu='a10g'"
            )

        if workdir is not None and not workdir.startswith("/"):
            raise InvalidError(f"workdir must be an absolute path, got: {workdir}")

        # Validate volumes
        validated_volumes = validate_volumes(volumes)
        cloud_bucket_mounts = [(k, v) for k, v in validated_volumes if isinstance(v, _CloudBucketMount)]
        validated_volumes = [(k, v) for k, v in validated_volumes if isinstance(v, _Volume)]

        if pty:
            pty_info = _Sandbox._default_pty_info()

        def _deps() -> list[_Object]:
            deps: list[_Object] = [image] + list(mounts) + list(secrets)
            for _, vol in validated_network_file_systems:
                deps.append(vol)
            for _, vol in validated_volumes:
                deps.append(vol)
            for _, cloud_bucket_mount in cloud_bucket_mounts:
                if cloud_bucket_mount.secret:
                    deps.append(cloud_bucket_mount.secret)
            if proxy:
                deps.append(proxy)
            return deps

        async def _load(self: _Sandbox, resolver: Resolver, _existing_object_id: Optional[str]):
            # Relies on dicts being ordered (true as of Python 3.6).
            volume_mounts = [
                api_pb2.VolumeMount(
                    mount_path=path,
                    volume_id=volume.object_id,
                    allow_background_commits=True,
                    read_only=volume._read_only,
                )
                for path, volume in validated_volumes
            ]

            open_ports = [api_pb2.PortSpec(port=port, unencrypted=False) for port in encrypted_ports]
            open_ports.extend([api_pb2.PortSpec(port=port, unencrypted=True) for port in unencrypted_ports])
            open_ports.extend(
                [
                    api_pb2.PortSpec(port=port, unencrypted=False, tunnel_type=api_pb2.TUNNEL_TYPE_H2)
                    for port in h2_ports
                ]
            )

            if block_network:
                # If the network is blocked, cidr_allowlist is invalid as we don't allow any network access.
                if cidr_allowlist is not None:
                    raise InvalidError("`cidr_allowlist` cannot be used when `block_network` is enabled")
                network_access = api_pb2.NetworkAccess(
                    network_access_type=api_pb2.NetworkAccess.NetworkAccessType.BLOCKED,
                )
            elif cidr_allowlist is None:
                # If the allowlist is empty, we allow all network access.
                network_access = api_pb2.NetworkAccess(
                    network_access_type=api_pb2.NetworkAccess.NetworkAccessType.OPEN,
                )
            else:
                network_access = api_pb2.NetworkAccess(
                    network_access_type=api_pb2.NetworkAccess.NetworkAccessType.ALLOWLIST,
                    allowed_cidrs=cidr_allowlist,
                )

            ephemeral_disk = None  # Ephemeral disk requests not supported on Sandboxes.
            definition = api_pb2.Sandbox(
                entrypoint_args=args,
                image_id=image.object_id,
                mount_ids=[mount.object_id for mount in mounts] + [mount.object_id for mount in image._mount_layers],
                secret_ids=[secret.object_id for secret in secrets],
                timeout_secs=timeout,
                idle_timeout_secs=idle_timeout,
                workdir=workdir,
                resources=convert_fn_config_to_resources_config(
                    cpu=cpu, memory=memory, gpu=gpu, ephemeral_disk=ephemeral_disk
                ),
                cloud_provider_str=cloud if cloud else None,  # Supersedes cloud_provider
                nfs_mounts=network_file_system_mount_protos(validated_network_file_systems),
                runtime=config.get("function_runtime"),
                runtime_debug=config.get("function_runtime_debug"),
                cloud_bucket_mounts=cloud_bucket_mounts_to_proto(cloud_bucket_mounts),
                volume_mounts=volume_mounts,
                pty_info=pty_info,
                scheduler_placement=scheduler_placement.proto if scheduler_placement else None,
                worker_id=config.get("worker_id"),
                open_ports=api_pb2.PortSpecs(ports=open_ports),
                network_access=network_access,
                proxy_id=(proxy.object_id if proxy else None),
                enable_snapshot=enable_snapshot,
                verbose=verbose,
                name=name,
                experimental_options=experimental_options,
            )

            create_req = api_pb2.SandboxCreateRequest(app_id=resolver.app_id, definition=definition)
            try:
                create_resp = await retry_transient_errors(resolver.client.stub.SandboxCreate, create_req)
            except GRPCError as exc:
                if exc.status == Status.ALREADY_EXISTS:
                    raise AlreadyExistsError(exc.message)
                raise exc

            sandbox_id = create_resp.sandbox_id
            self._hydrate(sandbox_id, resolver.client, None)

        return _Sandbox._from_loader(_load, "Sandbox()", deps=_deps)

    @staticmethod
    async def create(
        *args: str,  # Set the CMD of the Sandbox, overriding any CMD of the container image.
        # Associate the sandbox with an app. Required unless creating from a container.
        app: Optional["modal.app._App"] = None,
        name: Optional[str] = None,  # Optionally give the sandbox a name. Unique within an app.
        image: Optional[_Image] = None,  # The image to run as the container for the sandbox.
        env: Optional[dict[str, Optional[str]]] = None,  # Environment variables to set in the Sandbox.
        secrets: Optional[Collection[_Secret]] = None,  # Secrets to inject into the Sandbox as environment variables.
        network_file_systems: dict[Union[str, os.PathLike], _NetworkFileSystem] = {},
        timeout: int = 300,  # Maximum lifetime of the sandbox in seconds.
        # The amount of time in seconds that a sandbox can be idle before being terminated.
        idle_timeout: Optional[int] = None,
        workdir: Optional[str] = None,  # Working directory of the sandbox.
        gpu: GPU_T = None,
        cloud: Optional[str] = None,
        region: Optional[Union[str, Sequence[str]]] = None,  # Region or regions to run the sandbox on.
        # Specify, in fractional CPU cores, how many CPU cores to request.
        # Or, pass (request, limit) to additionally specify a hard limit in fractional CPU cores.
        # CPU throttling will prevent a container from exceeding its specified limit.
        cpu: Optional[Union[float, tuple[float, float]]] = None,
        # Specify, in MiB, a memory request which is the minimum memory required.
        # Or, pass (request, limit) to additionally specify a hard limit in MiB.
        memory: Optional[Union[int, tuple[int, int]]] = None,
        block_network: bool = False,  # Whether to block network access
        # List of CIDRs the sandbox is allowed to access. If None, all CIDRs are allowed.
        cidr_allowlist: Optional[Sequence[str]] = None,
        volumes: dict[
            Union[str, os.PathLike], Union[_Volume, _CloudBucketMount]
        ] = {},  # Mount points for Modal Volumes and CloudBucketMounts
        pty: bool = False,  # Enable a PTY for the Sandbox
        # List of ports to tunnel into the sandbox. Encrypted ports are tunneled with TLS.
        encrypted_ports: Sequence[int] = [],
        # List of encrypted ports to tunnel into the sandbox, using HTTP/2.
        h2_ports: Sequence[int] = [],
        # List of ports to tunnel into the sandbox without encryption.
        unencrypted_ports: Sequence[int] = [],
        # Reference to a Modal Proxy to use in front of this Sandbox.
        proxy: Optional[_Proxy] = None,
        # Enable verbose logging for sandbox operations.
        verbose: bool = False,
        experimental_options: Optional[dict[str, bool]] = None,
        # Enable memory snapshots.
        _experimental_enable_snapshot: bool = False,
        _experimental_scheduler_placement: Optional[
            SchedulerPlacement
        ] = None,  # Experimental controls over fine-grained scheduling (alpha).
        client: Optional[_Client] = None,
        environment_name: Optional[str] = None,  # *DEPRECATED* Optionally override the default environment
        pty_info: Optional[api_pb2.PTYInfo] = None,  # *DEPRECATED* Use `pty` instead. `pty` will override `pty_info`.
    ) -> "_Sandbox":
        """
        Create a new Sandbox to run untrusted, arbitrary code.

        The Sandbox's corresponding container will be created asynchronously.

        **Usage**

        ```python
        app = modal.App.lookup('sandbox-hello-world', create_if_missing=True)
        sandbox = modal.Sandbox.create("echo", "hello world", app=app)
        print(sandbox.stdout.read())
        sandbox.wait()
        ```
        """
        if environment_name is not None:
            deprecation_warning(
                (2025, 7, 16),
                "Passing `environment_name` to `Sandbox.create` is deprecated and will be removed in a future release. "
                "A sandbox's environment is determined by the app it is associated with.",
            )

        if pty_info is not None:
            deprecation_warning(
                (2025, 9, 12),
                "The `pty_info` parameter is deprecated and will be removed in a future release. "
                "Set the `pty` parameter to `True` instead.",
            )

        secrets = secrets or []
        if env:
            secrets = [*secrets, _Secret.from_dict(env)]

        return await _Sandbox._create(
            *args,
            app=app,
            name=name,
            image=image,
            secrets=secrets,
            network_file_systems=network_file_systems,
            timeout=timeout,
            idle_timeout=idle_timeout,
            workdir=workdir,
            gpu=gpu,
            cloud=cloud,
            region=region,
            cpu=cpu,
            memory=memory,
            block_network=block_network,
            cidr_allowlist=cidr_allowlist,
            volumes=volumes,
            pty=pty,
            encrypted_ports=encrypted_ports,
            h2_ports=h2_ports,
            unencrypted_ports=unencrypted_ports,
            proxy=proxy,
            experimental_options=experimental_options,
            _experimental_enable_snapshot=_experimental_enable_snapshot,
            _experimental_scheduler_placement=_experimental_scheduler_placement,
            client=client,
            verbose=verbose,
            pty_info=pty_info,
        )

    @staticmethod
    async def _create(
        *args: str,
        app: Optional["modal.app._App"] = None,
        name: Optional[str] = None,
        image: Optional[_Image] = None,
        env: Optional[dict[str, Optional[str]]] = None,
        secrets: Optional[Collection[_Secret]] = None,
        mounts: Sequence[_Mount] = (),
        network_file_systems: dict[Union[str, os.PathLike], _NetworkFileSystem] = {},
        timeout: int = 300,
        idle_timeout: Optional[int] = None,
        workdir: Optional[str] = None,
        gpu: GPU_T = None,
        cloud: Optional[str] = None,
        region: Optional[Union[str, Sequence[str]]] = None,
        cpu: Optional[Union[float, tuple[float, float]]] = None,
        memory: Optional[Union[int, tuple[int, int]]] = None,
        block_network: bool = False,
        cidr_allowlist: Optional[Sequence[str]] = None,
        volumes: dict[Union[str, os.PathLike], Union[_Volume, _CloudBucketMount]] = {},
        pty: bool = False,
        encrypted_ports: Sequence[int] = [],
        h2_ports: Sequence[int] = [],
        unencrypted_ports: Sequence[int] = [],
        proxy: Optional[_Proxy] = None,
        experimental_options: Optional[dict[str, bool]] = None,
        _experimental_enable_snapshot: bool = False,
        _experimental_scheduler_placement: Optional[SchedulerPlacement] = None,
        client: Optional[_Client] = None,
        verbose: bool = False,
        pty_info: Optional[api_pb2.PTYInfo] = None,
    ):
        """Private method used internally.

        This method exposes some internal arguments (currently `mounts`) which are not in the public API.
        `mounts` is currently only used by modal shell (cli) to provide a function's mounts to the
        sandbox that runs the shell session.
        """
        from .app import _App

        _validate_exec_args(args)
        if name is not None:
            _warn_if_invalid_name(name)

        if block_network and (encrypted_ports or h2_ports or unencrypted_ports):
            raise InvalidError("Cannot specify open ports when `block_network` is enabled")

        secrets = secrets or []
        if env:
            secrets = [*secrets, _Secret.from_dict(env)]

        # TODO(erikbern): Get rid of the `_new` method and create an already-hydrated object
        obj = _Sandbox._new(
            args,
            image=image or _default_image,
            secrets=secrets,
            name=name,
            timeout=timeout,
            idle_timeout=idle_timeout,
            workdir=workdir,
            gpu=gpu,
            cloud=cloud,
            region=region,
            cpu=cpu,
            memory=memory,
            mounts=mounts,
            network_file_systems=network_file_systems,
            block_network=block_network,
            cidr_allowlist=cidr_allowlist,
            volumes=volumes,
            pty=pty,
            pty_info=pty_info,
            encrypted_ports=encrypted_ports,
            h2_ports=h2_ports,
            unencrypted_ports=unencrypted_ports,
            proxy=proxy,
            experimental_options=experimental_options,
            _experimental_scheduler_placement=_experimental_scheduler_placement,
            enable_snapshot=_experimental_enable_snapshot,
            verbose=verbose,
        )
        obj._enable_snapshot = _experimental_enable_snapshot

        app_id: Optional[str] = None
        app_client: Optional[_Client] = None

        if app is not None:
            if app.app_id is None:
                raise ValueError(
                    "App has not been initialized yet. To create an App lazily, use `App.lookup`: \n"
                    "app = modal.App.lookup('my-app', create_if_missing=True)\n"
                    "modal.Sandbox.create('echo', 'hi', app=app)\n"
                    "In order to initialize an existing `App` object, refer to our docs: https://modal.com/docs/guide/apps"
                )

            app_id = app.app_id
            app_client = app._client
        elif (container_app := _App._get_container_app()) is not None:
            app_id = container_app.app_id
            app_client = container_app._client
        else:
            raise InvalidError(
                "Sandboxes require an App when created outside of a Modal container.\n\n"
                "Run an ephemeral App (`with app.run(): ...`), or reference a deployed App using `App.lookup`:\n\n"
                "```\n"
                'app = modal.App.lookup("sandbox-app", create_if_missing=True)\n'
                "sb = modal.Sandbox.create(..., app=app)\n"
                "```",
            )

        client = client or app_client or await _Client.from_env()

        resolver = Resolver(client, app_id=app_id)
        await resolver.load(obj)
        return obj

    def _hydrate_metadata(self, handle_metadata: Optional[Message]):
        self._stdout: _StreamReader[str] = StreamReader[str](
            api_pb2.FILE_DESCRIPTOR_STDOUT, self.object_id, "sandbox", self._client, by_line=True
        )
        self._stderr: _StreamReader[str] = StreamReader[str](
            api_pb2.FILE_DESCRIPTOR_STDERR, self.object_id, "sandbox", self._client, by_line=True
        )
        self._stdin = StreamWriter(self.object_id, "sandbox", self._client)
        self._result = None

    @staticmethod
    async def from_name(
        app_name: str,
        name: str,
        *,
        environment_name: Optional[str] = None,
        client: Optional[_Client] = None,
    ) -> "_Sandbox":
        """Get a running Sandbox by name from a deployed App.

        Raises a modal.exception.NotFoundError if no running sandbox is found with the given name.
        A Sandbox's name is the `name` argument passed to `Sandbox.create`.
        """
        if client is None:
            client = await _Client.from_env()
        env_name = _get_environment_name(environment_name)

        req = api_pb2.SandboxGetFromNameRequest(sandbox_name=name, app_name=app_name, environment_name=env_name)
        resp = await retry_transient_errors(client.stub.SandboxGetFromName, req)
        return _Sandbox._new_hydrated(resp.sandbox_id, client, None)

    @staticmethod
    async def from_id(sandbox_id: str, client: Optional[_Client] = None) -> "_Sandbox":
        """Construct a Sandbox from an id and look up the Sandbox result.

        The ID of a Sandbox object can be accessed using `.object_id`.
        """
        if client is None:
            client = await _Client.from_env()

        req = api_pb2.SandboxWaitRequest(sandbox_id=sandbox_id, timeout=0)
        resp = await retry_transient_errors(client.stub.SandboxWait, req)

        obj = _Sandbox._new_hydrated(sandbox_id, client, None)

        if resp.result.status:
            obj._result = resp.result

        return obj

    async def get_tags(self) -> dict[str, str]:
        """Fetches any tags (key-value pairs) currently attached to this Sandbox from the server."""
        req = api_pb2.SandboxTagsGetRequest(sandbox_id=self.object_id)
        try:
            resp = await retry_transient_errors(self._client.stub.SandboxTagsGet, req)
        except GRPCError as exc:
            raise InvalidError(exc.message) if exc.status == Status.INVALID_ARGUMENT else exc

        return {tag.tag_name: tag.tag_value for tag in resp.tags}

    async def set_tags(self, tags: dict[str, str], *, client: Optional[_Client] = None) -> None:
        """Set tags (key-value pairs) on the Sandbox. Tags can be used to filter results in `Sandbox.list`."""
        environment_name = _get_environment_name()
        if client is not None:
            deprecation_warning(
                (2025, 9, 18),
                "The `client` parameter is deprecated. Set `client` when creating the Sandbox instead "
                "(in e.g. `Sandbox.create()`/`.from_id()`/`.from_name()`).",
            )

        tags_list = [api_pb2.SandboxTag(tag_name=name, tag_value=value) for name, value in tags.items()]

        req = api_pb2.SandboxTagsSetRequest(
            environment_name=environment_name,
            sandbox_id=self.object_id,
            tags=tags_list,
        )
        try:
            await retry_transient_errors(self._client.stub.SandboxTagsSet, req)
        except GRPCError as exc:
            raise InvalidError(exc.message) if exc.status == Status.INVALID_ARGUMENT else exc

    async def snapshot_filesystem(self, timeout: int = 55) -> _Image:
        """Snapshot the filesystem of the Sandbox.

        Returns an [`Image`](https://modal.com/docs/reference/modal.Image) object which
        can be used to spawn a new Sandbox with the same filesystem.
        """
        await self._get_task_id()  # Ensure the sandbox has started
        req = api_pb2.SandboxSnapshotFsRequest(sandbox_id=self.object_id, timeout=timeout)
        resp = await retry_transient_errors(self._client.stub.SandboxSnapshotFs, req)

        if resp.result.status != api_pb2.GenericResult.GENERIC_STATUS_SUCCESS:
            raise ExecutionError(resp.result.exception)

        image_id = resp.image_id
        metadata = resp.image_metadata

        async def _load(self: _Image, resolver: Resolver, existing_object_id: Optional[str]):
            # no need to hydrate again since we do it eagerly below
            pass

        rep = "Image()"
        image = _Image._from_loader(_load, rep, hydrate_lazily=True)
        image._hydrate(image_id, self._client, metadata)  # hydrating eagerly since we have all of the data

        return image

    # Live handle methods

    async def wait(self, raise_on_termination: bool = True):
        """Wait for the Sandbox to finish running."""

        while True:
            req = api_pb2.SandboxWaitRequest(sandbox_id=self.object_id, timeout=10)
            resp = await retry_transient_errors(self._client.stub.SandboxWait, req)
            if resp.result.status:
                logger.debug(f"Sandbox {self.object_id} wait completed with status {resp.result.status}")
                self._result = resp.result

                if resp.result.status == api_pb2.GenericResult.GENERIC_STATUS_TIMEOUT:
                    raise SandboxTimeoutError()
                elif resp.result.status == api_pb2.GenericResult.GENERIC_STATUS_TERMINATED and raise_on_termination:
                    raise SandboxTerminatedError()
                break

    async def tunnels(self, timeout: int = 50) -> dict[int, Tunnel]:
        """Get Tunnel metadata for the sandbox.

        Raises `SandboxTimeoutError` if the tunnels are not available after the timeout.

        Returns a dictionary of `Tunnel` objects which are keyed by the container port.

        NOTE: Previous to client [v0.64.153](https://modal.com/docs/reference/changelog#064153-2024-09-30), this
        returned a list of `TunnelData` objects.
        """

        if self._tunnels:
            return self._tunnels

        req = api_pb2.SandboxGetTunnelsRequest(sandbox_id=self.object_id, timeout=timeout)
        resp = await retry_transient_errors(self._client.stub.SandboxGetTunnels, req)

        # If we couldn't get the tunnels in time, report the timeout.
        if resp.result.status == api_pb2.GenericResult.GENERIC_STATUS_TIMEOUT:
            raise SandboxTimeoutError()

        # Otherwise, we got the tunnels and can report the result.
        self._tunnels = {
            t.container_port: Tunnel(t.host, t.port, t.unencrypted_host, t.unencrypted_port) for t in resp.tunnels
        }

        return self._tunnels

    async def create_connect_token(
        self, user_metadata: Optional[Union[str, dict[str, Any]]] = None
    ) -> SandboxConnectCredentials:
        """mdmd:hidden
        [Alpha] Create a token for making HTTP connections to the sandbox.

        Also accepts an optional user_metadata string or dict to associate with the token. This metadata
        will be added to the headers by the proxy when forwarding requests to the sandbox."""
        if user_metadata is not None and isinstance(user_metadata, dict):
            try:
                user_metadata = json.dumps(user_metadata)
            except Exception as e:
                raise InvalidError(f"Failed to serialize user_metadata: {e}")

        req = api_pb2.SandboxCreateConnectTokenRequest(sandbox_id=self.object_id, user_metadata=user_metadata)
        resp = await retry_transient_errors(self._client.stub.SandboxCreateConnectToken, req)
        return SandboxConnectCredentials(resp.url, resp.token)

    async def reload_volumes(self) -> None:
        """Reload all Volumes mounted in the Sandbox.

        Added in v1.1.0.
        """
        task_id = await self._get_task_id()
        await retry_transient_errors(
            self._client.stub.ContainerReloadVolumes,
            api_pb2.ContainerReloadVolumesRequest(
                task_id=task_id,
            ),
        )

    async def terminate(self) -> None:
        """Terminate Sandbox execution.

        This is a no-op if the Sandbox has already finished running."""

        await retry_transient_errors(
            self._client.stub.SandboxTerminate, api_pb2.SandboxTerminateRequest(sandbox_id=self.object_id)
        )

    async def poll(self) -> Optional[int]:
        """Check if the Sandbox has finished running.

        Returns `None` if the Sandbox is still running, else returns the exit code.
        """

        req = api_pb2.SandboxWaitRequest(sandbox_id=self.object_id, timeout=0)
        resp = await retry_transient_errors(self._client.stub.SandboxWait, req)

        if resp.result.status:
            self._result = resp.result

        return self.returncode

    async def _get_task_id(self) -> str:
        while not self._task_id:
            resp = await retry_transient_errors(
                self._client.stub.SandboxGetTaskId, api_pb2.SandboxGetTaskIdRequest(sandbox_id=self.object_id)
            )
            self._task_id = resp.task_id
            if not self._task_id:
                await asyncio.sleep(0.5)
        return self._task_id

    @overload
    async def exec(
        self,
        *args: str,
        stdout: StreamType = StreamType.PIPE,
        stderr: StreamType = StreamType.PIPE,
        timeout: Optional[int] = None,
        workdir: Optional[str] = None,
        env: Optional[dict[str, Optional[str]]] = None,
        secrets: Optional[Collection[_Secret]] = None,
        text: Literal[True] = True,
        bufsize: Literal[-1, 1] = -1,
        pty: bool = False,
        pty_info: Optional[api_pb2.PTYInfo] = None,
        _pty_info: Optional[api_pb2.PTYInfo] = None,
    ) -> _ContainerProcess[str]: ...

    @overload
    async def exec(
        self,
        *args: str,
        stdout: StreamType = StreamType.PIPE,
        stderr: StreamType = StreamType.PIPE,
        timeout: Optional[int] = None,
        workdir: Optional[str] = None,
        env: Optional[dict[str, Optional[str]]] = None,
        secrets: Optional[Collection[_Secret]] = None,
        text: Literal[False] = False,
        bufsize: Literal[-1, 1] = -1,
        pty: bool = False,
        pty_info: Optional[api_pb2.PTYInfo] = None,
        _pty_info: Optional[api_pb2.PTYInfo] = None,
    ) -> _ContainerProcess[bytes]: ...

    async def exec(
        self,
        *args: str,
        stdout: StreamType = StreamType.PIPE,
        stderr: StreamType = StreamType.PIPE,
        timeout: Optional[int] = None,
        workdir: Optional[str] = None,
        env: Optional[dict[str, Optional[str]]] = None,  # Environment variables to set during command execution.
        secrets: Optional[
            Collection[_Secret]
        ] = None,  # Secrets to inject as environment variables during command execution.
        # Encode output as text.
        text: bool = True,
        # Control line-buffered output.
        # -1 means unbuffered, 1 means line-buffered (only available if `text=True`).
        bufsize: Literal[-1, 1] = -1,
        pty: bool = False,  # Enable a PTY for the command
        _pty_info: Optional[api_pb2.PTYInfo] = None,  # *DEPRECATED* Use `pty` instead. `pty` will override `pty_info`.
        pty_info: Optional[api_pb2.PTYInfo] = None,  # *DEPRECATED* Use `pty` instead. `pty` will override `pty_info`.
    ):
        """Execute a command in the Sandbox and return a ContainerProcess handle.

        See the [`ContainerProcess`](https://modal.com/docs/reference/modal.container_process#modalcontainer_processcontainerprocess)
        docs for more information.

        **Usage**

        ```python
        app = modal.App.lookup("my-app", create_if_missing=True)

        sandbox = modal.Sandbox.create("sleep", "infinity", app=app)

        process = sandbox.exec("bash", "-c", "for i in $(seq 1 10); do echo foo $i; sleep 0.5; done")

        for line in process.stdout:
            print(line)
        ```
        """
        if pty_info is not None or _pty_info is not None:
            deprecation_warning(
                (2025, 9, 12),
                "The `_pty_info` and `pty_info` parameters are deprecated and will be removed in a future release. "
                "Set the `pty` parameter to `True` instead.",
            )
        pty_info = _pty_info or pty_info
        if pty:
            pty_info = self._default_pty_info()

        return await self._exec(
            *args,
            pty_info=pty_info,
            stdout=stdout,
            stderr=stderr,
            timeout=timeout,
            workdir=workdir,
            env=env,
            secrets=secrets,
            text=text,
            bufsize=bufsize,
        )

    async def _exec(
        self,
        *args: str,
        pty_info: Optional[api_pb2.PTYInfo] = None,
        stdout: StreamType = StreamType.PIPE,
        stderr: StreamType = StreamType.PIPE,
        timeout: Optional[int] = None,
        workdir: Optional[str] = None,
        env: Optional[dict[str, Optional[str]]] = None,
        secrets: Optional[Collection[_Secret]] = None,
        text: bool = True,
        bufsize: Literal[-1, 1] = -1,
    ) -> Union[_ContainerProcess[bytes], _ContainerProcess[str]]:
        """Private method used internally.

        This method exposes some internal arguments (currently `pty_info`) which are not in the public API.
        """
        if workdir is not None and not workdir.startswith("/"):
            raise InvalidError(f"workdir must be an absolute path, got: {workdir}")
        _validate_exec_args(args)

        secrets = secrets or []
        if env:
            secrets = [*secrets, _Secret.from_dict(env)]

        # Force secret resolution so we can pass the secret IDs to the backend.
        secret_coros = [secret.hydrate(client=self._client) for secret in secrets]
        await TaskContext.gather(*secret_coros)

        task_id = await self._get_task_id()
        req = api_pb2.ContainerExecRequest(
            task_id=task_id,
            command=args,
            pty_info=pty_info,
            runtime_debug=config.get("function_runtime_debug"),
            timeout_secs=timeout or 0,
            workdir=workdir,
            secret_ids=[secret.object_id for secret in secrets],
        )
        resp = await retry_transient_errors(self._client.stub.ContainerExec, req)
        by_line = bufsize == 1
        exec_deadline = time.monotonic() + int(timeout) + CONTAINER_EXEC_TIMEOUT_BUFFER if timeout else None
        logger.debug(f"Created ContainerProcess for exec_id {resp.exec_id} on Sandbox {self.object_id}")
        return _ContainerProcess(
            resp.exec_id,
            self._client,
            stdout=stdout,
            stderr=stderr,
            text=text,
            exec_deadline=exec_deadline,
            by_line=by_line,
        )

    async def _experimental_snapshot(self) -> _SandboxSnapshot:
        await self._get_task_id()
        snap_req = api_pb2.SandboxSnapshotRequest(sandbox_id=self.object_id)
        snap_resp = await retry_transient_errors(self._client.stub.SandboxSnapshot, snap_req)

        snapshot_id = snap_resp.snapshot_id

        # wait for the snapshot to succeed. this is implemented as a second idempotent rpc
        # because the snapshot itself may take a while to complete.
        wait_req = api_pb2.SandboxSnapshotWaitRequest(snapshot_id=snapshot_id, timeout=55.0)
        wait_resp = await retry_transient_errors(self._client.stub.SandboxSnapshotWait, wait_req)
        if wait_resp.result.status != api_pb2.GenericResult.GENERIC_STATUS_SUCCESS:
            raise ExecutionError(wait_resp.result.exception)

        async def _load(self: _SandboxSnapshot, resolver: Resolver, existing_object_id: Optional[str]):
            # we eagerly hydrate the sandbox snapshot below
            pass

        rep = "SandboxSnapshot()"
        obj = _SandboxSnapshot._from_loader(_load, rep, hydrate_lazily=True)
        obj._hydrate(snapshot_id, self._client, None)

        return obj

    @staticmethod
    async def _experimental_from_snapshot(
        snapshot: _SandboxSnapshot,
        client: Optional[_Client] = None,
        *,
        name: Optional[str] = _DEFAULT_SANDBOX_NAME_OVERRIDE,
    ):
        client = client or await _Client.from_env()

        if name is not None and name != _DEFAULT_SANDBOX_NAME_OVERRIDE:
            _warn_if_invalid_name(name)

        if name is _DEFAULT_SANDBOX_NAME_OVERRIDE:
            restore_req = api_pb2.SandboxRestoreRequest(
                snapshot_id=snapshot.object_id,
                sandbox_name_override_type=api_pb2.SandboxRestoreRequest.SANDBOX_NAME_OVERRIDE_TYPE_UNSPECIFIED,
            )
        elif name is None:
            restore_req = api_pb2.SandboxRestoreRequest(
                snapshot_id=snapshot.object_id,
                sandbox_name_override_type=api_pb2.SandboxRestoreRequest.SANDBOX_NAME_OVERRIDE_TYPE_NONE,
            )
        else:
            restore_req = api_pb2.SandboxRestoreRequest(
                snapshot_id=snapshot.object_id,
                sandbox_name_override=name,
                sandbox_name_override_type=api_pb2.SandboxRestoreRequest.SANDBOX_NAME_OVERRIDE_TYPE_STRING,
            )
        try:
            restore_resp: api_pb2.SandboxRestoreResponse = await retry_transient_errors(
                client.stub.SandboxRestore, restore_req
            )
        except GRPCError as exc:
            if exc.status == Status.ALREADY_EXISTS:
                raise AlreadyExistsError(exc.message)
            raise exc

        sandbox = await _Sandbox.from_id(restore_resp.sandbox_id, client)

        task_id_req = api_pb2.SandboxGetTaskIdRequest(
            sandbox_id=restore_resp.sandbox_id, wait_until_ready=True, timeout=55.0
        )
        resp = await retry_transient_errors(client.stub.SandboxGetTaskId, task_id_req)
        if resp.task_result.status not in [
            api_pb2.GenericResult.GENERIC_STATUS_UNSPECIFIED,
            api_pb2.GenericResult.GENERIC_STATUS_SUCCESS,
        ]:
            raise ExecutionError(resp.task_result.exception)
        return sandbox

    @overload
    async def open(
        self,
        path: str,
        mode: "_typeshed.OpenTextMode",
    ) -> _FileIO[str]: ...

    @overload
    async def open(
        self,
        path: str,
        mode: "_typeshed.OpenBinaryMode",
    ) -> _FileIO[bytes]: ...

    async def open(
        self,
        path: str,
        mode: Union["_typeshed.OpenTextMode", "_typeshed.OpenBinaryMode"] = "r",
    ):
        """[Alpha] Open a file in the Sandbox and return a FileIO handle.

        See the [`FileIO`](https://modal.com/docs/reference/modal.file_io#modalfile_iofileio) docs for more information.

        **Usage**

        ```python notest
        sb = modal.Sandbox.create(app=sb_app)
        f = sb.open("/test.txt", "w")
        f.write("hello")
        f.close()
        ```
        """
        task_id = await self._get_task_id()
        return await _FileIO.create(path, mode, self._client, task_id)

    async def ls(self, path: str) -> list[str]:
        """[Alpha] List the contents of a directory in the Sandbox."""
        task_id = await self._get_task_id()
        return await _FileIO.ls(path, self._client, task_id)

    async def mkdir(self, path: str, parents: bool = False) -> None:
        """[Alpha] Create a new directory in the Sandbox."""
        task_id = await self._get_task_id()
        return await _FileIO.mkdir(path, self._client, task_id, parents)

    async def rm(self, path: str, recursive: bool = False) -> None:
        """[Alpha] Remove a file or directory in the Sandbox."""
        task_id = await self._get_task_id()
        return await _FileIO.rm(path, self._client, task_id, recursive)

    async def watch(
        self,
        path: str,
        filter: Optional[list[FileWatchEventType]] = None,
        recursive: Optional[bool] = None,
        timeout: Optional[int] = None,
    ) -> AsyncIterator[FileWatchEvent]:
        """[Alpha] Watch a file or directory in the Sandbox for changes."""
        task_id = await self._get_task_id()
        async for event in _FileIO.watch(path, self._client, task_id, filter, recursive, timeout):
            yield event

    @property
    def stdout(self) -> _StreamReader[str]:
        """
        [`StreamReader`](https://modal.com/docs/reference/modal.io_streams#modalio_streamsstreamreader) for
        the sandbox's stdout stream.
        """

        return self._stdout

    @property
    def stderr(self) -> _StreamReader[str]:
        """[`StreamReader`](https://modal.com/docs/reference/modal.io_streams#modalio_streamsstreamreader) for
        the Sandbox's stderr stream.
        """

        return self._stderr

    @property
    def stdin(self) -> _StreamWriter:
        """
        [`StreamWriter`](https://modal.com/docs/reference/modal.io_streams#modalio_streamsstreamwriter) for
        the Sandbox's stdin stream.
        """

        return self._stdin

    @property
    def returncode(self) -> Optional[int]:
        """Return code of the Sandbox process if it has finished running, else `None`."""
        if self._result is None or self._result.status == api_pb2.GenericResult.GENERIC_STATUS_UNSPECIFIED:
            return None

        # Statuses are converted to exitcodes so we can conform to subprocess API.
        # TODO: perhaps there should be a separate property that returns an enum directly?
        elif self._result.status == api_pb2.GenericResult.GENERIC_STATUS_TIMEOUT:
            return 124
        elif self._result.status == api_pb2.GenericResult.GENERIC_STATUS_TERMINATED:
            return 137
        else:
            return self._result.exitcode

    @staticmethod
    async def list(
        *, app_id: Optional[str] = None, tags: Optional[dict[str, str]] = None, client: Optional[_Client] = None
    ) -> AsyncGenerator["_Sandbox", None]:
        """List all Sandboxes for the current Environment or App ID (if specified). If tags are specified, only
        Sandboxes that have at least those tags are returned. Returns an iterator over `Sandbox` objects."""
        before_timestamp = None
        environment_name = _get_environment_name()
        if client is None:
            client = await _Client.from_env()

        tags_list = [api_pb2.SandboxTag(tag_name=name, tag_value=value) for name, value in tags.items()] if tags else []

        while True:
            req = api_pb2.SandboxListRequest(
                app_id=app_id,
                before_timestamp=before_timestamp,
                environment_name=environment_name,
                include_finished=False,
                tags=tags_list,
            )

            # Fetches a batch of sandboxes.
            try:
                resp = await retry_transient_errors(client.stub.SandboxList, req)
            except GRPCError as exc:
                raise InvalidError(exc.message) if exc.status == Status.INVALID_ARGUMENT else exc

            if not resp.sandboxes:
                return

            for sandbox_info in resp.sandboxes:
                sandbox_info: api_pb2.SandboxInfo
                obj = _Sandbox._new_hydrated(sandbox_info.id, client, None)
                obj._result = sandbox_info.task_info.result  # TODO: send SandboxInfo as metadata to _new_hydrated?
                yield obj

            # Fetch the next batch starting from the end of the current one.
            before_timestamp = resp.sandboxes[-1].created_at


Sandbox = synchronize_api(_Sandbox)



================================================
FILE: modal/schedule.py
================================================
# Copyright Modal Labs 2022
from modal_proto import api_pb2


class Schedule:
    """Schedules represent a time frame to repeatedly run a Modal function."""

    def __init__(self, proto_message):
        self.proto_message = proto_message


class Cron(Schedule):
    """Cron jobs are a type of schedule, specified using the
    [Unix cron tab](https://crontab.guru/) syntax.

    The alternative schedule type is the [`modal.Period`](https://modal.com/docs/reference/modal.Period).

    **Usage**

    ```python
    import modal
    app = modal.App()


    @app.function(schedule=modal.Cron("* * * * *"))
    def f():
        print("This function will run every minute")
    ```

    We can specify different schedules with cron strings, for example:

    ```python
    modal.Cron("5 4 * * *")  # run at 4:05am UTC every night
    modal.Cron("0 9 * * 4")  # runs every Thursday at 9am UTC
    ```

    We can also optionally specify a timezone, for example:

    ```python
    # Run daily at 6am New York time, regardless of whether daylight saving
    # is in effect (i.e. at 11am UTC in the winter, and 10am UTC in the summer):
    modal.Cron("0 6 * * *", timezone="America/New_York")
    ```

    If no timezone is specified, the default is UTC.
    """

    def __init__(
        self,
        cron_string: str,
        timezone: str = "UTC",
    ) -> None:
        """Construct a schedule that runs according to a cron expression string."""
        cron = api_pb2.Schedule.Cron(cron_string=cron_string, timezone=timezone)
        super().__init__(api_pb2.Schedule(cron=cron))


class Period(Schedule):
    """Create a schedule that runs every given time interval.

    **Usage**

    ```python
    import modal
    app = modal.App()

    @app.function(schedule=modal.Period(days=1))
    def f():
        print("This function will run every day")

    modal.Period(hours=4)          # runs every 4 hours
    modal.Period(minutes=15)       # runs every 15 minutes
    modal.Period(seconds=math.pi)  # runs every 3.141592653589793 seconds
    ```

    Only `seconds` can be a float. All other arguments are integers.

    Note that `days=1` will trigger the function the same time every day.
    This does not have the same behavior as `seconds=84000` since days have
    different lengths due to daylight savings and leap seconds. Similarly,
    using `months=1` will trigger the function on the same day each month.

    This behaves similar to the
    [dateutil](https://dateutil.readthedocs.io/en/latest/relativedelta.html)
    package.
    """

    def __init__(
        self,
        *,
        years: int = 0,
        months: int = 0,
        weeks: int = 0,
        days: int = 0,
        hours: int = 0,
        minutes: int = 0,
        seconds: float = 0,
    ) -> None:
        period = api_pb2.Schedule.Period(
            years=years,
            months=months,
            weeks=weeks,
            days=days,
            hours=hours,
            minutes=minutes,
            seconds=seconds,
        )
        super().__init__(api_pb2.Schedule(period=period))



================================================
FILE: modal/scheduler_placement.py
================================================
# Copyright Modal Labs 2024
from collections.abc import Sequence
from typing import Optional, Union

from modal_proto import api_pb2


class SchedulerPlacement:
    """mdmd:hidden This is an experimental feature."""

    proto: api_pb2.SchedulerPlacement

    def __init__(
        self,
        region: Optional[Union[str, Sequence[str]]] = None,
        zone: Optional[str] = None,
        spot: Optional[bool] = None,
        instance_type: Optional[Union[str, Sequence[str]]] = None,
    ):
        """mdmd:hidden"""
        _lifecycle: Optional[str] = None
        if spot is not None:
            _lifecycle = "spot" if spot else "on-demand"

        regions = []
        if region:
            if isinstance(region, str):
                regions = [region]
            else:
                regions = list(region)

        instance_types = []
        if instance_type:
            if isinstance(instance_type, str):
                instance_types = [instance_type]
            else:
                instance_types = list(instance_type)

        self.proto = api_pb2.SchedulerPlacement(
            regions=regions,
            _zone=zone,
            _lifecycle=_lifecycle,
            _instance_types=instance_types,
        )



================================================
FILE: modal/secret.py
================================================
# Copyright Modal Labs 2022
import os
from dataclasses import dataclass
from datetime import datetime
from typing import Optional, Union

from google.protobuf.message import Message
from grpclib import GRPCError, Status
from synchronicity import classproperty

from modal_proto import api_pb2

from ._object import _get_environment_name, _Object, live_method
from ._resolver import Resolver
from ._runtime.execution_context import is_local
from ._utils.async_utils import synchronize_api
from ._utils.deprecation import deprecation_warning, warn_if_passing_namespace
from ._utils.grpc_utils import retry_transient_errors
from ._utils.name_utils import check_object_name
from ._utils.time_utils import as_timestamp, timestamp_to_localized_dt
from .client import _Client
from .exception import AlreadyExistsError, InvalidError, NotFoundError

ENV_DICT_WRONG_TYPE_ERR = "the env_dict argument to Secret has to be a dict[str, Union[str, None]]"


@dataclass
class SecretInfo:
    """Information about the Secret object."""

    # This dataclass should be limited to information that is unchanging over the lifetime of the Secret,
    # since it is transmitted from the server when the object is hydrated and could be stale when accessed.

    name: Optional[str]
    created_at: datetime
    created_by: Optional[str]


class _SecretManager:
    """Namespace with methods for managing named Secret objects."""

    @staticmethod
    async def create(
        name: str,  # Name to use for the new Secret
        env_dict: dict[str, str],  # Key-value pairs to set in the Secret
        *,
        allow_existing: bool = False,  # If True, no-op when the Secret already exists
        environment_name: Optional[str] = None,  # Uses active environment if not specified
        client: Optional[_Client] = None,  # Optional client with Modal credentials
    ) -> None:
        """Create a new Secret object.

        **Examples:**

        ```python notest
        contents = {"MY_KEY": "my-value", "MY_OTHER_KEY": "my-other-value"}
        modal.Secret.objects.create("my-secret", contents)
        ```

        Secrets will be created in the active environment, or another one can be specified:

        ```python notest
        modal.Secret.objects.create("my-secret", contents, environment_name="dev")
        ```

        By default, an error will be raised if the Secret already exists, but passing
        `allow_existing=True` will make the creation attempt a no-op in this case.
        If the `env_dict` data differs from the existing Secret, it will be ignored.

        ```python notest
        modal.Secret.objects.create("my-secret", contents, allow_existing=True)
        ```

        Note that this method does not return a local instance of the Secret. You can use
        `modal.Secret.from_name` to perform a lookup after creation.

        Added in v1.1.2.

        """
        check_object_name(name, "Secret")
        client = await _Client.from_env() if client is None else client
        object_creation_type = (
            api_pb2.OBJECT_CREATION_TYPE_CREATE_IF_MISSING
            if allow_existing
            else api_pb2.OBJECT_CREATION_TYPE_CREATE_FAIL_IF_EXISTS
        )
        req = api_pb2.SecretGetOrCreateRequest(
            deployment_name=name,
            environment_name=_get_environment_name(environment_name),
            object_creation_type=object_creation_type,
            env_dict=env_dict,
        )
        try:
            await retry_transient_errors(client.stub.SecretGetOrCreate, req)
        except GRPCError as exc:
            if exc.status == Status.ALREADY_EXISTS and not allow_existing:
                raise AlreadyExistsError(exc.message)
            else:
                raise

    @staticmethod
    async def list(
        *,
        max_objects: Optional[int] = None,  # Limit requests to this size
        created_before: Optional[Union[datetime, str]] = None,  # Limit based on creation date
        environment_name: str = "",  # Uses active environment if not specified
        client: Optional[_Client] = None,  # Optional client with Modal credentials
    ) -> list["_Secret"]:
        """Return a list of hydrated Secret objects.

        **Examples:**

        ```python
        secrets = modal.Secret.objects.list()
        print([s.name for s in secrets])
        ```

        Secrets will be retreived from the active environment, or another one can be specified:

        ```python notest
        dev_secrets = modal.Secret.objects.list(environment_name="dev")
        ```

        By default, all named Secrets are returned, newest to oldest. It's also possible to limit the
        number of results and to filter by creation date:

        ```python
        secrets = modal.Secret.objects.list(max_objects=10, created_before="2025-01-01")
        ```

        Added in v1.1.2.

        """
        client = await _Client.from_env() if client is None else client
        if max_objects is not None and max_objects < 0:
            raise InvalidError("max_objects cannot be negative")

        items: list[api_pb2.SecretListItem] = []

        async def retrieve_page(created_before: float) -> bool:
            max_page_size = 100 if max_objects is None else min(100, max_objects - len(items))
            pagination = api_pb2.ListPagination(max_objects=max_page_size, created_before=created_before)
            req = api_pb2.SecretListRequest(
                environment_name=_get_environment_name(environment_name), pagination=pagination
            )
            resp = await retry_transient_errors(client.stub.SecretList, req)
            items.extend(resp.items)
            finished = (len(resp.items) < max_page_size) or (max_objects is not None and len(items) >= max_objects)
            return finished

        finished = await retrieve_page(as_timestamp(created_before))
        while True:
            if finished:
                break
            finished = await retrieve_page(items[-1].metadata.creation_info.created_at)

        secrets = [
            _Secret._new_hydrated(
                item.secret_id,
                client,
                item.metadata,
                is_another_app=True,
                rep=_Secret._repr(item.label, environment_name),
            )
            for item in items
        ]
        return secrets[:max_objects] if max_objects is not None else secrets

    @staticmethod
    async def delete(
        name: str,  # Name of the Secret to delete
        *,
        allow_missing: bool = False,  # If True, don't raise an error if the Secret doesn't exist
        environment_name: Optional[str] = None,  # Uses active environment if not specified
        client: Optional[_Client] = None,  # Optional client with Modal credentials
    ):
        """Delete a named Secret.

        Warning: Deletion is irreversible and will affect any Apps currently using the Secret.

        **Examples:**

        ```python notest
        await modal.Secret.objects.delete("my-secret")
        ```

        Secrets will be deleted from the active environment, or another one can be specified:

        ```python notest
        await modal.Secret.objects.delete("my-secret", environment_name="dev")
        ```

        Added in v1.1.2.

        """
        try:
            obj = await _Secret.from_name(name, environment_name=environment_name).hydrate(client)
        except NotFoundError:
            if not allow_missing:
                raise
        else:
            req = api_pb2.SecretDeleteRequest(secret_id=obj.object_id)
            await retry_transient_errors(obj._client.stub.SecretDelete, req)


SecretManager = synchronize_api(_SecretManager)


class _Secret(_Object, type_prefix="st"):
    """Secrets provide a dictionary of environment variables for images.

    Secrets are a secure way to add credentials and other sensitive information
    to the containers your functions run in. You can create and edit secrets on
    [the dashboard](https://modal.com/secrets), or programmatically from Python code.

    See [the secrets guide page](https://modal.com/docs/guide/secrets) for more information.
    """

    _metadata: Optional[api_pb2.SecretMetadata] = None

    @classproperty
    def objects(cls) -> _SecretManager:
        return _SecretManager

    @property
    def name(self) -> Optional[str]:
        return self._name

    def _hydrate_metadata(self, metadata: Optional[Message]):
        if metadata:
            assert isinstance(metadata, api_pb2.SecretMetadata)
            self._metadata = metadata
            self._name = metadata.name

    def _get_metadata(self) -> api_pb2.SecretMetadata:
        assert self._metadata
        return self._metadata

    @staticmethod
    def from_dict(
        env_dict: dict[
            str, Optional[str]
        ] = {},  # dict of entries to be inserted as environment variables in functions using the secret
    ) -> "_Secret":
        """Create a secret from a str-str dictionary. Values can also be `None`, which is ignored.

        Usage:
        ```python
        @app.function(secrets=[modal.Secret.from_dict({"FOO": "bar"})])
        def run():
            print(os.environ["FOO"])
        ```
        """
        if not isinstance(env_dict, dict):
            raise InvalidError(ENV_DICT_WRONG_TYPE_ERR)

        env_dict_filtered: dict[str, str] = {k: v for k, v in env_dict.items() if v is not None}
        if not all(isinstance(k, str) for k in env_dict_filtered.keys()):
            raise InvalidError(ENV_DICT_WRONG_TYPE_ERR)
        if not all(isinstance(v, str) for v in env_dict_filtered.values()):
            raise InvalidError(ENV_DICT_WRONG_TYPE_ERR)

        async def _load(self: _Secret, resolver: Resolver, existing_object_id: Optional[str]):
            if resolver.app_id is not None:
                object_creation_type = api_pb2.OBJECT_CREATION_TYPE_ANONYMOUS_OWNED_BY_APP
            else:
                object_creation_type = api_pb2.OBJECT_CREATION_TYPE_EPHEMERAL

            req = api_pb2.SecretGetOrCreateRequest(
                object_creation_type=object_creation_type,
                env_dict=env_dict_filtered,
                app_id=resolver.app_id,
                environment_name=resolver.environment_name,
            )
            try:
                resp = await resolver.client.stub.SecretGetOrCreate(req)
            except GRPCError as exc:
                if exc.status == Status.INVALID_ARGUMENT:
                    raise InvalidError(exc.message)
                if exc.status == Status.FAILED_PRECONDITION:
                    raise InvalidError(exc.message)
                raise
            self._hydrate(resp.secret_id, resolver.client, resp.metadata)

        rep = f"Secret.from_dict([{', '.join(env_dict.keys())}])"
        return _Secret._from_loader(_load, rep, hydrate_lazily=True)

    @staticmethod
    def from_local_environ(
        env_keys: list[str],  # list of local env vars to be included for remote execution
    ) -> "_Secret":
        """Create secrets from local environment variables automatically."""

        if is_local():
            try:
                return _Secret.from_dict({k: os.environ[k] for k in env_keys})
            except KeyError as exc:
                missing_key = exc.args[0]
                raise InvalidError(
                    f"Could not find local environment variable '{missing_key}' for Secret.from_local_environ"
                )

        return _Secret.from_dict({})

    @staticmethod
    def from_dotenv(path=None, *, filename=".env") -> "_Secret":
        """Create secrets from a .env file automatically.

        If no argument is provided, it will use the current working directory as the starting
        point for finding a `.env` file. Note that it does not use the location of the module
        calling `Secret.from_dotenv`.

        If called with an argument, it will use that as a starting point for finding `.env` files.
        In particular, you can call it like this:
        ```python
        @app.function(secrets=[modal.Secret.from_dotenv(__file__)])
        def run():
            print(os.environ["USERNAME"])  # Assumes USERNAME is defined in your .env file
        ```

        This will use the location of the script calling `modal.Secret.from_dotenv` as a
        starting point for finding the `.env` file.

        A file named `.env` is expected by default, but this can be overridden with the `filename`
        keyword argument:

        ```python
        @app.function(secrets=[modal.Secret.from_dotenv(filename=".env-dev")])
        def run():
            ...
        ```
        """

        async def _load(self: _Secret, resolver: Resolver, existing_object_id: Optional[str]):
            try:
                from dotenv import dotenv_values, find_dotenv
                from dotenv.main import _walk_to_root
            except ImportError:
                raise ImportError(
                    "Need the `dotenv` package installed. You can install it by running `pip install python-dotenv`."
                )

            if path is not None:
                # This basically implements the logic in find_dotenv
                for dirname in _walk_to_root(path):
                    check_path = os.path.join(dirname, filename)
                    if os.path.isfile(check_path):
                        dotenv_path = check_path
                        break
                else:
                    dotenv_path = ""
            else:
                # TODO(erikbern): dotenv tries to locate .env files based on location of the file in the stack frame.
                # Since the modal code "intermediates" this, a .env file in user's local directory won't be picked up.
                # To simplify this, we just support the cwd and don't do any automatic path inference.
                dotenv_path = find_dotenv(filename, usecwd=True)

            env_dict = dotenv_values(dotenv_path)

            req = api_pb2.SecretGetOrCreateRequest(
                object_creation_type=api_pb2.OBJECT_CREATION_TYPE_ANONYMOUS_OWNED_BY_APP,
                env_dict=env_dict,
                app_id=resolver.app_id,
            )
            resp = await resolver.client.stub.SecretGetOrCreate(req)

            self._hydrate(resp.secret_id, resolver.client, resp.metadata)

        return _Secret._from_loader(_load, "Secret.from_dotenv()", hydrate_lazily=True)

    @staticmethod
    def from_name(
        name: str,
        *,
        namespace=None,  # mdmd:line-hidden
        environment_name: Optional[str] = None,
        required_keys: list[
            str
        ] = [],  # Optionally, a list of required environment variables (will be asserted server-side)
    ) -> "_Secret":
        """Reference a Secret by its name.

        In contrast to most other Modal objects, named Secrets must be provisioned
        from the Dashboard. See other methods for alternate ways of creating a new
        Secret from code.

        ```python
        secret = modal.Secret.from_name("my-secret")

        @app.function(secrets=[secret])
        def run():
           ...
        ```
        """
        warn_if_passing_namespace(namespace, "modal.Secret.from_name")

        async def _load(self: _Secret, resolver: Resolver, existing_object_id: Optional[str]):
            req = api_pb2.SecretGetOrCreateRequest(
                deployment_name=name,
                environment_name=_get_environment_name(environment_name, resolver),
                required_keys=required_keys,
            )
            try:
                response = await resolver.client.stub.SecretGetOrCreate(req)
            except GRPCError as exc:
                if exc.status == Status.NOT_FOUND:
                    raise NotFoundError(exc.message)
                else:
                    raise
            self._hydrate(response.secret_id, resolver.client, response.metadata)

        rep = _Secret._repr(name, environment_name)
        return _Secret._from_loader(_load, rep, hydrate_lazily=True, name=name)

    @staticmethod
    async def create_deployed(
        deployment_name: str,
        env_dict: dict[str, str],
        namespace=None,  # mdmd:line-hidden
        client: Optional[_Client] = None,
        environment_name: Optional[str] = None,
        overwrite: bool = False,
    ) -> str:
        """mdmd:hidden"""
        deprecation_warning(
            (2025, 8, 13),
            "The undocumented `modal.Secret.create_deployed` method is deprecated and will be removed "
            "in a future release. It can be replaced with `modal.Secret.objects.create`.",
        )
        return await _Secret._create_deployed(deployment_name, env_dict, namespace, client, environment_name, overwrite)

    @staticmethod
    async def _create_deployed(
        deployment_name: str,
        env_dict: dict[str, str],
        namespace=None,  # mdmd:line-hidden
        client: Optional[_Client] = None,
        environment_name: Optional[str] = None,
        overwrite: bool = False,
    ) -> str:
        """mdmd:hidden"""
        warn_if_passing_namespace(namespace, "modal.Secret.create_deployed")

        check_object_name(deployment_name, "Secret")
        if client is None:
            client = await _Client.from_env()
        if overwrite:
            object_creation_type = api_pb2.OBJECT_CREATION_TYPE_CREATE_OVERWRITE_IF_EXISTS
        else:
            object_creation_type = api_pb2.OBJECT_CREATION_TYPE_CREATE_FAIL_IF_EXISTS
        request = api_pb2.SecretGetOrCreateRequest(
            deployment_name=deployment_name,
            environment_name=_get_environment_name(environment_name),
            object_creation_type=object_creation_type,
            env_dict=env_dict,
        )
        resp = await retry_transient_errors(client.stub.SecretGetOrCreate, request)
        return resp.secret_id

    @live_method
    async def info(self) -> SecretInfo:
        """Return information about the Secret object."""
        metadata = self._get_metadata()
        creation_info = metadata.creation_info
        return SecretInfo(
            name=metadata.name or None,
            created_at=timestamp_to_localized_dt(creation_info.created_at),
            created_by=creation_info.created_by or None,
        )


Secret = synchronize_api(_Secret)



================================================
FILE: modal/serving.py
================================================
# Copyright Modal Labs 2023
import multiprocessing
import platform
from collections.abc import AsyncGenerator
from multiprocessing.context import SpawnProcess
from multiprocessing.synchronize import Event
from typing import TYPE_CHECKING, Optional, TypeVar

from synchronicity.async_wrap import asynccontextmanager

from modal._output import OutputManager

from ._utils.async_utils import TaskContext, asyncify, synchronize_api, synchronizer
from ._utils.logger import logger
from ._watcher import watch
from .cli.import_refs import ImportRef, import_app_from_ref
from .client import _Client
from .config import config
from .output import _get_output_manager, enable_output
from .runner import _run_app, serve_update

if TYPE_CHECKING:
    from .app import _App
else:
    _App = TypeVar("_App")


def _run_serve(
    import_ref: ImportRef, existing_app_id: str, is_ready: Event, environment_name: str, show_progress: bool
):
    # subprocess entrypoint
    _app = import_app_from_ref(import_ref, base_cmd="modal serve")
    blocking_app = synchronizer._translate_out(_app)

    with enable_output(show_progress=show_progress):
        serve_update(blocking_app, existing_app_id, is_ready, environment_name)


async def _restart_serve(
    import_ref: ImportRef, *, existing_app_id: str, environment_name: str, timeout: float = 5.0
) -> SpawnProcess:
    ctx = multiprocessing.get_context("spawn")  # Needed to reload the interpreter
    is_ready = ctx.Event()
    output_mgr = OutputManager.get()
    show_progress = output_mgr is not None
    p = ctx.Process(target=_run_serve, args=(import_ref, existing_app_id, is_ready, environment_name, show_progress))
    p.start()
    await asyncify(is_ready.wait)(timeout)
    # TODO(erikbern): we don't fail if the above times out, but that's somewhat intentional, since
    # the child process might build a huge image or similar
    return p


async def _terminate(proc: Optional[SpawnProcess], timeout: float = 5.0):
    if proc is None:
        return
    try:
        proc.terminate()
        await asyncify(proc.join)(timeout)
        if proc.exitcode is not None:
            if output_mgr := _get_output_manager():
                output_mgr.print(f"Serve process {proc.pid} terminated")
        else:
            if output_mgr := _get_output_manager():
                output_mgr.print(f"[red]Serve process {proc.pid} didn't terminate after {timeout}s, killing it[/red]")
            proc.kill()
    except ProcessLookupError:
        pass  # Child process already finished


async def _run_watch_loop(
    import_ref: ImportRef,
    *,
    app_id: str,
    watcher: AsyncGenerator[set[str], None],
    environment_name: str,
):
    unsupported_msg = None
    if platform.system() == "Windows":
        unsupported_msg = "Live-reload skipped. This feature is currently unsupported on Windows"
        " This can hopefully be fixed in a future version of Modal."

    if unsupported_msg:
        if output_mgr := _get_output_manager():
            async for _ in watcher:
                output_mgr.print(unsupported_msg)
    else:
        curr_proc = None
        try:
            async for trigger_files in watcher:
                logger.debug(f"The following files triggered an app update: {', '.join(trigger_files)}")
                await _terminate(curr_proc)
                curr_proc = await _restart_serve(import_ref, existing_app_id=app_id, environment_name=environment_name)
        finally:
            await _terminate(curr_proc)


@asynccontextmanager
async def _serve_app(
    app: "_App",
    import_ref: ImportRef,
    *,
    _watcher: Optional[AsyncGenerator[set[str], None]] = None,  # for testing
    environment_name: Optional[str] = None,
) -> AsyncGenerator["_App", None]:
    if environment_name is None:
        environment_name = config.get("environment")

    client = await _Client.from_env()

    async with _run_app(app, client=client, environment_name=environment_name):
        if _watcher is not None:
            watcher = _watcher  # Only used by tests
        else:
            mounts_to_watch = app._get_watch_mounts()
            watcher = watch(mounts_to_watch)
        async with TaskContext(grace=0.1) as tc:
            tc.create_task(
                _run_watch_loop(import_ref, app_id=app.app_id, watcher=watcher, environment_name=environment_name)
            )
            yield app


serve_app = synchronize_api(_serve_app)



================================================
FILE: modal/snapshot.py
================================================
# Copyright Modal Labs 2024
from typing import Optional

from modal_proto import api_pb2

from ._object import _Object
from ._resolver import Resolver
from ._utils.async_utils import synchronize_api
from ._utils.grpc_utils import retry_transient_errors
from .client import _Client


class _SandboxSnapshot(_Object, type_prefix="sn"):
    """
    > Sandbox memory snapshots are in **early preview**.

    A `SandboxSnapshot` object lets you interact with a stored Sandbox snapshot that was created by calling
    `._experimental_snapshot()` on a Sandbox instance. This includes both the filesystem and memory state of
    the original Sandbox at the time the snapshot was taken.
    """

    @staticmethod
    async def from_id(sandbox_snapshot_id: str, client: Optional[_Client] = None):
        """
        Construct a `SandboxSnapshot` object from a sandbox snapshot ID.
        """
        if client is None:
            client = await _Client.from_env()

        async def _load(self: _SandboxSnapshot, resolver: Resolver, existing_object_id: Optional[str]):
            await retry_transient_errors(
                client.stub.SandboxSnapshotGet, api_pb2.SandboxSnapshotGetRequest(snapshot_id=sandbox_snapshot_id)
            )

        rep = "SandboxSnapshot()"
        obj = _SandboxSnapshot._from_loader(_load, rep)
        obj._hydrate(sandbox_snapshot_id, client, None)

        return obj


SandboxSnapshot = synchronize_api(_SandboxSnapshot)



================================================
FILE: modal/stream_type.py
================================================
# Copyright Modal Labs 2022
import subprocess
from enum import Enum


class StreamType(Enum):
    # Discard all logs from the stream.
    DEVNULL = subprocess.DEVNULL
    # Store logs in a pipe to be read by the client.
    PIPE = subprocess.PIPE
    # Print logs to stdout immediately.
    STDOUT = subprocess.STDOUT

    def __repr__(self):
        return f"{self.__module__}.{self.__class__.__name__}.{self.name}"



================================================
FILE: modal/token_flow.py
================================================
# Copyright Modal Labs 2023
import itertools
import os
import webbrowser
from collections.abc import AsyncGenerator
from typing import Optional

import aiohttp.web
from synchronicity.async_wrap import asynccontextmanager

from modal_proto import api_pb2

from ._output import make_console
from ._utils.async_utils import synchronize_api
from ._utils.http_utils import run_temporary_http_server
from .client import _Client
from .config import DEFAULT_SERVER_URL, _lookup_workspace, _store_user_config, config, config_profiles, user_config_path
from .exception import AuthError


class _TokenFlow:
    def __init__(self, client: _Client):
        self.stub = client.stub

    @asynccontextmanager
    async def start(
        self, utm_source: Optional[str] = None, next_url: Optional[str] = None
    ) -> AsyncGenerator[tuple[str, str, str], None]:
        """mdmd:hidden"""
        # Run a temporary http server returning the token id on /
        # This helps us add direct validation later
        # TODO(erikbern): handle failure launching server

        async def slash(request):
            headers = {"Access-Control-Allow-Origin": "*"}
            return aiohttp.web.Response(text=self.token_flow_id, headers=headers)

        app = aiohttp.web.Application()
        app.add_routes([aiohttp.web.get("/", slash)])
        async with run_temporary_http_server(app) as url:
            req = api_pb2.TokenFlowCreateRequest(
                utm_source=utm_source,
                next_url=next_url,
                localhost_port=int(url.split(":")[-1]),
            )
            resp = await self.stub.TokenFlowCreate(req)
            self.token_flow_id = resp.token_flow_id
            self.wait_secret = resp.wait_secret
            yield (resp.token_flow_id, resp.web_url, resp.code)

    async def finish(
        self, timeout: float = 40.0, grpc_extra_timeout: float = 5.0
    ) -> Optional[api_pb2.TokenFlowWaitResponse]:
        """mdmd:hidden"""
        # Wait for token flow to finish
        req = api_pb2.TokenFlowWaitRequest(
            token_flow_id=self.token_flow_id, timeout=timeout, wait_secret=self.wait_secret
        )
        resp = await self.stub.TokenFlowWait(req, timeout=(timeout + grpc_extra_timeout))
        if not resp.timeout:
            return resp
        else:
            return None


TokenFlow = synchronize_api(_TokenFlow)


async def _new_token(
    *,
    profile: Optional[str] = None,
    activate: bool = True,
    verify: bool = True,
    source: Optional[str] = None,
    next_url: Optional[str] = None,
):
    server_url = config.get("server_url", profile=profile)

    console = make_console()

    result: Optional[api_pb2.TokenFlowWaitResponse] = None
    async with _Client.anonymous(server_url) as client:
        token_flow = _TokenFlow(client)

        async with token_flow.start(source, next_url) as (_, web_url, code):
            with console.status("Waiting for authentication in the web browser", spinner="dots"):
                # Open the web url in the browser
                if _open_url(web_url):
                    console.print(
                        "The web browser should have opened for you to authenticate and get an API token.\n"
                        "If it didn't, please copy this URL into your web browser manually:\n"
                    )
                else:
                    console.print(
                        "[red]Was not able to launch web browser[/red]\n"
                        "Please go to this URL manually and complete the flow:\n"
                    )
                console.print(f"[link={web_url}]{web_url}[/link]\n")
                if code:
                    console.print(f"Enter this code: [yellow]{code}[/yellow]\n")

            with console.status("Waiting for token flow to complete...", spinner="dots") as status:
                for attempt in itertools.count():
                    result = await token_flow.finish()
                    if result is not None:
                        break
                    status.update(f"Waiting for token flow to complete... (attempt {attempt + 2})")

        console.print("[green]Web authentication finished successfully![/green]")

        server_url = client.server_url

    assert result is not None

    if result.workspace_username:
        console.print(
            f"[green]Token is connected to the [magenta]{result.workspace_username}[/magenta] workspace.[/green]"
        )

    await _set_token(
        result.token_id, result.token_secret, profile=profile, activate=activate, verify=verify, server_url=server_url
    )


async def _set_token(
    token_id: str,
    token_secret: str,
    *,
    profile: Optional[str] = None,
    activate: bool = True,
    verify: bool = True,
    server_url: Optional[str] = None,
):
    # TODO add server_url as a parameter for verification?
    server_url = config.get("server_url", profile=profile)
    console = make_console()
    if verify:
        console.print(f"Verifying token against [blue]{server_url}[/blue]")
        await _Client.verify(server_url, (token_id, token_secret))
        console.print("[green]Token verified successfully![/green]")

    if profile is None:
        if "MODAL_PROFILE" in os.environ:
            profile = os.environ["MODAL_PROFILE"]
        else:
            try:
                workspace = await _lookup_workspace(server_url, token_id, token_secret)
            except AuthError as exc:
                if not verify:
                    # Improve the error message for verification failure with --no-verify to reduce surprise
                    msg = "No profile name given, but could not authenticate client to look up workspace name."
                    raise AuthError(msg) from exc
                raise exc
            profile = workspace.username

    config_data = {"token_id": token_id, "token_secret": token_secret}
    if server_url is not None and server_url != DEFAULT_SERVER_URL:
        config_data["server_url"] = server_url
    # Activate the profile when requested or if no other profiles currently exist
    active_profile = profile if (activate or not config_profiles()) else None
    with console.status("Storing token", spinner="dots"):
        _store_user_config(config_data, profile=profile, active_profile=active_profile)
    console.print(
        f"[green]Token written to [magenta]{user_config_path}[/magenta] in profile "
        f"[magenta]{profile}[/magenta].[/green]"
    )

    # Warn the user if their token will be ignored
    env_vars = [var if os.environ.get(var) else None for var in ["MODAL_TOKEN_ID", "MODAL_TOKEN_SECRET"]]
    env_vars_used = [var for var in env_vars if var is not None]
    env_vars_str = " / ".join(env_vars_used)
    if env_vars_used:
        s = "s" if len(env_vars_used) > 1 else ""
        verb = "are" if len(env_vars_used) > 1 else "is"
        console.print(
            f"Warning: The {env_vars_str} environment variable{s} {verb} set; this will override your new credentials.",
            style="yellow",
        )


def _open_url(url: str) -> bool:
    """Opens url in web browser, making sure we use a modern one (not Lynx etc)"""
    if "PYTEST_CURRENT_TEST" in os.environ:
        return False
    try:
        browser = webbrowser.get()
        # zpresto defines `BROWSER=open` by default on macOS, which causes `webbrowser` to return `GenericBrowser`.
        if isinstance(browser, webbrowser.GenericBrowser) and browser.name != "open":
            return False
        else:
            return browser.open_new_tab(url)
    except webbrowser.Error:
        return False



================================================
FILE: modal/_runtime/__init__.py
================================================
# Copyright Modal Labs 2024



================================================
FILE: modal/_runtime/asgi.py
================================================
# Copyright Modal Labs 2022

# Note: this module isn't imported unless it's needed.
# This is because aiohttp is a pretty big dependency that adds significant latency when imported

import asyncio
from collections.abc import AsyncGenerator
from typing import Any, Callable, NoReturn, Optional, cast

import aiohttp

from modal._utils.async_utils import TaskContext
from modal._utils.blob_utils import MAX_OBJECT_SIZE_BYTES
from modal._utils.package_utils import parse_major_minor_version
from modal.config import logger
from modal.exception import ExecutionError, InvalidError
from modal.experimental import stop_fetching_inputs

from .execution_context import current_attempt_token, current_function_call_id

FIRST_MESSAGE_TIMEOUT_SECONDS = 5.0


class LifespanManager:
    _startup: asyncio.Future
    _shutdown: asyncio.Future
    _queue: asyncio.Queue
    _has_run_init: bool = False
    _lifespan_supported: bool = False

    def __init__(self, asgi_app, state):
        self.asgi_app = asgi_app
        self.state = state

    async def ensure_init(self):
        # making this async even though
        # no async code since it has to run inside
        # the event loop to tie the
        # objects to the correct loop in python 3.9
        if not self._has_run_init:
            self._queue = asyncio.Queue()
            self._startup = asyncio.Future()
            self._shutdown = asyncio.Future()
            self._has_run_init = True

    async def background_task(self):
        await self.ensure_init()

        async def receive():
            self._lifespan_supported = True
            return await self._queue.get()

        async def send(message):
            if message["type"] == "lifespan.startup.complete":
                self._startup.set_result(None)
            elif message["type"] == "lifespan.startup.failed":
                self._startup.set_exception(ExecutionError("ASGI lifespan startup failed"))
            elif message["type"] == "lifespan.shutdown.complete":
                self._shutdown.set_result(None)
            elif message["type"] == "lifespan.shutdown.failed":
                self._shutdown.set_exception(ExecutionError("ASGI lifespan shutdown failed"))
            else:
                raise ExecutionError(f"Unexpected message type: {message['type']}")

        try:
            await self.asgi_app({"type": "lifespan", "state": self.state}, receive, send)
        except Exception as e:
            if not self._lifespan_supported:
                logger.info(f"ASGI lifespan task exited before receiving any messages with exception:\n{e}")
                if not self._startup.done():
                    self._startup.set_result(None)
                if not self._shutdown.done():
                    self._shutdown.set_result(None)
                return

            logger.error(f"Error in ASGI lifespan task: {e}", exc_info=True)
            if not self._startup.done():
                self._startup.set_exception(ExecutionError("ASGI lifespan task exited startup"))
            if not self._shutdown.done():
                self._shutdown.set_exception(ExecutionError("ASGI lifespan task exited shutdown"))
        else:
            logger.info("ASGI Lifespan protocol is probably not supported by this library")
            if not self._startup.done():
                self._startup.set_result(None)
            if not self._shutdown.done():
                self._shutdown.set_result(None)

    async def lifespan_startup(self):
        await self.ensure_init()
        self._queue.put_nowait({"type": "lifespan.startup"})
        await self._startup

    async def lifespan_shutdown(self):
        await self.ensure_init()
        self._queue.put_nowait({"type": "lifespan.shutdown"})
        await self._shutdown


def asgi_app_wrapper(asgi_app, container_io_manager) -> tuple[Callable[..., AsyncGenerator], LifespanManager]:
    state: dict[str, Any] = {}  # used for lifespan state

    async def fn(scope):
        if "state" in scope:
            # we don't expect users to set state in ASGI scope
            # this should be handled internally by the LifespanManager
            raise ExecutionError("Unpexected state in ASGI scope")
        scope["state"] = state
        function_call_id = current_function_call_id()
        attempt_token = current_attempt_token()
        assert function_call_id, "internal error: function_call_id not set in asgi_app() scope"

        messages_from_app: asyncio.Queue[dict[str, Any]] = asyncio.Queue(1)
        messages_to_app: asyncio.Queue[dict[str, Any]] = asyncio.Queue(1)

        async def disconnect_app():
            if scope["type"] == "http":
                await messages_to_app.put({"type": "http.disconnect"})
            elif scope["type"] == "websocket":
                await messages_to_app.put({"type": "websocket.disconnect"})

        async def handle_first_input_timeout():
            if scope["type"] == "http":
                await messages_from_app.put({"type": "http.response.start", "status": 408})
                await messages_from_app.put(
                    {
                        "type": "http.response.body",
                        "body": b"Missing request, possibly due to expiry or cancellation",
                    }
                )
            elif scope["type"] == "websocket":
                await messages_from_app.put(
                    {
                        "type": "websocket.close",
                        "code": 1011,
                        "reason": "Missing request, possibly due to expiry or cancellation",
                    }
                )
            await disconnect_app()

        async def fetch_data_in():
            # Cancel an ASGI app call if the initial message is not received within a short timeout.
            #
            # This initial message, "http.request" or "websocket.connect", should be sent
            # immediately after starting the ASGI app's function call. If it is not received, that
            # indicates a request cancellation or other abnormal circumstance.
            message_gen = container_io_manager.get_data_in.aio(function_call_id, attempt_token)
            first_message_task = asyncio.create_task(message_gen.__anext__())

            try:
                # we are intentionally shielding + manually cancelling first_message_task, since cancellations
                # can otherwise get ignored in case the cancellation and an awaited future resolve gets
                # triggered in the same sequence before handing back control to the event loop.
                first_message = await asyncio.shield(
                    asyncio.wait_for(first_message_task, FIRST_MESSAGE_TIMEOUT_SECONDS)
                )
            except asyncio.CancelledError:
                if not first_message_task.done():
                    # see comment above about manual cancellation
                    first_message_task.cancel()
                raise
            except (asyncio.TimeoutError, StopAsyncIteration):
                # About `StopAsyncIteration` above: The generator shouldn't typically exit,
                # but if it does, we handle it like a timeout in that case.
                await handle_first_input_timeout()
                return
            except Exception:
                logger.exception("Internal error in asgi_app_wrapper")
                await disconnect_app()
                return

            await messages_to_app.put(first_message)
            async for message in message_gen:
                await messages_to_app.put(message)

        async def send(msg):
            # Automatically split body chunks that are greater than the output size limit, to
            # prevent them from being uploaded to S3.
            if msg["type"] == "http.response.body":
                body_chunk_size = MAX_OBJECT_SIZE_BYTES - 1024  # reserve 1 KiB for framing
                body_chunk_limit = 20 * body_chunk_size
                s3_chunk_size = 50 * body_chunk_size

                size = len(msg.get("body", b""))
                if size <= body_chunk_limit:
                    chunk_size = body_chunk_size
                else:
                    # If the body is _very large_, we should still split it up to avoid sending all
                    # of the data in a huge chunk in S3.
                    chunk_size = s3_chunk_size

                if size > chunk_size:
                    indices = list(range(0, size, chunk_size))
                    for i in indices[:-1]:
                        chunk = msg["body"][i : i + chunk_size]
                        await messages_from_app.put({"type": "http.response.body", "body": chunk, "more_body": True})
                    msg["body"] = msg["body"][indices[-1] :]

            await messages_from_app.put(msg)

        # Run the ASGI app, while draining the send message queue at the same time,
        # and yielding results.
        async with TaskContext() as tc:
            tc.create_task(fetch_data_in())

            async def receive():
                return await messages_to_app.get()

            app_task = tc.create_task(asgi_app(scope, receive, send))
            pop_task = None
            while True:
                pop_task = tc.create_task(messages_from_app.get())

                try:
                    done, pending = await asyncio.wait([pop_task, app_task], return_when=asyncio.FIRST_COMPLETED)
                except asyncio.CancelledError:
                    break

                if pop_task in done:
                    yield pop_task.result()
                else:
                    # clean up the popping task, or we will leak unresolved tasks every loop iteration
                    pop_task.cancel()

                if app_task in done:
                    while not messages_from_app.empty():
                        yield messages_from_app.get_nowait()
                    app_task.result()  # consume/raise exceptions if there are any!
                    break

    return fn, LifespanManager(asgi_app, state)


def wsgi_app_wrapper(wsgi_app, container_io_manager):
    from modal._vendor.a2wsgi_wsgi import WSGIMiddleware

    asgi_app = WSGIMiddleware(wsgi_app, workers=10000, send_queue_size=1)  # unlimited workers
    return asgi_app_wrapper(asgi_app, container_io_manager)


def magic_fastapi_app(fn: Callable[..., Any], method: str, docs: bool):
    """Return a FastAPI app wrapping a function handler."""
    try:
        from fastapi import FastAPI
        from fastapi.middleware.cors import CORSMiddleware
    except ImportError as exc:
        message = (
            "Modal functions decorated with `fastapi_endpoint` require FastAPI to be installed in the modal.Image."
            ' Please update your Image definition code, e.g. with `.pip_install("fastapi[standard]")`.'
        )
        raise InvalidError(message) from exc

    app = FastAPI(openapi_url="/openapi.json" if docs else None)  # disabling openapi spec disables all docs
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )
    app.add_api_route("/", fn, methods=[method])
    return app


def get_ip_address(ifname: bytes):
    """Get the IP address associated with a network interface in Linux."""
    import fcntl
    import socket
    import struct

    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    return socket.inet_ntoa(
        fcntl.ioctl(
            s.fileno(),
            0x8915,  # SIOCGIFADDR
            struct.pack("256s", ifname[:15]),
        )[20:24]
    )


def wait_for_web_server(host: str, port: int, *, timeout: float) -> None:
    """Wait until a web server port starts accepting TCP connections."""
    import socket
    import time

    start_time = time.monotonic()
    while True:
        try:
            with socket.create_connection((host, port), timeout=timeout):
                break
        except OSError as ex:
            time.sleep(0.01)
            if time.monotonic() - start_time >= timeout:
                raise TimeoutError(
                    f"Waited too long for port {port} to start accepting connections. "
                    "Make sure the web server is bound to 0.0.0.0 (rather than localhost or 127.0.0.1), "
                    "or adjust `startup_timeout`."
                ) from ex


def _add_forwarded_for_header(scope):
    # we strip X-Forwarded-For headers from the scope
    # but we can add it back from the ASGI scope
    # https://asgi.readthedocs.io/en/latest/specs/www.html#http-connection-scope

    # X-Forwarded-For headers is a comma separated list of IP addresses
    # there may be multiple X-Forwarded-For headers
    # we want to prepend the client IP to the first one
    # but only if it doesn't exist in one of the headers already

    first_x_forwarded_for_idx = None
    if "headers" in scope and "client" in scope:
        client_host = scope["client"][0]

        for idx, header in enumerate(scope["headers"]):
            if header[0] == b"X-Forwarded-For":
                if first_x_forwarded_for_idx is None:
                    first_x_forwarded_for_idx = idx
                values = header[1].decode().split(", ")

                if client_host in values:
                    # we already have the client IP in this header
                    # return early
                    return scope

        if first_x_forwarded_for_idx is not None:
            # we have X-Forwarded-For headers but they don't have the client IP
            # we need to prepend the client IP to the first one
            values = [client_host] + scope["headers"][first_x_forwarded_for_idx][1].decode().split(", ")
            scope["headers"][first_x_forwarded_for_idx] = (b"X-Forwarded-For", ", ".join(values).encode())
        else:
            # we don't have X-Forwarded-For headers, we need to add one
            scope["headers"].append((b"X-Forwarded-For", client_host.encode()))

    return scope


async def _proxy_http_request(session: aiohttp.ClientSession, scope, receive, send) -> None:
    proxy_response: aiohttp.ClientResponse

    scope = _add_forwarded_for_header(scope)

    async def request_generator() -> AsyncGenerator[bytes, None]:
        while True:
            message = await receive()
            if message["type"] == "http.request":
                body = message.get("body", b"")
                if body:
                    yield body
                if not message.get("more_body", False):
                    break
            elif message["type"] == "http.disconnect":
                raise ConnectionAbortedError("Disconnect message received")
            else:
                raise ExecutionError(f"Unexpected message type: {message['type']}")

    path = scope["path"]
    if scope.get("query_string"):
        path += "?" + scope["query_string"].decode()

    try:
        proxy_response = await session.request(
            method=scope["method"],
            url=path,
            headers=[(k.decode(), v.decode()) for k, v in scope["headers"]],
            data=None if scope["method"] in aiohttp.ClientRequest.GET_METHODS else request_generator(),
            allow_redirects=False,
        )
    except ConnectionAbortedError:
        return
    except aiohttp.ClientConnectionError as e:  # some versions of aiohttp wrap the error
        if isinstance(e.__cause__, ConnectionAbortedError):
            return
        raise

    async def send_response() -> None:
        msg = {
            "type": "http.response.start",
            "status": proxy_response.status,
            "headers": [(k.encode(), v.encode()) for k, v in proxy_response.headers.items()],
        }
        await send(msg)
        async for data in proxy_response.content.iter_any():
            msg = {"type": "http.response.body", "body": data, "more_body": True}
            await send(msg)
        await send({"type": "http.response.body"})

    async def listen_for_disconnect() -> NoReturn:
        while True:
            message = await receive()
            if (
                message["type"] == "http.disconnect"
                and proxy_response.connection is not None
                and proxy_response.connection.transport is not None
            ):
                proxy_response.connection.transport.abort()

    async with TaskContext() as tc:
        send_response_task = tc.create_task(send_response())
        disconnect_task = tc.create_task(listen_for_disconnect())
        await asyncio.wait([send_response_task, disconnect_task], return_when=asyncio.FIRST_COMPLETED)


async def _proxy_websocket_request(session: aiohttp.ClientSession, scope, receive, send) -> None:
    first_message = await receive()  # Consume the initial "websocket.connect" message.
    if first_message["type"] == "websocket.disconnect":
        return
    elif first_message["type"] != "websocket.connect":
        raise ExecutionError(f"Unexpected message type: {first_message['type']}")

    path = scope["path"]
    if scope.get("query_string"):
        path += "?" + scope["query_string"].decode()

    async with session.ws_connect(
        url=path,
        headers=[(k.decode(), v.decode()) for k, v in scope["headers"]],  # type: ignore
        protocols=scope.get("subprotocols", []),
    ) as upstream_ws:

        async def client_to_upstream():
            while True:
                client_message = await receive()
                if client_message["type"] == "websocket.disconnect":
                    await upstream_ws.close(code=client_message.get("code", 1005))
                    break
                elif client_message["type"] == "websocket.receive":
                    if client_message.get("text") is not None:
                        await upstream_ws.send_str(client_message["text"])
                    elif client_message.get("bytes") is not None:
                        await upstream_ws.send_bytes(client_message["bytes"])
                else:
                    raise ExecutionError(f"Unexpected message type: {client_message['type']}")

        async def upstream_to_client():
            msg: dict[str, Any] = {
                "type": "websocket.accept",
                "subprotocol": upstream_ws.protocol,
            }
            await send(msg)

            while True:
                upstream_message = await upstream_ws.receive()
                if upstream_message.type == aiohttp.WSMsgType.closed:
                    msg = {"type": "websocket.close"}
                    if upstream_message.data is not None:
                        msg["code"] = cast(aiohttp.WSCloseCode, upstream_message.data).value
                        msg["reason"] = upstream_message.extra
                    await send(msg)
                    break
                elif upstream_message.type == aiohttp.WSMsgType.text:
                    await send({"type": "websocket.send", "text": upstream_message.data})
                elif upstream_message.type == aiohttp.WSMsgType.binary:
                    await send({"type": "websocket.send", "bytes": upstream_message.data})
                else:
                    pass  # Ignore all other upstream WebSocket message types.

        async with TaskContext() as tc:
            client_to_upstream_task = tc.create_task(client_to_upstream())
            upstream_to_client_task = tc.create_task(upstream_to_client())
            await asyncio.wait([client_to_upstream_task, upstream_to_client_task], return_when=asyncio.FIRST_COMPLETED)


async def _proxy_lifespan_request(base_url, scope, receive, send) -> None:
    session: Optional[aiohttp.ClientSession] = None
    while True:
        message = await receive()
        if message["type"] == "lifespan.startup":
            if session is None:
                session = aiohttp.ClientSession(
                    base_url,
                    cookie_jar=aiohttp.DummyCookieJar(),
                    # "Disable" timeout, since timeouts are handled on input level instead.
                    timeout=aiohttp.ClientTimeout(total=None),
                    auto_decompress=False,
                    read_bufsize=1024 * 1024,  # 1 MiB
                    connector=aiohttp.TCPConnector(
                        limit=1000
                    ),  # 100 is the default max, but 1000 is the max for `@modal.concurrent`.
                    # Note: these values will need to be kept in sync.
                    **(
                        # These options were introduced in aiohttp 3.9, and we can remove the
                        # conditional after deprecating image builder version 2023.12.
                        dict(  # type: ignore
                            max_line_size=64 * 1024,  # 64 KiB
                            max_field_size=64 * 1024,  # 64 KiB
                        )
                        if parse_major_minor_version(aiohttp.__version__) >= (3, 9)
                        else {}
                    ),
                )
                scope["state"]["session"] = session
            await send({"type": "lifespan.startup.complete"})
        elif message["type"] == "lifespan.shutdown":
            if session is not None:
                await session.close()
            await send({"type": "lifespan.shutdown.complete"})
            break
        else:
            raise ExecutionError(f"Unexpected message type: {message['type']}")


def web_server_proxy(host: str, port: int):
    """Return an ASGI app that proxies requests to a web server running on the same host."""
    if not 0 < port < 65536:
        raise InvalidError(f"Invalid port number: {port}")

    base_url = f"http://{host}:{port}"

    async def web_server_proxy_app(scope, receive, send):
        try:
            if scope["type"] == "lifespan":
                await _proxy_lifespan_request(base_url, scope, receive, send)
            elif scope["type"] == "http":
                await _proxy_http_request(scope["state"]["session"], scope, receive, send)
            elif scope["type"] == "websocket":
                await _proxy_websocket_request(scope["state"]["session"], scope, receive, send)
            else:
                raise NotImplementedError(f"Scope {scope} is not understood")

        except aiohttp.ClientConnectorError as exc:
            # If the server is not running or not reachable, we should stop fetching new inputs.
            logger.warning(f"Terminating runner due to @web_server connection issue: {exc}")
            stop_fetching_inputs()

    return web_server_proxy_app



================================================
FILE: modal/_runtime/execution_context.py
================================================
# Copyright Modal Labs 2024
from contextlib import contextmanager
from contextvars import ContextVar
from typing import Callable, Optional

from modal._utils.async_utils import synchronize_api
from modal.exception import InvalidError

from .container_io_manager import _ContainerIOManager


def is_local() -> bool:
    """Returns if we are currently on the machine launching/deploying a Modal app

    Returns `True` when executed locally on the user's machine.
    Returns `False` when executed from a Modal container in the cloud.
    """
    return not _ContainerIOManager._singleton


async def _interact() -> None:
    """Enable interactivity with user input inside a Modal container.

    See the [interactivity guide](https://modal.com/docs/guide/developing-debugging#interactivity)
    for more information on how to use this function.
    """
    container_io_manager = _ContainerIOManager._singleton
    if not container_io_manager:
        raise InvalidError("Interactivity only works inside a Modal container.")
    else:
        await container_io_manager.interact()


interact = synchronize_api(_interact)


def current_input_id() -> Optional[str]:
    """Returns the input ID for the current input.

    Can only be called from Modal function (i.e. in a container context).

    ```python
    from modal import current_input_id

    @app.function()
    def process_stuff():
        print(f"Starting to process {current_input_id()}")
    ```
    """
    try:
        return _current_input_id.get()
    except LookupError:
        return None


def current_function_call_id() -> Optional[str]:
    """Returns the function call ID for the current input.

    Can only be called from Modal function (i.e. in a container context).

    ```python
    from modal import current_function_call_id

    @app.function()
    def process_stuff():
        print(f"Starting to process input from {current_function_call_id()}")
    ```
    """
    try:
        return _current_function_call_id.get()
    except LookupError:
        return None


def current_attempt_token() -> Optional[str]:
    # This ContextVar isn't useful to expose to users.
    try:
        return _current_attempt_token.get()
    except LookupError:
        return None


def _set_current_context_ids(
    input_ids: list[str], function_call_ids: list[str], attempt_tokens: list[str]
) -> Callable[[], None]:
    assert len(input_ids) == len(function_call_ids) == len(attempt_tokens) and input_ids

    input_id = input_ids[0]
    function_call_id = function_call_ids[0]
    attempt_token = attempt_tokens[0]

    input_token = _current_input_id.set(input_id)
    function_call_token = _current_function_call_id.set(function_call_id)
    attempt_token_token = _current_attempt_token.set(attempt_token)

    def _reset_current_context_ids():
        _current_input_id.reset(input_token)
        _current_function_call_id.reset(function_call_token)
        _current_attempt_token.reset(attempt_token_token)

    return _reset_current_context_ids


_current_input_id: ContextVar = ContextVar("_current_input_id")
_current_function_call_id: ContextVar = ContextVar("_current_function_call_id")
_current_attempt_token: ContextVar = ContextVar("_current_attempt_token")

_is_currently_importing = False  # we set this to True while a container is importing user code


@contextmanager
def _import_context():
    global _is_currently_importing
    _is_currently_importing = True
    try:
        yield
    finally:
        _is_currently_importing = False



================================================
FILE: modal/_runtime/gpu_memory_snapshot.py
================================================
# Copyright Modal Labs 2022
#
# This module provides a simple interface for creating GPU memory snapshots,
# providing a convenient interface to `cuda-checkpoint` [1]. This is intended
# to be used in conjunction with memory snapshots.
#
# [1] https://github.com/NVIDIA/cuda-checkpoint

import subprocess
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from enum import Enum
from pathlib import Path
from typing import List, Optional

from modal.config import config, logger

CUDA_CHECKPOINT_PATH: str = config.get("cuda_checkpoint_path")

# Maximum total duration for an entire toggle operation.
CUDA_CHECKPOINT_TOGGLE_TIMEOUT: float = 5 * 60.0

# Maximum total duration for each individual `cuda-checkpoint` invocation.
CUDA_CHECKPOINT_TIMEOUT: float = 90


class CudaCheckpointState(Enum):
    """State representation from the CUDA API [1].

    [1] https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__TYPES.html"""

    RUNNING = "running"
    LOCKED = "locked"
    CHECKPOINTED = "checkpointed"
    FAILED = "failed"


class CudaCheckpointException(Exception):
    """Exception raised for CUDA checkpoint operations."""

    pass


@dataclass
class CudaCheckpointProcess:
    """Contains a reference to a PID with active CUDA session. This also provides
    methods for checkpointing and restoring GPU memory."""

    pid: int
    state: CudaCheckpointState

    def toggle(self, target_state: CudaCheckpointState, skip_first_refresh: bool = False) -> None:
        """Toggle CUDA checkpoint state for current process, moving GPU memory to the
        CPU and back depending on the current process state when called.
        """
        logger.debug(f"PID: {self.pid} Toggling CUDA checkpoint state to {target_state.value}")

        start_time = time.monotonic()
        retry_count = 0
        max_retries = 3

        attempts = 0
        while self._should_continue_toggle(
            target_state, start_time, refresh=not (skip_first_refresh and attempts == 0)
        ):
            attempts += 1
            try:
                self._execute_toggle_command()
                # Use exponential backoff for retries
                sleep_time = min(0.1 * (2**retry_count), 1.0)
                time.sleep(sleep_time)
                retry_count = 0
            except CudaCheckpointException as e:
                retry_count += 1
                if retry_count >= max_retries:
                    raise CudaCheckpointException(
                        f"PID: {self.pid} Failed to toggle state after {max_retries} retries: {e}"
                    )
                logger.debug(f"PID: {self.pid} Retry {retry_count}/{max_retries} after error: {e}")
                time.sleep(0.5 * retry_count)

        logger.debug(f"PID: {self.pid} Target state {target_state.value} reached")

    def _should_continue_toggle(
        self, target_state: CudaCheckpointState, start_time: float, refresh: bool = True
    ) -> bool:
        """Check if toggle operation should continue based on current state and timeout."""
        if refresh:
            self.refresh_state()

        if self.state == target_state:
            return False

        if self.state == CudaCheckpointState.FAILED:
            raise CudaCheckpointException(f"PID: {self.pid} CUDA process state is {self.state}")

        elapsed = time.monotonic() - start_time
        if elapsed >= CUDA_CHECKPOINT_TOGGLE_TIMEOUT:
            raise CudaCheckpointException(
                f"PID: {self.pid} Timeout after {elapsed:.2f}s waiting for state {target_state.value}. "
                f"Current state: {self.state}"
            )

        return True

    def _execute_toggle_command(self) -> None:
        """Execute the cuda-checkpoint toggle command."""
        try:
            _ = subprocess.run(
                [CUDA_CHECKPOINT_PATH, "--toggle", "--pid", str(self.pid)],
                check=True,
                capture_output=True,
                text=True,
                timeout=CUDA_CHECKPOINT_TIMEOUT,
            )
            logger.debug(f"PID: {self.pid} Successfully toggled CUDA checkpoint state")
        except subprocess.CalledProcessError as e:
            error_msg = f"PID: {self.pid} Failed to toggle CUDA checkpoint state: {e.stderr}"
            logger.debug(error_msg)
            raise CudaCheckpointException(error_msg)
        except subprocess.TimeoutExpired:
            error_msg = f"PID: {self.pid} Toggle command timed out"
            logger.debug(error_msg)
            raise CudaCheckpointException(error_msg)

    def refresh_state(self) -> None:
        """Refreshes the current CUDA checkpoint state for this process."""
        try:
            result = subprocess.run(
                [CUDA_CHECKPOINT_PATH, "--get-state", "--pid", str(self.pid)],
                check=True,
                capture_output=True,
                text=True,
                timeout=CUDA_CHECKPOINT_TIMEOUT,
            )

            state_str = result.stdout.strip().lower()
            self.state = CudaCheckpointState(state_str)

        except subprocess.CalledProcessError as e:
            error_msg = f"PID: {self.pid} Failed to get CUDA checkpoint state: {e.stderr}"
            logger.debug(error_msg)
            raise CudaCheckpointException(error_msg)
        except subprocess.TimeoutExpired:
            error_msg = f"PID: {self.pid} Get state command timed out"
            logger.debug(error_msg)
            raise CudaCheckpointException(error_msg)


class CudaCheckpointSession:
    """Manages the checkpointing state of processes with active CUDA sessions."""

    def __init__(self):
        self.cuda_processes = self._get_cuda_pids()
        if self.cuda_processes:
            logger.debug(
                f"Found {len(self.cuda_processes)} PID(s) with CUDA sessions: {[c.pid for c in self.cuda_processes]}"
            )
        else:
            logger.debug("No CUDA sessions found.")

    def _get_cuda_pids(self) -> List[CudaCheckpointProcess]:
        """Iterates over all PIDs and identifies the ones that have running
        CUDA sessions."""
        cuda_pids: List[CudaCheckpointProcess] = []

        # Get all active process IDs from /proc directory
        proc_dir = Path("/proc")
        if not proc_dir.exists():
            raise CudaCheckpointException(
                "OS does not have /proc path rendering it incompatible with GPU memory snapshots."
            )

        # Get all numeric directories (PIDs) from /proc
        pid_dirs = [entry for entry in proc_dir.iterdir() if entry.name.isdigit()]

        # Use ThreadPoolExecutor to check PIDs in parallel for better performance
        with ThreadPoolExecutor(max_workers=min(50, len(pid_dirs))) as executor:
            future_to_pid = {
                executor.submit(self._check_cuda_session, int(entry.name)): int(entry.name) for entry in pid_dirs
            }

            for future in as_completed(future_to_pid):
                pid = future_to_pid[future]
                try:
                    cuda_process = future.result()
                    if cuda_process:
                        cuda_pids.append(cuda_process)
                except Exception as e:
                    logger.debug(f"Error checking PID {pid}: {e}")

        # Sort PIDs for ordered checkpointing
        cuda_pids.sort(key=lambda x: x.pid)
        return cuda_pids

    def _check_cuda_session(self, pid: int) -> Optional[CudaCheckpointProcess]:
        """Check if a specific PID has a CUDA session."""
        try:
            result = subprocess.run(
                [CUDA_CHECKPOINT_PATH, "--get-state", "--pid", str(pid)],
                capture_output=True,
                text=True,
                # This should be quick since no checkpoint has taken place yet
                timeout=5,
            )

            # If the command succeeds (return code 0), this PID has a CUDA session
            if result.returncode == 0:
                state_str = result.stdout.strip().lower()
                state = CudaCheckpointState(state_str)
                return CudaCheckpointProcess(pid=pid, state=state)

        except subprocess.CalledProcessError:
            # Command failed, which is expected for PIDs without CUDA sessions
            pass
        except subprocess.TimeoutExpired:
            logger.debug(f"Timeout checking CUDA state for PID {pid}")
        except Exception as e:
            logger.debug(f"Error checking PID {pid}: {e}")

        return None

    def checkpoint(self) -> None:
        """Checkpoint all CUDA processes, moving GPU memory to CPU."""
        if not self.cuda_processes:
            logger.debug("No CUDA processes to checkpoint.")
            return

        # Validate all states first
        for proc in self.cuda_processes:
            proc.refresh_state()  # Refresh state before validation
            if proc.state != CudaCheckpointState.RUNNING:
                raise CudaCheckpointException(
                    f"PID {proc.pid}: CUDA session not in {CudaCheckpointState.RUNNING.value} state. "
                    f"Current state: {proc.state.value}"
                )

        # Moving state from GPU to CPU can take several seconds per CUDA session.
        # Make a parallel call per CUDA session.
        start = time.perf_counter()

        def checkpoint_impl(proc: CudaCheckpointProcess) -> None:
            proc.toggle(CudaCheckpointState.CHECKPOINTED)

        with ThreadPoolExecutor() as executor:
            futures = [executor.submit(checkpoint_impl, proc) for proc in self.cuda_processes]

            # Wait for all futures and collect any exceptions
            exceptions = []
            for future in as_completed(futures):
                try:
                    future.result()
                except Exception as e:
                    exceptions.append(e)

            if exceptions:
                raise CudaCheckpointException(
                    f"Failed to checkpoint {len(exceptions)} processes: {'; '.join(str(e) for e in exceptions)}"
                )

        elapsed = time.perf_counter() - start
        logger.debug(f"Checkpointing {len(self.cuda_processes)} CUDA sessions took => {elapsed:.3f}s")

    def restore(self) -> None:
        """Restore all CUDA processes, moving memory back from CPU to GPU."""
        if not self.cuda_processes:
            logger.debug("No CUDA sessions to restore.")
            return

        # See checkpoint() for rationale about parallelism.
        start = time.perf_counter()

        def restore_process(proc: CudaCheckpointProcess) -> None:
            proc.toggle(CudaCheckpointState.RUNNING, skip_first_refresh=True)

        with ThreadPoolExecutor() as executor:
            futures = [executor.submit(restore_process, proc) for proc in self.cuda_processes]

            # Wait for all futures and collect any exceptions
            exceptions = []
            for future in as_completed(futures):
                try:
                    future.result()
                except Exception as e:
                    exceptions.append(e)

            if exceptions:
                raise CudaCheckpointException(
                    f"Failed to restore {len(exceptions)} processes: {'; '.join(str(e) for e in exceptions)}"
                )

        elapsed = time.perf_counter() - start
        logger.debug(f"Restoring {len(self.cuda_processes)} CUDA session(s) took => {elapsed:.3f}s")

    def get_process_count(self) -> int:
        """Get the number of CUDA processes managed by this session."""
        return len(self.cuda_processes)

    def get_process_states(self) -> List[tuple[int, CudaCheckpointState]]:
        """Get current states of all managed processes."""
        states = []
        for proc in self.cuda_processes:
            proc.refresh_state()
            states.append((proc.pid, proc.state))
        return states



================================================
FILE: modal/_runtime/telemetry.py
================================================
# Copyright Modal Labs 2024

import importlib.abc
import json
import queue
import socket
import sys
import threading
import time
import uuid
from importlib.util import find_spec, module_from_spec
from struct import pack

from modal.config import logger

MODULE_LOAD_START = "module_load_start"
MODULE_LOAD_END = "module_load_end"

MESSAGE_HEADER_FORMAT = "<I"
MESSAGE_HEADER_LEN = 4


class InterceptedModuleLoader(importlib.abc.Loader):
    def __init__(self, name, loader, interceptor):
        self.name = name
        self.loader = loader
        self.interceptor = interceptor

    def exec_module(self, module):
        if self.loader is None:
            return
        try:
            self.loader.exec_module(module)
        finally:
            self.interceptor.load_end(self.name)

    def create_module(self, spec):
        spec.loader = self.loader
        module = module_from_spec(spec)
        spec.loader = self
        return module

    def get_data(self, path: str) -> bytes:
        """
        Implementation is required to support pkgutil.get_data.

        > If the package cannot be located or loaded, or it uses a loader which does
        > not support get_data, then None is returned.

        ref: https://docs.python.org/3/library/pkgutil.html#pkgutil.get_data
        """
        return self.loader.get_data(path)

    def get_resource_reader(self, fullname: str):
        """
        Support reading a binary artifact that is shipped within a package.

        > Loaders that wish to support resource reading are expected to provide a method called
        > get_resource_reader(fullname) which returns an object implementing this ABCâ€™s interface.

        ref: docs.python.org/3.10/library/importlib.html?highlight=traversableresources#importlib.abc.ResourceReader
        """
        return self.loader.get_resource_reader(fullname)


class ImportInterceptor(importlib.abc.MetaPathFinder):
    loading: dict[str, tuple[str, float]]
    tracing_socket: socket.socket
    events: queue.Queue

    @classmethod
    def connect(cls, socket_filename: str) -> "ImportInterceptor":
        tracing_socket = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
        tracing_socket.connect(socket_filename)
        return cls(tracing_socket)

    def __init__(self, tracing_socket: socket.socket):
        self.loading = {}
        self.tracing_socket = tracing_socket
        self.events = queue.Queue(maxsize=16 * 1024)
        sender = threading.Thread(target=self._send, daemon=True)
        sender.start()

    def find_spec(self, fullname, path, target=None):
        if fullname in self.loading:
            return None
        self.load_start(fullname)
        spec = find_spec(fullname)
        if spec is None:
            self.load_end(fullname)
            return None
        spec.loader = InterceptedModuleLoader(fullname, spec.loader, self)
        return spec

    def load_start(self, name):
        t0 = time.monotonic()
        span_id = str(uuid.uuid4())
        self.emit(
            {"span_id": span_id, "timestamp": time.time(), "event": MODULE_LOAD_START, "attributes": {"name": name}}
        )
        self.loading[name] = (span_id, t0)

    def load_end(self, name):
        span_id, t0 = self.loading.pop(name, (None, None))
        if t0 is None:
            return
        latency = time.monotonic() - t0
        self.emit(
            {
                "span_id": span_id,
                "timestamp": time.time(),
                "event": MODULE_LOAD_END,
                "attributes": {
                    "name": name,
                    "latency": latency,
                },
            }
        )

    def emit(self, event):
        try:
            self.events.put_nowait(event)
        except queue.Full:
            logger.debug("failed to emit event: queue full")

    def _send(self):
        while True:
            event = self.events.get()
            try:
                msg = json.dumps(event).encode("utf-8")
            except BaseException as e:
                logger.debug(f"failed to serialize event: {e}")
                continue
            try:
                encoded_len = pack(MESSAGE_HEADER_FORMAT, len(msg))
                self.tracing_socket.send(encoded_len + msg)
            except OSError as e:
                logger.debug(f"failed to send event: {e}")

    def install(self):
        sys.meta_path = [self] + sys.meta_path  # type: ignore

    def remove(self):
        sys.meta_path.remove(self)  # type: ignore

    def __enter__(self):
        self.install()

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.remove()


def _instrument_imports(socket_filename: str):
    if not supported_platform():
        logger.debug("unsupported platform, not instrumenting imports")
        return
    interceptor = ImportInterceptor.connect(socket_filename)
    interceptor.install()


def instrument_imports(socket_filename: str):
    try:
        _instrument_imports(socket_filename)
    except BaseException as e:
        logger.warning(f"failed to instrument imports: {e}")


def supported_platform():
    return sys.platform in ("linux", "darwin")



================================================
FILE: modal/_runtime/user_code_imports.py
================================================
# Copyright Modal Labs 2024
import importlib
import typing
from abc import ABCMeta, abstractmethod
from dataclasses import dataclass
from typing import Any, Callable, Optional, Sequence

import modal._object
import modal._runtime.container_io_manager
import modal.cls
from modal import Function
from modal._functions import _Function
from modal._utils.async_utils import synchronizer
from modal._utils.function_utils import LocalFunctionError, is_async as get_is_async, is_global_object
from modal.app import _App
from modal.config import logger
from modal.exception import ExecutionError, InvalidError
from modal_proto import api_pb2

if typing.TYPE_CHECKING:
    import modal._functions
    import modal._partial_function
    import modal.app
    from modal._runtime.asgi import LifespanManager


@dataclass
class FinalizedFunction:
    callable: Callable[..., Any]
    is_async: bool
    is_generator: bool
    supported_output_formats: Sequence["api_pb2.DataFormat.ValueType"]
    lifespan_manager: Optional["LifespanManager"] = None


class Service(metaclass=ABCMeta):
    """Common interface for singular functions and class-based "services"

    There are differences in the importing/finalization logic, and this
    "protocol"/abc basically defines a common interface for the two types
    of "Services" after the point of import.
    """

    user_cls_instance: Any
    app: "modal.app._App"
    service_deps: Optional[Sequence["modal._object._Object"]]

    @abstractmethod
    def get_finalized_functions(
        self, fun_def: api_pb2.Function, container_io_manager: "modal._runtime.container_io_manager.ContainerIOManager"
    ) -> dict[str, "FinalizedFunction"]: ...


def construct_webhook_callable(
    user_defined_callable: Callable,
    webhook_config: api_pb2.WebhookConfig,
    container_io_manager: "modal._runtime.container_io_manager.ContainerIOManager",
):
    # Note: aiohttp is a significant dependency of the `asgi` module, so we import it locally
    from modal._runtime import asgi

    # For webhooks, the user function is used to construct an asgi app:
    if webhook_config.type == api_pb2.WEBHOOK_TYPE_ASGI_APP:
        # Function returns an asgi_app, which we can use as a callable.
        return asgi.asgi_app_wrapper(user_defined_callable(), container_io_manager)

    elif webhook_config.type == api_pb2.WEBHOOK_TYPE_WSGI_APP:
        # Function returns an wsgi_app, which we can use as a callable
        return asgi.wsgi_app_wrapper(user_defined_callable(), container_io_manager)

    elif webhook_config.type == api_pb2.WEBHOOK_TYPE_FUNCTION:
        # Function is a webhook without an ASGI app. Create one for it.
        return asgi.asgi_app_wrapper(
            asgi.magic_fastapi_app(user_defined_callable, webhook_config.method, webhook_config.web_endpoint_docs),
            container_io_manager,
        )

    elif webhook_config.type == api_pb2.WEBHOOK_TYPE_WEB_SERVER:
        # Function spawns an HTTP web server listening at a port.
        user_defined_callable()

        # We intentionally try to connect to the external interface instead of the loopback
        # interface here so users are forced to expose the server. This allows us to potentially
        # change the implementation to use an external bridge in the future.
        host = asgi.get_ip_address(b"eth0")
        port = webhook_config.web_server_port
        startup_timeout = webhook_config.web_server_startup_timeout
        asgi.wait_for_web_server(host, port, timeout=startup_timeout)
        return asgi.asgi_app_wrapper(asgi.web_server_proxy(host, port), container_io_manager)
    else:
        raise InvalidError(f"Unrecognized web endpoint type {webhook_config.type}")


@dataclass
class ImportedFunction(Service):
    app: modal.app._App
    service_deps: Optional[Sequence["modal._object._Object"]]
    user_cls_instance = None

    _user_defined_callable: Callable[..., Any]

    def get_finalized_functions(
        self, fun_def: api_pb2.Function, container_io_manager: "modal._runtime.container_io_manager.ContainerIOManager"
    ) -> dict[str, "FinalizedFunction"]:
        # Check this property before we turn it into a method (overriden by webhooks)
        is_async = get_is_async(self._user_defined_callable)
        # Use the function definition for whether this is a generator (overriden by webhooks)
        is_generator = fun_def.function_type == api_pb2.Function.FUNCTION_TYPE_GENERATOR

        webhook_config = fun_def.webhook_config

        if not webhook_config.type:
            # for non-webhooks, the runnable is straight forward:
            return {
                "": FinalizedFunction(
                    callable=self._user_defined_callable,
                    is_async=is_async,
                    is_generator=is_generator,
                    supported_output_formats=fun_def.supported_output_formats
                    # FIXME (elias): the following `or [api_pb2.DATA_FORMAT_PICKLE, api_pb2.DATA_FORMAT_CBOR]` is only
                    # needed for tests
                    or [api_pb2.DATA_FORMAT_PICKLE, api_pb2.DATA_FORMAT_CBOR],
                )
            }

        web_callable, lifespan_manager = construct_webhook_callable(
            self._user_defined_callable, fun_def.webhook_config, container_io_manager
        )

        return {
            "": FinalizedFunction(
                callable=web_callable,
                lifespan_manager=lifespan_manager,
                is_async=True,
                is_generator=True,
                # FIXME (elias): the following `or [api_pb2.DATA_FORMAT_ASGI]` is only needed for tests
                supported_output_formats=fun_def.supported_output_formats or [api_pb2.DATA_FORMAT_ASGI],
            )
        }


@dataclass
class ImportedClass(Service):
    user_cls_instance: Any
    app: "modal.app._App"
    service_deps: Optional[Sequence["modal._object._Object"]]

    _partial_functions: dict[str, "modal._partial_function._PartialFunction"]

    def get_finalized_functions(
        self, fun_def: api_pb2.Function, container_io_manager: "modal._runtime.container_io_manager.ContainerIOManager"
    ) -> dict[str, "FinalizedFunction"]:
        finalized_functions = {}
        for method_name, _partial in self._partial_functions.items():
            user_func = _partial.raw_f
            assert user_func
            # Check this property before we turn it into a method (overriden by webhooks)
            is_async = get_is_async(user_func)
            # Use the function definition for whether this is a generator (overriden by webhooks)
            is_generator = _partial.params.is_generator
            webhook_config = _partial.params.webhook_config
            method_def = fun_def.method_definitions[method_name]

            bound_func = user_func.__get__(self.user_cls_instance)

            if not webhook_config or webhook_config.type == api_pb2.WEBHOOK_TYPE_UNSPECIFIED:
                # for non-webhooks, the runnable is straight forward:
                finalized_function = FinalizedFunction(
                    callable=bound_func,
                    is_async=is_async,
                    is_generator=bool(is_generator),
                    # FIXME (elias): the following `or [api_pb2.DATA_FORMAT_PICKLE, api_pb2.DATA_FORMAT_CBOR]` is only
                    # needed for tests
                    supported_output_formats=method_def.supported_output_formats
                    or [api_pb2.DATA_FORMAT_PICKLE, api_pb2.DATA_FORMAT_CBOR],
                )
            else:
                web_callable, lifespan_manager = construct_webhook_callable(
                    bound_func, webhook_config, container_io_manager
                )
                finalized_function = FinalizedFunction(
                    callable=web_callable,
                    lifespan_manager=lifespan_manager,
                    is_async=True,
                    is_generator=True,
                    # FIXME (elias): the following `or [api_pb2.DATA_FORMAT_ASGI]` is only needed for tests
                    supported_output_formats=method_def.supported_output_formats or [api_pb2.DATA_FORMAT_ASGI],
                )
            finalized_functions[method_name] = finalized_function
        return finalized_functions


def get_user_class_instance(_cls: modal.cls._Cls, args: tuple[Any, ...], kwargs: dict[str, Any]) -> typing.Any:
    """Returns instance of the underlying class to be used as the `self`

    For the time being, this is an instance of the underlying user defined type, with
    some extra attributes like parameter values and _modal_functions set, allowing
    its methods to be used as modal Function objects with .remote() and .local() etc.

    TODO: Could possibly change this to use an Obj to clean up the data model? would invalidate isinstance checks though
    """
    cls = typing.cast(modal.cls.Cls, synchronizer._translate_out(_cls))  # ugly
    modal_obj: modal.cls.Obj = cls(*args, **kwargs)
    modal_obj._entered = True  # ugly but prevents .local() from triggering additional enter-logic
    # TODO: unify lifecycle logic between .local() and container_entrypoint
    user_cls_instance = modal_obj._cached_user_cls_instance()
    return user_cls_instance


def import_single_function_service(
    function_def: api_pb2.Function,
    ser_fun: Optional[Callable[..., Any]],
) -> Service:
    """Imports a function dynamically, and locates the app.

    This is somewhat complex because we're dealing with 3 quite different type of functions:
    1. Functions defined in global scope and decorated in global scope (Function objects)
    2. Functions defined in global scope but decorated elsewhere (these will be raw callables)
    3. Serialized functions

    In addition, we also need to handle
    * Normal functions
    * Methods on classes (in which case we need to instantiate the object)

    This helper also handles web endpoints, ASGI/WSGI servers, and HTTP servers.

    In order to locate the app, we try two things:
    * If the function is a Function, we can get the app directly from it
    * Otherwise, use the app name and look it up from a global list of apps: this
      typically only happens in case 2 above, or in sometimes for case 3

    Note that `import_function` is *not* synchronized, because we need it to run on the main
    thread. This is so that any user code running in global scope (which executes as a part of
    the import) runs on the right thread.
    """
    user_defined_callable: Callable
    service_deps: Optional[Sequence["modal._object._Object"]] = None
    active_app: modal.app._App

    if ser_fun is not None:
        # This is a serialized function we already fetched from the server
        user_defined_callable = ser_fun
        active_app = get_active_app_fallback(function_def)
    else:
        # Load the module dynamically
        module = importlib.import_module(function_def.module_name)
        qual_name: str = function_def.function_name

        if not is_global_object(qual_name):
            raise LocalFunctionError("Attempted to load a function defined in a function scope")

        parts = qual_name.split(".")
        if len(parts) != 1:
            raise InvalidError(f"Invalid function qualname {qual_name}")

        f = getattr(module, qual_name)
        if isinstance(f, Function):
            _function: modal._functions._Function[Any, Any, Any] = synchronizer._translate_in(f)  # type: ignore
            service_deps = _function.deps(only_explicit_mounts=True)
            user_defined_callable = _function.get_raw_f()
            assert _function._app  # app should always be set on a decorated function
            active_app = _function._app
        else:
            # function isn't decorated in global scope
            user_defined_callable = f
            active_app = get_active_app_fallback(function_def)

    return ImportedFunction(
        active_app,
        service_deps,
        user_defined_callable,
    )


def import_class_service(
    function_def: api_pb2.Function,
    service_function_hydration_data: api_pb2.Object,
    class_id: str,
    client: "modal.client.Client",
    ser_user_cls: Optional[type],
    cls_args,
    cls_kwargs,
) -> Service:
    """
    This imports a full class to be able to execute any @method or webhook decorated methods.

    See import_function.
    """
    active_app: Optional["modal.app._App"]
    service_deps: Optional[Sequence["modal._object._Object"]]
    cls_or_user_cls: typing.Union[type, modal.cls.Cls]

    if function_def.definition_type == api_pb2.Function.DEFINITION_TYPE_SERIALIZED:
        assert ser_user_cls is not None
        cls_or_user_cls = ser_user_cls
    else:
        # Load the module dynamically
        module = importlib.import_module(function_def.module_name)
        qual_name: str = function_def.function_name

        if not is_global_object(qual_name):
            raise LocalFunctionError("Attempted to load a class defined in a function scope")

        parts = qual_name.split(".")
        if not (
            len(parts) == 2 and parts[1] == "*"
        ):  # the "function name" of a class service "function placeholder" is expected to be "ClassName.*"
            raise ExecutionError(
                f"Internal error: Invalid 'service function' identifier {qual_name}. Please contact Modal support"
            )

        assert not function_def.use_method_name  # new "placeholder methods" should not be invoked directly!
        cls_name = parts[0]
        cls_or_user_cls = getattr(module, cls_name)

    if isinstance(cls_or_user_cls, modal.cls.Cls):
        _cls = typing.cast(modal.cls._Cls, synchronizer._translate_in(cls_or_user_cls))
        class_service_function: _Function = _cls._get_class_service_function()
        service_deps = class_service_function.deps(only_explicit_mounts=True)
        active_app = class_service_function.app
    else:
        # Undecorated user class (serialized or local scope-decoration).
        service_deps = None  # we can't infer service deps for now
        active_app = get_active_app_fallback(function_def)
        _client = typing.cast("modal.client._Client", synchronizer._translate_in(client))
        _service_function: modal._functions._Function[Any, Any, Any] = modal._functions._Function._new_hydrated(
            service_function_hydration_data.object_id,
            _client,
            service_function_hydration_data.function_handle_metadata,
            is_another_app=True,  # this skips re-loading the function, which is required since it doesn't have a loader
        )
        _cls = modal.cls._Cls.from_local(cls_or_user_cls, active_app, _service_function)
        # hydration of the class itself - just sets the id and triggers some side effects
        # that transfers metadata from the service function to the class. TODO: cleanup!
        _cls._hydrate(class_id, _client, api_pb2.ClassHandleMetadata())

    method_partials: dict[str, "modal._partial_function._PartialFunction"] = _cls._get_partial_functions()
    user_cls_instance = get_user_class_instance(_cls, cls_args, cls_kwargs)

    return ImportedClass(
        user_cls_instance,
        active_app,
        service_deps,
        # TODO (elias/deven): instead of using method_partials here we should use a set of api_pb2.MethodDefinition
        method_partials,
    )


def get_active_app_fallback(function_def: api_pb2.Function) -> _App:
    # This branch is reached in the special case that the imported function/class is:
    # 1) not serialized, and
    # 2) isn't a FunctionHandle - i.e, not decorated at definition time
    # Look at all instantiated apps - if there is only one with the indicated name, use that one
    app_name: Optional[str] = function_def.app_name or None  # coalesce protobuf field to None
    matching_apps = _App._all_apps.get(app_name, [])
    if len(matching_apps) == 1:
        active_app: _App = matching_apps[0]
        return active_app

    if len(matching_apps) > 1:
        if app_name is not None:
            warning_sub_message = f"app with the same name ('{app_name}')"
        else:
            warning_sub_message = "unnamed app"
        logger.warning(
            f"You have more than one {warning_sub_message}. "
            "It's recommended to name all your Apps uniquely when using multiple apps"
        )

    # If we don't have an active app, create one on the fly
    # The app object is used to carry the app layout etc
    return _App()



================================================
FILE: modal/_utils/__init__.py
================================================
# Copyright Modal Labs 2022



================================================
FILE: modal/_utils/app_utils.py
================================================
# Copyright Modal Labs 2024
# Temporary shim as we use this in the server
from .name_utils import *  # noqa



================================================
FILE: modal/_utils/async_utils.py
================================================
# Copyright Modal Labs 2022
import asyncio
import concurrent.futures
import functools
import inspect
import itertools
import sys
import time
import typing
from collections.abc import AsyncGenerator, AsyncIterable, Awaitable, Iterable, Iterator
from contextlib import asynccontextmanager
from dataclasses import dataclass
from typing import (
    Any,
    Callable,
    Generic,
    Optional,
    TypeVar,
    Union,
    cast,
)

import synchronicity
from synchronicity.async_utils import Runner
from synchronicity.exceptions import NestedEventLoops
from typing_extensions import ParamSpec, assert_type

from ..exception import InvalidError
from .logger import logger

T = TypeVar("T")
P = ParamSpec("P")
V = TypeVar("V")

if sys.platform == "win32":
    # quick workaround for deadlocks on shutdown - need to investigate further
    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

synchronizer = synchronicity.Synchronizer()


def synchronize_api(obj, target_module=None):
    if inspect.isclass(obj) or inspect.isfunction(obj):
        blocking_name = obj.__name__.lstrip("_")
    elif isinstance(obj, TypeVar):
        blocking_name = "_BLOCKING_" + obj.__name__
    else:
        blocking_name = None
    if target_module is None:
        target_module = obj.__module__
    return synchronizer.create_blocking(obj, blocking_name, target_module=target_module)


def retry(direct_fn=None, *, n_attempts=3, base_delay=0, delay_factor=2, timeout=90):
    """Decorator that calls an async function multiple times, with a given timeout.

    If a `base_delay` is provided, the function is given an exponentially
    increasing delay on each run, up until the maximum number of attempts.

    Usage:

    ```
    @retry
    async def may_fail_default():
        # ...
        pass

    @retry(n_attempts=5, base_delay=1)
    async def may_fail_delay():
        # ...
        pass
    ```
    """

    def decorator(fn):
        @functools.wraps(fn)
        async def f_wrapped(*args, **kwargs):
            delay = base_delay
            for i in range(n_attempts):
                t0 = time.time()
                try:
                    return await asyncio.wait_for(fn(*args, **kwargs), timeout=timeout)
                except asyncio.CancelledError:
                    logger.debug(f"Function {fn} was cancelled")
                    raise
                except Exception as e:
                    if i >= n_attempts - 1:
                        raise
                    logger.debug(
                        f"Failed invoking function {fn}: {e}"
                        f" (took {time.time() - t0}s, sleeping {delay}s"
                        f" and trying {n_attempts - i - 1} more times)"
                    )
                await asyncio.sleep(delay)
                delay *= delay_factor

        return f_wrapped

    if direct_fn is not None:
        # It's invoked like @retry
        return decorator(direct_fn)
    else:
        # It's invoked like @retry(n_attempts=...)
        return decorator


class TaskContext:
    """A structured group that helps manage stray tasks.

    This differs from the standard library `asyncio.TaskGroup` in that it cancels all tasks still
    running after exiting the context manager, rather than waiting for them to finish.

    A `TaskContext` can have an optional `grace` period in seconds, which will wait for a certain
    amount of time before cancelling all remaining tasks. This is useful for allowing tasks to
    gracefully exit when they determine that the context is shutting down.

    Usage:

    ```python notest
    async with TaskContext() as task_context:
        task = task_context.create_task(coro())
    ```
    """

    _loops: set[asyncio.Task]

    def __init__(self, grace: Optional[float] = None):
        self._grace = grace
        self._loops = set()

    async def start(self):
        # TODO: this only exists as a standalone method because Client doesn't have a proper ctx mgr
        self._tasks: set[asyncio.Task] = set()
        self._exited: asyncio.Event = asyncio.Event()  # Used to stop infinite loops

    @property
    def exited(self) -> bool:
        return self._exited.is_set()

    async def __aenter__(self):
        await self.start()
        return self

    async def stop(self):
        self._exited.set()
        await asyncio.sleep(0)  # Causes any just-created tasks to get started
        unfinished_tasks = [t for t in self._tasks if not t.done()]
        gather_future = None
        try:
            if self._grace is not None and unfinished_tasks:
                gather_future = asyncio.gather(*unfinished_tasks, return_exceptions=True)
                await asyncio.wait_for(gather_future, timeout=self._grace)
        except asyncio.TimeoutError:
            pass
        finally:
            # asyncio.wait_for cancels the future, but the CancelledError
            # still needs to be handled
            # (https://stackoverflow.com/a/63356323/2475114)
            if gather_future:
                try:
                    await gather_future
                except asyncio.CancelledError:
                    pass

            for task in self._tasks:
                if task.done() and not task.cancelled():
                    # Raise any exceptions if they happened.
                    # Only tasks without a done_callback will still be present in self._tasks
                    task.result()

                if task.done() or task in self._loops:  # Note: Legacy code, we can probably cancel loops.
                    continue

                # Cancel any remaining unfinished tasks.
                task.cancel()
            await asyncio.sleep(0)  # wake up coroutines waiting for cancellations

    async def __aexit__(self, exc_type, value, tb):
        await self.stop()

    def create_task(self, coro_or_task) -> asyncio.Task:
        if isinstance(coro_or_task, asyncio.Task):
            task = coro_or_task
        elif asyncio.iscoroutine(coro_or_task):
            loop = asyncio.get_event_loop()
            task = loop.create_task(coro_or_task)
        else:
            raise Exception(f"Object of type {type(coro_or_task)} is not a coroutine or Task")
        self._tasks.add(task)
        task.add_done_callback(self._tasks.discard)
        return task

    def infinite_loop(
        self, async_f, timeout: Optional[float] = 90, sleep: float = 10, log_exception: bool = True
    ) -> asyncio.Task:
        if isinstance(async_f, functools.partial):
            function_name = async_f.func.__qualname__
        else:
            function_name = async_f.__qualname__

        async def loop_coro() -> None:
            logger.debug(f"Starting infinite loop {function_name}")
            while not self.exited:
                try:
                    await asyncio.wait_for(async_f(), timeout=timeout)
                except Exception as exc:
                    if log_exception and isinstance(exc, asyncio.TimeoutError):
                        # Asyncio sends an empty message in this case, so let's use logger.error
                        logger.error(f"Loop attempt for {function_name} timed out")
                    elif log_exception:
                        # Propagate the exception to the logger
                        logger.exception(f"Loop attempt for {function_name} failed")
                try:
                    await asyncio.wait_for(self._exited.wait(), timeout=sleep)
                except asyncio.TimeoutError:
                    continue

            logger.debug(f"Exiting infinite loop for {function_name}")

        t = self.create_task(loop_coro())
        t.set_name(f"{function_name} loop")
        self._loops.add(t)
        t.add_done_callback(self._loops.discard)
        return t

    @staticmethod
    async def gather(*coros: Awaitable) -> Any:
        """Wait for a sequence of coroutines to finish, concurrently.

        This is similar to `asyncio.gather()`, but it uses TaskContext to cancel all remaining tasks
        if one fails with an exception other than `asyncio.CancelledError`. The native `asyncio`
        function does not cancel remaining tasks in this case, which can lead to surprises.

        For example, if you use `asyncio.gather(t1, t2, t3)` and t2 raises an exception, then t1 and
        t3 would continue running. With `TaskContext.gather(t1, t2, t3)`, they are cancelled.

        (It's still acceptable to use `asyncio.gather()` if you don't need cancellation â€” for
        example, if you're just gathering quick coroutines with no side-effects. Or if you're
        gathering the tasks with `return_exceptions=True`.)

        Usage:

        ```python notest
        # Example 1: Await three coroutines
        created_object, other_work, new_plumbing = await TaskContext.gather(
            create_my_object(),
            do_some_other_work(),
            fix_plumbing(),
        )

        # Example 2: Gather a list of coroutines
        coros = [a.load() for a in objects]
        results = await TaskContext.gather(*coros)
        ```
        """
        async with TaskContext() as tc:
            results = await asyncio.gather(*(tc.create_task(coro) for coro in coros))
        return results


def run_coro_blocking(coro):
    """Fairly hacky thing that's needed in some extreme cases.

    It's basically works like asyncio.run but unlike asyncio.run it also works
    with in the case an event loop is already running. It does this by basically
    moving the whole thing to a separate thread.
    """
    with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
        fut = executor.submit(asyncio.run, coro)
        return fut.result()


class TimestampPriorityQueue(Generic[T]):
    """
    A priority queue that schedules items to be processed at specific timestamps.
    """

    _MAX_PRIORITY = float("inf")

    def __init__(self, maxsize: int = 0):
        self.condition = asyncio.Condition()
        self._queue: asyncio.PriorityQueue[tuple[float, int, Union[T, None]]] = asyncio.PriorityQueue(maxsize=maxsize)
        # Used to tiebreak items with the same timestamp that are not comparable. (eg. protos)
        self._counter = itertools.count()

    async def close(self):
        await self.put(self._MAX_PRIORITY, None)

    async def put(self, timestamp: float, item: Union[T, None]):
        """
        Add an item to the queue to be processed at a specific timestamp.
        """
        await self._queue.put((timestamp, next(self._counter), item))
        async with self.condition:
            self.condition.notify_all()  # notify any waiting coroutines

    async def get(self) -> Union[T, None]:
        """
        Get the next item from the queue that is ready to be processed.
        """
        while True:
            async with self.condition:
                while self.empty():
                    await self.condition.wait()
                # peek at the next item
                timestamp, counter, item = await self._queue.get()
                now = time.time()
                if timestamp < now:
                    return item
                if timestamp == self._MAX_PRIORITY:
                    return None
                # not ready yet, calculate sleep time
                sleep_time = timestamp - now
                self._queue.put_nowait((timestamp, counter, item))  # put it back
                # wait until either the timeout or a new item is added
                try:
                    await asyncio.wait_for(self.condition.wait(), timeout=sleep_time)
                except asyncio.TimeoutError:
                    continue

    def empty(self) -> bool:
        return self._queue.empty()

    def qsize(self) -> int:
        return self._queue.qsize()

    async def clear(self):
        """
        Clear the retry queue. Used for testing to simulate reading all elements from queue using queue_batch_iterator.
        """
        while not self.empty():
            await self.get()

    def __len__(self):
        return self._queue.qsize()


async def queue_batch_iterator(
    q: Union[asyncio.Queue, TimestampPriorityQueue], max_batch_size=100, debounce_time=0.015
):
    """
    Read from a queue but return lists of items when queue is large

    Treats a None value as end of queue items
    """
    item_list: list[Any] = []

    while True:
        if q.empty() and len(item_list) > 0:
            yield item_list
            item_list = []
            await asyncio.sleep(debounce_time)

        res = await q.get()

        if len(item_list) >= max_batch_size:
            yield item_list
            item_list = []

        if res is None:
            if len(item_list) > 0:
                yield item_list
            break
        item_list.append(res)


class _WarnIfGeneratorIsNotConsumed:
    def __init__(self, gen, function_name: str):
        self.gen = gen
        self.function_name = function_name
        self.iterated = False
        self.warned = False

    def __aiter__(self):
        self.iterated = True
        return self.gen.__aiter__()

    async def __anext__(self):
        self.iterated = True
        return await self.gen.__anext__()

    async def asend(self, value):
        self.iterated = True
        return await self.gen.asend(value)

    def __repr__(self):
        return repr(self.gen)

    def __del__(self):
        if not self.iterated and not self.warned:
            self.warned = True
            logger.warning(
                f"Warning: the results of a call to {self.function_name} was not consumed, "
                "so the call will never be executed."
                f" Consider a for-loop like `for x in {self.function_name}(...)` or "
                "unpacking the generator using `list(...)`"
            )

    async def athrow(self, exc):
        return await self.gen.athrow(exc)

    async def aclose(self):
        return await self.gen.aclose()


_BlockingWarnIfGeneratorIsNotConsumed = synchronize_api(_WarnIfGeneratorIsNotConsumed)


class _WarnIfNonWrappedGeneratorIsNotConsumed(_WarnIfGeneratorIsNotConsumed):
    # used for non-synchronicity-wrapped generators and iterators
    def __iter__(self):
        self.iterated = True
        return iter(self.gen)

    def __next__(self):
        self.iterated = True
        return self.gen.__next__()

    def send(self, value):
        self.iterated = True
        return self.gen.send(value)


def warn_if_generator_is_not_consumed(function_name: Optional[str] = None):
    # https://gist.github.com/erikbern/01ae78d15f89edfa7f77e5c0a827a94d
    def decorator(gen_f):
        presented_func_name = function_name if function_name is not None else gen_f.__name__

        @functools.wraps(gen_f)
        def f_wrapped(*args, **kwargs):
            gen = gen_f(*args, **kwargs)
            if inspect.isasyncgen(gen):
                return _WarnIfGeneratorIsNotConsumed(gen, presented_func_name)
            else:
                return _WarnIfNonWrappedGeneratorIsNotConsumed(gen, presented_func_name)

        return f_wrapped

    return decorator


def run_coroutine_in_temporary_event_loop(coro: typing.Coroutine[None, None, T], nested_async_message: str) -> T:
    """Compatibility function to run an async coroutine in a temporary event loop.

    This is needed for compatibility with the async implementation of Function.spawn_map. The future plan is
    to have separate implementations so there is no issue with nested event loops.
    """
    try:
        with Runner() as runner:
            return runner.run(coro)
    except NestedEventLoops:
        raise InvalidError(nested_async_message)


class AsyncOrSyncIterable:
    """Compatibility class for non-synchronicity wrapped async iterables to get
    both async and sync interfaces in the same way that synchronicity does (but on the main thread)
    so they can be "lazily" iterated using either `for _ in x` or `async for _ in x`

    nested_async_message is raised as an InvalidError if the async variant is called
    from an already async context, since that would otherwise deadlock the event loop
    """

    def __init__(self, async_iterable: typing.AsyncGenerator[Any, None], nested_async_message):
        self._async_iterable = async_iterable
        self.nested_async_message = nested_async_message

    def __aiter__(self):
        return self._async_iterable

    def __iter__(self):
        try:
            with Runner() as runner:
                yield from run_async_gen(runner, self._async_iterable)
        except NestedEventLoops:
            raise InvalidError(self.nested_async_message)

    async def aclose(self):
        if hasattr(self._async_iterable, "aclose"):
            await self._async_iterable.aclose()


_shutdown_tasks = []


def on_shutdown(coro):
    # hook into event loop shutdown when all active tasks get cancelled
    async def wrapper():
        try:
            await asyncio.sleep(1e10)  # never awake except for exceptions
        finally:
            await coro
            raise

    _shutdown_tasks.append(asyncio.create_task(wrapper()))


def asyncify(f: Callable[P, T]) -> Callable[P, typing.Coroutine[None, None, T]]:
    """Convert a blocking function into one that runs in the current loop's executor."""

    @functools.wraps(f)
    async def wrapper(*args: P.args, **kwargs: P.kwargs):
        loop = asyncio.get_running_loop()
        return await loop.run_in_executor(None, functools.partial(f, *args, **kwargs))

    return wrapper


async def iterate_blocking(iterator: Iterator[T]) -> AsyncGenerator[T, None]:
    """Iterate over a blocking iterator in an async context."""

    loop = asyncio.get_running_loop()
    DONE = object()
    while True:
        obj = await loop.run_in_executor(None, next, iterator, DONE)
        if obj is DONE:
            break
        yield cast(T, obj)


@asynccontextmanager
async def asyncnullcontext(*args, **kwargs):
    """Async noop context manager.

    Note that for Python 3.10+ you can use contextlib.nullcontext() instead.

    Usage:
    async with asyncnullcontext():
        pass
    """
    yield


YIELD_TYPE = typing.TypeVar("YIELD_TYPE")
SEND_TYPE = typing.TypeVar("SEND_TYPE")


def run_async_gen(
    runner: Runner,
    gen: typing.AsyncGenerator[YIELD_TYPE, SEND_TYPE],
) -> typing.Generator[YIELD_TYPE, SEND_TYPE, None]:
    """Convert an async generator into a sync one"""
    # more or less copied from synchronicity's implementation:
    next_send: typing.Union[SEND_TYPE, None] = None
    next_yield: YIELD_TYPE
    exc: Optional[BaseException] = None
    while True:
        try:
            if exc:
                next_yield = runner.run(gen.athrow(exc))
            else:
                next_yield = runner.run(gen.asend(next_send))  # type: ignore[arg-type]
        except KeyboardInterrupt as e:
            raise e from None
        except StopAsyncIteration:
            break  # typically a graceful exit of the async generator
        try:
            next_send = yield next_yield
            exc = None
        except BaseException as err:
            exc = err


class aclosing(typing.Generic[T]):  # noqa
    # backport of Python contextlib.aclosing from Python 3.10
    def __init__(self, agen: AsyncGenerator[T, None]):
        self.agen = agen

    async def __aenter__(self) -> AsyncGenerator[T, None]:
        return self.agen

    async def __aexit__(self, exc, exc_type, tb):
        await self.agen.aclose()


async def sync_or_async_iter(iter: Union[Iterable[T], AsyncIterable[T]]) -> AsyncGenerator[T, None]:
    if hasattr(iter, "__aiter__"):
        agen = typing.cast(AsyncGenerator[T, None], iter)
        try:
            async for item in agen:
                yield item
        finally:
            if hasattr(agen, "aclose"):
                # All AsyncGenerator's have an aclose method
                # but some AsyncIterable's don't necessarily
                await agen.aclose()
    else:
        assert hasattr(iter, "__iter__"), "sync_or_async_iter requires an Iterable or AsyncGenerator"
        # This intentionally could block the event loop for the duration of calling __iter__ and __next__,
        # so in non-trivial cases (like passing lists and ranges) this could be quite a foot gun for users #
        # w/ async code (but they can work around it by always using async iterators)
        for item in typing.cast(Iterable[T], iter):
            yield item


@typing.overload
def async_zip(g1: AsyncGenerator[T, None], g2: AsyncGenerator[V, None], /) -> AsyncGenerator[tuple[T, V], None]: ...


@typing.overload
def async_zip(*generators: AsyncGenerator[T, None]) -> AsyncGenerator[tuple[T, ...], None]: ...


async def async_zip(*generators):
    tasks = []
    try:
        while True:
            try:

                async def next_item(gen):
                    return await gen.__anext__()

                tasks = [asyncio.create_task(next_item(gen)) for gen in generators]
                items = await asyncio.gather(*tasks)
                yield tuple(items)
            except StopAsyncIteration:
                break
    finally:
        cancelled_tasks = []
        for task in tasks:
            if not task.done():
                task.cancel()
                cancelled_tasks.append(task)
        try:
            await asyncio.gather(*cancelled_tasks)
        except asyncio.CancelledError:
            pass

        first_exception = None
        for gen in generators:
            try:
                await gen.aclose()
            except BaseException as e:
                if first_exception is None:
                    first_exception = e
                logger.exception(f"Error closing async generator: {e}")
        if first_exception is not None:
            raise first_exception


@dataclass
class ValueWrapper(typing.Generic[T]):
    value: T


@dataclass
class ExceptionWrapper:
    value: Exception


class StopSentinelType: ...


STOP_SENTINEL = StopSentinelType()


async def async_merge(
    *generators: AsyncGenerator[T, None], cancellation_timeout: float = 10.0
) -> AsyncGenerator[T, None]:
    """
    Asynchronously merges multiple async generators into a single async generator.

    This function takes multiple async generators and yields their values in the order
    they are produced. If any generator raises an exception, the exception is propagated.

    Args:
        *generators: One or more async generators to be merged.

    Yields:
        The values produced by the input async generators.

    Raises:
        Exception: If any of the input generators raises an exception, it is propagated.

    Usage:
    ```python
    import asyncio
    from modal._utils.async_utils import async_merge

    async def gen1():
        yield 1
        yield 2

    async def gen2():
        yield "a"
        yield "b"

    async def example():
        values = set()
        async for value in async_merge(gen1(), gen2()):
            values.add(value)

        return values

    # Output could be: {1, "a", 2, "b"} (order may vary)
    values = asyncio.run(example())
    assert values == {1, "a", 2, "b"}
    ```
    """
    queue: asyncio.Queue[Union[ValueWrapper[T], ExceptionWrapper]] = asyncio.Queue(maxsize=len(generators) * 10)

    async def producer(generator: AsyncGenerator[T, None]):
        try:
            async with aclosing(generator) as stream:
                async for item in stream:
                    await queue.put(ValueWrapper(item))
        except Exception as e:
            await queue.put(ExceptionWrapper(e))

    tasks = {asyncio.create_task(producer(gen)) for gen in generators}
    new_output_task = asyncio.create_task(queue.get())

    try:
        while tasks:
            done, _ = await asyncio.wait(
                [*tasks, new_output_task],
                return_when=asyncio.FIRST_COMPLETED,
            )

            if new_output_task in done:
                item = new_output_task.result()
                if isinstance(item, ValueWrapper):
                    yield item.value
                else:
                    assert_type(item, ExceptionWrapper)
                    raise item.value

                new_output_task = asyncio.create_task(queue.get())

            finished_producers = done & tasks
            tasks -= finished_producers
            for finished_producer in finished_producers:
                # this is done in order to catch potential raised errors/cancellations
                # from within worker tasks as soon as they happen.
                await finished_producer

        while not queue.empty():
            item = await new_output_task
            if isinstance(item, ValueWrapper):
                yield item.value
            else:
                assert_type(item, ExceptionWrapper)
                raise item.value

            new_output_task = asyncio.create_task(queue.get())

    finally:
        unfinished_tasks = [t for t in tasks | {new_output_task} if not t.done()]
        for t in unfinished_tasks:
            t.cancel()
        try:
            await asyncio.wait_for(
                asyncio.shield(
                    # we need to `shield` the `gather` to ensure cooperation with the timeout
                    # all underlying tasks have been marked as cancelled at this point anyway
                    asyncio.gather(*unfinished_tasks, return_exceptions=True)
                ),
                timeout=cancellation_timeout,
            )
        except asyncio.TimeoutError:
            logger.debug("Timed out while cleaning up async_merge")


async def callable_to_agen(awaitable: Callable[[], Awaitable[T]]) -> AsyncGenerator[T, None]:
    yield await awaitable()


async def gather_cancel_on_exc(*coros_or_futures):
    input_tasks = [asyncio.ensure_future(t) for t in coros_or_futures]
    try:
        return await asyncio.gather(*input_tasks)
    except BaseException:
        for t in input_tasks:
            t.cancel()
        await asyncio.gather(*input_tasks, return_exceptions=False)  # handle cancellations
        raise


async def prevent_cancellation_abortion(coro):
    # if this is cancelled, it will wait for coro cancellation handling
    # and then unconditionally re-raises a CancelledError, even if the underlying coro
    # doesn't re-raise the cancellation itself
    t = asyncio.create_task(coro)
    try:
        return await asyncio.shield(t)
    except asyncio.CancelledError:
        if t.cancelled():
            # coro cancelled itself - reraise
            raise
        t.cancel()  # cancel task
        await t  # this *normally* reraises
        raise  # if the above somehow resolved, by swallowing cancellation - we still raise


async def async_map(
    input_generator: AsyncGenerator[T, None],
    async_mapper_func: Callable[[T], Awaitable[V]],
    concurrency: int,
    cancellation_timeout: float = 10.0,
) -> AsyncGenerator[V, None]:
    queue: asyncio.Queue[Union[ValueWrapper[T], StopSentinelType]] = asyncio.Queue(maxsize=concurrency * 2)

    async def producer() -> AsyncGenerator[V, None]:
        async with aclosing(input_generator) as stream:
            async for item in stream:
                await queue.put(ValueWrapper(item))

        for _ in range(concurrency):
            await queue.put(STOP_SENTINEL)

        if False:
            # Need it to be an async generator for async_merge
            # but we don't want to yield anything
            yield

    async def worker() -> AsyncGenerator[V, None]:
        while True:
            item = await queue.get()
            if isinstance(item, ValueWrapper):
                res = await prevent_cancellation_abortion(async_mapper_func(item.value))
                yield res
            elif isinstance(item, ExceptionWrapper):
                raise item.value
            else:
                assert_type(item, StopSentinelType)
                break

    async with aclosing(
        async_merge(*[worker() for i in range(concurrency)], producer(), cancellation_timeout=cancellation_timeout)
    ) as stream:
        async for item in stream:
            yield item


async def async_map_ordered(
    input_generator: AsyncGenerator[T, None],
    async_mapper_func: Callable[[T], Awaitable[V]],
    concurrency: int,
    buffer_size: Optional[int] = None,
) -> AsyncGenerator[V, None]:
    semaphore = asyncio.Semaphore(buffer_size or concurrency)

    async def mapper_func_wrapper(tup: tuple[int, T]) -> tuple[int, V]:
        return (tup[0], await async_mapper_func(tup[1]))

    async def counter() -> AsyncGenerator[int, None]:
        for i in itertools.count():
            await semaphore.acquire()
            yield i

    next_idx = 0
    buffer = {}

    async with aclosing(async_map(async_zip(counter(), input_generator), mapper_func_wrapper, concurrency)) as stream:
        async for output_idx, output_item in stream:
            buffer[output_idx] = output_item

            while next_idx in buffer:
                yield buffer[next_idx]
                semaphore.release()
                del buffer[next_idx]
                next_idx += 1


async def async_chain(*generators: AsyncGenerator[T, None]) -> AsyncGenerator[T, None]:
    try:
        for gen in generators:
            async for item in gen:
                yield item
    finally:
        first_exception = None
        for gen in generators:
            try:
                await gen.aclose()
            except BaseException as e:
                if first_exception is None:
                    first_exception = e
                logger.exception(f"Error closing async generator: {e}")
        if first_exception is not None:
            raise first_exception



================================================
FILE: modal/_utils/auth_token_manager.py
================================================
# Copyright Modal Labs 2025
import asyncio
import base64
import json
import time
import typing
from typing import Any

from modal.exception import ExecutionError
from modal_proto import api_pb2, modal_api_grpc

from .grpc_utils import retry_transient_errors
from .logger import logger


class _AuthTokenManager:
    """Handles fetching and refreshing of the input plane auth token."""

    # Start refreshing this many seconds before the token expires
    REFRESH_WINDOW = 5 * 60
    # If the token doesn't have an expiry field, default to current time plus this value (not expected).
    DEFAULT_EXPIRY_OFFSET = 20 * 60

    def __init__(self, stub: "modal_api_grpc.ModalClientModal"):
        self._stub = stub
        self._token = ""
        self._expiry = 0.0
        self._lock: typing.Union[asyncio.Lock, None] = None

    async def get_token(self) -> str:
        """
        When called, the AuthTokenManager can be in one of three states:
        1. Has a valid cached token. It is returned to the caller.
        2. Has no cached token, or the token is expired. We fetch a new one and cache it. If `get_token` is called
        concurrently by multiple coroutines, all requests will block until the token has been fetched. But only one
        coroutine will actually make a request to the control plane to fetch the new token. This ensures we do not hit
        the control plane with more requests than needed.
        3. Has a valid cached token, but it is going to expire in the next 5 minutes. In this case we fetch a new token
        and cache it. If `get_token` is called concurrently, only one request will fetch the new token, and the others
        will be given the old (but still valid) token - i.e. they will not block.
        """
        if not self._token or self._is_expired():
            # We either have no token or it is expired - block everyone until we get a new token
            await self._refresh_token()
        elif self._needs_refresh():
            # The token hasn't expired yet, but will soon, so it needs a refresh.
            lock = await self._get_lock()
            if lock.locked():
                # The lock is taken, so someone else is refreshing. Continue to use the old token.
                return self._token
            else:
                # The lock is not taken, so we need to fetch a new token.
                await self._refresh_token()

        return self._token

    async def _refresh_token(self):
        """
        Fetch a new token from the control plane. If called concurrently, only one coroutine will make a request for a
        new token. The others will block on a lock, until the first coroutine has fetched the new token.
        """
        lock = await self._get_lock()
        async with lock:
            # Double check inside lock - maybe another coroutine refreshed already. This happens the first time we fetch
            # the token. The first coroutine will fetch the token, while the others block on the lock, waiting for the
            # new token. Once we have a new token, the other coroutines will unblock and return from here.
            if self._token and not self._needs_refresh():
                return
            resp: api_pb2.AuthTokenGetResponse = await retry_transient_errors(
                self._stub.AuthTokenGet, api_pb2.AuthTokenGetRequest()
            )
            if not resp.token:
                # Not expected
                raise ExecutionError(
                    "Internal error: Did not receive auth token from server. Please contact Modal support."
                )

            self._token = resp.token
            if exp := self._decode_jwt(resp.token).get("exp"):
                self._expiry = float(exp)
            else:
                # This should never happen.
                logger.warning("x-modal-auth-token does not contain exp field")
                # We'll use the token, and set the expiry to 20 min from now.
                self._expiry = time.time() + self.DEFAULT_EXPIRY_OFFSET

    async def _get_lock(self) -> asyncio.Lock:
        # Note: this function runs no async code but is marked as async to ensure it's
        # being run inside the synchronicity event loop and binds the lock to the
        # correct event loop on Python 3.9 which eagerly assigns event loops on
        # constructions of locks
        if self._lock is None:
            self._lock = asyncio.Lock()
        return self._lock

    @staticmethod
    def _decode_jwt(token: str) -> dict[str, Any]:
        """
        Decodes a JWT into a dict without verifying signature. We do this manually instead of using a library to avoid
        adding another dependency to the client.
        """
        try:
            payload = token.split(".")[1]
            padding = "=" * (-len(payload) % 4)
            decoded_bytes = base64.urlsafe_b64decode(payload + padding)
            return json.loads(decoded_bytes)
        except Exception as e:
            raise ValueError("Internal error: Cannot parse auth token. Please contact Modal support.") from e

    def _needs_refresh(self):
        return time.time() >= (self._expiry - self.REFRESH_WINDOW)

    def _is_expired(self):
        return time.time() >= self._expiry



================================================
FILE: modal/_utils/blob_utils.py
================================================
# Copyright Modal Labs 2022
import asyncio
import dataclasses
import hashlib
import os
import platform
import random
import time
from collections.abc import AsyncIterator
from contextlib import AbstractContextManager, contextmanager
from io import BytesIO, FileIO
from pathlib import Path, PurePosixPath
from typing import (
    TYPE_CHECKING,
    Any,
    BinaryIO,
    Callable,
    ContextManager,
    Optional,
    Union,
    cast,
)
from urllib.parse import urlparse

from modal_proto import api_pb2
from modal_proto.modal_api_grpc import ModalClientModal

from ..exception import ExecutionError
from .async_utils import TaskContext, retry
from .grpc_utils import retry_transient_errors
from .hash_utils import UploadHashes, get_upload_hashes
from .http_utils import ClientSessionRegistry
from .logger import logger

if TYPE_CHECKING:
    from .bytes_io_segment_payload import BytesIOSegmentPayload

# Max size for function inputs and outputs.
MAX_OBJECT_SIZE_BYTES = 2 * 1024 * 1024  # 2 MiB

# Max size for async function inputs and outputs.
MAX_ASYNC_OBJECT_SIZE_BYTES = 8 * 1024  # 8 KiB

#  If a file is LARGE_FILE_LIMIT bytes or larger, it's uploaded to blob store (s3) instead of going through grpc
#  It will also make sure to chunk the hash calculation to avoid reading the entire file into memory
LARGE_FILE_LIMIT = 4 * 1024 * 1024  # 4 MiB

# Max parallelism during map calls
BLOB_MAX_PARALLELISM = 20

# read ~16MiB chunks by default
DEFAULT_SEGMENT_CHUNK_SIZE = 2**24

# Files larger than this will be multipart uploaded. The server might request multipart upload for smaller files as
# well, but the limit will never be raised.
# TODO(dano): remove this once we stop requiring md5 for blobs
MULTIPART_UPLOAD_THRESHOLD = 1024**3

# For block based storage like volumefs2: the size of a block
BLOCK_SIZE: int = 8 * 1024 * 1024

HEALTHY_R2_UPLOAD_PERCENTAGE = 0.95


@retry(n_attempts=5, base_delay=0.5, timeout=None)
async def _upload_to_s3_url(
    upload_url,
    payload: "BytesIOSegmentPayload",
    content_md5_b64: Optional[str] = None,
    content_type: Optional[str] = "application/octet-stream",  # set to None to force omission of ContentType header
) -> str:
    """Returns etag of s3 object which is a md5 hex checksum of the uploaded content"""
    with payload.reset_on_error():  # ensure retries read the same data
        headers = {}
        if content_md5_b64 and use_md5(upload_url):
            headers["Content-MD5"] = content_md5_b64
        if content_type:
            headers["Content-Type"] = content_type

        async with ClientSessionRegistry.get_session().put(
            upload_url,
            data=payload,
            headers=headers,
            skip_auto_headers=["content-type"] if content_type is None else [],
        ) as resp:
            # S3 signal to slow down request rate.
            if resp.status == 503:
                logger.debug("Received SlowDown signal from S3, sleeping for 1 second before retrying.")
                await asyncio.sleep(1)

            if resp.status != 200:
                try:
                    text = await resp.text()
                except Exception:
                    text = "<no body>"
                raise ExecutionError(f"Put to url {upload_url} failed with status {resp.status}: {text}")

            # client side ETag checksum verification
            # the s3 ETag of a single part upload is a quoted md5 hex of the uploaded content
            etag = resp.headers["ETag"].strip()
            if etag.startswith(("W/", "w/")):  # see https://www.rfc-editor.org/rfc/rfc7232#section-2.3
                etag = etag[2:]
            if etag[0] == '"' and etag[-1] == '"':
                etag = etag[1:-1]
            remote_md5 = etag

            local_md5_hex = payload.md5_checksum().hexdigest()
            if local_md5_hex != remote_md5:
                raise ExecutionError(f"Local data and remote data checksum mismatch ({local_md5_hex} vs {remote_md5})")

            return remote_md5


async def perform_multipart_upload(
    data_file: Union[BinaryIO, BytesIO, FileIO],
    *,
    content_length: int,
    max_part_size: int,
    part_urls: list[str],
    completion_url: str,
    upload_chunk_size: int = DEFAULT_SEGMENT_CHUNK_SIZE,
    progress_report_cb: Optional[Callable] = None,
) -> None:
    from .bytes_io_segment_payload import BytesIOSegmentPayload

    upload_coros = []
    file_offset = 0
    num_bytes_left = content_length

    # Give each part its own IO reader object to avoid needing to
    # lock access to the reader's position pointer.
    data_file_readers: list[BinaryIO]
    if isinstance(data_file, BytesIO):
        view = data_file.getbuffer()  # does not copy data
        data_file_readers = [BytesIO(view) for _ in range(len(part_urls))]
    else:
        filename = data_file.name
        data_file_readers = [open(filename, "rb") for _ in range(len(part_urls))]

    for part_number, (data_file_rdr, part_url) in enumerate(zip(data_file_readers, part_urls), start=1):
        part_length_bytes = min(num_bytes_left, max_part_size)
        part_payload = BytesIOSegmentPayload(
            data_file_rdr,
            segment_start=file_offset,
            segment_length=part_length_bytes,
            chunk_size=upload_chunk_size,
            progress_report_cb=progress_report_cb,
        )
        upload_coros.append(_upload_to_s3_url(part_url, payload=part_payload, content_type=None))
        num_bytes_left -= part_length_bytes
        file_offset += part_length_bytes

    part_etags = await TaskContext.gather(*upload_coros)

    # The body of the complete_multipart_upload command needs some data in xml format:
    completion_body = "<CompleteMultipartUpload>\n"
    for part_number, etag in enumerate(part_etags, 1):
        completion_body += f"""<Part>\n<PartNumber>{part_number}</PartNumber>\n<ETag>"{etag}"</ETag>\n</Part>\n"""
    completion_body += "</CompleteMultipartUpload>"

    # etag of combined object should be md5 hex of concatendated md5 *bytes* from parts + `-{num_parts}`
    bin_hash_parts = [bytes.fromhex(etag) for etag in part_etags]

    expected_multipart_etag = hashlib.md5(b"".join(bin_hash_parts)).hexdigest() + f"-{len(part_etags)}"
    resp = await ClientSessionRegistry.get_session().post(
        completion_url, data=completion_body.encode("ascii"), skip_auto_headers=["content-type"]
    )
    if resp.status != 200:
        try:
            msg = await resp.text()
        except Exception:
            msg = "<no body>"
        raise ExecutionError(f"Error when completing multipart upload: {resp.status}\n{msg}")
    else:
        response_body = await resp.text()
        if expected_multipart_etag not in response_body:
            raise ExecutionError(
                f"Hash mismatch on multipart upload assembly: {expected_multipart_etag} not in {response_body}"
            )


def get_content_length(data: BinaryIO) -> int:
    # *Remaining* length of file from current seek position
    pos = data.tell()
    data.seek(0, os.SEEK_END)
    content_length = data.tell()
    data.seek(pos)
    return content_length - pos


async def _blob_upload_with_fallback(
    items, blob_ids: list[str], callback, content_length: int
) -> tuple[str, bool, int]:
    r2_throughput_bytes_s = 0
    r2_failed = False
    for idx, (item, blob_id) in enumerate(zip(items, blob_ids)):
        # We want to default to R2 95% of the time and S3 5% of the time.
        # To ensure the failure path is continuously exercised.
        if idx == 0 and len(items) > 1 and random.random() > HEALTHY_R2_UPLOAD_PERCENTAGE:
            continue
        try:
            if blob_id.endswith(":r2"):
                t0 = time.monotonic_ns()
                await callback(item)
                dt_ns = time.monotonic_ns() - t0
                r2_throughput_bytes_s = (content_length * 1_000_000_000) // max(dt_ns, 1)
            else:
                await callback(item)
            return blob_id, r2_failed, r2_throughput_bytes_s
        except Exception as _:
            if blob_id.endswith(":r2"):
                r2_failed = True
            # Ignore all errors except the last one, since we're out of fallback options.
            if idx == len(items) - 1:
                raise
    raise ExecutionError("Failed to upload blob")


async def _blob_upload(
    upload_hashes: UploadHashes, data: Union[bytes, BinaryIO], stub, progress_report_cb: Optional[Callable] = None
) -> tuple[str, bool, int]:
    if isinstance(data, bytes):
        data = BytesIO(data)

    content_length = get_content_length(data)

    req = api_pb2.BlobCreateRequest(
        content_md5=upload_hashes.md5_base64,
        content_sha256_base64=upload_hashes.sha256_base64,
        content_length=content_length,
    )
    resp = await retry_transient_errors(stub.BlobCreate, req)

    if resp.WhichOneof("upload_types_oneof") == "multiparts":

        async def upload_multipart_upload(part):
            return await perform_multipart_upload(
                data,
                content_length=content_length,
                max_part_size=part.part_length,
                part_urls=part.upload_urls,
                completion_url=part.completion_url,
                upload_chunk_size=DEFAULT_SEGMENT_CHUNK_SIZE,
                progress_report_cb=progress_report_cb,
            )

        blob_id, r2_failed, r2_throughput_bytes_s = await _blob_upload_with_fallback(
            resp.multiparts.items,
            resp.blob_ids,
            upload_multipart_upload,
            content_length=content_length,
        )
    else:
        from .bytes_io_segment_payload import BytesIOSegmentPayload

        payload = BytesIOSegmentPayload(
            data, segment_start=0, segment_length=content_length, progress_report_cb=progress_report_cb
        )

        async def upload_to_s3_url(url):
            return await _upload_to_s3_url(
                url,
                payload,
                # for single part uploads, we use server side md5 checksums
                content_md5_b64=upload_hashes.md5_base64,
            )

        blob_id, r2_failed, r2_throughput_bytes_s = await _blob_upload_with_fallback(
            resp.upload_urls.items,
            resp.blob_ids,
            upload_to_s3_url,
            content_length=content_length,
        )

    if progress_report_cb:
        progress_report_cb(complete=True)

    return blob_id, r2_failed, r2_throughput_bytes_s


async def blob_upload_with_r2_failure_info(payload: bytes, stub: ModalClientModal) -> tuple[str, bool, int]:
    size_mib = len(payload) / 1024 / 1024
    logger.debug(f"Uploading large blob of size {size_mib:.2f} MiB")
    t0 = time.time()
    if isinstance(payload, str):
        logger.debug("Blob uploading string, not bytes - auto-encoding as utf8")
        payload = payload.encode("utf8")
    upload_hashes = get_upload_hashes(payload)
    blob_id, r2_failed, r2_throughput_bytes_s = await _blob_upload(upload_hashes, payload, stub)
    dur_s = max(time.time() - t0, 0.001)  # avoid division by zero
    throughput_mib_s = (size_mib) / dur_s
    logger.debug(
        f"Uploaded large blob of size {size_mib:.2f} MiB ({throughput_mib_s:.2f} MiB/s, total {dur_s:.2f}s). {blob_id}"
    )
    return blob_id, r2_failed, r2_throughput_bytes_s


async def blob_upload(payload: bytes, stub: ModalClientModal) -> str:
    blob_id, _, _ = await blob_upload_with_r2_failure_info(payload, stub)
    return blob_id


async def format_blob_data(data: bytes, api_stub: ModalClientModal) -> dict[str, Any]:
    return {"data_blob_id": await blob_upload(data, api_stub)} if len(data) > MAX_OBJECT_SIZE_BYTES else {"data": data}


async def blob_upload_file(
    file_obj: BinaryIO,
    stub: ModalClientModal,
    progress_report_cb: Optional[Callable] = None,
    sha256_hex: Optional[str] = None,
    md5_hex: Optional[str] = None,
) -> str:
    upload_hashes = get_upload_hashes(file_obj, sha256_hex=sha256_hex, md5_hex=md5_hex)
    blob_id, _, _ = await _blob_upload(upload_hashes, file_obj, stub, progress_report_cb)
    return blob_id


@retry(n_attempts=5, base_delay=0.1, timeout=None)
async def _download_from_url(download_url: str) -> bytes:
    async with ClientSessionRegistry.get_session().get(download_url) as s3_resp:
        # S3 signal to slow down request rate.
        if s3_resp.status == 503:
            logger.debug("Received SlowDown signal from S3, sleeping for 1 second before retrying.")
            await asyncio.sleep(1)

        if s3_resp.status != 200:
            text = await s3_resp.text()
            raise ExecutionError(f"Get from url failed with status {s3_resp.status}: {text}")
        return await s3_resp.read()


async def blob_download(blob_id: str, stub: ModalClientModal) -> bytes:
    """Convenience function for reading all of the downloaded file into memory."""
    logger.debug(f"Downloading large blob {blob_id}")
    t0 = time.time()
    req = api_pb2.BlobGetRequest(blob_id=blob_id)
    resp = await retry_transient_errors(stub.BlobGet, req)
    data = await _download_from_url(resp.download_url)
    size_mib = len(data) / 1024 / 1024
    dur_s = max(time.time() - t0, 0.001)  # avoid division by zero
    throughput_mib_s = size_mib / dur_s
    logger.debug(
        f"Downloaded large blob {blob_id} of size {size_mib:.2f} MiB ({throughput_mib_s:.2f} MiB/s, total {dur_s:.2f}s)"
    )
    return data


async def blob_iter(blob_id: str, stub: ModalClientModal) -> AsyncIterator[bytes]:
    req = api_pb2.BlobGetRequest(blob_id=blob_id)
    resp = await retry_transient_errors(stub.BlobGet, req)
    download_url = resp.download_url
    async with ClientSessionRegistry.get_session().get(download_url) as s3_resp:
        # S3 signal to slow down request rate.
        if s3_resp.status == 503:
            logger.debug("Received SlowDown signal from S3, sleeping for 1 second before retrying.")
            await asyncio.sleep(1)

        if s3_resp.status != 200:
            text = await s3_resp.text()
            raise ExecutionError(f"Get from url failed with status {s3_resp.status}: {text}")

        async for chunk in s3_resp.content.iter_any():
            yield chunk


@dataclasses.dataclass
class FileUploadSpec:
    source: Callable[[], Union[AbstractContextManager, BinaryIO]]
    source_description: Any
    source_is_path: bool
    mount_filename: str

    use_blob: bool
    content: Optional[bytes]  # typically None if using blob, required otherwise
    sha256_hex: str
    md5_hex: str
    mode: int  # file permission bits (last 12 bits of st_mode)
    size: int


def _get_file_upload_spec(
    source: Callable[[], Union[AbstractContextManager, BinaryIO]],
    source_description: Any,
    mount_filename: PurePosixPath,
    mode: int,
) -> FileUploadSpec:
    with source() as fp:
        # Current position is ignored - we always upload from position 0
        fp.seek(0, os.SEEK_END)
        size = fp.tell()
        fp.seek(0)

        if size >= LARGE_FILE_LIMIT:
            # TODO(dano): remove the placeholder md5 once we stop requiring md5 for blobs
            md5_hex = "baadbaadbaadbaadbaadbaadbaadbaad" if size > MULTIPART_UPLOAD_THRESHOLD else None
            use_blob = True
            content = None
            hashes = get_upload_hashes(fp, md5_hex=md5_hex)
        else:
            use_blob = False
            content = fp.read()
            hashes = get_upload_hashes(content)

    return FileUploadSpec(
        source=source,
        source_description=source_description,
        source_is_path=isinstance(source_description, Path),
        mount_filename=mount_filename.as_posix(),
        use_blob=use_blob,
        content=content,
        sha256_hex=hashes.sha256_hex(),
        md5_hex=hashes.md5_hex(),
        mode=mode & 0o7777,
        size=size,
    )


def get_file_upload_spec_from_path(
    filename: Path, mount_filename: PurePosixPath, mode: Optional[int] = None
) -> FileUploadSpec:
    # Python appears to give files 0o666 bits on Windows (equal for user, group, and global),
    # so we mask those out to 0o755 for compatibility with POSIX-based permissions.
    mode = mode or os.stat(filename).st_mode & (0o7777 if platform.system() != "Windows" else 0o7755)
    return _get_file_upload_spec(
        lambda: open(filename, "rb"),
        filename,
        mount_filename,
        mode,
    )


def get_file_upload_spec_from_fileobj(fp: BinaryIO, mount_filename: PurePosixPath, mode: int) -> FileUploadSpec:
    @contextmanager
    def source():
        # We ignore position in stream and always upload from position 0
        fp.seek(0)
        yield fp

    return _get_file_upload_spec(
        source,
        str(fp),
        mount_filename,
        mode,
    )


_FileUploadSource2 = Callable[[], ContextManager[BinaryIO]]


@dataclasses.dataclass
class FileUploadBlock:
    # The start (byte offset, inclusive) of the block within the file
    start: int
    # The end (byte offset, exclusive) of the block, after having removed any trailing zeroes
    end: int
    # Raw (unencoded 32 byte) SHA256 sum of the block, not including trailing zeroes
    contents_sha256: bytes


@dataclasses.dataclass
class FileUploadSpec2:
    source: _FileUploadSource2
    source_description: Union[str, Path]

    path: str
    # 8MiB file blocks
    blocks: list[FileUploadBlock]
    mode: int  # file permission bits (last 12 bits of st_mode)
    size: int

    @staticmethod
    async def from_path(
        filename: Path,
        mount_filename: PurePosixPath,
        hash_semaphore: asyncio.Semaphore,
        mode: Optional[int] = None,
    ) -> "FileUploadSpec2":
        # Python appears to give files 0o666 bits on Windows (equal for user, group, and global),
        # so we mask those out to 0o755 for compatibility with POSIX-based permissions.
        mode = mode or os.stat(filename).st_mode & (0o7777 if platform.system() != "Windows" else 0o7755)

        def source():
            return open(filename, "rb")

        return await FileUploadSpec2._create(
            source,
            filename,
            mount_filename,
            mode,
            hash_semaphore,
        )

    @staticmethod
    async def from_fileobj(
        source_fp: Union[BinaryIO, BytesIO],
        mount_filename: PurePosixPath,
        hash_semaphore: asyncio.Semaphore,
        mode: int,
    ) -> "FileUploadSpec2":
        try:
            fileno = source_fp.fileno()

            def source():
                new_fd = os.dup(fileno)
                fp = os.fdopen(new_fd, "rb")
                fp.seek(0)
                return fp

        except OSError:
            # `.fileno()` not available; assume BytesIO-like type
            source_fp = cast(BytesIO, source_fp)
            buffer = source_fp.getbuffer()

            def source():
                return BytesIO(buffer)

        return await FileUploadSpec2._create(
            source,
            str(source),
            mount_filename,
            mode,
            hash_semaphore,
        )

    @staticmethod
    async def _create(
        source: _FileUploadSource2,
        source_description: Union[str, Path],
        mount_filename: PurePosixPath,
        mode: int,
        hash_semaphore: asyncio.Semaphore,
    ) -> "FileUploadSpec2":
        # Current position is ignored - we always upload from position 0
        with source() as source_fp:
            source_fp.seek(0, os.SEEK_END)
            size = source_fp.tell()

        blocks = await _gather_blocks(source, size, hash_semaphore)

        return FileUploadSpec2(
            source=source,
            source_description=source_description,
            path=mount_filename.as_posix(),
            blocks=blocks,
            mode=mode & 0o7777,
            size=size,
        )


async def _gather_blocks(
    source: _FileUploadSource2,
    size: int,
    hash_semaphore: asyncio.Semaphore,
) -> list[FileUploadBlock]:
    def ceildiv(a: int, b: int) -> int:
        return -(a // -b)

    num_blocks = ceildiv(size, BLOCK_SIZE)

    async def gather_block(block_idx: int) -> FileUploadBlock:
        async with hash_semaphore:
            return await asyncio.to_thread(_gather_block, source, block_idx)

    tasks = (gather_block(idx) for idx in range(num_blocks))
    return await asyncio.gather(*tasks)


def _gather_block(source: _FileUploadSource2, block_idx: int) -> FileUploadBlock:
    start = block_idx * BLOCK_SIZE
    end = _find_end_of_block(source, start, start + BLOCK_SIZE)
    contents_sha256 = _hash_range_sha256(source, start, end)
    return FileUploadBlock(start=start, end=end, contents_sha256=contents_sha256)


def _hash_range_sha256(source: _FileUploadSource2, start, end):
    sha256_hash = hashlib.sha256()
    range_size = end - start

    with source() as fp:
        fp.seek(start)

        num_bytes_read = 0
        while num_bytes_read < range_size:
            chunk = fp.read(range_size - num_bytes_read)

            if not chunk:
                break

            num_bytes_read += len(chunk)
            sha256_hash.update(chunk)

    return sha256_hash.digest()


def _find_end_of_block(source: _FileUploadSource2, start: int, end: int) -> Optional[int]:
    """Finds the appropriate end of a block, which is the index of the byte just past the last non-zero byte in the
    block.

    >>> _find_end_of_block(lambda: BytesIO(b"abc123\0\0\0"), 0, 1024)
    6
    >>> _find_end_of_block(lambda: BytesIO(b"abc123\0\0\0"), 3, 1024)
    6
    >>> _find_end_of_block(lambda: BytesIO(b"abc123\0\0\0"), 0, 3)
    4
    >>> _find_end_of_block(lambda: BytesIO(b"abc123\0\0\0a"), 0, 9)
    6
    >>> _find_end_of_block(lambda: BytesIO(b"\0\0\0"), 0, 3)
    0
    >>> _find_end_of_block(lambda: BytesIO(b"\0\0\0\0\0\0"), 3, 6)
    3
    >>> _find_end_of_block(lambda: BytesIO(b""), 0, 1024)
    0
    """
    size = end - start
    new_end = start

    with source() as block_fp:
        block_fp.seek(start)

        num_bytes_read = 0
        while num_bytes_read < size:
            chunk = block_fp.read(size - num_bytes_read)

            if not chunk:
                break

            stripped_chunk = chunk.rstrip(b"\0")
            if stripped_chunk:
                new_end = start + num_bytes_read + len(stripped_chunk)

            num_bytes_read += len(chunk)

    return new_end


def use_md5(url: str) -> bool:
    """This takes an upload URL in S3 and returns whether we should attach a checksum.

    It's only a workaround for missing functionality in moto.
    https://github.com/spulec/moto/issues/816
    """
    host = urlparse(url).netloc.split(":")[0]
    if host.endswith(".amazonaws.com") or host.endswith(".r2.cloudflarestorage.com"):
        return True
    elif host in ["127.0.0.1", "localhost", "172.21.0.1"]:
        return False
    else:
        raise Exception(f"Unknown S3 host: {host}")



================================================
FILE: modal/_utils/bytes_io_segment_payload.py
================================================
# Copyright Modal Labs 2024

import asyncio
import hashlib
from contextlib import contextmanager
from typing import BinaryIO, Callable, Optional

# Note: this module needs to import aiohttp in global scope
# This takes about 50ms and isn't needed in many cases for Modal execution
# To avoid this, we import it in local scope when needed (blob_utils.py)
from aiohttp import Payload
from aiohttp.abc import AbstractStreamWriter

# read ~16MiB chunks by default
DEFAULT_SEGMENT_CHUNK_SIZE = 2**24


class BytesIOSegmentPayload(Payload):
    """Modified bytes payload for concurrent sends of chunks from the same file.

    Adds:
    * read limit using remaining_bytes, in order to split files across streams
    * larger read chunk (to prevent excessive read contention between parts)
    * calculates an md5 for the segment

    Feels like this should be in some standard lib...
    """

    _value: BinaryIO

    def __init__(
        self,
        bytes_io: BinaryIO,  # should *not* be shared as IO position modification is not locked
        segment_start: int,
        segment_length: int,
        chunk_size: int = DEFAULT_SEGMENT_CHUNK_SIZE,
        progress_report_cb: Optional[Callable] = None,
    ):
        # not thread safe constructor!
        super().__init__(bytes_io)
        self._size = segment_length
        self.initial_seek_pos = bytes_io.tell()
        self.segment_start = segment_start
        self.segment_length = segment_length
        # seek to start of file segment we are interested in, in order to make .size() evaluate correctly
        self._value.seek(self.initial_seek_pos + segment_start)
        assert self.segment_length <= super().size
        self.chunk_size = chunk_size
        self.progress_report_cb = progress_report_cb or (lambda *_, **__: None)
        self.reset_state()

    def decode(self, encoding: str = "utf-8", errors: str = "strict") -> str:
        self._value.seek(self.initial_seek_pos)
        return self._value.read().decode(encoding, errors)

    def reset_state(self):
        self._md5_checksum = hashlib.md5()
        self.num_bytes_read = 0
        self._value.seek(self.initial_seek_pos)

    @contextmanager
    def reset_on_error(self, subtract_progress: bool = False):
        try:
            yield
        except Exception as exc:
            try:
                if subtract_progress:
                    negative_progress = -self.num_bytes_read
                    self.progress_report_cb(advance=negative_progress)
                else:
                    self.progress_report_cb(reset=True)
            except Exception as cb_exc:
                raise cb_exc from exc
            raise exc
        finally:
            self.reset_state()

    @property
    def size(self) -> int:
        return self.segment_length

    def md5_checksum(self):
        return self._md5_checksum

    async def write(self, writer: "AbstractStreamWriter"):
        # On aiohttp < 3.12.0 - this is the method that's being called on a custom payload,
        # but on aiohttp 3.12+ `write_with_length` is called directly.
        await self.write_with_length(writer, None)

    async def write_with_length(self, writer: AbstractStreamWriter, content_length: Optional[int]):
        loop = asyncio.get_event_loop()

        async def safe_read():
            read_start = self.initial_seek_pos + self.segment_start + self.num_bytes_read
            self._value.seek(read_start)
            num_bytes = min(self.chunk_size, self.remaining_bytes())
            if content_length is not None:
                num_bytes = min(num_bytes, content_length)

            chunk = await loop.run_in_executor(None, self._value.read, num_bytes)
            await loop.run_in_executor(None, self._md5_checksum.update, chunk)
            self.num_bytes_read += len(chunk)
            return chunk

        chunk = await safe_read()
        while chunk and self.remaining_bytes() > 0:
            await writer.write(chunk)
            self.progress_report_cb(advance=len(chunk))
            chunk = await safe_read()
        if chunk:
            await writer.write(chunk)
            self.progress_report_cb(advance=len(chunk))

    def remaining_bytes(self):
        return self.segment_length - self.num_bytes_read



================================================
FILE: modal/_utils/deprecation.py
================================================
# Copyright Modal Labs 2024
import functools
import sys
import warnings
from datetime import date
from typing import Any, Callable, TypeVar

from typing_extensions import ParamSpec  # Needed for Python 3.9

from ..exception import DeprecationError, PendingDeprecationError

_INTERNAL_MODULES = ["modal", "synchronicity"]


def _is_internal_frame(frame):
    module = frame.f_globals["__name__"].split(".")[0]
    return module in _INTERNAL_MODULES


def deprecation_error(deprecated_on: tuple[int, int, int], msg: str):
    raise DeprecationError(f"Deprecated on {date(*deprecated_on)}: {msg}")


def deprecation_warning(
    deprecated_on: tuple[int, int, int], msg: str, *, pending: bool = False, show_source: bool = True
) -> None:
    """Issue a Modal deprecation warning with source optionally attributed to user code.

    See the implementation of the built-in [warnings.warn](https://docs.python.org/3/library/warnings.html#available-functions).
    """
    filename, lineno = "<unknown>", 0
    if show_source:
        # Find the last non-Modal line that triggered the warning
        try:
            frame = sys._getframe()
            while frame is not None and _is_internal_frame(frame):
                frame = frame.f_back
            if frame is not None:
                filename = frame.f_code.co_filename
                lineno = frame.f_lineno
        except ValueError:
            # Use the defaults from above
            pass

    warning_cls = PendingDeprecationError if pending else DeprecationError

    # This is a lower-level function that warnings.warn uses
    warnings.warn_explicit(f"{date(*deprecated_on)}: {msg}", warning_cls, filename, lineno)


P = ParamSpec("P")
R = TypeVar("R")


def renamed_parameter(
    date: tuple[int, int, int],
    old_name: str,
    new_name: str,
    show_source: bool = True,
) -> Callable[[Callable[P, R]], Callable[P, R]]:
    """Decorator for semi-gracefully changing a parameter name.

    Functions wrapped with this decorator can be defined using only the `new_name` of the parameter.
    If the function is invoked with the `old_name`, the wrapper will pass the value as a keyword
    argument for `new_name` and issue a Modal deprecation warning about the change.

    Note that this only prevents parameter renamings from breaking code at runtime.
    Type checking will fail when code uses `old_name`. To avoid this, the `old_name` can be
    preserved in the function signature with an `Annotated` type hint indicating the renaming.
    """

    def decorator(func: Callable[P, R]) -> Callable[P, R]:
        @functools.wraps(func)
        def wrapper(*args: P.args, **kwargs: P.kwargs) -> R:
            mut_kwargs: dict[str, Any] = locals()["kwargs"]  # Avoid referencing kwargs directly due to bug in sigtools
            if old_name in mut_kwargs:
                mut_kwargs[new_name] = mut_kwargs.pop(old_name)
                func_name = func.__qualname__.removeprefix("_")  # Avoid confusion when synchronicity-wrapped
                message = (
                    f"The '{old_name}' parameter of `{func_name}` has been renamed to '{new_name}'."
                    "\nUsing the old name will become an error in a future release. Please update your code."
                )
                deprecation_warning(date, message, show_source=show_source)

            return func(*args, **kwargs)

        return wrapper

    return decorator


def warn_on_renamed_autoscaler_settings(func: Callable[P, R]) -> Callable[P, R]:
    name_map = {
        "keep_warm": "min_containers",
        "concurrency_limit": "max_containers",
        "_experimental_buffer_containers": "buffer_containers",
        "container_idle_timeout": "scaledown_window",
    }

    @functools.wraps(func)
    def wrapper(*args: P.args, **kwargs: P.kwargs) -> R:
        mut_kwargs: dict[str, Any] = locals()["kwargs"]  # Avoid referencing kwargs directly due to bug in sigtools

        substitutions = []
        old_params_used = name_map.keys() & mut_kwargs.keys()
        for old_param, new_param in name_map.items():
            if old_param in old_params_used:
                new_param = name_map[old_param]
                mut_kwargs[new_param] = mut_kwargs.pop(old_param)
                substitutions.append(f"- {old_param} -> {new_param}")

        if substitutions:
            substitution_string = "\n".join(substitutions)
            message = (
                "We have renamed several parameters related to autoscaling."
                " Please update your code to use the following new names:"
                f"\n\n{substitution_string}"
                "\n\nSee https://modal.com/docs/guide/modal-1-0-migration for more details."
            )
            deprecation_warning((2025, 2, 24), message, show_source=True)

        return func(*args, **kwargs)

    return wrapper


def warn_if_passing_namespace(
    namespace: Any,
    resource_name: str,
) -> None:
    """Issue deprecation warning for namespace parameter if non-None value is passed.

    Args:
        namespace: The namespace parameter value (may be None or actual value)
        resource_name: Name of the resource type for the warning message
    """
    if namespace is not None:
        deprecation_warning(
            (2025, 6, 30),
            f"The `namespace` parameter for `{resource_name}` is deprecated and will be"
            " removed in a future release. It is no longer needed, so can be removed"
            " from your code.",
        )



================================================
FILE: modal/_utils/docker_utils.py
================================================
# Copyright Modal Labs 2024
import re
import shlex
from pathlib import Path
from typing import Optional, Sequence

from ..exception import InvalidError


def extract_copy_command_patterns(dockerfile_lines: Sequence[str]) -> list[str]:
    """
    Extract all COPY command sources from a Dockerfile.
    Combines multiline COPY commands into a single line.
    """
    copy_source_patterns: set[str] = set()
    current_command = ""
    copy_pattern = re.compile(r"^\s*COPY\s+(.+)$", re.IGNORECASE)

    # First pass: handle line continuations and collect full commands
    for line in dockerfile_lines:
        line = line.strip()
        if not line or line.startswith("#"):
            # ignore comments and empty lines
            continue

        if current_command:
            # Continue previous line
            current_command += " " + line.rstrip("\\").strip()
        else:
            # Start new command
            current_command = line.rstrip("\\").strip()

        if not line.endswith("\\"):
            # Command is complete

            match = copy_pattern.match(current_command)
            if match:
                args = match.group(1)
                parts = shlex.split(args)

                # COPY --from=... commands reference external sources and do not need a context mount.
                # https://docs.docker.com/reference/dockerfile/#copy---from
                if parts[0].startswith("--from="):
                    current_command = ""
                    continue

                if len(parts) >= 2:
                    # Last part is destination, everything else is a mount source
                    sources = parts[:-1]

                    for source in sources:
                        special_pattern = re.compile(r"^\s*--|\$\s*")
                        if special_pattern.match(source):
                            raise InvalidError(
                                f"COPY command: {source} using special flags/arguments/variables are not supported"
                            )

                        if source == ".":
                            copy_source_patterns.add("./**")
                        else:
                            copy_source_patterns.add(source)

            current_command = ""

    return list(copy_source_patterns)


def find_dockerignore_file(context_directory: Path, dockerfile_path: Optional[Path] = None) -> Optional[Path]:
    """
    Find dockerignore file relative to current context directory
    and if dockerfile path is provided, check if specific <dockerfile_name>.dockerignore
    file exists in the same directory as <dockerfile_name>
    Finds the most specific dockerignore file that exists.
    """

    def valid_dockerignore_file(fp):
        # fp has to exist
        if not fp.exists():
            return False
        # fp has to be subpath to current working directory
        if not fp.is_relative_to(context_directory):
            return False

        return True

    generic_name = ".dockerignore"
    possible_locations = []
    if dockerfile_path:
        specific_name = f"{dockerfile_path.name}.dockerignore"
        # 1. check if specific <dockerfile_name>.dockerignore file exists in the same directory as <dockerfile_name>
        possible_locations.append(dockerfile_path.parent / specific_name)
        # 2. check if generic .dockerignore file exists in the same directory as <dockerfile_name>
        possible_locations.append(dockerfile_path.parent / generic_name)

    # 3. check if generic .dockerignore file exists in current working directory
    possible_locations.append(context_directory / generic_name)

    return next((e for e in possible_locations if valid_dockerignore_file(e)), None)



================================================
FILE: modal/_utils/function_utils.py
================================================
# Copyright Modal Labs 2022
import asyncio
import inspect
import os
import typing
from collections.abc import AsyncGenerator
from enum import Enum
from pathlib import Path, PurePosixPath
from typing import Any, Callable, Literal, Optional

from grpclib import GRPCError
from grpclib.exceptions import StreamTerminatedError

import modal_proto
from modal_proto import api_pb2
from modal_proto.modal_api_grpc import ModalClientModal

from .._serialization import (
    deserialize,
    deserialize_data_format,
    get_preferred_payload_format,
    serialize,
    serialize_data_format as _serialize_data_format,
    signature_to_parameter_specs,
)
from .._traceback import append_modal_tb
from ..config import logger
from ..exception import (
    DeserializationError,
    ExecutionError,
    FunctionTimeoutError,
    InternalFailure,
    InvalidError,
    RemoteError,
)
from ..mount import ROOT_DIR, _is_modal_path, _Mount
from .blob_utils import (
    MAX_ASYNC_OBJECT_SIZE_BYTES,
    blob_download,
    blob_upload_with_r2_failure_info,
)
from .grpc_utils import RETRYABLE_GRPC_STATUS_CODES

if typing.TYPE_CHECKING:
    import modal._functions


class FunctionInfoType(Enum):
    PACKAGE = "package"
    FILE = "file"
    SERIALIZED = "serialized"
    NOTEBOOK = "notebook"


class LocalFunctionError(InvalidError):
    """Raised if a function declared in a non-global scope is used in an impermissible way"""


def entrypoint_only_package_mount_condition(entrypoint_file):
    entrypoint_path = Path(entrypoint_file)

    def inner(filename):
        path = Path(filename)
        if path == entrypoint_path:
            return True
        if path.name == "__init__.py" and path.parent in entrypoint_path.parents:
            # ancestor __init__.py are included
            return True
        return False

    return inner


def is_global_object(object_qual_name: str):
    return "<locals>" not in object_qual_name.split(".")


def is_method_fn(object_qual_name: str):
    # methods have names like Cls.foo.
    if "<locals>" in object_qual_name:
        # functions can be nested in multiple local scopes.
        rest = object_qual_name.split("<locals>.")[-1]
        return len(rest.split(".")) > 1
    return len(object_qual_name.split(".")) > 1


def is_top_level_function(f: Callable) -> bool:
    """Returns True if this function is defined in global scope.

    Returns False if this function is locally scoped (including on a class).
    """
    return f.__name__ == f.__qualname__


def is_async(function):
    # TODO: this is somewhat hacky. We need to know whether the function is async or not in order to
    # coerce the input arguments to the right type. The proper way to do is to call the function and
    # see if you get a coroutine (or async generator) back. However at this point, it's too late to
    # coerce the type. For now let's make a determination based on inspecting the function definition.
    # This sometimes isn't correct, since a "vanilla" Python function can return a coroutine if it
    # wraps async code or similar. Let's revisit this shortly.
    if inspect.ismethod(function):
        function = function.__func__  # inspect the underlying function
    if inspect.iscoroutinefunction(function) or inspect.isasyncgenfunction(function):
        return True
    elif inspect.isfunction(function) or inspect.isgeneratorfunction(function):
        return False
    else:
        raise RuntimeError(f"Function {function} is a strange type {type(function)}")


def get_function_type(is_generator: Optional[bool]) -> "api_pb2.Function.FunctionType.ValueType":
    return api_pb2.Function.FUNCTION_TYPE_GENERATOR if is_generator else api_pb2.Function.FUNCTION_TYPE_FUNCTION


class FunctionInfo:
    """Utility that determines serialization/deserialization mechanisms for functions

    * Stored as file vs serialized
    * If serialized: how to serialize the function
    * If file: which module/function name should be used to retrieve

    Used for populating the definition of a remote function
    """

    raw_f: Optional[Callable[..., Any]]  # if None - this is a "class service function"
    function_name: str
    user_cls: Optional[type[Any]]
    module_name: Optional[str]

    _type: FunctionInfoType
    _file: Optional[str]
    _base_dir: str
    _remote_dir: Optional[PurePosixPath] = None

    def get_definition_type(self) -> "modal_proto.api_pb2.Function.DefinitionType.ValueType":
        if self.is_serialized():
            return modal_proto.api_pb2.Function.DEFINITION_TYPE_SERIALIZED
        else:
            return modal_proto.api_pb2.Function.DEFINITION_TYPE_FILE

    def is_service_class(self):
        if self.raw_f is None:
            assert self.user_cls
            return True
        return False

    # TODO: we should have a bunch of unit tests for this
    def __init__(
        self,
        f: Optional[Callable[..., Any]],
        serialized: bool = False,
        name_override: Optional[str] = None,
        user_cls: Optional[type] = None,
    ):
        self.raw_f = f
        self.user_cls = user_cls

        if name_override is not None:
            if not serialized:
                # We may relax this constraint in the future, but currently we don't track the distinction between
                # the Function's name inside modal and the name of the object that we need to import in a container.
                raise InvalidError("Setting a custom `name=` also requires setting `serialized=True`")
            self.function_name = name_override
        elif f is None and user_cls:
            # "service function" for running all methods of a class
            self.function_name = f"{user_cls.__name__}.*"
        elif f and user_cls:
            # Method may be defined on superclass of the wrapped class
            self.function_name = f"{user_cls.__name__}.{f.__name__}"
        else:
            self.function_name = f.__qualname__

        # If it's a cls, the @method could be defined in a base class in a different file.
        if user_cls is not None:
            module = inspect.getmodule(user_cls)
        else:
            module = inspect.getmodule(f)

        if getattr(module, "__package__", None) and not serialized:
            # This is a "real" module, eg. examples.logs.f
            # Get the package path
            # Note: __import__ always returns the top-level package.
            self._file = os.path.abspath(module.__file__)
            package_paths = {os.path.abspath(p) for p in __import__(module.__package__).__path__}
            # There might be multiple package paths in some weird cases
            base_dirs = [
                base_dir for base_dir in package_paths if os.path.commonpath((base_dir, self._file)) == base_dir
            ]

            if not base_dirs:
                logger.info(f"Module files: {self._file}")
                logger.info(f"Package paths: {package_paths}")
                logger.info(f"Base dirs: {base_dirs}")
                raise Exception("Wasn't able to find the package directory!")
            elif len(base_dirs) > 1:
                # Base_dirs should all be prefixes of each other since they all contain `module_file`.
                base_dirs.sort(key=len)
            self._base_dir = base_dirs[0]
            self.module_name = module.__spec__.name
            self._remote_dir = ROOT_DIR / PurePosixPath(module.__package__.split(".")[0])
            self._is_serialized = False
            self._type = FunctionInfoType.PACKAGE
        elif hasattr(module, "__file__") and not serialized:
            # This generally covers the case where it's invoked with
            # python foo/bar/baz.py

            # If it's a cls, the @method could be defined in a base class in a different file.
            self._file = os.path.abspath(inspect.getfile(module))
            self.module_name = inspect.getmodulename(self._file)
            self._base_dir = os.path.dirname(self._file)
            self._is_serialized = False
            self._type = FunctionInfoType.FILE
        else:
            self.module_name = None
            self._base_dir = os.path.abspath("")  # get current dir
            self._is_serialized = True  # either explicitly, or by being in a notebook
            if serialized:  # if explicit
                self._type = FunctionInfoType.SERIALIZED
            else:
                # notebook, or in general any exec() on a function definition
                self._type = FunctionInfoType.NOTEBOOK

        if not self.is_serialized():
            # Sanity check that this function is defined in global scope
            # Unfortunately, there's no "clean" way to do this in Python
            qualname = f.__qualname__ if f else user_cls.__qualname__
            if not is_global_object(qualname):
                raise LocalFunctionError(
                    "Modal can only import functions defined in global scope unless they are `serialized=True`"
                )

    def is_serialized(self) -> bool:
        return self._is_serialized

    def serialized_function(self) -> bytes:
        # Note: this should only be called from .load() and not at function decoration time
        #       otherwise the serialized function won't have access to variables/side effect
        #        defined after it in the same file
        assert self.is_serialized()
        if self.raw_f:
            serialized_bytes = serialize(self.raw_f)
            logger.debug(f"Serializing {self.raw_f.__qualname__}, size is {len(serialized_bytes)}")
            return serialized_bytes
        else:
            logger.debug(f"Serializing function for class service function {self.user_cls.__qualname__} as empty")
            return b""

    def get_cls_vars(self) -> dict[str, Any]:
        if self.user_cls is not None:
            cls_vars = {
                attr: getattr(self.user_cls, attr)
                for attr in dir(self.user_cls)
                if not callable(getattr(self.user_cls, attr)) and not attr.startswith("__")
            }
            return cls_vars
        return {}

    def get_cls_var_attrs(self) -> dict[str, Any]:
        import dis
        import opcode

        LOAD_ATTR = opcode.opmap["LOAD_ATTR"]
        STORE_ATTR = opcode.opmap["STORE_ATTR"]

        func = self.raw_f
        code = func.__code__
        f_attr_ops = set()
        for instr in dis.get_instructions(code):
            if instr.opcode == LOAD_ATTR:
                f_attr_ops.add(instr.argval)
            elif instr.opcode == STORE_ATTR:
                f_attr_ops.add(instr.argval)

        cls_vars = self.get_cls_vars()
        f_attrs = {k: cls_vars[k] for k in cls_vars if k in f_attr_ops}
        return f_attrs

    def get_globals(self) -> dict[str, Any]:
        from .._vendor.cloudpickle import _extract_code_globals

        if self.raw_f is None:
            return {}

        func = self.raw_f
        while hasattr(func, "__wrapped__") and func is not func.__wrapped__:
            # Unwrap functions decorated using functools.wrapped (potentially multiple times)
            func = func.__wrapped__
        f_globals_ref = _extract_code_globals(func.__code__)
        f_globals = {k: func.__globals__[k] for k in f_globals_ref if k in func.__globals__}
        return f_globals

    def class_parameter_info(self) -> api_pb2.ClassParameterInfo:
        if not self.user_cls:
            return api_pb2.ClassParameterInfo()

        # TODO(elias): Resolve circular dependencies... maybe we'll need some cls_utils module
        from modal.cls import _get_class_constructor_signature, _use_annotation_parameters

        if not _use_annotation_parameters(self.user_cls):
            return api_pb2.ClassParameterInfo(format=api_pb2.ClassParameterInfo.PARAM_SERIALIZATION_FORMAT_PICKLE)

        # annotation parameters trigger strictly typed parametrization
        # which enables web endpoint for parametrized classes
        signature = _get_class_constructor_signature(self.user_cls)
        # at this point, the types in the signature should already have been validated (see Cls.from_local())
        parameter_specs = signature_to_parameter_specs(signature)

        return api_pb2.ClassParameterInfo(
            format=api_pb2.ClassParameterInfo.PARAM_SERIALIZATION_FORMAT_PROTO,
            schema=parameter_specs,
        )

    def get_entrypoint_mount(self) -> dict[str, _Mount]:
        """
        Includes:
        * Implicit mount of the function itself (the module or package that the function is part of)

        Does not include:
        * Client mount
        * Explicit mounts added to the stub or function declaration
        * "Auto mounted" mounts, i.e. all mounts in sys.modules that are *not* installed in site-packages.
            These are typically local modules which are imported but not part of the running package

        """
        if self.is_serialized():
            # Don't auto-mount anything for serialized functions (including notebooks)
            return {}

        # make sure the function's own entrypoint is included:
        if self._type == FunctionInfoType.PACKAGE:
            top_level_package = self.module_name.split(".")[0]
            # TODO: add deprecation warning if the following entrypoint mount
            #  includes non-.py files, since we'll want to migrate to .py-only
            #  soon to get it consistent with the `add_local_python_source()`
            #  defaults.
            return {top_level_package: _Mount._from_local_python_packages(top_level_package)}
        elif self._type == FunctionInfoType.FILE:
            # TODO: inspect if this file is already included as part of
            #  a package mount, and skip it + reference that package
            #  instead if that's the case. This avoids possible module
            #  duplication bugs
            module_file = Path(self._file)
            container_module_name = module_file.stem
            remote_path = ROOT_DIR / module_file.name
            if not _is_modal_path(remote_path):
                return {
                    container_module_name: _Mount._from_local_file(
                        self._file,
                        remote_path=remote_path,
                    )
                }
        return {}  # this should never be reached...

    def get_tag(self):
        return self.function_name

    def is_nullary(self):
        signature = inspect.signature(self.raw_f)
        for param in signature.parameters.values():
            if param.kind in (inspect.Parameter.VAR_POSITIONAL, inspect.Parameter.VAR_KEYWORD):
                # variadic parameters are nullary
                continue
            if param.default is param.empty:
                return False
        return True


def callable_has_non_self_params(f: Callable[..., Any]) -> bool:
    """Return True if a callable (function, bound method, or unbound method) has parameters other than self.

    Used to ensure that @exit(), @asgi_app, and @wsgi_app functions don't have parameters.
    """
    return any(param.name != "self" for param in inspect.signature(f).parameters.values())


def callable_has_non_self_non_default_params(f: Callable[..., Any]) -> bool:
    """Return True if a callable (function, bound method, or unbound method) has non-default parameters other than self.

    Used for deprecation of default parameters in @asgi_app and @wsgi_app functions.
    """
    for param in inspect.signature(f).parameters.values():
        if param.name == "self":
            continue

        if param.default != inspect.Parameter.empty:
            continue

        return True
    return False


async def _stream_function_call_data(
    client,
    stub,
    function_call_id: Optional[str],
    variant: Literal["data_in", "data_out"],
    attempt_token: Optional[str] = None,
) -> AsyncGenerator[Any, None]:
    """Read from the `data_in` or `data_out` stream of a function call."""
    if not function_call_id and not attempt_token:
        raise ValueError("function_call_id or attempt_token is required to read from a data stream")

    if stub is None:
        stub = client.stub

    last_index = 0

    # TODO(gongy): generalize this logic as util for unary streams
    retries_remaining = 10
    delay_ms = 1

    if variant == "data_in":
        stub_fn = stub.FunctionCallGetDataIn
    elif variant == "data_out":
        stub_fn = stub.FunctionCallGetDataOut
    else:
        raise ValueError(f"Invalid variant {variant}")

    while True:
        req = api_pb2.FunctionCallGetDataRequest(
            function_call_id=function_call_id,
            last_index=last_index,
        )
        if attempt_token:
            req.attempt_token = attempt_token  # oneof clears function_call_id.
        try:
            async for chunk in stub_fn.unary_stream(req):
                if chunk.index <= last_index:
                    continue
                if chunk.data_blob_id:
                    message_bytes = await blob_download(chunk.data_blob_id, client.stub)
                else:
                    message_bytes = chunk.data
                message = deserialize_data_format(message_bytes, chunk.data_format, client)

                last_index = chunk.index
                yield message
        except (GRPCError, StreamTerminatedError) as exc:
            if retries_remaining > 0:
                retries_remaining -= 1
                if isinstance(exc, GRPCError):
                    if exc.status in RETRYABLE_GRPC_STATUS_CODES:
                        logger.debug(f"{variant} stream retrying with delay {delay_ms}ms due to {exc}")
                        await asyncio.sleep(delay_ms / 1000)
                        delay_ms = min(1000, delay_ms * 10)
                        continue
                elif isinstance(exc, StreamTerminatedError):
                    continue
            raise
        else:
            delay_ms = 1


OUTPUTS_TIMEOUT = 55.0  # seconds
ATTEMPT_TIMEOUT_GRACE_PERIOD = 5  # seconds


def exc_with_hints(exc: BaseException):
    """mdmd:hidden"""
    if isinstance(exc, ImportError) and exc.msg == "attempted relative import with no known parent package":
        exc.msg += """\n
HINT: For relative imports to work, you might need to run your modal app as a module. Try:
- `python -m my_pkg.my_app` instead of `python my_pkg/my_app.py`
- `modal deploy my_pkg.my_app` instead of `modal deploy my_pkg/my_app.py`
"""
    elif isinstance(
        exc, RuntimeError
    ) and "CUDA error: no kernel image is available for execution on the device" in str(exc):
        msg = (
            exc.args[0]
            + """\n
HINT: This error usually indicates an outdated CUDA version. Older versions of torch (<=1.12)
come with CUDA 10.2 by default. If pinning to an older torch version, you can specify a CUDA version
manually, for example:
-  image.pip_install("torch==1.12.1+cu116", find_links="https://download.pytorch.org/whl/torch_stable.html")
"""
        )
        exc.args = (msg,)

    return exc


async def _process_result(result: api_pb2.GenericResult, data_format: int, stub, client=None):
    if result.WhichOneof("data_oneof") == "data_blob_id":
        data = await blob_download(result.data_blob_id, stub)
    else:
        data = result.data

    if result.status == api_pb2.GenericResult.GENERIC_STATUS_TIMEOUT:
        raise FunctionTimeoutError(result.exception)
    elif result.status == api_pb2.GenericResult.GENERIC_STATUS_INTERNAL_FAILURE:
        raise InternalFailure(result.exception)
    elif result.status != api_pb2.GenericResult.GENERIC_STATUS_SUCCESS:
        if data and data_format in (api_pb2.DATA_FORMAT_PICKLE, api_pb2.DATA_FORMAT_UNSPECIFIED):
            # *Unspecified data format here but data present usually means that the exception
            # was created by the server representing an exception that occurred during container
            # startup (crash looping) that eventually got escalated to input failures.
            # TaskResult doesn't specify data format, so these results don't have that metadata
            # the moment.
            try:
                exc = deserialize(data, client)
            except DeserializationError as deser_exc:
                raise ExecutionError(
                    "Could not deserialize remote exception due to local error:\n"
                    + f"{deser_exc}\n"
                    + "This can happen if your local environment does not have the remote exception definitions.\n"
                    + "Here is the remote traceback:\n"
                    + f"{result.traceback}"
                ) from deser_exc.__cause__
            except Exception as deser_exc:
                raise ExecutionError(
                    "Could not deserialize remote exception due to local error:\n"
                    + f"{deser_exc}\n"
                    + "Here is the remote traceback:\n"
                    + f"{result.traceback}"
                ) from deser_exc
            if not isinstance(exc, BaseException):
                raise ExecutionError(f"Got remote exception of incorrect type {type(exc)}")

            if result.serialized_tb:
                try:
                    tb_dict = deserialize(result.serialized_tb, client)
                    line_cache = deserialize(result.tb_line_cache, client)
                    append_modal_tb(exc, tb_dict, line_cache)
                except Exception:
                    pass

            raise exc_with_hints(exc)

        raise RemoteError(result.exception)

    try:
        return deserialize_data_format(data, data_format, client)
    except ModuleNotFoundError as deser_exc:
        raise ExecutionError(
            "Could not deserialize result due to error:\n"
            f"{deser_exc}\n"
            "This can happen if your local environment does not have a module that was used to construct the result. \n"
        ) from deser_exc


def should_upload(
    num_bytes: int,
    max_object_size_bytes: int,
    function_call_invocation_type: Optional["api_pb2.FunctionCallInvocationType.ValueType"],
) -> bool:
    """
    Determine if the input should be uploaded to blob storage.
    """
    return num_bytes > max_object_size_bytes or (
        function_call_invocation_type == api_pb2.FUNCTION_CALL_INVOCATION_TYPE_ASYNC
        and num_bytes > MAX_ASYNC_OBJECT_SIZE_BYTES
    )


# This must be called against the client stub, not the input-plane stub.
async def _create_input(
    args,
    kwargs,
    stub: ModalClientModal,
    *,
    function: "modal._functions._Function",
    idx: Optional[int] = None,
    function_call_invocation_type: Optional["api_pb2.FunctionCallInvocationType.ValueType"] = None,
) -> api_pb2.FunctionPutInputsItem:
    """Serialize function arguments and create a FunctionInput protobuf,
    uploading to blob storage if needed.
    """
    method_name = function._use_method_name
    max_object_size_bytes = function._max_object_size_bytes

    if idx is None:
        idx = 0

    data_format = get_preferred_payload_format()
    if not function._metadata:
        raise ExecutionError("Attempted to call function that has not been hydrated with metadata")

    supported_input_formats = function._metadata.supported_input_formats or [api_pb2.DATA_FORMAT_PICKLE]
    if data_format not in supported_input_formats:
        data_format = supported_input_formats[0]

    args_serialized = _serialize_data_format((args, kwargs), data_format)

    if should_upload(len(args_serialized), max_object_size_bytes, function_call_invocation_type):
        args_blob_id, r2_failed, r2_throughput_bytes_s = await blob_upload_with_r2_failure_info(args_serialized, stub)
        return api_pb2.FunctionPutInputsItem(
            input=api_pb2.FunctionInput(
                args_blob_id=args_blob_id,
                data_format=data_format,
                method_name=method_name,
            ),
            idx=idx,
            r2_failed=r2_failed,
            r2_throughput_bytes_s=r2_throughput_bytes_s,
        )
    else:
        return api_pb2.FunctionPutInputsItem(
            input=api_pb2.FunctionInput(
                args=args_serialized,
                data_format=data_format,
                method_name=method_name,
            ),
            idx=idx,
        )


def _get_suffix_from_web_url_info(url_info: api_pb2.WebUrlInfo) -> str:
    if url_info.truncated:
        suffix = " [grey70](label truncated)[/grey70]"
    elif url_info.label_stolen:
        suffix = " [grey70](label stolen)[/grey70]"
    else:
        suffix = ""
    return suffix


class FunctionCreationStatus:
    # TODO(michael) this really belongs with other output-related code
    # but moving it here so we can use it when loading a function with output disabled
    tag: str
    response: Optional[api_pb2.FunctionCreateResponse] = None

    def __init__(self, resolver, tag):
        self.resolver = resolver
        self.tag = tag

    def __enter__(self):
        self.status_row = self.resolver.add_status_row()
        self.status_row.message(f"Creating function {self.tag}...")
        return self

    def set_response(self, resp: api_pb2.FunctionCreateResponse):
        self.response = resp

    def __exit__(self, exc_type, exc_val, exc_tb):
        if exc_type:
            raise exc_val

        if not self.response:
            self.status_row.finish(f"Unknown error when creating function {self.tag}")

        elif self.response.function.web_url:
            url_info = self.response.function.web_url_info
            requires_proxy_auth = self.response.function.webhook_config.requires_proxy_auth
            proxy_auth_suffix = " ðŸ”‘" if requires_proxy_auth else ""
            # Ensure terms used here match terms used in modal.com/docs/guide/webhook-urls doc.
            suffix = _get_suffix_from_web_url_info(url_info)
            # TODO: this is only printed when we're showing progress. Maybe move this somewhere else.
            web_url = self.response.handle_metadata.web_url
            for warning in self.response.server_warnings:
                self.status_row.warning(warning)
            self.status_row.finish(
                f"Created web function {self.tag} => [magenta underline]{web_url}[/magenta underline]"
                f"{proxy_auth_suffix}{suffix}"
            )

            # Print custom domain in terminal
            for custom_domain in self.response.function.custom_domain_info:
                custom_domain_status_row = self.resolver.add_status_row()
                custom_domain_status_row.finish(
                    f"Custom domain for {self.tag} => [magenta underline]{custom_domain.url}[/magenta underline]"
                )

        elif self.response.function.flash_service_urls:
            for flash_service_url in self.response.function.flash_service_urls:
                flash_service_url_status_row = self.resolver.add_status_row()
                flash_service_url_status_row.finish(
                    f"Created flash service endpoint for {self.tag} => "
                    f"[magenta underline]{flash_service_url}[/magenta underline]"
                )

        else:
            for warning in self.response.server_warnings:
                self.status_row.warning(warning)
            self.status_row.finish(f"Created function {self.tag}.")
            if self.response.function.method_definitions_set:
                for method_definition in self.response.function.method_definitions.values():
                    if method_definition.web_url:
                        url_info = method_definition.web_url_info
                        suffix = _get_suffix_from_web_url_info(url_info)
                        class_web_endpoint_method_status_row = self.resolver.add_status_row()
                        class_web_endpoint_method_status_row.finish(
                            f"Created web endpoint for {method_definition.function_name} => [magenta underline]"
                            f"{method_definition.web_url}[/magenta underline]{suffix}"
                        )
                        for custom_domain in method_definition.custom_domain_info:
                            custom_domain_status_row = self.resolver.add_status_row()
                            custom_domain_status_row.finish(
                                f"Custom domain for {method_definition.function_name} => [magenta underline]"
                                f"{custom_domain.url}[/magenta underline]"
                            )



================================================
FILE: modal/_utils/git_utils.py
================================================
# Copyright Modal Labs 2025
import asyncio
from typing import Optional

from modal.config import logger
from modal_proto import api_pb2


async def run_command_fallible(args: list[str]) -> Optional[str]:
    try:
        process = await asyncio.create_subprocess_exec(
            *args, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE
        )
        stdout_bytes, _ = await process.communicate()

        if process.returncode != 0:
            logger.debug(f"Command {args} exited with code {process.returncode}")
            return None

        return stdout_bytes.decode("utf-8").strip()

    except Exception as e:
        logger.debug(f"Command {args} failed", exc_info=e)
        return None


def is_valid_commit_info(commit_info: api_pb2.CommitInfo) -> tuple[bool, str]:
    # returns (valid, error_message)
    if commit_info.vcs != "git":
        return False, "Invalid VCS"
    if len(commit_info.commit_hash) != 40:
        return False, "Invalid commit hash"
    if len(commit_info.branch) > 255:
        # Git doesn't enforce a max length for branch names, but github does, so use their limit
        # https://stackoverflow.com/questions/24014361/max-length-of-git-branch-name
        return False, "Branch name too long"
    if len(commit_info.repo_url) > 200:
        return False, "Repo URL too long"
    if len(commit_info.author_name) > 200:
        return False, "Author name too long"
    if len(commit_info.author_email) > 200:
        return False, "Author email too long"
    return True, ""


async def get_git_commit_info() -> Optional[api_pb2.CommitInfo]:
    """Collect git information about the current repository asynchronously."""
    git_info: api_pb2.CommitInfo = api_pb2.CommitInfo(vcs="git")

    commands = [
        # Get commit hash, timestamp, author name, and author email
        ["git", "log", "-1", "--format=%H%n%ct%n%an%n%ae", "HEAD"],
        # Get branch name
        ["git", "rev-parse", "--abbrev-ref", "HEAD"],
        # Check if working directory is dirty
        ["git", "status", "--porcelain"],
        ["git", "remote", "get-url", "origin"],
    ]

    tasks = (run_command_fallible(cmd) for cmd in commands)
    (log_info, branch, status, origin_url) = await asyncio.gather(*tasks)

    if not branch:
        return None

    git_info.branch = branch

    if not log_info:
        return None

    info_lines = log_info.split("\n")
    if len(info_lines) < 4:
        # If we didn't get all expected lines, bail
        logger.debug(f"Log info returned only {len(info_lines)} lines")
        return None

    try:
        git_info.commit_hash = info_lines[0]
        git_info.commit_timestamp = int(info_lines[1])
        git_info.author_name = info_lines[2]
        git_info.author_email = info_lines[3]
    except (ValueError, IndexError):
        logger.debug(f"Failed to parse git log info: {log_info}")
        return None

    git_info.dirty = bool(status)

    if origin_url:
        git_info.repo_url = origin_url

    valid, error_message = is_valid_commit_info(git_info)
    if not valid:
        logger.warning(f"Invalid commit info: {error_message}")
        return None

    return git_info



================================================
FILE: modal/_utils/grpc_testing.py
================================================
# Copyright Modal Labs 2023
import contextlib
import inspect
import logging
import typing
from collections import Counter, defaultdict
from collections.abc import Awaitable
from typing import Any, Callable

import grpclib.server
from grpclib import GRPCError, Status

from modal.config import logger

if typing.TYPE_CHECKING:
    from test.conftest import MockClientServicer


def patch_mock_servicer(cls):
    """Adds an `.intercept()` context manager method

    This allows for context-local tracking and assertions of all calls
    performed on the servicer during a context, e.g.:

    ```python notest
    with servicer.intercept() as ctx:
        await some_complex_method()
    assert ctx.calls == [("SomeMethod", MyMessage(foo="bar"))]
    ```
    Also allows to set a predefined queue of responses, temporarily replacing
    a mock servicer's default responses for a method:

    ```python notest
    with servicer.intercept() as ctx:
        ctx.add_response("SomeMethod", [
            MyResponse(bar="baz")
        ])
        ctx.add_response("SomeMethod", [
            MyResponse(bar="baz2")
        ])
        await service_stub.SomeMethod(Empty())  # receives MyResponse(bar="baz")
        await service_stub.SomeMethod(Empty())  # receives MyResponse(bar="baz2")
    ```

    Also patches all unimplemented abstract methods in a mock servicer with default error implementations.
    """

    async def fallback(self, stream) -> None:
        raise GRPCError(Status.UNIMPLEMENTED, "Not implemented in mock servicer " + repr(cls))

    @contextlib.contextmanager
    def intercept(servicer):
        ctx = InterceptionContext(servicer)
        servicer.interception_context = ctx
        yield ctx
        ctx._assert_responses_consumed()
        servicer.interception_context = None

    cls.intercept = intercept
    cls.interception_context = None

    def patch_grpc_method(method_name, original_method):
        async def patched_method(servicer_self, stream):
            try:
                ctx = servicer_self.interception_context
                if ctx:
                    intercepted_stream = await InterceptedStream(ctx, method_name, stream).initialize()
                    custom_responder = ctx._next_custom_responder(method_name, intercepted_stream.request_message)
                    if custom_responder:
                        return await custom_responder(servicer_self, intercepted_stream)
                    else:
                        # use default servicer, but intercept messages for assertions
                        return await original_method(servicer_self, intercepted_stream)
                else:
                    return await original_method(servicer_self, stream)
            except GRPCError:
                raise
            except Exception:
                logger.exception("Error in mock servicer responder:")
                raise

        return patched_method

    # Fill in the remaining methods on the class
    for name in dir(cls):
        method = getattr(cls, name)
        if getattr(method, "__isabstractmethod__", False):
            setattr(cls, name, patch_grpc_method(name, fallback))
        elif name[0].isupper() and inspect.isfunction(method):
            setattr(cls, name, patch_grpc_method(name, method))

    cls.__abstractmethods__ = frozenset()
    return cls


class ResponseNotConsumed(Exception):
    def __init__(self, unconsumed_requests: list[str]):
        self.unconsumed_requests = unconsumed_requests
        request_count = Counter(unconsumed_requests)
        super().__init__(f"Expected but did not receive the following requests: {request_count}")


class InterceptionContext:
    def __init__(self, servicer):
        self._servicer = servicer
        self.calls: list[tuple[str, Any]] = []  # List[Tuple[method_name, message]]
        self.custom_responses: dict[str, list[tuple[Callable[[Any], bool], list[Any]]]] = defaultdict(list)
        self.custom_defaults: dict[str, Callable[["MockClientServicer", grpclib.server.Stream], Awaitable[None]]] = {}

    def add_response(
        self, method_name: str, first_payload, *, request_filter: Callable[[Any], bool] = lambda req: True
    ):
        """Adds one response payload to an expected queue of responses for a method.

        These responses will be used once each instead of calling the MockServicer's
        implementation of the method.

        The interception context will throw an exception on exit if not all of the added
        responses have been consumed.
        """
        self.custom_responses[method_name].append((request_filter, [first_payload]))

    def set_responder(
        self, method_name: str, responder: Callable[["MockClientServicer", grpclib.server.Stream], Awaitable[None]]
    ):
        """Replace the default responder from the MockClientServicer with a custom implementation

        ```python notest
        def custom_responder(servicer, stream):
            request = stream.recv_message()
            await stream.send_message(api_pb2.SomeMethodResponse(foo=123))

        with servicer.intercept() as ctx:
            ctx.set_responder("SomeMethod", custom_responder)
        ```

        Responses added via `.add_response()` take precedence over the use of this replacement
        """
        self.custom_defaults[method_name] = responder

    def pop_request(self, method_name):
        # fast forward to the next request of type method_name
        # dropping any preceding requests if there is a match
        # returns the payload of the request
        for i, (_method_name, msg) in enumerate(self.calls):
            if _method_name == method_name:
                self.calls = self.calls[i + 1 :]
                return msg

        raise KeyError(f"No message of that type in call list: {self.calls}")

    def get_requests(self, method_name: str) -> list[Any]:
        if not hasattr(self._servicer, method_name):
            # we check this to prevent things like `assert ctx.get_requests("ASdfFunctionCreate") == 0` passing
            raise ValueError(f"{method_name} not in MockServicer - did you spell it right?")
        return [msg for _method_name, msg in self.calls if _method_name == method_name]

    def _add_recv(self, method_name: str, msg):
        self.calls.append((method_name, msg))

    def _next_custom_responder(self, method_name, request):
        method_responses = self.custom_responses[method_name]
        for i, (request_filter, response_messages) in enumerate(method_responses):
            try:
                request_matches = request_filter(request)
            except Exception:
                logging.exception("Error when filtering requests")
                raise

            if request_matches:
                next_response_messages = response_messages
                self.custom_responses[method_name] = method_responses[:i] + method_responses[i + 1 :]
                break
        else:
            custom_default = self.custom_defaults.get(method_name)
            if not custom_default:
                return None
            return custom_default

        # build a new temporary responder based on the next queued response messages (added via add_response)
        async def responder(servicer_self, stream):
            await stream.recv_message()  # get the input message so we can track that
            for msg in next_response_messages:
                await stream.send_message(msg)

        return responder

    def _assert_responses_consumed(self):
        unconsumed = []
        for method_name, queued_responses in self.custom_responses.items():
            unconsumed += [method_name] * len(queued_responses)

        if unconsumed:
            raise ResponseNotConsumed(unconsumed)


class InterceptedStream:
    def __init__(self, interception_context: InterceptionContext, method_name: str, stream):
        self.interception_context = interception_context
        self.method_name = method_name
        self.stream = stream
        self.request_message = None

    async def initialize(self):
        self.request_message = await self.recv_message()
        return self

    async def recv_message(self):
        if self.request_message:
            ret = self.request_message
            self.request_message = None
            return ret

        msg = await self.stream.recv_message()
        self.interception_context._add_recv(self.method_name, msg)
        return msg

    async def send_message(self, msg):
        await self.stream.send_message(msg)

    def __getattr__(self, attr):
        return getattr(self.stream, attr)



================================================
FILE: modal/_utils/grpc_utils.py
================================================
# Copyright Modal Labs 2022
import asyncio
import contextlib
import platform
import socket
import time
import typing
import urllib.parse
import uuid
from collections.abc import AsyncIterator
from dataclasses import dataclass
from typing import (
    Any,
    Optional,
    TypeVar,
)

import grpclib.client
import grpclib.config
import grpclib.events
import grpclib.protocol
import grpclib.stream
from google.protobuf.message import Message
from grpclib import GRPCError, Status
from grpclib.exceptions import StreamTerminatedError
from grpclib.protocol import H2Protocol

from modal.exception import AuthError, ConnectionError
from modal_version import __version__

from .async_utils import retry
from .logger import logger

RequestType = TypeVar("RequestType", bound=Message)
ResponseType = TypeVar("ResponseType", bound=Message)

if typing.TYPE_CHECKING:
    import modal.client

# Monkey patches grpclib to have a Modal User Agent header.
grpclib.client.USER_AGENT = "modal-client/{version} ({sys}; {py}/{py_ver})'".format(
    version=__version__,
    sys=platform.system(),
    py=platform.python_implementation(),
    py_ver=platform.python_version(),
).lower()


class Subchannel:
    protocol: H2Protocol
    created_at: float
    requests: int

    def __init__(self, protocol: H2Protocol) -> None:
        self.protocol = protocol
        self.created_at = time.time()
        self.requests = 0

    def connected(self):
        if hasattr(self.protocol.handler, "connection_lost"):
            # AbstractHandler doesn't have connection_lost, but Handler does
            return not self.protocol.handler.connection_lost  # type: ignore
        return True


RETRYABLE_GRPC_STATUS_CODES = [
    Status.DEADLINE_EXCEEDED,
    Status.UNAVAILABLE,
    Status.CANCELLED,
    Status.INTERNAL,
    Status.UNKNOWN,
]


@dataclass
class RetryWarningMessage:
    message: str
    warning_interval: int
    errors_to_warn_for: typing.List[Status]


class ConnectionManager:
    """ConnectionManager is a helper class for sharing connections to the Modal server.

    It can create, cache, and close channels to the Modal server. This is useful since
    multiple ModalClientModal stubs may target the same server URL, in which case they
    should share the same connection.
    """

    def __init__(self, client: "modal.client._Client", metadata: dict[str, str] = {}):
        self._client = client
        # Warning: This metadata is shared across all channels! If the metadata is mutated
        # in one `create_channel` call, the mutation will be reflected in all channels.
        self._metadata = metadata
        self._channels: dict[str, grpclib.client.Channel] = {}

    async def get_or_create_channel(self, server_url: str) -> grpclib.client.Channel:
        if server_url not in self._channels:
            self._channels[server_url] = create_channel(server_url, self._metadata)
            try:
                await connect_channel(self._channels[server_url])
            except OSError as exc:
                raise ConnectionError("Could not connect to the Modal server.") from exc
        return self._channels[server_url]

    def close(self):
        for channel in self._channels.values():
            channel.close()
        self._channels.clear()


def create_channel(
    server_url: str,
    metadata: dict[str, str] = {},
) -> grpclib.client.Channel:
    """Creates a grpclib.Channel to be used by a GRPC stub.

    Note that this function mutates the given metadata argument by adding an x-modal-auth-token
    if one is present in the trailing metadata of any response.
    """
    o = urllib.parse.urlparse(server_url)

    channel: grpclib.client.Channel
    config = grpclib.config.Configuration(
        http2_connection_window_size=64 * 1024 * 1024,  # 64 MiB
        http2_stream_window_size=64 * 1024 * 1024,  # 64 MiB
    )

    if o.scheme == "unix":
        channel = grpclib.client.Channel(path=o.path, config=config)  # probably pointless to use a pool ever
    elif o.scheme in ("http", "https"):
        target = o.netloc
        parts = target.split(":")
        assert 1 <= len(parts) <= 2, "Invalid target location: " + target
        ssl = o.scheme.endswith("s")
        host = parts[0]
        port = int(parts[1]) if len(parts) == 2 else 443 if ssl else 80
        channel = grpclib.client.Channel(host, port, ssl=ssl, config=config)
    else:
        raise Exception(f"Unknown scheme: {o.scheme}")

    target = o.path if o.scheme == "unix" else o.netloc
    logger.debug(f"Connecting to {target} using scheme {o.scheme}")

    # Inject metadata for the client.
    async def send_request(event: grpclib.events.SendRequest) -> None:
        for k, v in metadata.items():
            event.metadata[k] = v

        logger.debug(f"Sending request to {event.method_name}")

    grpclib.events.listen(channel, grpclib.events.SendRequest, send_request)

    return channel


@retry(n_attempts=5, base_delay=0.1)
async def connect_channel(channel: grpclib.client.Channel):
    """Connect to socket and raise exceptions when there is a connection issue."""
    await channel.__connect__()


if typing.TYPE_CHECKING:
    import modal.client


async def unary_stream(
    method: "modal.client.UnaryStreamWrapper[RequestType, ResponseType]",
    request: RequestType,
    metadata: Optional[Any] = None,
) -> AsyncIterator[ResponseType]:
    # TODO: remove this, since we have a method now
    async for item in method.unary_stream(request, metadata):
        yield item


async def retry_transient_errors(
    fn: "modal.client.UnaryUnaryWrapper[RequestType, ResponseType]",
    *args,
    base_delay: float = 0.1,
    max_delay: float = 1,
    delay_factor: float = 2,
    max_retries: Optional[int] = 3,
    additional_status_codes: list = [],
    attempt_timeout: Optional[float] = None,  # timeout for each attempt
    total_timeout: Optional[float] = None,  # timeout for the entire function call
    attempt_timeout_floor=2.0,  # always have at least this much timeout (only for total_timeout)
    retry_warning_message: Optional[RetryWarningMessage] = None,
    metadata: list[tuple[str, str]] = [],
) -> ResponseType:
    """Retry on transient gRPC failures with back-off until max_retries is reached.
    If max_retries is None, retry forever."""

    delay = base_delay
    n_retries = 0

    status_codes = [*RETRYABLE_GRPC_STATUS_CODES, *additional_status_codes]

    idempotency_key = str(uuid.uuid4())

    t0 = time.time()
    if total_timeout is not None:
        total_deadline = t0 + total_timeout
    else:
        total_deadline = None

    metadata = metadata + [("x-modal-timestamp", str(time.time()))]
    while True:
        attempt_metadata = [
            ("x-idempotency-key", idempotency_key),
            ("x-retry-attempt", str(n_retries)),
            *metadata,
        ]
        if n_retries > 0:
            attempt_metadata.append(("x-retry-delay", str(time.time() - t0)))
        timeouts = []
        if attempt_timeout is not None:
            timeouts.append(attempt_timeout)
        if total_timeout is not None:
            timeouts.append(max(total_deadline - time.time(), attempt_timeout_floor))
        if timeouts:
            timeout = min(timeouts)  # In case the function provided both types of timeouts
        else:
            timeout = None
        try:
            return await fn(*args, metadata=attempt_metadata, timeout=timeout)
        except (StreamTerminatedError, GRPCError, OSError, asyncio.TimeoutError, AttributeError) as exc:
            if isinstance(exc, GRPCError) and exc.status not in status_codes:
                if exc.status == Status.UNAUTHENTICATED:
                    raise AuthError(exc.message)
                else:
                    raise exc

            if max_retries is not None and n_retries >= max_retries:
                final_attempt = True
            elif total_deadline is not None and time.time() + delay + attempt_timeout_floor >= total_deadline:
                final_attempt = True
            else:
                final_attempt = False

            if final_attempt:
                logger.debug(
                    f"Final attempt failed with {repr(exc)} {n_retries=} {delay=} "
                    f"{total_deadline=} for {fn.name} ({idempotency_key[:8]})"
                )
                if isinstance(exc, OSError):
                    raise ConnectionError(str(exc))
                elif isinstance(exc, asyncio.TimeoutError):
                    raise ConnectionError(str(exc))
                else:
                    raise exc

            if isinstance(exc, AttributeError) and "_write_appdata" not in str(exc):
                # StreamTerminatedError are not properly raised in grpclib<=0.4.7
                # fixed in https://github.com/vmagamedov/grpclib/issues/185
                # TODO: update to newer version (>=0.4.8) once stable
                raise exc

            logger.debug(f"Retryable failure {repr(exc)} {n_retries=} {delay=} for {fn.name} ({idempotency_key[:8]})")

            n_retries += 1

            if (
                retry_warning_message
                and n_retries % retry_warning_message.warning_interval == 0
                and isinstance(exc, GRPCError)
                and exc.status in retry_warning_message.errors_to_warn_for
            ):
                logger.warning(retry_warning_message.message)

            await asyncio.sleep(delay)
            delay = min(delay * delay_factor, max_delay)


def find_free_port() -> int:
    """
    Find a free TCP port, useful for testing.

    WARN: if a returned free port is not bound immediately by the caller, that same port
    may be returned in subsequent calls to this function, potentially creating port collisions.
    """
    with contextlib.closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as s:
        s.bind(("", 0))
        s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        return s.getsockname()[1]


def get_proto_oneof(message: Message, oneof_group: str) -> Optional[Message]:
    oneof_field = message.WhichOneof(oneof_group)
    if oneof_field is None:
        return None

    return getattr(message, oneof_field)



================================================
FILE: modal/_utils/hash_utils.py
================================================
# Copyright Modal Labs 2022
import base64
import dataclasses
import hashlib
import time
from typing import BinaryIO, Callable, Optional, Sequence, Union

from modal.config import logger

HASH_CHUNK_SIZE = 65536


def _update(hashers: Sequence[Callable[[bytes], None]], data: Union[bytes, BinaryIO]) -> None:
    if isinstance(data, bytes):
        for hasher in hashers:
            hasher(data)
    else:
        assert not isinstance(data, (bytearray, memoryview))  # https://github.com/microsoft/pyright/issues/5697
        pos = data.tell()
        while True:
            chunk = data.read(HASH_CHUNK_SIZE)
            if not isinstance(chunk, bytes):
                raise ValueError(f"Only accepts bytes or byte buffer objects, not {type(chunk)} buffers")
            if not chunk:
                break
            for hasher in hashers:
                hasher(chunk)
        data.seek(pos)


def get_sha256_hex(data: Union[bytes, BinaryIO]) -> str:
    t0 = time.monotonic()
    hasher = hashlib.sha256()
    _update([hasher.update], data)
    logger.debug("get_sha256_hex took %.3fs", time.monotonic() - t0)
    return hasher.hexdigest()


def get_sha256_base64(data: Union[bytes, BinaryIO]) -> str:
    t0 = time.monotonic()
    hasher = hashlib.sha256()
    _update([hasher.update], data)
    logger.debug("get_sha256_base64 took %.3fs", time.monotonic() - t0)
    return base64.b64encode(hasher.digest()).decode("ascii")


def get_md5_base64(data: Union[bytes, BinaryIO]) -> str:
    t0 = time.monotonic()
    hasher = hashlib.md5()
    _update([hasher.update], data)
    logger.debug("get_md5_base64 took %.3fs", time.monotonic() - t0)
    return base64.b64encode(hasher.digest()).decode("utf-8")


@dataclasses.dataclass
class UploadHashes:
    md5_base64: str
    sha256_base64: str

    def md5_hex(self) -> str:
        return base64.b64decode(self.md5_base64).hex()

    def sha256_hex(self) -> str:
        return base64.b64decode(self.sha256_base64).hex()


def get_upload_hashes(
    data: Union[bytes, BinaryIO], sha256_hex: Optional[str] = None, md5_hex: Optional[str] = None
) -> UploadHashes:
    t0 = time.monotonic()
    hashers = {}

    if not sha256_hex:
        sha256 = hashlib.sha256()
        hashers["sha256"] = sha256
    if not md5_hex:
        md5 = hashlib.md5()
        hashers["md5"] = md5

    if hashers:
        updaters = [h.update for h in hashers.values()]
        _update(updaters, data)

    if sha256_hex:
        sha256_base64 = base64.b64encode(bytes.fromhex(sha256_hex)).decode("ascii")
    else:
        sha256_base64 = base64.b64encode(hashers["sha256"].digest()).decode("ascii")

    if md5_hex:
        md5_base64 = base64.b64encode(bytes.fromhex(md5_hex)).decode("ascii")
    else:
        md5_base64 = base64.b64encode(hashers["md5"].digest()).decode("ascii")

    hashes = UploadHashes(
        md5_base64=md5_base64,
        sha256_base64=sha256_base64,
    )

    logger.debug("get_upload_hashes took %.3fs (%s)", time.monotonic() - t0, hashers.keys())
    return hashes



================================================
FILE: modal/_utils/http_utils.py
================================================
# Copyright Modal Labs 2022
import contextlib
from typing import TYPE_CHECKING, Optional

# Note: importing aiohttp seems to take about 100ms, and it's not really necessarily,
# unless we need to work with blobs. So that's why we import it lazily instead.

if TYPE_CHECKING:
    from aiohttp import ClientSession
    from aiohttp.web import Application

from .async_utils import on_shutdown


def _http_client_with_tls(timeout: Optional[float]) -> "ClientSession":
    """Create a new HTTP client session with standard, bundled TLS certificates.

    This is necessary to prevent client issues on some system where Python does
    not come pre-installed with specific TLS certificates that are necessary to
    connect to AWS S3 bucket URLs.

    Specifically: the error "unable to get local issuer certificate" when making
    an aiohttp request.
    """
    import ssl

    import certifi
    from aiohttp import ClientSession, ClientTimeout, TCPConnector

    ssl_context = ssl.create_default_context(cafile=certifi.where())
    connector = TCPConnector(ssl=ssl_context)
    return ClientSession(connector=connector, timeout=ClientTimeout(total=timeout))


class ClientSessionRegistry:
    _client_session: "ClientSession"
    _client_session_active: bool = False

    @staticmethod
    def get_session():
        if not ClientSessionRegistry._client_session_active:
            ClientSessionRegistry._client_session = _http_client_with_tls(timeout=None)
            ClientSessionRegistry._client_session_active = True
            on_shutdown(ClientSessionRegistry.close_session())
        return ClientSessionRegistry._client_session

    @staticmethod
    async def close_session():
        if ClientSessionRegistry._client_session_active:
            await ClientSessionRegistry._client_session.close()
            ClientSessionRegistry._client_session_active = False


@contextlib.asynccontextmanager
async def run_temporary_http_server(app: "Application"):
    # Allocates a random port, runs a server in a context manager
    # This is used in various tests
    import socket

    from aiohttp.web_runner import AppRunner, SockSite

    sock = socket.socket()
    sock.bind(("", 0))
    port = sock.getsockname()[1]
    host = f"http://127.0.0.1:{port}"

    runner = AppRunner(app)
    await runner.setup()
    site = SockSite(runner, sock=sock)
    await site.start()
    try:
        yield host
    finally:
        await runner.cleanup()



================================================
FILE: modal/_utils/jwt_utils.py
================================================
# Copyright Modal Labs 2025
import base64
import json
from dataclasses import dataclass
from typing import Any, Dict


@dataclass
class DecodedJwt:
    header: Dict[str, Any]
    payload: Dict[str, Any]

    @staticmethod
    def decode_without_verification(token: str) -> "DecodedJwt":
        # Split the JWT into its three parts
        header_b64, payload_b64, _ = token.split(".")

        # Decode Base64 (with padding handling)
        header_json = base64.urlsafe_b64decode(header_b64 + "==").decode("utf-8")
        payload_json = base64.urlsafe_b64decode(payload_b64 + "==").decode("utf-8")

        # Convert JSON strings to dictionaries
        header = json.loads(header_json)
        payload = json.loads(payload_json)

        return DecodedJwt(header, payload)

    @staticmethod
    def _base64url_encode(data: str) -> str:
        """Encodes data to Base64 URL-safe format without padding."""
        return base64.urlsafe_b64encode(data.encode()).rstrip(b"=").decode()

    @staticmethod
    def encode_without_signature(fields: Dict[str, Any]) -> str:
        """Encodes an Unsecured JWT (without a signature)."""
        header_b64 = DecodedJwt._base64url_encode(json.dumps({"alg": "none", "typ": "JWT"}))
        payload_b64 = DecodedJwt._base64url_encode(json.dumps(fields))
        return f"{header_b64}.{payload_b64}."  # No signature



================================================
FILE: modal/_utils/logger.py
================================================
# Copyright Modal Labs 2022
import logging
import os


def configure_logger(logger: logging.Logger, log_level: str, log_format: str):
    from modal.config import config

    ch = logging.StreamHandler()
    log_level_numeric = logging.getLevelName(log_level.upper())
    logger.setLevel(log_level_numeric)
    ch.setLevel(log_level_numeric)
    datefmt = "%Y-%m-%dT%H:%M:%S%z"
    if log_format.upper() == "JSON":
        # This is primarily for modal internal use.
        # pythonjsonlogger is already installed in the environment.
        from pythonjsonlogger import jsonlogger

        if not (log_format_pattern := config.get("log_pattern")):
            log_format_pattern = (
                "%(asctime)s %(levelname)s [%(name)s] [%(filename)s:%(lineno)d] "
                "[dd.service=%(dd.service)s dd.env=%(dd.env)s dd.version=%(dd.version)s dd.trace_id=%(dd.trace_id)s "
                "dd.span_id=%(dd.span_id)s] "
                "- %(message)s"
            )

        json_formatter = jsonlogger.JsonFormatter(
            fmt=log_format_pattern,
            datefmt=datefmt,
        )
        ch.setFormatter(json_formatter)
    else:
        if not (log_format_pattern := config.get("log_pattern")):
            # TODO: use `%(name)s` instead of `modal-client` as soon as we unify the loggers we use
            log_format_pattern = "[modal-client] %(asctime)s %(message)s"

        ch.setFormatter(logging.Formatter(log_format_pattern, datefmt=datefmt))

    logger.addHandler(ch)


# TODO: remove this distinct logger in favor of the one in modal.config?
log_level = os.environ.get("MODAL_LOGLEVEL", "WARNING")
log_format = os.environ.get("MODAL_LOG_FORMAT", "STRING")

logger = logging.getLogger("modal-utils")
configure_logger(logger, log_level, log_format)



================================================
FILE: modal/_utils/mount_utils.py
================================================
# Copyright Modal Labs 2022
import posixpath
import typing
from collections.abc import Mapping, Sequence
from pathlib import PurePath, PurePosixPath
from typing import Union

from ..cloud_bucket_mount import _CloudBucketMount
from ..exception import InvalidError
from ..network_file_system import _NetworkFileSystem
from ..volume import _Volume

T = typing.TypeVar("T", bound=Union[_Volume, _NetworkFileSystem, _CloudBucketMount])


def validate_mount_points(
    display_name: str,
    volume_likes: Mapping[Union[str, PurePosixPath], T],
) -> list[tuple[str, T]]:
    """Mount point path validation for volumes and network file systems."""

    if not isinstance(volume_likes, dict):
        raise InvalidError(
            f"`volume_likes` should be a dict[str | PurePosixPath, {display_name}], got {type(volume_likes)} instead"
        )

    validated = []
    for path, vol in volume_likes.items():
        path = PurePath(path).as_posix()
        abs_path = posixpath.abspath(path)

        if path != abs_path:
            raise InvalidError(f"{display_name} {path} must be a canonical, absolute path.")
        elif abs_path == "/":
            raise InvalidError(f"{display_name} {path} cannot be mounted into root directory.")
        elif abs_path == "/root":
            raise InvalidError(f"{display_name} {path} cannot be mounted at '/root'.")
        elif abs_path == "/tmp":
            raise InvalidError(f"{display_name} {path} cannot be mounted at '/tmp'.")
        validated.append((path, vol))
    return validated


def validate_network_file_systems(
    network_file_systems: Mapping[Union[str, PurePosixPath], _NetworkFileSystem],
):
    validated_network_file_systems = validate_mount_points("NetworkFileSystem", network_file_systems)

    for path, network_file_system in validated_network_file_systems:
        if not isinstance(network_file_system, (_NetworkFileSystem)):
            raise InvalidError(
                f"Object of type {type(network_file_system)} mounted at '{path}' "
                + "is not useable as a network file system."
            )

    return validated_network_file_systems


def validate_volumes(
    volumes: Mapping[Union[str, PurePosixPath], Union[_Volume, _CloudBucketMount]],
) -> Sequence[tuple[str, Union[_Volume, _CloudBucketMount]]]:
    validated_volumes = validate_mount_points("Volume", volumes)
    # We don't support mounting a modal.Volume in more than one location,
    # but the same CloudBucketMount object can be used in more than one location.
    volume_to_paths: dict[_Volume, list[str]] = {}
    for path, volume in validated_volumes:
        if not isinstance(volume, (_Volume, _CloudBucketMount)):
            raise InvalidError(f"Object of type {type(volume)} mounted at '{path}' is not usable as a volume.")
        elif isinstance(volume, (_Volume)):
            volume_to_paths.setdefault(volume, []).append(path)
    for paths in volume_to_paths.values():
        if len(paths) > 1:
            conflicting = ", ".join(paths)
            raise InvalidError(
                f"The same Volume cannot be mounted in multiple locations for the same function: {conflicting}"
            )

    return validated_volumes



================================================
FILE: modal/_utils/name_utils.py
================================================
# Copyright Modal Labs 2022
import re

from ..exception import InvalidError

# https://www.rfc-editor.org/rfc/rfc1035
subdomain_regex = re.compile("^(?![0-9]+$)(?!-)[a-z0-9-]{,63}(?<!-)$")


def is_valid_subdomain_label(label: str) -> bool:
    return subdomain_regex.match(label) is not None


def replace_invalid_subdomain_chars(label: str) -> str:
    return re.sub("[^a-z0-9-]", "-", label.lower())


def is_valid_object_name(name: str) -> bool:
    return (
        # Limit object name length
        len(name) <= 64
        # Limit character set
        and re.match("^[a-zA-Z0-9-_.]+$", name) is not None
        # Avoid collisions with App IDs
        and re.match("^ap-[a-zA-Z0-9]{22}$", name) is None
    )


def is_valid_environment_name(name: str) -> bool:
    # first char is alnum, the rest allows other chars
    return len(name) <= 64 and re.match(r"^[a-zA-Z0-9][a-zA-Z0-9-_.]+$", name) is not None


def is_valid_tag(tag: str, max_length: int = 50) -> bool:
    """Tags are alphanumeric, dashes, periods, and underscores, and not longer than the max_length."""
    pattern = rf"^[a-zA-Z0-9._-]{{1,{max_length}}}$"
    return bool(re.match(pattern, tag))


def check_tag_dict(tags: dict[str, str]) -> dict[str, str]:
    rules = (
        "\n\nTags may contain only alphanumeric characters, dashes, periods, or underscores, "
        "and must be 63 characters or less."
    )
    max_length = 63
    for key, value in tags.items():
        if not is_valid_tag(key, max_length):
            raise InvalidError(f"Invalid tag key: {key!r}.{rules}")
        if not is_valid_tag(value, max_length):
            raise InvalidError(f"Invalid tag value: {value!r}.{rules}")

    return tags


def check_object_name(name: str, object_type: str) -> None:
    message = (
        f"Invalid {object_type} name: '{name}'."
        "\n\nNames may contain only alphanumeric characters, dashes, periods, and underscores,"
        " must be shorter than 64 characters, and cannot conflict with App ID strings."
    )
    if not is_valid_object_name(name):
        raise InvalidError(message)


def check_environment_name(name: str) -> None:
    message = (
        f"Invalid environment name: '{name}'."
        "\n\nEnvironment names can only start with alphanumeric characters,"
        " may contain only alphanumeric characters, dashes, periods, and underscores,"
        " and must be shorter than 64 characters."
    )
    if not is_valid_environment_name(name):
        raise InvalidError(message)



================================================
FILE: modal/_utils/package_utils.py
================================================
# Copyright Modal Labs 2022
import importlib
import importlib.util
import typing
from importlib.metadata import PackageNotFoundError, files
from pathlib import Path

from ..exception import ModuleNotMountable


def get_file_formats(module):
    try:
        module_files = files(module)
        if not module_files:
            return []

        endings = [str(p).split(".")[-1] for p in module_files if "." in str(p)]
        return list(set(endings))
    except PackageNotFoundError:
        return []


BINARY_FORMATS = ["so", "S", "s", "asm"]  # TODO


def get_module_mount_info(module_name: str) -> typing.Sequence[tuple[bool, Path]]:
    """Returns a list of tuples [(is_dir, path)] describing how to mount a given module."""
    file_formats = get_file_formats(module_name)
    if set(BINARY_FORMATS) & set(file_formats):
        raise ModuleNotMountable(f"{module_name} can't be mounted because it contains binary file(s).")
    try:
        spec = importlib.util.find_spec(module_name)
    except Exception as exc:
        raise ModuleNotMountable(str(exc))

    entries = []
    if spec is None:
        raise ModuleNotMountable(f"{module_name} has no spec - might not be installed?")
    elif spec.submodule_search_locations:
        entries = [(True, Path(path)) for path in spec.submodule_search_locations if Path(path).exists()]
    else:
        # Individual file
        filename = spec.origin
        if filename is not None and Path(filename).exists():
            entries = [(False, Path(filename))]
    if not entries:
        raise ModuleNotMountable(f"{module_name} has no mountable paths")
    return entries


def parse_major_minor_version(version_string: str) -> tuple[int, int]:
    parts = version_string.split(".")
    if len(parts) < 2:
        raise ValueError("version_string must have at least an 'X.Y' format")
    try:
        major = int(parts[0])
        minor = int(parts[1])
    except ValueError:
        raise ValueError("version_string must have at least an 'X.Y' format with integral major/minor values")

    return major, minor



================================================
FILE: modal/_utils/pattern_utils.py
================================================
# Copyright Modal Labs 2024
"""Pattern matching library ported from https://github.com/moby/patternmatcher.

This is the same pattern-matching logic used by Docker, except it is written in
Python rather than Go. Also, the original Go library has a couple deprecated
functions that we don't implement in this port.

The main way to use this library is by constructing a `FilePatternMatcher` object,
then asking it whether file paths match any of its patterns.
"""

import enum
import os
import re
from typing import Optional, TextIO

escape_chars = frozenset(".+()|{}$")


class MatchType(enum.IntEnum):
    UNKNOWN = 0
    EXACT = 1
    PREFIX = 2
    SUFFIX = 3
    REGEXP = 4


class Pattern:
    """Defines a single regex pattern used to filter file paths."""

    def __init__(self) -> None:
        """Initialize a new Pattern instance."""
        self.match_type = MatchType.UNKNOWN
        self.cleaned_pattern = ""
        self.dirs: list[str] = []
        self.regexp: Optional[re.Pattern] = None
        self.exclusion = False

    def __str__(self) -> str:
        """Return the cleaned pattern as the string representation."""
        return self.cleaned_pattern

    def compile(self, separator: str) -> None:
        """Compile the pattern into a regular expression.

        Args:
            separator (str): The path separator (e.g., '/' or '\\').

        Raises:
            ValueError: If the pattern is invalid.
        """
        reg_str = "^"
        pattern = self.cleaned_pattern

        esc_separator = separator
        if separator == "\\":
            esc_separator = "\\\\"

        self.match_type = MatchType.EXACT
        i = 0
        pattern_length = len(pattern)
        while i < pattern_length:
            ch = pattern[i]
            if ch == "*":
                if (i + 1) < pattern_length and pattern[i + 1] == "*":
                    # Handle '**'
                    i += 1  # Skip the second '*'
                    # Treat '**/' as '**' so eat the '/'
                    if (i + 1) < pattern_length and pattern[i + 1] == separator:
                        i += 1  # Skip the '/'
                    if i + 1 == pattern_length:
                        # Pattern ends with '**'
                        if self.match_type == MatchType.EXACT:
                            self.match_type = MatchType.PREFIX
                        else:
                            reg_str += ".*"
                            self.match_type = MatchType.REGEXP
                    else:
                        # '**' in the middle
                        reg_str += f"(.*{esc_separator})?"
                        self.match_type = MatchType.REGEXP

                    if i == 1:
                        self.match_type = MatchType.SUFFIX
                else:
                    # Single '*'
                    reg_str += f"[^{esc_separator}]*"
                    self.match_type = MatchType.REGEXP
            elif ch == "?":
                # Single '?'
                reg_str += f"[^{esc_separator}]"
                self.match_type = MatchType.REGEXP
            elif ch in escape_chars:
                reg_str += "\\" + ch
            elif ch == "\\":
                # Escape next character
                if separator == "\\":
                    reg_str += esc_separator
                    i += 1
                    continue
                if (i + 1) < pattern_length:
                    reg_str += "\\" + pattern[i + 1]
                    i += 1  # Skip the escaped character
                    self.match_type = MatchType.REGEXP
                else:
                    reg_str += "\\"
            elif ch == "[" or ch == "]":
                reg_str += ch
                self.match_type = MatchType.REGEXP
            else:
                reg_str += ch
            i += 1

        if self.match_type != MatchType.REGEXP:
            return

        reg_str += "$"

        try:
            self.regexp = re.compile(reg_str)
            self.match_type = MatchType.REGEXP
        except re.error as e:
            raise ValueError(f"Bad pattern: {pattern}") from e

    def match(self, path: str) -> bool:
        """Check if the path matches the pattern."""
        if self.match_type == MatchType.UNKNOWN:
            self.compile(os.path.sep)

        if self.match_type == MatchType.EXACT:
            return path == self.cleaned_pattern
        elif self.match_type == MatchType.PREFIX:
            # Strip trailing '**'
            return path.startswith(self.cleaned_pattern[:-2])
        elif self.match_type == MatchType.SUFFIX:
            # Strip leading '**'
            suffix = self.cleaned_pattern[2:]
            if path.endswith(suffix):
                return True
            # '**/foo' matches 'foo'
            if suffix[0] == os.path.sep and path == suffix[1:]:
                return True
            else:
                return False
        elif self.match_type == MatchType.REGEXP:
            return self.regexp.match(path) is not None
        else:
            return False


def read_ignorefile(reader: TextIO) -> list[str]:
    """Read an ignore file from a reader and return the list of file patterns to
    ignore, applying the following rules:

    - An UTF8 BOM header (if present) is stripped. (Python does this already)
    - Lines starting with "#" are considered comments and are skipped.

    For remaining lines:

    - Leading and trailing whitespace is removed from each ignore pattern.
    - It uses `os.path.normpath` to get the shortest/cleanest path for ignore
      patterns.
    - Leading forward-slashes ("/") are removed from ignore patterns, so
      "/some/path" and "some/path" are considered equivalent.

    Args:
        reader (file-like object): The input stream to read from.

    Returns:
        list: A list of patterns to ignore.
    """
    if reader is None:
        return []

    excludes: list[str] = []

    for line in reader:
        pattern = line.rstrip("\n\r")

        # Lines starting with "#" are ignored
        if pattern.startswith("#"):
            continue

        pattern = pattern.strip()
        if pattern == "":
            continue

        # Normalize absolute paths to paths relative to the context
        # (taking care of '!' prefix)
        invert = pattern[0] == "!"
        if invert:
            pattern = pattern[1:].strip()

        if len(pattern) > 0:
            pattern = os.path.normpath(pattern)
            pattern = pattern.replace(os.sep, "/")
            if len(pattern) > 1 and pattern[0] == "/":
                pattern = pattern[1:]

        if invert:
            pattern = "!" + pattern

        excludes.append(pattern)

    return excludes



================================================
FILE: modal/_utils/rand_pb_testing.py
================================================
# Copyright Modal Labs 2023
"""Utilities to generate random valid Protobuf messages for testing.

This is based on https://github.com/yupingso/randomproto but customizable for
Modal, with random seeds, and it supports oneofs, and Protobuf v4.
"""

import string
from random import Random
from typing import Any, Callable, Optional, TypeVar, Union

from google.protobuf.descriptor import Descriptor, FieldDescriptor

T = TypeVar("T")

_FIELD_RANDOM_GENERATOR: dict[int, Callable[[Random], Any]] = {
    FieldDescriptor.TYPE_DOUBLE: lambda rand: rand.normalvariate(0, 1),
    FieldDescriptor.TYPE_FLOAT: lambda rand: rand.normalvariate(0, 1),
    FieldDescriptor.TYPE_INT32: lambda rand: int.from_bytes(rand.randbytes(4), "little", signed=True),
    FieldDescriptor.TYPE_INT64: lambda rand: int.from_bytes(rand.randbytes(8), "little", signed=True),
    FieldDescriptor.TYPE_UINT32: lambda rand: int.from_bytes(rand.randbytes(4), "little"),
    FieldDescriptor.TYPE_UINT64: lambda rand: int.from_bytes(rand.randbytes(8), "little"),
    FieldDescriptor.TYPE_SINT32: lambda rand: int.from_bytes(rand.randbytes(4), "little", signed=True),
    FieldDescriptor.TYPE_SINT64: lambda rand: int.from_bytes(rand.randbytes(8), "little", signed=True),
    FieldDescriptor.TYPE_FIXED32: lambda rand: int.from_bytes(rand.randbytes(4), "little"),
    FieldDescriptor.TYPE_FIXED64: lambda rand: int.from_bytes(rand.randbytes(8), "little"),
    FieldDescriptor.TYPE_SFIXED32: lambda rand: int.from_bytes(rand.randbytes(4), "little", signed=True),
    FieldDescriptor.TYPE_SFIXED64: lambda rand: int.from_bytes(rand.randbytes(8), "little", signed=True),
    FieldDescriptor.TYPE_BOOL: lambda rand: rand.choice([True, False]),
    FieldDescriptor.TYPE_STRING: lambda rand: "".join(
        rand.choice(string.printable) for _ in range(int(rand.expovariate(0.15)))
    ),
    FieldDescriptor.TYPE_BYTES: lambda rand: rand.randbytes(int(rand.expovariate(0.15))),
}


def _fill(msg, desc: Descriptor, rand: Random) -> None:
    field: FieldDescriptor
    oneof_fields: set[str] = set()
    for oneof in desc.oneofs:
        oneof_field: Union[FieldDescriptor, None] = rand.choice(list(oneof.fields) + [None])
        if oneof_field is not None:
            oneof_fields.add(oneof_field.name)
    for field in desc.fields:
        if field.containing_oneof is not None and field.name not in oneof_fields:
            continue
        is_message = field.type == FieldDescriptor.TYPE_MESSAGE
        is_repeated = field.label == FieldDescriptor.LABEL_REPEATED
        if is_message:
            msg_field = getattr(msg, field.name)
            if is_repeated:
                num = rand.randint(0, 2)
                for _ in range(num):
                    element = msg_field.add()
                    _fill(element, field.message_type, rand)
            else:
                _fill(msg_field, field.message_type, rand)
        else:
            if field.type == FieldDescriptor.TYPE_ENUM:
                enum_values = [x.number for x in field.enum_type.values]
                generator = lambda rand: rand.choice(enum_values)  # noqa: E731

            else:
                generator = _FIELD_RANDOM_GENERATOR[field.type]
            if is_repeated:
                num = rand.randint(0, 2)
                msg_field = getattr(msg, field.name)
                for _ in range(num):
                    msg_field.append(generator(rand))
            else:
                setattr(msg, field.name, generator(rand))


def rand_pb(proto: type[T], rand: Optional[Random] = None) -> T:
    """Generate a pseudorandom protobuf message.

    ```python notest
    rand = random.Random(42)
    definition = rand_pb(api_pb2.Function, rand)
    ```
    """
    if rand is None:
        rand = Random(0)  # note: deterministic seed if not specified
    msg = proto()
    _fill(msg, proto.DESCRIPTOR, rand)  # type: ignore
    return msg



================================================
FILE: modal/_utils/shell_utils.py
================================================
# Copyright Modal Labs 2024

import asyncio
import contextlib
import errno
import os
import select
import sys
from collections.abc import Coroutine
from typing import Callable, Optional

from modal._pty import raw_terminal, set_nonblocking

from .async_utils import asyncify


def write_to_fd(fd: int, data: bytes):
    loop = asyncio.get_event_loop()
    future = loop.create_future()

    def try_write():
        nonlocal data
        try:
            nbytes = os.write(fd, data)
            data = data[nbytes:]
            if not data:
                loop.remove_writer(fd)
                future.set_result(None)
        except OSError as e:
            if e.errno == errno.EAGAIN:
                # Wait for the next write notification
                return
            # Fail if it's not EAGAIN
            loop.remove_writer(fd)
            future.set_exception(e)

    loop.add_writer(fd, try_write)
    return future


@contextlib.asynccontextmanager
async def stream_from_stdin(handle_input: Callable[[bytes, int], Coroutine], use_raw_terminal=False):
    """Stream from terminal stdin to the handle_input provided by the method"""
    quit_pipe_read, quit_pipe_write = os.pipe()

    set_nonblocking(sys.stdin.fileno())

    @asyncify
    def _read_stdin() -> Optional[bytes]:
        nonlocal quit_pipe_read
        # TODO: Windows support.
        (readable, _, _) = select.select([sys.stdin.buffer, quit_pipe_read], [], [], 5)
        if quit_pipe_read in readable:
            return None
        if sys.stdin.buffer in readable:
            return sys.stdin.buffer.read()
        # we had 5 seconds of no input. send an empty string as a "heartbeat" to the server.
        return b""

    async def _write():
        message_index = 1
        while True:
            data = await _read_stdin()
            if data is None:
                return

            await handle_input(data, message_index)

            message_index += 1

    write_task = asyncio.create_task(_write())

    if use_raw_terminal:
        with raw_terminal():
            yield
    else:
        yield
    os.write(quit_pipe_write, b"\n")
    write_task.cancel()



================================================
FILE: modal/_utils/time_utils.py
================================================
# Copyright Modal Labs 2025
from datetime import datetime, tzinfo
from typing import Optional, Union


def locale_tz() -> tzinfo:
    return datetime.now().astimezone().tzinfo


def as_timestamp(arg: Optional[Union[datetime, str]]) -> float:
    """Coerce a user-provided argument to a timestamp.

    An argument provided without timezone information will be treated as local time.

    When the argument is null, returns the current time.
    """
    if arg is None:
        dt = datetime.now().astimezone()
    elif isinstance(arg, str):
        dt = datetime.fromisoformat(arg)
    elif isinstance(arg, datetime):
        dt = arg
    else:
        raise TypeError(f"Invalid argument: {arg}")

    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=locale_tz())
    return dt.timestamp()


def timestamp_to_localized_dt(ts: float) -> datetime:
    return datetime.fromtimestamp(ts, tz=locale_tz())


def timestamp_to_localized_str(ts: float, isotz: bool = True) -> Optional[str]:
    if ts > 0:
        dt = timestamp_to_localized_dt(ts)
        if isotz:
            return dt.isoformat(sep=" ", timespec="seconds")
        else:
            return f"{dt:%Y-%m-%d %H:%M %Z}"
    else:
        return None



================================================
FILE: modal/_vendor/__init__.py
================================================
# Copyright Modal Labs 2024



================================================
FILE: modal/_vendor/a2wsgi_wsgi.py
================================================
"""
Vendored version of a2wsgi v1.10.2.

We vendor only a2wsgi/wsgi.py, plus type annotations, to convert WSGI apps into ASGI protocol
versions using the `WSGIMiddleware` class.

This is a well-tested library marked as an optional dependency of uvicorn. It doesn't have the
issues with buffering request streams that asgiref has, and it also is simpler, only requiring a
standard `concurrent.futures.ThreadPoolExecutor`.

---

   Copyright 2022 abersheeran

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
"""

import asyncio
import contextvars
import functools
import os
import sys
import typing
from concurrent.futures import ThreadPoolExecutor
from types import TracebackType
from typing import (
    Any,
    Callable,
    Dict,
    List,
    Literal,
    Optional,
    Protocol,
    Tuple,
    Type,
    TypedDict,
    Union,
)
from collections.abc import Awaitable, Iterable


## BEGIN a2wsgi/asgi_typing.py

if sys.version_info >= (3, 11):
    from typing import NotRequired
else:
    from typing_extensions import NotRequired


class ASGIVersions(TypedDict):
    spec_version: str
    version: Literal["3.0"]


class HTTPScope(TypedDict):
    type: Literal["http"]
    asgi: ASGIVersions
    http_version: str
    method: str
    scheme: str
    path: str
    raw_path: NotRequired[bytes]
    query_string: bytes
    root_path: str
    headers: Iterable[tuple[bytes, bytes]]
    client: NotRequired[tuple[str, int]]
    server: NotRequired[tuple[str, Optional[int]]]
    state: NotRequired[dict[str, Any]]
    extensions: NotRequired[dict[str, dict[object, object]]]


class WebSocketScope(TypedDict):
    type: Literal["websocket"]
    asgi: ASGIVersions
    http_version: str
    scheme: str
    path: str
    raw_path: bytes
    query_string: bytes
    root_path: str
    headers: Iterable[tuple[bytes, bytes]]
    client: NotRequired[tuple[str, int]]
    server: NotRequired[tuple[str, Optional[int]]]
    subprotocols: Iterable[str]
    state: NotRequired[dict[str, Any]]
    extensions: NotRequired[dict[str, dict[object, object]]]


class LifespanScope(TypedDict):
    type: Literal["lifespan"]
    asgi: ASGIVersions
    state: NotRequired[dict[str, Any]]


WWWScope = Union[HTTPScope, WebSocketScope]
Scope = Union[HTTPScope, WebSocketScope, LifespanScope]


class HTTPRequestEvent(TypedDict):
    type: Literal["http.request"]
    body: bytes
    more_body: NotRequired[bool]


class HTTPResponseStartEvent(TypedDict):
    type: Literal["http.response.start"]
    status: int
    headers: NotRequired[Iterable[tuple[bytes, bytes]]]
    trailers: NotRequired[bool]


class HTTPResponseBodyEvent(TypedDict):
    type: Literal["http.response.body"]
    body: NotRequired[bytes]
    more_body: NotRequired[bool]


class HTTPDisconnectEvent(TypedDict):
    type: Literal["http.disconnect"]


class WebSocketConnectEvent(TypedDict):
    type: Literal["websocket.connect"]


class WebSocketAcceptEvent(TypedDict):
    type: Literal["websocket.accept"]
    subprotocol: NotRequired[str]
    headers: NotRequired[Iterable[tuple[bytes, bytes]]]


class WebSocketReceiveEvent(TypedDict):
    type: Literal["websocket.receive"]
    bytes: NotRequired[bytes]
    text: NotRequired[str]


class WebSocketSendEvent(TypedDict):
    type: Literal["websocket.send"]
    bytes: NotRequired[bytes]
    text: NotRequired[str]


class WebSocketDisconnectEvent(TypedDict):
    type: Literal["websocket.disconnect"]
    code: int


class WebSocketCloseEvent(TypedDict):
    type: Literal["websocket.close"]
    code: NotRequired[int]
    reason: NotRequired[str]


class LifespanStartupEvent(TypedDict):
    type: Literal["lifespan.startup"]


class LifespanShutdownEvent(TypedDict):
    type: Literal["lifespan.shutdown"]


class LifespanStartupCompleteEvent(TypedDict):
    type: Literal["lifespan.startup.complete"]


class LifespanStartupFailedEvent(TypedDict):
    type: Literal["lifespan.startup.failed"]
    message: str


class LifespanShutdownCompleteEvent(TypedDict):
    type: Literal["lifespan.shutdown.complete"]


class LifespanShutdownFailedEvent(TypedDict):
    type: Literal["lifespan.shutdown.failed"]
    message: str


ReceiveEvent = Union[
    HTTPRequestEvent,
    HTTPDisconnectEvent,
    WebSocketConnectEvent,
    WebSocketReceiveEvent,
    WebSocketDisconnectEvent,
    LifespanStartupEvent,
    LifespanShutdownEvent,
]

SendEvent = Union[
    HTTPResponseStartEvent,
    HTTPResponseBodyEvent,
    HTTPDisconnectEvent,
    WebSocketAcceptEvent,
    WebSocketSendEvent,
    WebSocketCloseEvent,
    LifespanStartupCompleteEvent,
    LifespanStartupFailedEvent,
    LifespanShutdownCompleteEvent,
    LifespanShutdownFailedEvent,
]

Receive = Callable[[], Awaitable[ReceiveEvent]]

Send = Callable[[SendEvent], Awaitable[None]]

ASGIApp = Callable[[Scope, Receive, Send], Awaitable[None]]

## END a2wsgi/asgi_typing.py


## BEGIN a2wsgi/wsgi_typing.py

class CGIRequiredDefined(TypedDict):
    # The HTTP request method, such as GET or POST. This cannot ever be an
    # empty string, and so is always required.
    REQUEST_METHOD: str
    # When HTTP_HOST is not set, these variables can be combined to determine
    # a default.
    # SERVER_NAME and SERVER_PORT are required strings and must never be empty.
    SERVER_NAME: str
    SERVER_PORT: str
    # The version of the protocol the client used to send the request.
    # Typically this will be something like "HTTP/1.0" or "HTTP/1.1" and
    # may be used by the application to determine how to treat any HTTP
    # request headers. (This variable should probably be called REQUEST_PROTOCOL,
    # since it denotes the protocol used in the request, and is not necessarily
    # the protocol that will be used in the server's response. However, for
    # compatibility with CGI we have to keep the existing name.)
    SERVER_PROTOCOL: str

class CGIOptionalDefined(TypedDict, total=False):
    REQUEST_URI: str
    REMOTE_ADDR: str
    REMOTE_PORT: str
    # The initial portion of the request URLâ€™s â€œpathâ€ that corresponds to the
    # application object, so that the application knows its virtual â€œlocationâ€.
    # This may be an empty string, if the application corresponds to the â€œrootâ€
    # of the server.
    SCRIPT_NAME: str
    # The remainder of the request URLâ€™s â€œpathâ€, designating the virtual
    # â€œlocationâ€ of the requestâ€™s target within the application. This may be an
    # empty string, if the request URL targets the application root and does
    # not have a trailing slash.
    PATH_INFO: str
    # The portion of the request URL that follows the â€œ?â€, if any. May be empty
    # or absent.
    QUERY_STRING: str
    # The contents of any Content-Type fields in the HTTP request. May be empty
    # or absent.
    CONTENT_TYPE: str
    # The contents of any Content-Length fields in the HTTP request. May be empty
    # or absent.
    CONTENT_LENGTH: str


class InputStream(Protocol):
    """
    An input stream (file-like object) from which the HTTP request body bytes can be
    read. (The server or gateway may perform reads on-demand as requested by the
    application, or it may pre- read the client's request body and buffer it in-memory
    or on disk, or use any other technique for providing such an input stream, according
    to its preference.)
    """

    def read(self, size: int = -1, /) -> bytes:
        """
        The server is not required to read past the client's specified Content-Length,
        and should simulate an end-of-file condition if the application attempts to read
        past that point. The application should not attempt to read more data than is
        specified by the CONTENT_LENGTH variable.
        A server should allow read() to be called without an argument, and return the
        remainder of the client's input stream.
        A server should return empty bytestrings from any attempt to read from an empty
        or exhausted input stream.
        """
        raise NotImplementedError

    def readline(self, limit: int = -1, /) -> bytes:
        """
        Servers should support the optional "size" argument to readline(), but as in
        WSGI 1.0, they are allowed to omit support for it.
        (In WSGI 1.0, the size argument was not supported, on the grounds that it might
        have been complex to implement, and was not often used in practice... but then
        the cgi module started using it, and so practical servers had to start
        supporting it anyway!)
        """
        raise NotImplementedError

    def readlines(self, hint: int = -1, /) -> list[bytes]:
        """
        Note that the hint argument to readlines() is optional for both caller and
        implementer. The application is free not to supply it, and the server or gateway
        is free to ignore it.
        """
        raise NotImplementedError


class ErrorStream(Protocol):
    """
    An output stream (file-like object) to which error output can be written,
    for the purpose of recording program or other errors in a standardized and
    possibly centralized location. This should be a "text mode" stream;
    i.e., applications should use "\n" as a line ending, and assume that it will
    be converted to the correct line ending by the server/gateway.
    (On platforms where the str type is unicode, the error stream should accept
    and log arbitrary unicode without raising an error; it is allowed, however,
    to substitute characters that cannot be rendered in the stream's encoding.)
    For many servers, wsgi.errors will be the server's main error log. Alternatively,
    this may be sys.stderr, or a log file of some sort. The server's documentation
    should include an explanation of how to configure this or where to find the
    recorded output. A server or gateway may supply different error streams to
    different applications, if this is desired.
    """

    def flush(self) -> None:
        """
        Since the errors stream may not be rewound, servers and gateways are free to
        forward write operations immediately, without buffering. In this case, the
        flush() method may be a no-op. Portable applications, however, cannot assume
        that output is unbuffered or that flush() is a no-op. They must call flush()
        if they need to ensure that output has in fact been written.
        (For example, to minimize intermingling of data from multiple processes writing
        to the same error log.)
        """
        raise NotImplementedError

    def write(self, s: str, /) -> Any:
        raise NotImplementedError

    def writelines(self, seq: list[str], /) -> Any:
        raise NotImplementedError


WSGIDefined = TypedDict(
    "WSGIDefined",
    {
        "wsgi.version": tuple[int, int],  # e.g. (1, 0)
        "wsgi.url_scheme": str,  # e.g. "http" or "https"
        "wsgi.input": InputStream,
        "wsgi.errors": ErrorStream,
        # This value should evaluate true if the application object may be simultaneously
        # invoked by another thread in the same process, and should evaluate false otherwise.
        "wsgi.multithread": bool,
        # This value should evaluate true if an equivalent application object may be
        # simultaneously invoked by another process, and should evaluate false otherwise.
        "wsgi.multiprocess": bool,
        # This value should evaluate true if the server or gateway expects (but does
        # not guarantee!) that the application will only be invoked this one time during
        # the life of its containing process. Normally, this will only be true for a
        # gateway based on CGI (or something similar).
        "wsgi.run_once": bool,
    },
)


class Environ(CGIRequiredDefined, CGIOptionalDefined, WSGIDefined):
    """
    WSGI Environ
    """


ExceptionInfo = tuple[type[BaseException], BaseException, Optional[TracebackType]]

# https://peps.python.org/pep-3333/#the-write-callable
WriteCallable = Callable[[bytes], None]


class StartResponse(Protocol):
    def __call__(
        self,
        status: str,
        response_headers: list[tuple[str, str]],
        exc_info: Optional[ExceptionInfo] = None,
        /,
    ) -> WriteCallable:
        raise NotImplementedError


IterableChunks = Iterable[bytes]

WSGIApp = Callable[[Environ, StartResponse], IterableChunks]

## END a2wsgi/wsgi_typing.py


## BEGIN a2wsgi/wsgi.py

class Body:
    def __init__(self, loop: asyncio.AbstractEventLoop, receive: Receive) -> None:
        self.buffer = bytearray()
        self.loop = loop
        self.receive = receive
        self._has_more = True

    @property
    def has_more(self) -> bool:
        if self._has_more or self.buffer:
            return True
        return False

    def _receive_more_data(self) -> bytes:
        if not self._has_more:
            return b""
        future = asyncio.run_coroutine_threadsafe(self.receive(), loop=self.loop)
        message = future.result()
        self._has_more = message.get("more_body", False)
        return message.get("body", b"")

    def read(self, size: int = -1) -> bytes:
        while size == -1 or size > len(self.buffer):
            self.buffer.extend(self._receive_more_data())
            if not self._has_more:
                break
        if size == -1:
            result = bytes(self.buffer)
            self.buffer.clear()
        else:
            result = bytes(self.buffer[:size])
            del self.buffer[:size]
        return result

    def readline(self, limit: int = -1) -> bytes:
        while True:
            lf_index = self.buffer.find(b"\n", 0, limit if limit > -1 else None)
            if lf_index != -1:
                result = bytes(self.buffer[: lf_index + 1])
                del self.buffer[: lf_index + 1]
                return result
            elif limit != -1:
                result = bytes(self.buffer[:limit])
                del self.buffer[:limit]
                return result
            if not self._has_more:
                break
            self.buffer.extend(self._receive_more_data())

        result = bytes(self.buffer)
        self.buffer.clear()
        return result

    def readlines(self, hint: int = -1) -> list[bytes]:
        if not self.has_more:
            return []
        if hint == -1:
            raw_data = self.read(-1)
            bytelist = raw_data.split(b"\n")
            if raw_data[-1] == 10:  # 10 -> b"\n"
                bytelist.pop(len(bytelist) - 1)
            return [line + b"\n" for line in bytelist]
        return [self.readline() for _ in range(hint)]

    def __iter__(self) -> typing.Generator[bytes, None, None]:
        while self.has_more:
            yield self.readline()


ENC, ESC = sys.getfilesystemencoding(), "surrogateescape"


def unicode_to_wsgi(u):
    """Convert an environment variable to a WSGI "bytes-as-unicode" string"""
    return u.encode(ENC, ESC).decode("iso-8859-1")


def build_environ(scope: HTTPScope, body: Body) -> Environ:
    """
    Builds a scope and request body into a WSGI environ object.
    """
    script_name = scope.get("root_path", "").encode("utf8").decode("latin1")
    path_info = scope["path"].encode("utf8").decode("latin1")
    if path_info.startswith(script_name):
        path_info = path_info[len(script_name) :]

    script_name_environ_var = os.environ.get("SCRIPT_NAME", "")
    if script_name_environ_var:
        script_name = unicode_to_wsgi(script_name_environ_var)

    environ: Environ = {
        "asgi.scope": scope,  # type: ignore a2wsgi
        "REQUEST_METHOD": scope["method"],
        "SCRIPT_NAME": script_name,
        "PATH_INFO": path_info,
        "QUERY_STRING": scope["query_string"].decode("ascii"),
        "SERVER_PROTOCOL": f"HTTP/{scope['http_version']}",
        "wsgi.version": (1, 0),
        "wsgi.url_scheme": scope.get("scheme", "http"),
        "wsgi.input": body,
        "wsgi.errors": sys.stdout,
        "wsgi.multithread": True,
        "wsgi.multiprocess": True,
        "wsgi.run_once": False,
    }

    # Get server name and port - required in WSGI, not in ASGI
    server_addr, server_port = scope.get("server") or ("localhost", 80)
    environ["SERVER_NAME"] = server_addr
    environ["SERVER_PORT"] = str(server_port or 0)

    # Get client IP address
    client = scope.get("client")
    if client is not None:
        addr, port = client
        environ["REMOTE_ADDR"] = addr
        environ["REMOTE_PORT"] = str(port)

    # Go through headers and make them into environ entries
    for name, value in scope.get("headers", []):
        name = name.decode("latin1")
        if name == "content-length":
            corrected_name = "CONTENT_LENGTH"
        elif name == "content-type":
            corrected_name = "CONTENT_TYPE"
        else:
            corrected_name = f"HTTP_{name}".upper().replace("-", "_")
        # HTTPbis say only ASCII chars are allowed in headers, but we latin1 just in case
        value = value.decode("latin1")
        if corrected_name in environ:
            value = environ[corrected_name] + "," + value
        environ[corrected_name] = value
    return environ


class WSGIMiddleware:
    """
    Convert WSGIApp to ASGIApp.
    """

    def __init__(
        self, app: WSGIApp, workers: int = 10, send_queue_size: int = 10
    ) -> None:
        self.app = app
        self.send_queue_size = send_queue_size
        self.executor = ThreadPoolExecutor(
            thread_name_prefix="WSGI", max_workers=workers
        )

    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:
        if scope["type"] == "http":
            responder = WSGIResponder(self.app, self.executor, self.send_queue_size)
            return await responder(scope, receive, send)

        if scope["type"] == "websocket":
            await send({"type": "websocket.close", "code": 1000})
            return

        if scope["type"] == "lifespan":
            message = await receive()
            assert message["type"] == "lifespan.startup"
            await send({"type": "lifespan.startup.complete"})
            message = await receive()
            assert message["type"] == "lifespan.shutdown"
            await send({"type": "lifespan.shutdown.complete"})
            return


class WSGIResponder:
    def __init__(
        self, app: WSGIApp, executor: ThreadPoolExecutor, send_queue_size: int
    ) -> None:
        self.app = app
        self.executor = executor
        self.loop = asyncio.get_event_loop()
        self.send_queue = asyncio.Queue(send_queue_size)
        self.response_started = False
        self.exc_info: typing.Any = None

    async def __call__(self, scope: HTTPScope, receive: Receive, send: Send) -> None:
        body = Body(self.loop, receive)
        environ = build_environ(scope, body)
        sender = None
        try:
            sender = self.loop.create_task(self.sender(send))
            context = contextvars.copy_context()
            func = functools.partial(context.run, self.wsgi)
            await self.loop.run_in_executor(
                self.executor, func, environ, self.start_response
            )
            await self.send_queue.put(None)
            await self.send_queue.join()
            await asyncio.wait_for(sender, None)
            if self.exc_info is not None:
                raise self.exc_info[0].with_traceback(
                    self.exc_info[1], self.exc_info[2]
                )
        finally:
            if sender and not sender.done():
                sender.cancel()  # pragma: no cover

    def send(self, message: typing.Optional[SendEvent]) -> None:
        future = asyncio.run_coroutine_threadsafe(
            self.send_queue.put(message),
            loop=self.loop,
        )
        future.result()

    async def sender(self, send: Send) -> None:
        while True:
            message = await self.send_queue.get()
            self.send_queue.task_done()
            if message is None:
                return
            await send(message)

    def start_response(
        self,
        status: str,
        response_headers: list[tuple[str, str]],
        exc_info: typing.Optional[ExceptionInfo] = None,
    ) -> WriteCallable:
        self.exc_info = exc_info
        if not self.response_started:
            self.response_started = True
            status_code_string, _ = status.split(" ", 1)
            status_code = int(status_code_string)
            headers = [
                (name.strip().encode("latin1").lower(), value.strip().encode("latin1"))
                for name, value in response_headers
            ]
            self.send(
                {
                    "type": "http.response.start",
                    "status": status_code,
                    "headers": headers,
                }
            )
        return lambda chunk: self.send(
            {"type": "http.response.body", "body": chunk, "more_body": True}
        )

    def wsgi(self, environ: Environ, start_response: StartResponse) -> None:
        iterable = self.app(environ, start_response)
        try:
            for chunk in iterable:
                self.send(
                    {"type": "http.response.body", "body": chunk, "more_body": True}
                )

            self.send({"type": "http.response.body", "body": b""})
        finally:
            getattr(iterable, "close", lambda: None)()

## END a2wsgi/wsgi.py




================================================
FILE: modal/_vendor/tblib.py
================================================
"""
Vendored version of tblib v3.0.0.

We vendor only tblib/__init__.py because we don't use tblib's pickling features (we use cloudpickle instead).

---

BSD 2-Clause License

Copyright (c) 2013-2023, Ionel Cristian MÄƒrieÈ™. All rights reserved.

Redistribution and use in source and binary forms, with or without modification, are permitted provided that the
following conditions are met:

1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following
disclaimer.

2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following
disclaimer in the documentation and/or other materials provided with the distribution.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES,
INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
"""
import re
import sys
from types import CodeType

__version__ = '3.0.0'
__all__ = 'Traceback', 'TracebackParseError', 'Frame', 'Code'

FRAME_RE = re.compile(r'^\s*File "(?P<co_filename>.+)", line (?P<tb_lineno>\d+)(, in (?P<co_name>.+))?$')


class _AttrDict(dict):
    __slots__ = ()

    def __getattr__(self, name):
        try:
            return self[name]
        except KeyError:
            raise AttributeError(name) from None


# noinspection PyPep8Naming
class __traceback_maker(Exception):
    pass


class TracebackParseError(Exception):
    pass


class Code:
    """
    Class that replicates just enough of the builtin Code object to enable serialization and traceback rendering.
    """

    co_code = None

    def __init__(self, code):
        self.co_filename = code.co_filename
        self.co_name = code.co_name
        self.co_argcount = 0
        self.co_kwonlyargcount = 0
        self.co_varnames = ()
        self.co_nlocals = 0
        self.co_stacksize = 0
        self.co_flags = 64
        self.co_firstlineno = 0


class Frame:
    """
    Class that replicates just enough of the builtin Frame object to enable serialization and traceback rendering.

    Args:

        get_locals (callable): A function that take a frame argument and returns a dict.

            See :class:`Traceback` class for example.
    """

    def __init__(self, frame, *, get_locals=None):
        self.f_locals = {} if get_locals is None else get_locals(frame)
        self.f_globals = {k: v for k, v in frame.f_globals.items() if k in ('__file__', '__name__')}
        self.f_code = Code(frame.f_code)
        self.f_lineno = frame.f_lineno

    def clear(self):
        """
        For compatibility with PyPy 3.5;
        clear() was added to frame in Python 3.4
        and is called by traceback.clear_frames(), which
        in turn is called by unittest.TestCase.assertRaises
        """


class Traceback:
    """
    Class that wraps builtin Traceback objects.

    Args:
        get_locals (callable): A function that take a frame argument and returns a dict.

            Ideally you will only return exactly what you need, and only with simple types that can be json serializable.

            Example:

            .. code:: python

                def get_locals(frame):
                    if frame.f_locals.get("__tracebackhide__"):
                        return {"__tracebackhide__": True}
                    else:
                        return {}
    """

    tb_next = None

    def __init__(self, tb, *, get_locals=None):
        self.tb_frame = Frame(tb.tb_frame, get_locals=get_locals)
        self.tb_lineno = int(tb.tb_lineno)

        # Build in place to avoid exceeding the recursion limit
        tb = tb.tb_next
        prev_traceback = self
        cls = type(self)
        while tb is not None:
            traceback = object.__new__(cls)
            traceback.tb_frame = Frame(tb.tb_frame, get_locals=get_locals)
            traceback.tb_lineno = int(tb.tb_lineno)
            prev_traceback.tb_next = traceback
            prev_traceback = traceback
            tb = tb.tb_next

    def as_traceback(self):
        """
        Convert to a builtin Traceback object that is usable for raising or rendering a stacktrace.
        """
        current = self
        top_tb = None
        tb = None
        while current:
            f_code = current.tb_frame.f_code
            code = compile('\n' * (current.tb_lineno - 1) + 'raise __traceback_maker', current.tb_frame.f_code.co_filename, 'exec')
            if hasattr(code, 'replace'):
                # Python 3.8 and newer
                code = code.replace(co_argcount=0, co_filename=f_code.co_filename, co_name=f_code.co_name, co_freevars=(), co_cellvars=())
            else:
                code = CodeType(
                    0,
                    code.co_kwonlyargcount,
                    code.co_nlocals,
                    code.co_stacksize,
                    code.co_flags,
                    code.co_code,
                    code.co_consts,
                    code.co_names,
                    code.co_varnames,
                    f_code.co_filename,
                    f_code.co_name,
                    code.co_firstlineno,
                    code.co_lnotab,
                    (),
                    (),
                )

            # noinspection PyBroadException
            try:
                exec(code, dict(current.tb_frame.f_globals), dict(current.tb_frame.f_locals))  # noqa: S102
            except Exception:
                next_tb = sys.exc_info()[2].tb_next
                if top_tb is None:
                    top_tb = next_tb
                if tb is not None:
                    tb.tb_next = next_tb
                tb = next_tb
                del next_tb

            current = current.tb_next
        try:
            return top_tb
        finally:
            del top_tb
            del tb

    to_traceback = as_traceback

    def as_dict(self):
        """
        Converts to a dictionary representation. You can serialize the result to JSON as it only has
        builtin objects like dicts, lists, ints or strings.
        """
        if self.tb_next is None:
            tb_next = None
        else:
            tb_next = self.tb_next.as_dict()

        code = {
            'co_filename': self.tb_frame.f_code.co_filename,
            'co_name': self.tb_frame.f_code.co_name,
        }
        frame = {
            'f_globals': self.tb_frame.f_globals,
            'f_locals': self.tb_frame.f_locals,
            'f_code': code,
            'f_lineno': self.tb_frame.f_lineno,
        }
        return {
            'tb_frame': frame,
            'tb_lineno': self.tb_lineno,
            'tb_next': tb_next,
        }

    to_dict = as_dict

    @classmethod
    def from_dict(cls, dct):
        """
        Creates an instance from a dictionary with the same structure as ``.as_dict()`` returns.
        """
        if dct['tb_next']:
            tb_next = cls.from_dict(dct['tb_next'])
        else:
            tb_next = None

        code = _AttrDict(
            co_filename=dct['tb_frame']['f_code']['co_filename'],
            co_name=dct['tb_frame']['f_code']['co_name'],
        )
        frame = _AttrDict(
            f_globals=dct['tb_frame']['f_globals'],
            f_locals=dct['tb_frame'].get('f_locals', {}),
            f_code=code,
            f_lineno=dct['tb_frame']['f_lineno'],
        )
        tb = _AttrDict(
            tb_frame=frame,
            tb_lineno=dct['tb_lineno'],
            tb_next=tb_next,
        )
        return cls(tb, get_locals=get_all_locals)

    @classmethod
    def from_string(cls, string, strict=True):
        """
        Creates an instance by parsing a stacktrace. Strict means that parsing stops when lines are not indented by at least two spaces
        anymore.
        """
        frames = []
        header = strict

        for line in string.splitlines():
            line = line.rstrip()
            if header:
                if line == 'Traceback (most recent call last):':
                    header = False
                continue
            frame_match = FRAME_RE.match(line)
            if frame_match:
                frames.append(frame_match.groupdict())
            elif line.startswith('  '):
                pass
            elif strict:
                break  # traceback ended

        if frames:
            previous = None
            for frame in reversed(frames):
                previous = _AttrDict(
                    frame,
                    tb_frame=_AttrDict(
                        frame,
                        f_globals=_AttrDict(
                            __file__=frame['co_filename'],
                            __name__='?',
                        ),
                        f_locals={},
                        f_code=_AttrDict(frame),
                        f_lineno=int(frame['tb_lineno']),
                    ),
                    tb_next=previous,
                )
            return cls(previous)
        else:
            raise TracebackParseError('Could not find any frames in %r.' % string)


def get_all_locals(frame):
    return dict(frame.f_locals)


================================================
FILE: modal/builder/README.md
================================================
# Modal Image builder configuration

This directory contains `modal.Image` specifications that vary across
"image builder" versions.

The `base-images.json` file specifies the versions used for Modal's
various `Image` constructor methods.

The versioned requirements files enumerate the dependencies needed by
the Modal client library when it is running inside a Modal container.

The container requirements are a subset of the dependencies required by the
client for local operation (i.e., to run or deploy Modal apps). Additionally,
we aim to pin specific versions rather than allowing a range as we do for the
installation dependencies.

From version `2024.04`, the requirements specify the entire dependency tree,
and not just the first-order dependencies.

Note that for `2023.12`, there is a separate requirements file that is used for
Python 3.12.


================================================
FILE: modal/builder/2023.12.312.txt
================================================
# Pins Modal dependencies installed within the container runtime.
aiohttp==3.9.1
aiostream==0.4.4
asgiref==3.5.2
certifi>=2022.12.07
cloudpickle==2.2.0
fastapi==0.88.0
fastprogress==1.0.0
grpclib==0.4.7
importlib_metadata==4.8.1
ipython>=7.34.0
protobuf>=3.19.0
python-multipart>=0.0.5
rich==12.3.0
tblib==1.7.0
toml==0.10.2
typer==0.6.1
types-certifi==2021.10.8.3
types-toml==0.10.4
typeguard>=3.0.0


================================================
FILE: modal/builder/2023.12.txt
================================================
# Pins Modal dependencies installed within the container runtime.
aiohttp==3.8.3
aiostream==0.4.4
asgiref==3.5.2
certifi>=2022.12.07
cloudpickle==2.0.0;python_version<'3.11'
cloudpickle==2.2.0;python_version>='3.11'
ddtrace==1.5.2;python_version<'3.11'
fastapi==0.88.0
fastprogress==1.0.0
grpclib==0.4.3
importlib_metadata==4.8.1
ipython>=7.34.0
protobuf>=3.19.0
python-multipart>=0.0.5
rich==12.3.0
tblib==1.7.0
toml==0.10.2
typer==0.6.1
types-certifi==2021.10.8.3
types-toml==0.10.4
typeguard>=3.0.0



================================================
FILE: modal/builder/2024.04.txt
================================================
aiohttp==3.9.3
aiosignal==1.3.1
aiostream==0.5.2
annotated-types==0.6.0
anyio==4.3.0
async-timeout==4.0.3 ; python_version < "3.11"
attrs==23.2.0
certifi==2024.2.2
exceptiongroup==1.2.0 ; python_version < "3.11"
fastapi==0.110.0
frozenlist==1.4.1
grpclib==0.4.7
h2==4.1.0
hpack==4.0.0
hyperframe==6.0.1
idna==3.6
markdown-it-py==3.0.0
mdurl==0.1.2
multidict==6.0.5
protobuf==4.25.3
pydantic==2.6.4
pydantic_core==2.16.3
Pygments==2.17.2
python-multipart==0.0.9
rich==13.7.1
sniffio==1.3.1
starlette==0.36.3
typing_extensions==4.10.0
yarl==1.9.4


================================================
FILE: modal/builder/2024.10.txt
================================================
aiohappyeyeballs==2.4.3
aiohttp==3.10.8
aiosignal==1.3.1
async-timeout==4.0.3 ; python_version < "3.11"
attrs==24.2.0
certifi==2024.8.30
frozenlist==1.4.1
grpclib==0.4.7
h2==4.1.0
hpack==4.0.0
hyperframe==6.0.1
idna==3.10
multidict==6.1.0
protobuf>=3.20,<6
typing_extensions==4.12.2
yarl==1.13.1



================================================
FILE: modal/builder/2025.06.txt
================================================
aiohappyeyeballs==2.6.1
aiohttp==3.12.7
aiosignal==1.3.2
async-timeout==5.0.1 ; python_version < "3.11"
attrs==25.3.0
cbor2==5.7.0
certifi==2025.4.26
frozenlist==1.6.0
grpclib==0.4.8
h2==4.2.0
hpack==4.1.0
hyperframe==6.1.0
idna==3.10
multidict==6.4.4
propcache==0.3.1
protobuf==6.31.1
typing_extensions==4.13.2
yarl==1.20.0



================================================
FILE: modal/builder/base-images.json
================================================
{
    "debian": {
        "PREVIEW": "bookworm",
        "2025.06": "bookworm",
        "2024.10": "bookworm",
        "2024.04": "bookworm",
        "2023.12": "bullseye"
    },
    "python": {
        "PREVIEW": [
            "3.9.22",
            "3.10.17",
            "3.11.12",
            "3.12.10",
            "3.13.3"
        ],
        "2025.06": [
            "3.9.22",
            "3.10.17",
            "3.11.12",
            "3.12.10",
            "3.13.3"
        ],
        "2024.10": [
            "3.9.20",
            "3.10.15",
            "3.11.10",
            "3.12.6",
            "3.13.0"
        ],
        "2024.04": [
            "3.9.19",
            "3.10.14",
            "3.11.8",
            "3.12.2"
        ],
        "2023.12": [
            "3.9.15",
            "3.10.8",
            "3.11.0",
            "3.12.1"
        ]
    },
    "micromamba": {
        "PREVIEW": "2.1.1-debian12-slim",
        "2025.06": "2.1.1-debian12-slim",
        "2024.10": "1.5.10-bookworm-slim",
        "2024.04": "1.5.8-bookworm-slim",
        "2023.12": "1.3.1-bullseye-slim"
    },
    "package_tools": {
        "PREVIEW": "pip wheel uv",
        "2025.06": "pip wheel uv",
        "2024.10": "pip wheel uv",
        "2024.04": "pip wheel uv",
        "2023.12": "pip"
    }
}



================================================
FILE: modal/builder/PREVIEW.txt
================================================
aiohappyeyeballs==2.6.1
aiohttp==3.12.7
aiosignal==1.3.2
async-timeout==5.0.1 ; python_version < "3.11"
attrs==25.3.0
cbor2==5.7.0
certifi==2025.4.26
frozenlist==1.6.0
grpclib==0.4.8
h2==4.2.0
hpack==4.1.0
hyperframe==6.1.0
idna==3.10
multidict==6.4.4
propcache==0.3.1
protobuf==6.31.1
typing_extensions==4.13.2
yarl==1.20.0



================================================
FILE: modal/cli/__init__.py
================================================
# Copyright Modal Labs 2022
"""
The `modal.cli` package contains the Python implementations for Modal's CLI.

The functions defined in this package are intended to be called via the CLI,
not from Python code. No backwards compatibility guarantees are made with
respect to Python calling patterns (e.g., the order of parameters corresponding
to CLI options may change in the Python signature without warning).

Any utility or helper functions defined in this package are intended for use by
the CLI commands and are not for external consumption; they should be
considered private even if they do not use a private naming convention.

Over time, we aspire to support feature parity between the CLI and the public
Python API, although this remains a work in progress.
"""



================================================
FILE: modal/cli/_download.py
================================================
# Copyright Modal Labs 2023
import asyncio
import functools
import os
import shutil
import sys
from collections.abc import AsyncIterator
from pathlib import Path, PurePosixPath
from typing import Callable, Optional, Union

from click import UsageError

from modal._utils.async_utils import TaskContext
from modal.config import logger
from modal.network_file_system import _NetworkFileSystem
from modal.volume import FileEntry, FileEntryType, _Volume

PIPE_PATH = Path("-")


async def _volume_download(
    volume: Union[_NetworkFileSystem, _Volume],
    remote_path: str,
    local_destination: Path,
    overwrite: bool,
    progress_cb: Callable,
):
    is_pipe = local_destination == PIPE_PATH

    q: asyncio.Queue[tuple[Optional[Path], Optional[FileEntry]]] = asyncio.Queue()
    num_consumers = 1 if is_pipe else 10  # concurrency limit for downloading files

    async def producer():
        iterator: AsyncIterator[FileEntry]
        if isinstance(volume, _Volume):
            iterator = volume.iterdir(remote_path, recursive=True)
        else:
            iterator = volume.iterdir(remote_path)  # NFS still supports "glob" paths

        async for entry in iterator:
            if is_pipe:
                await q.put((None, entry))
            else:
                start_path = Path(remote_path).parent.as_posix().split("*")[0]
                rel_path = PurePosixPath(entry.path).relative_to(start_path.lstrip("/"))
                if local_destination.is_dir():
                    output_path = local_destination / rel_path
                else:
                    output_path = local_destination
                if output_path.exists():
                    if overwrite:
                        if output_path.is_file():
                            os.remove(output_path)
                        else:
                            shutil.rmtree(output_path)
                    else:
                        raise UsageError(
                            f"Output path '{output_path}' already exists. Use --force to overwrite the output directory"
                        )
                await q.put((output_path, entry))
        # No more entries to process; issue one shutdown message for each consumer.
        for _ in range(num_consumers):
            await q.put((None, None))

    async def consumer():
        while True:
            output_path, entry = await q.get()
            if entry is None:
                return
            try:
                if is_pipe:
                    if entry.type == FileEntryType.FILE:
                        progress_task_id = progress_cb(name=entry.path, size=entry.size)
                        file_progress_cb = functools.partial(progress_cb, task_id=progress_task_id)

                        async for chunk in volume.read_file(entry.path):
                            sys.stdout.buffer.write(chunk)
                            file_progress_cb(advance=len(chunk))

                        file_progress_cb(complete=True)
                else:
                    if entry.type == FileEntryType.FILE:
                        progress_task_id = progress_cb(name=entry.path, size=entry.size)
                        output_path.parent.mkdir(parents=True, exist_ok=True)
                        file_progress_cb = functools.partial(progress_cb, task_id=progress_task_id)

                        with output_path.open("wb") as fp:
                            if isinstance(volume, _Volume):
                                b = await volume.read_file_into_fileobj(entry.path, fp, file_progress_cb)
                            else:
                                b = 0
                                async for chunk in volume.read_file(entry.path):
                                    b += fp.write(chunk)
                                    file_progress_cb(advance=len(chunk))

                        logger.debug(f"Wrote {b} bytes to {output_path}")
                        file_progress_cb(complete=True)
                    elif entry.type == FileEntryType.DIRECTORY:
                        output_path.mkdir(parents=True, exist_ok=True)
            finally:
                q.task_done()

    consumers = [consumer() for _ in range(num_consumers)]
    await TaskContext.gather(producer(), *consumers)
    progress_cb(complete=True)
    sys.stdout.flush()



================================================
FILE: modal/cli/_traceback.py
================================================
# Copyright Modal Labs 2024
"""Helper functions related to displaying tracebacks in the CLI."""

import functools
import re
import warnings
from typing import Optional

from rich.console import RenderResult, group
from rich.panel import Panel
from rich.syntax import Syntax
from rich.text import Text
from rich.traceback import PathHighlighter, Stack, Traceback, install

from .._output import make_console
from ..exception import DeprecationError, PendingDeprecationError, ServerWarning


@group()
def _render_stack(self, stack: Stack) -> RenderResult:
    """Patched variant of rich.Traceback._render_stack that uses the line from the modal StackSummary,
    when the file isn't available to be read locally."""

    path_highlighter = PathHighlighter()
    theme = self.theme
    code_cache: dict[str, str] = {}
    line_cache = getattr(stack, "line_cache", {})
    task_id = None

    def read_code(filename: str) -> str:
        code = code_cache.get(filename)
        if code is None:
            with open(filename, encoding="utf-8", errors="replace") as code_file:
                code = code_file.read()
            code_cache[filename] = code
        return code

    exclude_frames: Optional[range] = None
    if self.max_frames != 0:
        exclude_frames = range(
            self.max_frames // 2,
            len(stack.frames) - self.max_frames // 2,
        )

    excluded = False
    for frame_index, frame in enumerate(stack.frames):
        if exclude_frames and frame_index in exclude_frames:
            excluded = True
            continue

        if excluded:
            assert exclude_frames is not None
            yield Text(
                f"\n... {len(exclude_frames)} frames hidden ...",
                justify="center",
                style="traceback.error",
            )
            excluded = False

        first = frame_index == 0
        # Patched Modal-specific code.
        if frame.filename.startswith("<") and ":" in frame.filename:
            next_task_id, frame_filename = frame.filename.split(":", 1)
            next_task_id = next_task_id.strip("<>")
        else:
            frame_filename = frame.filename
            next_task_id = None
        suppressed = any(frame_filename.startswith(path) for path in self.suppress)

        if next_task_id != task_id:
            task_id = next_task_id
            yield ""
            yield Text(
                f"...Remote call to Modal Function ({task_id})...",
                justify="center",
                style="green",
            )

        text = Text.assemble(
            path_highlighter(Text(frame_filename, style="pygments.string")),
            (":", "pygments.text"),
            (str(frame.lineno), "pygments.number"),
            " in ",
            (frame.name, "pygments.function"),
            style="pygments.text",
        )
        if not frame_filename.startswith("<") and not first:
            yield ""

        yield text
        if not suppressed:
            try:
                code = read_code(frame_filename)
                lexer_name = self._guess_lexer(frame_filename, code)
                syntax = Syntax(
                    code,
                    lexer_name,
                    theme=theme,
                    line_numbers=True,
                    line_range=(
                        frame.lineno - self.extra_lines,
                        frame.lineno + self.extra_lines,
                    ),
                    highlight_lines={frame.lineno},
                    word_wrap=self.word_wrap,
                    code_width=88,
                    indent_guides=self.indent_guides,
                    dedent=False,
                )
                yield ""
            except Exception as error:
                # Patched Modal-specific code.
                line = line_cache.get((frame_filename, frame.lineno))
                if line:
                    try:
                        lexer_name = self._guess_lexer(frame_filename, line)
                        yield ""
                        yield Syntax(
                            line,
                            lexer_name,
                            theme=theme,
                            line_numbers=True,
                            line_range=(0, 1),
                            highlight_lines={frame.lineno},
                            word_wrap=self.word_wrap,
                            code_width=88,
                            indent_guides=self.indent_guides,
                            dedent=False,
                            start_line=frame.lineno,
                        )
                    except Exception:
                        yield Text.assemble(
                            (f"\n{error}", "traceback.error"),
                        )
                yield ""
            else:
                yield syntax


def setup_rich_traceback() -> None:
    from_exception = Traceback.from_exception

    @functools.wraps(Traceback.from_exception)
    def _from_exception(exc_type, exc_value, *args, **kwargs):
        """Patch from_exception to grab the Modal line_cache and store it with the
        Stack object, so it's available to render_stack at display time."""

        line_cache = getattr(exc_value, "__line_cache__", {})
        tb = from_exception(exc_type, exc_value, *args, **kwargs)
        for stack in tb.trace.stacks:
            stack.line_cache = line_cache  # type: ignore
        return tb

    Traceback._render_stack = _render_stack  # type: ignore
    Traceback.from_exception = _from_exception  # type: ignore

    import click
    import grpclib
    import synchronicity
    import typer

    install(suppress=[synchronicity, grpclib, click, typer], extra_lines=1)


def highlight_modal_warnings() -> None:
    """Patch the warnings module to make certain warnings more salient in the CLI."""
    base_showwarning = warnings.showwarning

    def showwarning(warning, category, filename, lineno, file=None, line=None):
        if issubclass(category, (DeprecationError, PendingDeprecationError, ServerWarning)):
            content = str(warning)
            if re.match(r"^\d{4}-\d{2}-\d{2}", content):
                date = content[:10]
                message = content[11:].strip()
            else:
                date = ""
                message = content
            try:
                with open(filename, encoding="utf-8", errors="replace") as code_file:
                    source = code_file.readlines()[lineno - 1].strip()
                message = f"{message}\n\nSource: {filename}:{lineno}\n  {source}"
            except OSError:
                # e.g., when filename is "<unknown>"; raises FileNotFoundError on posix but OSError on windows
                pass
            if issubclass(category, ServerWarning):
                title = "Modal Warning"
            else:
                title = "Modal Deprecation Warning"
            if date:
                title += f" ({date})"
            panel = Panel(
                message,
                border_style="yellow",
                title=title,
                title_align="left",
            )
            make_console().print(panel)
        else:
            base_showwarning(warning, category, filename, lineno, file=None, line=None)

    warnings.showwarning = showwarning



================================================
FILE: modal/cli/app.py
================================================
# Copyright Modal Labs 2022
import re
from typing import Optional, Union

import rich
import typer
from click import UsageError
from rich.table import Column
from rich.text import Text
from typer import Argument

from modal._object import _get_environment_name
from modal._utils.async_utils import synchronizer
from modal.client import _Client
from modal.environments import ensure_env
from modal_proto import api_pb2

from .._utils.time_utils import timestamp_to_localized_str
from .utils import ENV_OPTION, display_table, get_app_id_from_name, stream_app_logs

APP_IDENTIFIER = Argument("", help="App name or ID")
NAME_OPTION = typer.Option("", "-n", "--name", help="Deprecated: Pass App name as a positional argument")

app_cli = typer.Typer(name="app", help="Manage deployed and running apps.", no_args_is_help=True)

APP_STATE_TO_MESSAGE = {
    api_pb2.APP_STATE_DEPLOYED: Text("deployed", style="green"),
    api_pb2.APP_STATE_DETACHED: Text("ephemeral (detached)", style="green"),
    api_pb2.APP_STATE_DETACHED_DISCONNECTED: Text("ephemeral (detached)", style="green"),
    api_pb2.APP_STATE_DISABLED: Text("disabled", style="dim"),
    api_pb2.APP_STATE_EPHEMERAL: Text("ephemeral", style="green"),
    api_pb2.APP_STATE_INITIALIZING: Text("initializing...", style="yellow"),
    api_pb2.APP_STATE_STOPPED: Text("stopped", style="blue"),
    api_pb2.APP_STATE_STOPPING: Text("stopping...", style="blue"),
}


@synchronizer.create_blocking
async def get_app_id(app_identifier: str, env: Optional[str], client: Optional[_Client] = None) -> str:
    """Resolve an app_identifier that may be a name or an ID into an ID."""
    if re.match(r"^ap-[a-zA-Z0-9]{22}$", app_identifier):
        return app_identifier
    return await get_app_id_from_name.aio(app_identifier, env, client)


@app_cli.command("list")
@synchronizer.create_blocking
async def list_(env: Optional[str] = ENV_OPTION, json: bool = False):
    """List Modal apps that are currently deployed/running or recently stopped."""
    env = ensure_env(env)
    client = await _Client.from_env()

    resp: api_pb2.AppListResponse = await client.stub.AppList(
        api_pb2.AppListRequest(environment_name=_get_environment_name(env))
    )

    columns: list[Union[Column, str]] = [
        Column("App ID", min_width=25),  # Ensure that App ID is not truncated in slim terminals
        "Description",
        "State",
        "Tasks",
        "Created at",
        "Stopped at",
    ]
    rows: list[list[Union[Text, str]]] = []
    for app_stats in resp.apps:
        state = APP_STATE_TO_MESSAGE.get(app_stats.state, Text("unknown", style="gray"))
        rows.append(
            [
                app_stats.app_id,
                app_stats.description,
                state,
                str(app_stats.n_running_tasks),
                timestamp_to_localized_str(app_stats.created_at, json),
                timestamp_to_localized_str(app_stats.stopped_at, json),
            ]
        )

    env_part = f" in environment '{env}'" if env else ""
    display_table(columns, rows, json, title=f"Apps{env_part}")


@app_cli.command("logs", no_args_is_help=True)
def logs(
    app_identifier: str = APP_IDENTIFIER,
    *,
    env: Optional[str] = ENV_OPTION,
    timestamps: bool = typer.Option(False, "--timestamps", help="Show timestamps for each log line"),
):
    """Show App logs, streaming while active.

    **Examples:**

    Get the logs based on an app ID:

    ```
    modal app logs ap-123456
    ```

    Get the logs for a currently deployed App based on its name:

    ```
    modal app logs my-app
    ```

    """
    app_id = get_app_id(app_identifier, env)
    stream_app_logs(app_id, show_timestamps=timestamps)


@app_cli.command("rollback", no_args_is_help=True, context_settings={"ignore_unknown_options": True})
@synchronizer.create_blocking
async def rollback(
    app_identifier: str = APP_IDENTIFIER,
    version: str = typer.Argument("", help="Target version for rollback."),
    *,
    env: Optional[str] = ENV_OPTION,
):
    """Redeploy a previous version of an App.

    Note that the App must currently be in a "deployed" state.
    Rollbacks will appear as a new deployment in the App history, although
    the App state will be reset to the state at the time of the previous deployment.

    **Examples:**

    Rollback an App to its previous version:

    ```
    modal app rollback my-app
    ```

    Rollback an App to a specific version:

    ```
    modal app rollback my-app v3
    ```

    Rollback an App using its App ID instead of its name:

    ```
    modal app rollback ap-abcdefghABCDEFGH123456
    ```

    """
    env = ensure_env(env)
    client = await _Client.from_env()
    app_id = await get_app_id.aio(app_identifier, env, client)
    if not version:
        version_number = -1
    else:
        if m := re.match(r"v(\d+)", version):
            version_number = int(m.group(1))
        else:
            raise UsageError(f"Invalid version specifer: {version}")
    req = api_pb2.AppRollbackRequest(app_id=app_id, version=version_number)
    await client.stub.AppRollback(req)
    rich.print("[green]âœ“[/green] Deployment rollback successful!")


@app_cli.command("stop", no_args_is_help=True)
@synchronizer.create_blocking
async def stop(
    app_identifier: str = APP_IDENTIFIER,
    *,
    env: Optional[str] = ENV_OPTION,
):
    """Stop an app."""
    client = await _Client.from_env()
    app_id = await get_app_id.aio(app_identifier, env)
    req = api_pb2.AppStopRequest(app_id=app_id, source=api_pb2.APP_STOP_SOURCE_CLI)
    await client.stub.AppStop(req)


@app_cli.command("history", no_args_is_help=True)
@synchronizer.create_blocking
async def history(
    app_identifier: str = APP_IDENTIFIER,
    *,
    env: Optional[str] = ENV_OPTION,
    json: bool = False,
):
    """Show App deployment history, for a currently deployed app

    **Examples:**

    Get the history based on an app ID:

    ```
    modal app history ap-123456
    ```

    Get the history for a currently deployed App based on its name:

    ```
    modal app history my-app
    ```

    """
    env = ensure_env(env)
    client = await _Client.from_env()
    app_id = await get_app_id.aio(app_identifier, env, client)
    resp = await client.stub.AppDeploymentHistory(api_pb2.AppDeploymentHistoryRequest(app_id=app_id))

    columns = [
        "Version",
        "Time deployed",
        "Client",
        "Deployed by",
        "Commit",
        "Tag",
    ]
    rows = []
    deployments_with_dirty_commit = False
    for idx, app_stats in enumerate(resp.app_deployment_histories):
        style = "bold green" if idx == 0 else ""

        row = [
            Text(f"v{app_stats.version}", style=style),
            Text(timestamp_to_localized_str(app_stats.deployed_at, json), style=style),
            Text(app_stats.client_version, style=style),
            Text(app_stats.deployed_by, style=style),
        ]

        if app_stats.commit_info.commit_hash:
            short_hash = app_stats.commit_info.commit_hash[:7]
            if app_stats.commit_info.dirty:
                deployments_with_dirty_commit = True
                short_hash = f"{short_hash}*"
            row.append(Text(short_hash, style=style))
        else:
            row.append(None)

        if app_stats.tag:
            row.append(Text(app_stats.tag, style=style))
        else:
            row.append(None)

        rows.append(row)

    # Suppress tag information when no deployments used one
    if not any(row[-1] for row in rows):
        rows = [row[:-1] for row in rows]
        columns = columns[:-1]

    rows = sorted(rows, key=lambda x: int(str(x[0])[1:]), reverse=True)
    display_table(columns, rows, json)

    if deployments_with_dirty_commit and not json:
        rich.print("* - repo had uncommitted changes")



================================================
FILE: modal/cli/cluster.py
================================================
# Copyright Modal Labs 2022
from typing import Optional, Union

import typer
from rich.table import Column
from rich.text import Text

from modal._object import _get_environment_name
from modal._output import make_console
from modal._pty import get_pty_info
from modal._utils.async_utils import synchronizer
from modal._utils.time_utils import timestamp_to_localized_str
from modal.cli.utils import ENV_OPTION, display_table, is_tty
from modal.client import _Client
from modal.config import config
from modal.container_process import _ContainerProcess
from modal.environments import ensure_env
from modal.stream_type import StreamType
from modal_proto import api_pb2

cluster_cli = typer.Typer(
    name="cluster", help="Manage and connect to running multi-node clusters.", no_args_is_help=True
)


@cluster_cli.command("list")
@synchronizer.create_blocking
async def list_(env: Optional[str] = ENV_OPTION, json: bool = False):
    """List all clusters that are currently running."""
    env = ensure_env(env)
    client = await _Client.from_env()
    environment_name = _get_environment_name(env)
    res: api_pb2.ClusterListResponse = await client.stub.ClusterList(
        api_pb2.ClusterListRequest(environment_name=environment_name)
    )

    column_names: list[Union[Column, str]] = [
        Column("Cluster ID", min_width=25),
        Column("App ID", min_width=25),
        "Start Time",
        "Nodes",
    ]
    rows: list[list[Union[Text, str]]] = []
    res.clusters.sort(key=lambda c: c.started_at, reverse=True)

    for c in res.clusters:
        rows.append(
            [
                c.cluster_id,
                c.app_id,
                timestamp_to_localized_str(c.started_at, json) if c.started_at else "Pending",
                str(len(c.task_ids)),
            ]
        )

    display_table(column_names, rows, json=json, title=f"Active Multi-node Clusters in environment: {environment_name}")


@cluster_cli.command("shell")
@synchronizer.create_blocking
async def shell(
    cluster_id: str = typer.Argument(help="Cluster ID"),
    rank: int = typer.Option(default=0, help="Rank of the node to shell into"),
):
    """Open a shell to a multi-node cluster node."""
    client = await _Client.from_env()
    res: api_pb2.ClusterGetResponse = await client.stub.ClusterGet(api_pb2.ClusterGetRequest(cluster_id=cluster_id))
    if len(res.cluster.task_ids) <= rank:
        raise typer.Abort(f"No node with rank {rank} in cluster {cluster_id}")
    task_id = res.cluster.task_ids[rank]
    console = make_console()
    is_main = "(main)" if rank == 0 else ""
    console.print(
        f"Opening shell to node {rank} {is_main} of cluster {cluster_id} (container {task_id})", style="green"
    )

    pty = is_tty()
    req = api_pb2.ContainerExecRequest(
        task_id=task_id,
        command=["/bin/bash"],
        pty_info=get_pty_info(shell=True) if pty else None,
        runtime_debug=config.get("function_runtime_debug"),
    )
    exec_res: api_pb2.ContainerExecResponse = await client.stub.ContainerExec(req)
    if pty:
        await _ContainerProcess(exec_res.exec_id, client).attach()
    else:
        # TODO: redirect stderr to its own stream?
        await _ContainerProcess(exec_res.exec_id, client, stdout=StreamType.STDOUT, stderr=StreamType.STDOUT).wait()



================================================
FILE: modal/cli/config.py
================================================
# Copyright Modal Labs 2022
import typer

from modal._output import make_console
from modal.config import _profile, _store_user_config, config
from modal.environments import Environment

config_cli = typer.Typer(
    name="config",
    help="""
    Manage client configuration for the current profile.

    Refer to https://modal.com/docs/reference/modal.config for a full explanation
    of what these options mean, and how to set them.
    """,
    no_args_is_help=True,
)


@config_cli.command(help="Show current configuration values (debugging command).")
def show(redact: bool = typer.Option(True, help="Redact the `token_secret` value.")):
    # This is just a test command
    config_dict = config.to_dict()
    if redact and config_dict.get("token_secret"):
        config_dict["token_secret"] = "***"

    console = make_console()
    console.print(config_dict)


SET_DEFAULT_ENV_HELP = """Set the default Modal environment for the active profile

The default environment of a profile is used when no --env flag is passed to `modal run`, `modal deploy` etc.

If no default environment is set, and there exists multiple environments in a workspace, an error will be raised
when running a command that requires an environment.
"""


@config_cli.command(help=SET_DEFAULT_ENV_HELP)
def set_environment(environment_name: str):
    # Confirm that the environment exists by looking it up
    Environment.from_name(environment_name).hydrate()
    _store_user_config({"environment": environment_name})
    typer.echo(f"New default environment for profile {_profile}: {environment_name}")


@config_cli.command(hidden=True)
def set(key: str, value: str):
    _store_user_config({key: value})



================================================
FILE: modal/cli/container.py
================================================
# Copyright Modal Labs 2022
from typing import Optional, Union

import typer
from rich.text import Text

from modal._object import _get_environment_name
from modal._pty import get_pty_info
from modal._utils.async_utils import synchronizer
from modal._utils.grpc_utils import retry_transient_errors
from modal._utils.time_utils import timestamp_to_localized_str
from modal.cli.utils import ENV_OPTION, display_table, is_tty, stream_app_logs
from modal.client import _Client
from modal.config import config
from modal.container_process import _ContainerProcess
from modal.environments import ensure_env
from modal.stream_type import StreamType
from modal_proto import api_pb2

container_cli = typer.Typer(name="container", help="Manage and connect to running containers.", no_args_is_help=True)


@container_cli.command("list")
@synchronizer.create_blocking
async def list_(env: Optional[str] = ENV_OPTION, json: bool = False):
    """List all containers that are currently running."""
    env = ensure_env(env)
    client = await _Client.from_env()
    environment_name = _get_environment_name(env)
    res: api_pb2.TaskListResponse = await client.stub.TaskList(
        api_pb2.TaskListRequest(environment_name=environment_name)
    )

    column_names = ["Container ID", "App ID", "App Name", "Start Time"]
    rows: list[list[Union[Text, str]]] = []
    res.tasks.sort(key=lambda task: task.started_at, reverse=True)
    for task_stats in res.tasks:
        rows.append(
            [
                task_stats.task_id,
                task_stats.app_id,
                task_stats.app_description,
                timestamp_to_localized_str(task_stats.started_at, json) if task_stats.started_at else "Pending",
            ]
        )

    display_table(column_names, rows, json=json, title=f"Active Containers in environment: {environment_name}")


@container_cli.command("logs")
def logs(container_id: str = typer.Argument(help="Container ID")):
    """Show logs for a specific container, streaming while active."""
    stream_app_logs(task_id=container_id)


@container_cli.command("exec")
@synchronizer.create_blocking
async def exec(
    pty: Optional[bool] = typer.Option(default=None, help="Run the command using a PTY."),
    container_id: str = typer.Argument(help="Container ID"),
    command: list[str] = typer.Argument(
        help="A command to run inside the container.\n\n"
        "To pass command-line flags or options, add `--` before the start of your commands. "
        "For example: `modal container exec <id> -- /bin/bash -c 'echo hi'`"
    ),
):
    """Execute a command in a container."""

    if pty is None:
        pty = is_tty()

    client = await _Client.from_env()

    req = api_pb2.ContainerExecRequest(
        task_id=container_id,
        command=command,
        pty_info=get_pty_info(shell=True) if pty else None,
        runtime_debug=config.get("function_runtime_debug"),
    )
    res: api_pb2.ContainerExecResponse = await client.stub.ContainerExec(req)

    if pty:
        await _ContainerProcess(res.exec_id, client).attach()
    else:
        # TODO: redirect stderr to its own stream?
        await _ContainerProcess(res.exec_id, client, stdout=StreamType.STDOUT, stderr=StreamType.STDOUT).wait()


@container_cli.command("stop")
@synchronizer.create_blocking
async def stop(container_id: str = typer.Argument(help="Container ID")):
    """Stop a currently-running container and reassign its in-progress inputs.

    This will send the container a SIGINT signal that Modal will handle.
    """
    client = await _Client.from_env()
    request = api_pb2.ContainerStopRequest(task_id=container_id)
    await retry_transient_errors(client.stub.ContainerStop, request)



================================================
FILE: modal/cli/dict.py
================================================
# Copyright Modal Labs 2024
from typing import Optional

import typer
from typer import Argument, Option, Typer

from modal._output import make_console
from modal._resolver import Resolver
from modal._utils.async_utils import synchronizer
from modal._utils.time_utils import timestamp_to_localized_str
from modal.cli.utils import ENV_OPTION, YES_OPTION, display_table
from modal.client import _Client
from modal.dict import _Dict
from modal.environments import ensure_env

dict_cli = Typer(
    name="dict",
    no_args_is_help=True,
    help="Manage `modal.Dict` objects and inspect their contents.",
)


@dict_cli.command(name="create", rich_help_panel="Management")
@synchronizer.create_blocking
async def create(name: str, *, env: Optional[str] = ENV_OPTION):
    """Create a named Dict object.

    Note: This is a no-op when the Dict already exists.
    """
    d = _Dict.from_name(name, environment_name=env, create_if_missing=True)
    client = await _Client.from_env()
    resolver = Resolver(client=client)
    await resolver.load(d)


@dict_cli.command(name="list", rich_help_panel="Management")
@synchronizer.create_blocking
async def list_(*, json: bool = False, env: Optional[str] = ENV_OPTION):
    """List all named Dicts."""
    env = ensure_env(env)
    dicts = await _Dict.objects.list(environment_name=env)
    rows = []
    for obj in dicts:
        info = await obj.info()
        rows.append((info.name, timestamp_to_localized_str(info.created_at.timestamp(), json), info.created_by))

    display_table(["Name", "Created at", "Created by"], rows, json)


@dict_cli.command("clear", rich_help_panel="Management")
@synchronizer.create_blocking
async def clear(name: str, *, yes: bool = YES_OPTION, env: Optional[str] = ENV_OPTION):
    """Clear the contents of a named Dict by deleting all of its data."""
    d = _Dict.from_name(name, environment_name=env)
    if not yes:
        typer.confirm(
            f"Are you sure you want to irrevocably delete the contents of modal.Dict '{name}'?",
            default=False,
            abort=True,
        )
    await d.clear()


@dict_cli.command(name="delete", rich_help_panel="Management")
@synchronizer.create_blocking
async def delete(
    name: str,
    *,
    allow_missing: bool = Option(False, "--allow-missing", help="Don't error if the Dict doesn't exist."),
    yes: bool = YES_OPTION,
    env: Optional[str] = ENV_OPTION,
):
    """Delete a named Dict and all of its data."""
    if not yes:
        typer.confirm(
            f"Are you sure you want to irrevocably delete the modal.Dict '{name}'?",
            default=False,
            abort=True,
        )
    await _Dict.objects.delete(name, environment_name=env, allow_missing=allow_missing)


@dict_cli.command(name="get", rich_help_panel="Inspection")
@synchronizer.create_blocking
async def get(name: str, key: str, *, env: Optional[str] = ENV_OPTION):
    """Print the value for a specific key.

    Note: When using the CLI, keys are always interpreted as having a string type.
    """
    d = _Dict.from_name(name, environment_name=env)
    console = make_console()
    val = await d.get(key)
    console.print(val)


def _display(input: str, use_repr: bool) -> str:
    val = repr(input) if use_repr else str(input)
    return val[:80] + "..." if len(val) > 80 else val


@dict_cli.command(name="items", rich_help_panel="Inspection")
@synchronizer.create_blocking
async def items(
    name: str,
    n: int = Argument(default=20, help="Limit the number of entries shown"),
    *,
    all: bool = Option(False, "-a", "--all", help="Ignore N and print all entries in the Dict (may be slow)"),
    use_repr: bool = Option(False, "-r", "--repr", help="Display items using `repr()` to see more details"),
    json: bool = False,
    env: Optional[str] = ENV_OPTION,
):
    """Print the contents of a Dict.

    Note: By default, this command truncates the contents. Use the `N` argument to control the
    amount of data shown or the `--all` option to retrieve the entire Dict, which may be slow.
    """
    d = _Dict.from_name(name, environment_name=env)

    i, items = 0, []
    async for key, val in d.items():
        i += 1
        if not json and not all and i > n:
            items.append(("...", "..."))
            break
        else:
            if json:
                display_item = key, val
            else:
                display_item = _display(key, use_repr), _display(val, use_repr)  # type: ignore  # mypy/issue/12056
            items.append(display_item)

    display_table(["Key", "Value"], items, json)



================================================
FILE: modal/cli/entry_point.py
================================================
# Copyright Modal Labs 2022
import subprocess
from typing import Optional

import typer
from rich.rule import Rule

from modal._output import make_console
from modal._utils.async_utils import synchronizer

from . import run
from .app import app_cli
from .cluster import cluster_cli
from .config import config_cli
from .container import container_cli
from .dict import dict_cli
from .environment import environment_cli
from .launch import launch_cli
from .network_file_system import nfs_cli
from .profile import profile_cli
from .queues import queue_cli
from .secret import secret_cli
from .token import _new_token, token_cli
from .volume import volume_cli


def version_callback(value: bool):
    if value:
        from modal_version import __version__

        typer.echo(f"modal client version: {__version__}")
        raise typer.Exit()


entrypoint_cli_typer = typer.Typer(
    no_args_is_help=False,
    add_completion=False,
    rich_markup_mode="markdown",
    help="""
    Modal is the fastest way to run code in the cloud.

    See the website at https://modal.com/ for documentation and more information
    about running code on Modal.
    """,
)


@entrypoint_cli_typer.callback(invoke_without_command=True)
def modal(
    ctx: typer.Context,
    version: bool = typer.Option(None, "--version", callback=version_callback),
):
    # TODO: When https://github.com/fastapi/typer/pull/1240 gets shipped, then
    # - set invoke_without_command=False in the callback decorator
    # - set no_args_is_help=True in entrypoint_cli_typer
    if ctx.invoked_subcommand is None:
        console = make_console()
        console.print(ctx.get_help())
        raise typer.Exit()


def check_path():
    """Checks whether the `modal` executable is on the path and usable."""
    url = "https://modal.com/docs/guide/troubleshooting#command-not-found-errors"
    try:
        subprocess.run(["modal", "--help"], capture_output=True)
        # TODO(erikbern): check returncode?
        return
    except FileNotFoundError:
        text = (
            "[red]The `[white]modal[/white]` command was not found on your path!\n"
            "You may need to add it to your path or use `[white]python -m modal[/white]` as a workaround.[/red]\n"
        )
    except PermissionError:
        text = (
            "[red]The `[white]modal[/white]` command is not executable!\n"
            "You may need to give it permissions or use `[white]python -m modal[/white]` as a workaround.[/red]\n"
        )
    text += f"See more information here:\n\n[link={url}]{url}[/link]\n"
    console = make_console()
    console.print(text)
    console.print(Rule(style="white"))


@synchronizer.create_blocking
async def setup(profile: Optional[str] = None):
    check_path()

    # Fetch a new token (same as `modal token new` but redirect to /home once finishes)
    await _new_token(profile=profile, next_url="/home")


# Commands
entrypoint_cli_typer.command("deploy", no_args_is_help=True)(run.deploy)
entrypoint_cli_typer.command("serve", no_args_is_help=True)(run.serve)
entrypoint_cli_typer.command("shell")(run.shell)
entrypoint_cli_typer.add_typer(launch_cli)

# Deployments
entrypoint_cli_typer.add_typer(app_cli, rich_help_panel="Deployments")
entrypoint_cli_typer.add_typer(container_cli, rich_help_panel="Deployments")
# TODO: cluster is hidden while multi-node is in beta/experimental
entrypoint_cli_typer.add_typer(cluster_cli, rich_help_panel="Deployments", hidden=True)

# Storage
entrypoint_cli_typer.add_typer(dict_cli, rich_help_panel="Storage")
entrypoint_cli_typer.add_typer(nfs_cli, rich_help_panel="Storage")
entrypoint_cli_typer.add_typer(secret_cli, rich_help_panel="Storage")
entrypoint_cli_typer.add_typer(queue_cli, rich_help_panel="Storage")
entrypoint_cli_typer.add_typer(volume_cli, rich_help_panel="Storage")

# Configuration
entrypoint_cli_typer.add_typer(config_cli, rich_help_panel="Configuration")
entrypoint_cli_typer.add_typer(environment_cli, rich_help_panel="Configuration")
entrypoint_cli_typer.add_typer(profile_cli, rich_help_panel="Configuration")
entrypoint_cli_typer.add_typer(token_cli, rich_help_panel="Configuration")

# Hide setup from help as it's redundant with modal token new, but nicer for onboarding
entrypoint_cli_typer.command("setup", help="Bootstrap Modal's configuration.", rich_help_panel="Onboarding")(setup)

# Special handling for modal run, which is more complicated
entrypoint_cli = typer.main.get_command(entrypoint_cli_typer)
entrypoint_cli.add_command(run.run, name="run")  # type: ignore
entrypoint_cli.list_commands(None)  # type: ignore

if __name__ == "__main__":
    # this module is only called from tests, otherwise the parent package __main__.py is used as the entrypoint
    entrypoint_cli()



================================================
FILE: modal/cli/environment.py
================================================
# Copyright Modal Labs 2023
from typing import Annotated, Optional, Union

import typer
from click import UsageError
from grpclib import GRPCError, Status
from rich.text import Text

from modal import environments
from modal._utils.name_utils import check_environment_name
from modal.cli.utils import YES_OPTION, display_table
from modal.config import config
from modal.exception import InvalidError

ENVIRONMENT_HELP_TEXT = """Create and interact with Environments

Environments are sub-divisons of workspaces, allowing you to deploy the same app
in different namespaces. Each environment has their own set of Secrets and any
lookups performed from an app in an environment will by default look for entities
in the same environment.

Typical use cases for environments include having one for development and one for
production, to prevent overwriting production apps when developing new features
while still being able to deploy changes to a live environment.
"""

environment_cli = typer.Typer(name="environment", help=ENVIRONMENT_HELP_TEXT, no_args_is_help=True)


class RenderableBool(Text):
    def __init__(self, value: bool):
        self.value = value

    def __rich__(self):
        return repr(self.value)


@environment_cli.command(name="list", help="List all environments in the current workspace")
def list_(json: Optional[bool] = False):
    envs = environments.list_environments()

    # determine which environment is currently active, prioritizing the local default
    # over the server default
    active_env = config.get("environment")
    for env in envs:
        if env.default is True and active_env is None:
            active_env = env.name

    table_data = []
    for item in envs:
        is_active = item.name == active_env
        is_active_display: Union[Text, str] = str(is_active) if json else RenderableBool(is_active)
        row = [item.name, item.webhook_suffix, is_active_display]
        table_data.append(row)
    display_table(["name", "web suffix", "active"], table_data, json=json)


ENVIRONMENT_CREATE_HELP = """Create a new environment in the current workspace"""


@environment_cli.command(name="create", help=ENVIRONMENT_CREATE_HELP)
def create(name: Annotated[str, typer.Argument(help="Name of the new environment. Must be unique. Case sensitive")]):
    check_environment_name(name)

    try:
        environments.create_environment(name)
    except GRPCError as exc:
        if exc.status == Status.INVALID_ARGUMENT:
            raise InvalidError(exc.message)
        raise
    typer.echo(f"Environment created: {name}")


ENVIRONMENT_DELETE_HELP = """Delete an environment in the current workspace

Deletes all apps in the selected environment and deletes the environment irrevocably.
"""


@environment_cli.command(name="delete", help=ENVIRONMENT_DELETE_HELP)
def delete(
    name: str = typer.Argument(help="Name of the environment to be deleted. Case sensitive"),
    *,
    yes: bool = YES_OPTION,
):
    if not yes:
        typer.confirm(
            (
                f"Are you sure you want to irrevocably delete the environment '{name}' and"
                " all its associated Apps, Secrets, Volumes, Dicts and Queues?"
            ),
            default=False,
            abort=True,
        )

    environments.delete_environment(name)
    typer.echo(f"Environment deleted: {name}")


ENVIRONMENT_UPDATE_HELP = """Update the name or web suffix of an environment"""


@environment_cli.command(name="update", help=ENVIRONMENT_UPDATE_HELP)
def update(
    current_name: str,
    set_name: Optional[str] = typer.Option(default=None, help="New name of the environment"),
    set_web_suffix: Optional[str] = typer.Option(
        default=None, help="New web suffix of environment (empty string is no suffix)"
    ),
):
    if set_name is None and set_web_suffix is None:
        raise UsageError("You need to at least one new property (using --set-name or --set-web-suffix)")

    if set_name:
        check_environment_name(set_name)

    try:
        environments.update_environment(current_name, new_name=set_name, new_web_suffix=set_web_suffix)
    except GRPCError as exc:
        if exc.status == Status.INVALID_ARGUMENT:
            raise InvalidError(exc.message)
        raise

    typer.echo("Environment updated")



================================================
FILE: modal/cli/import_refs.py
================================================
# Copyright Modal Labs 2023
"""Load or import Python modules from the CLI.

For example, the function reference of `modal run some_file.py::app.foo_func`
or the app lookup of `modal deploy some_file.py`.

These functions are only called by the Modal CLI, not in tasks.
"""

import dataclasses
import importlib
import importlib.util
import inspect
import sys
import typing
from collections import defaultdict
from dataclasses import dataclass
from pathlib import Path
from typing import Optional, Union, cast

import click
from rich.markdown import Markdown

from modal._output import make_console
from modal._utils.deprecation import deprecation_warning
from modal.app import App, LocalEntrypoint
from modal.cls import Cls
from modal.exception import InvalidError, _CliUserExecutionError
from modal.functions import Function


@dataclasses.dataclass
class ImportRef:
    file_or_module: str
    use_module_mode: bool  # i.e. using the -m flag

    # object_path is a .-delimited path to the object to execute, or a parent from which to infer the object
    # e.g.
    # function or local_entrypoint in module scope
    # app in module scope [+ method name]
    # app [+ function/entrypoint on that app]
    object_path: str = dataclasses.field(default="")


def parse_import_ref(object_ref: str, use_module_mode: bool = False) -> ImportRef:
    if object_ref.find("::") > 1:
        file_or_module, object_path = object_ref.split("::", 1)
    elif object_ref.find(":") > 1:
        raise InvalidError(f"Invalid object reference: {object_ref}. Did you mean '::' instead of ':'?")
    else:
        file_or_module, object_path = object_ref, ""

    return ImportRef(file_or_module, use_module_mode, object_path)


DEFAULT_APP_NAME = "app"


def import_file_or_module(import_ref: ImportRef, base_cmd: str = ""):
    if "" not in sys.path:
        # When running from a CLI like `modal run`
        # the current working directory isn't added to sys.path
        # so we add it in order to make module path specification possible
        sys.path.insert(0, "")  # "" means the current working directory

    if not import_ref.file_or_module.endswith(".py") or import_ref.use_module_mode:
        if not import_ref.use_module_mode:
            deprecation_warning(
                (2025, 2, 6),
                f"Using Python module paths will require using the -m flag in a future version of Modal.\n"
                f"Use `{base_cmd} -m {import_ref.file_or_module}` instead.",
                show_source=False,
            )
        try:
            module = importlib.import_module(import_ref.file_or_module)
        except Exception as exc:
            raise _CliUserExecutionError(import_ref.file_or_module) from exc
    else:
        # when using a script path, that scripts directory should also be on the path as it is
        # with `python some/script.py`
        full_path = Path(import_ref.file_or_module).resolve()
        if "." in full_path.name.removesuffix(".py"):
            raise InvalidError(
                f"Invalid Modal source filename: {full_path.name!r}."
                "\n\nSource filename cannot contain additional period characters."
            )
        sys.path.insert(0, str(full_path.parent))

        module_name = inspect.getmodulename(import_ref.file_or_module)
        assert module_name is not None
        # Import the module - see https://docs.python.org/3/library/importlib.html#importing-a-source-file-directly
        spec = importlib.util.spec_from_file_location(module_name, import_ref.file_or_module)
        assert spec is not None
        module = importlib.util.module_from_spec(spec)
        sys.modules[module_name] = module
        try:
            assert spec.loader
            spec.loader.exec_module(module)
        except Exception as exc:
            raise _CliUserExecutionError(str(full_path)) from exc

    return module


@dataclass(frozen=True)
class MethodReference:
    """This helps with deferring method reference until after the class gets instantiated by the CLI"""

    cls: Cls
    method_name: str


Runnable = Union[Function, MethodReference, LocalEntrypoint]


@dataclass(frozen=True)
class CLICommand:
    names: list[str]
    runnable: Runnable
    is_web_endpoint: bool
    priority: int


class AutoRunPriority:
    MODULE_LOCAL_ENTRYPOINT = 0
    MODULE_FUNCTION = 1
    APP_LOCAL_ENTRYPOINT = 2
    APP_FUNCTION = 3


def list_cli_commands(
    module_members: dict[str, typing.Any],
) -> list[CLICommand]:
    """
    Extracts all runnables found either directly in the input module, or in any of the Apps listed in that module

    Runnables includes all Functions, (class) Methods and Local Entrypoints, including web endpoints.

    The returned list consists of tuples:
    ([name1, name2...], Runnable)

    Where the first name is always the module level name if such a name exists
    """
    apps = cast(
        list[tuple[str, App]], [(name, member) for name, member in module_members.items() if isinstance(member, App)]
    )

    all_runnables: dict[Runnable, list[str]] = defaultdict(list)
    priorities: dict[Runnable, int] = defaultdict(lambda: AutoRunPriority.APP_FUNCTION)
    for app_name, app in apps:
        for name, local_entrypoint in app.registered_entrypoints.items():
            all_runnables[local_entrypoint].append(f"{app_name}.{name}")
            priorities[local_entrypoint] = AutoRunPriority.APP_LOCAL_ENTRYPOINT
        for name, function in app.registered_functions.items():
            if name.endswith(".*"):
                continue
            all_runnables[function].append(f"{app_name}.{name}")
            priorities[function] = AutoRunPriority.APP_FUNCTION
        for cls_name, cls in app.registered_classes.items():
            for method_name in cls._get_method_names():
                method_ref = MethodReference(cls, method_name)
                all_runnables[method_ref].append(f"{app_name}.{cls_name}.{method_name}")
                priorities[method_ref] = AutoRunPriority.APP_FUNCTION

    # If any class or function is exported as a module level object, use that
    # as the preferred name by putting it first in the list
    module_level_entities = [
        (name, member)
        for name, member in module_members.items()
        if isinstance(member, (Function, Cls, LocalEntrypoint))
    ]
    for name, entity in module_level_entities:
        if isinstance(entity, Cls) and entity._is_local():
            for method_name in entity._get_method_names():
                method_ref = MethodReference(entity, method_name)
                all_runnables.setdefault(method_ref, []).insert(0, f"{name}.{method_name}")
                priorities[method_ref] = AutoRunPriority.MODULE_FUNCTION
        elif isinstance(entity, Function) and entity._is_local():
            all_runnables.setdefault(entity, []).insert(0, name)
            priorities[entity] = AutoRunPriority.MODULE_FUNCTION
        elif isinstance(entity, LocalEntrypoint):
            all_runnables.setdefault(entity, []).insert(0, name)
            priorities[entity] = AutoRunPriority.MODULE_LOCAL_ENTRYPOINT

    def _is_web_endpoint(runnable: Runnable) -> bool:
        if isinstance(runnable, Function) and runnable._is_web_endpoint():
            return True
        elif isinstance(runnable, MethodReference):
            # this is a bit yucky but can hopefully get cleaned up with Cls cleanup:
            method_partial = runnable.cls._get_partial_functions()[runnable.method_name]
            if method_partial._is_web_endpoint():
                return True

        return False

    return [
        CLICommand(names, runnable, _is_web_endpoint(runnable), priority=priorities[runnable])
        for runnable, names in all_runnables.items()
    ]


def filter_cli_commands(
    cli_commands: list[CLICommand],
    name_prefix: str,
    accept_local_entrypoints: bool = True,
    accept_web_endpoints: bool = True,
) -> list[CLICommand]:
    """Filters by name and type of runnable

    Returns generator of (matching names list, CLICommand)
    """

    def _is_accepted_type(cli_command: CLICommand) -> bool:
        if not accept_local_entrypoints and isinstance(cli_command.runnable, LocalEntrypoint):
            return False
        if not accept_web_endpoints and cli_command.is_web_endpoint:
            return False
        return True

    res = []
    for cli_command in cli_commands:
        if not _is_accepted_type(cli_command):
            continue

        if name_prefix in cli_command.names:
            # exact name match
            res.append(cli_command)
            continue

        if not name_prefix:
            # no name specified, return all reachable runnables
            res.append(cli_command)
            continue

        # partial matches e.g. app or class name - should we even allow this?
        prefix_matches = [x for x in cli_command.names if x.startswith(f"{name_prefix}.")]
        if prefix_matches:
            res.append(cli_command)
    return res


def import_app(app_ref: str):
    # TODO: remove when integration tests have been migrated to import_app_from_ref
    return import_app_from_ref(parse_import_ref(app_ref))


def import_app_from_ref(import_ref: ImportRef, base_cmd: str = "") -> App:
    # TODO: default could be to just pick up any app regardless if it's called DEFAULT_APP_NAME
    #  as long as there is a single app in the module?
    import_path = import_ref.file_or_module
    object_path = import_ref.object_path or DEFAULT_APP_NAME

    module = import_file_or_module(import_ref, base_cmd)

    if "." in object_path:
        raise click.UsageError(f"{object_path} is not a Modal App")

    app = getattr(module, object_path)

    if app is None:
        error_console = make_console(stderr=True)
        error_console.print(f"[bold red]Could not find Modal app '{object_path}' in {import_path}.[/bold red]")

        if not object_path:
            guidance_msg = Markdown(
                f"Expected to find an app variable named **`{DEFAULT_APP_NAME}`** (the default app name). "
                "If your `modal.App` is assigned to a different variable name, "
                "you must specify it in the app ref argument. "
                f"For example an App variable `app_2 = modal.App()` in `{import_path}` would "
                f"be specified as `{import_path}::app_2`."
            )
            error_console.print(guidance_msg)

        sys.exit(1)

    if not isinstance(app, App):
        raise click.UsageError(f"{app} is not a Modal App")

    return app


def _show_function_ref_help(app_ref: ImportRef, base_cmd: str) -> None:
    object_path = app_ref.object_path
    import_path = app_ref.file_or_module
    error_console = make_console(stderr=True)
    if object_path:
        error_console.print(
            f"[bold red]Could not find Modal function or local entrypoint"
            f" '{object_path}' in '{import_path}'.[/bold red]"
        )
    else:
        error_console.print(
            f"[bold red]No function was specified, and no [green]`app`[/green] variable "
            f"could be found in '{import_path}'.[/bold red]"
        )
    guidance_msg = f"""
Usage:
{base_cmd} <file_or_module_path>::<function_name>

Given the following example `app.py`:
```
app = modal.App()

@app.function()
def foo():
    ...
```
You would run foo as [bold green]{base_cmd} app.py::foo[/bold green]"""
    error_console.print(guidance_msg)


def _get_runnable_app(runnable: Runnable) -> App:
    if isinstance(runnable, Function):
        return runnable.app
    elif isinstance(runnable, MethodReference):
        return runnable.cls._get_app()
    else:
        assert isinstance(runnable, LocalEntrypoint)
        return runnable.app


def import_and_filter(
    import_ref: ImportRef, *, base_cmd: str, accept_local_entrypoint=True, accept_webhook=False
) -> tuple[Optional[Runnable], list[CLICommand]]:
    """Takes a function ref string and returns a single determined "runnable" to use, and a list of all available
    runnables.

    The function ref can leave out partial information (apart from the file name/module)
    as long as the runnable is uniquely identifiable by the provided information.

    When there are multiple runnables within the provided ref, the following rules should
    be followed:

    1. if there is a single local_entrypoint, that one is used
    2. if there is a single {function, class} that one is used
    3. if there is a single method (within a class) that one is used
    """
    # all commands:
    module = import_file_or_module(import_ref, base_cmd)
    cli_commands = list_cli_commands(dict(inspect.getmembers(module)))

    # all commands that satisfy local entrypoint/accept webhook limitations AND object path prefix

    all_usable_commands = filter_cli_commands(cli_commands, "", accept_local_entrypoint, accept_webhook)
    inferred_runnable = infer_runnable(cli_commands, import_ref.object_path, accept_local_entrypoint, accept_webhook)

    if inferred_runnable:
        # if there is a single command with "highest run prio" - use that
        return inferred_runnable, all_usable_commands

    # otherwise, just return the list of all commands
    return None, all_usable_commands


def infer_runnable(
    cli_commands: list[CLICommand], object_path: str, accept_local_entrypoint: bool, accept_webhook: bool
) -> Optional[Runnable]:
    filtered_commands = filter_cli_commands(cli_commands, object_path, accept_local_entrypoint, accept_webhook)
    if len(filtered_commands) == 0:
        return None

    filtered_commands_by_prio = defaultdict(list)
    for cmd in filtered_commands:
        filtered_commands_by_prio[cmd.priority].append(cmd)

    _, highest_prio_commands = min(filtered_commands_by_prio.items())
    if len(highest_prio_commands) == 1:
        cli_command = highest_prio_commands[0]
        return cli_command.runnable

    return None



================================================
FILE: modal/cli/launch.py
================================================
# Copyright Modal Labs 2023
import asyncio
import inspect
import json
import os
import subprocess
import tempfile
from pathlib import Path
from typing import Any, Optional

import rich.panel
from rich.markdown import Markdown
from typer import Typer

from .._output import make_console
from ..exception import _CliUserExecutionError
from ..output import enable_output
from ..runner import run_app
from .import_refs import ImportRef, _get_runnable_app, import_file_or_module

launch_cli = Typer(
    name="launch",
    no_args_is_help=True,
    rich_markup_mode="markdown",
    help="""
    Open a serverless app instance on Modal.
    >âš ï¸  `modal launch` is **experimental** and may change in the future.
    """,
)


def _launch_program(
    name: str, filename: str, detach: bool, args: dict[str, Any], *, description: Optional[str] = None
) -> None:
    console = make_console()
    console.print(
        rich.panel.Panel(
            Markdown(f"âš ï¸  `modal launch {name}` is **experimental** and may change in the future."),
            border_style="yellow",
        ),
    )

    os.environ["MODAL_LAUNCH_ARGS"] = json.dumps(args)

    program_path = str(Path(__file__).parent / "programs" / filename)
    base_cmd = f"modal launch {name}"
    module = import_file_or_module(ImportRef(program_path, use_module_mode=False), base_cmd=base_cmd)
    entrypoint = module.main

    app = _get_runnable_app(entrypoint)
    app.set_description(description if description else base_cmd)

    # `launch/` scripts must have a `local_entrypoint()` with no args, for simplicity here.
    func = entrypoint.info.raw_f
    isasync = inspect.iscoroutinefunction(func)
    with enable_output():
        with run_app(app, detach=detach):
            try:
                if isasync:
                    asyncio.run(func())
                else:
                    func()
            except Exception as exc:
                raise _CliUserExecutionError(inspect.getsourcefile(func)) from exc


@launch_cli.command(name="jupyter", help="Start Jupyter Lab on Modal.")
def jupyter(
    cpu: int = 8,
    memory: int = 32768,
    gpu: Optional[str] = None,
    timeout: int = 3600,
    image: str = "ubuntu:22.04",
    add_python: Optional[str] = "3.11",
    mount: Optional[str] = None,  # Adds a local directory to the jupyter container
    volume: Optional[str] = None,  # Attach a persisted `modal.Volume` by name (creating if missing).
    detach: bool = False,  # Run the app in "detached" mode to persist after local client disconnects
):
    console = make_console()
    console.print(
        rich.panel.Panel(
            (
                "[link=https://modal.com/notebooks]Try Modal Notebooks! "
                "modal.com/notebooks[/link]\n"
                "Notebooks have a new UI, saved content, real-time collaboration and more."
            ),
        ),
        style="bold cyan",
    )
    args = {
        "cpu": cpu,
        "memory": memory,
        "gpu": gpu,
        "timeout": timeout,
        "image": image,
        "add_python": add_python,
        "mount": mount,
        "volume": volume,
    }
    _launch_program("jupyter", "run_jupyter.py", detach, args)


@launch_cli.command(name="vscode", help="Start Visual Studio Code on Modal.")
def vscode(
    cpu: int = 8,
    memory: int = 32768,
    gpu: Optional[str] = None,
    image: str = "debian:12",
    timeout: int = 3600,
    mount: Optional[str] = None,  # Create a `modal.Mount` from a local directory.
    volume: Optional[str] = None,  # Attach a persisted `modal.Volume` by name (creating if missing).
    detach: bool = False,  # Run the app in "detached" mode to persist after local client disconnects
):
    args = {
        "cpu": cpu,
        "memory": memory,
        "gpu": gpu,
        "image": image,
        "timeout": timeout,
        "mount": mount,
        "volume": volume,
    }
    _launch_program("vscode", "vscode.py", detach, args)


@launch_cli.command(name="machine", help="Start an instance on Modal, with direct SSH access.", hidden=True)
def machine(
    name: str,  # Name of the machine App.
    cpu: int = 8,  # Reservation of CPU cores (can burst above this value).
    memory: int = 32768,  # Reservation of memory in MiB (can burst above this value).
    gpu: Optional[str] = None,  # GPU type and count, e.g. "t4" or "h100:2".
    image: Optional[str] = None,  # Image tag to use from registry. Defaults to the notebook base image.
    timeout: int = 3600 * 24,  # Timeout in seconds for the instance.
    volume: str = "machine-vol",  # Attach a persisted `modal.Volume` at /workspace (created if missing).
):
    tempdir = Path(tempfile.gettempdir())
    key_path = tempdir / "modal-machine-keyfile.pem"
    # Generate a new SSH key pair for this machine instance.
    if not key_path.exists():
        subprocess.run(
            ["ssh-keygen", "-t", "ed25519", "-f", str(key_path), "-N", ""],
            check=True,
            stdout=subprocess.DEVNULL,
        )
    # Add the key with expiry 1d to ssh agent.
    subprocess.run(
        ["ssh-add", "-t", "1d", str(key_path)],
        check=True,
        stdout=subprocess.DEVNULL,
        stderr=subprocess.DEVNULL,
    )

    os.environ["SSH_PUBLIC_KEY"] = Path(str(key_path) + ".pub").read_text()
    os.environ["MODAL_LOGS_TIMEOUT"] = "0"  # hack to work with --detach

    args = {
        "cpu": cpu,
        "memory": memory,
        "gpu": gpu,
        "image": image,
        "timeout": timeout,
        "volume": volume,
    }
    _launch_program(
        "machine",
        "launch_instance_ssh.py",
        True,
        args,
        description=name,
    )


@launch_cli.command(name="marimo", help="Start a remote Marimo notebook on Modal.", hidden=True)
def marimo(
    cpu: int = 8,
    memory: int = 32768,
    gpu: Optional[str] = None,
    image: str = "debian:12",
    timeout: int = 3600,
    add_python: Optional[str] = "3.12",
    mount: Optional[str] = None,  # Create a `modal.Mount` from a local directory.
    volume: Optional[str] = None,  # Attach a persisted `modal.Volume` by name (creating if missing).
    detach: bool = False,  # Run the app in "detached" mode to persist after local client disconnects
):
    args = {
        "cpu": cpu,
        "memory": memory,
        "gpu": gpu,
        "timeout": timeout,
        "image": image,
        "add_python": add_python,
        "mount": mount,
        "volume": volume,
    }
    _launch_program("marimo", "run_marimo.py", detach, args)



================================================
FILE: modal/cli/network_file_system.py
================================================
# Copyright Modal Labs 2022
import os
import sys
from pathlib import Path
from typing import Optional

import typer
from click import UsageError
from grpclib import GRPCError, Status
from rich.syntax import Syntax
from rich.table import Table
from typer import Argument, Typer

import modal
from modal._location import display_location
from modal._output import OutputManager, ProgressHandler, make_console
from modal._utils.async_utils import synchronizer
from modal._utils.grpc_utils import retry_transient_errors
from modal._utils.time_utils import timestamp_to_localized_str
from modal.cli._download import _volume_download
from modal.cli.utils import ENV_OPTION, YES_OPTION, display_table
from modal.client import _Client
from modal.environments import ensure_env
from modal.network_file_system import _NetworkFileSystem
from modal_proto import api_pb2

nfs_cli = Typer(name="nfs", help="Read and edit `modal.NetworkFileSystem` file systems.", no_args_is_help=True)


@nfs_cli.command(name="list", help="List the names of all network file systems.", rich_help_panel="Management")
@synchronizer.create_blocking
async def list_(env: Optional[str] = ENV_OPTION, json: Optional[bool] = False):
    env = ensure_env(env)

    client = await _Client.from_env()
    response = await retry_transient_errors(
        client.stub.SharedVolumeList, api_pb2.SharedVolumeListRequest(environment_name=env)
    )
    env_part = f" in environment '{env}'" if env else ""
    column_names = ["Name", "Location", "Created at"]
    rows = []
    for item in response.items:
        rows.append(
            [
                item.label,
                display_location(item.cloud_provider),
                timestamp_to_localized_str(item.created_at, json),
            ]
        )
    display_table(column_names, rows, json, title=f"Shared Volumes{env_part}")


def gen_usage_code(label):
    return f"""
@app.function(network_file_systems={{"/my_vol": modal.NetworkFileSystem.from_name("{label}")}})
def some_func():
    os.listdir("/my_vol")
"""


@nfs_cli.command(name="create", help="Create a named network file system.", rich_help_panel="Management")
def create(
    name: str,
    env: Optional[str] = ENV_OPTION,
):
    ensure_env(env)
    modal.NetworkFileSystem.create_deployed(name, environment_name=env)
    console = make_console()
    console.print(f"Created volume '{name}'. \n\nCode example:\n")
    usage = Syntax(gen_usage_code(name), "python")
    console.print(usage)


@nfs_cli.command(
    name="ls",
    help="List files and directories in a network file system.",
    rich_help_panel="File operations",
)
@synchronizer.create_blocking
async def ls(
    volume_name: str,
    path: str = typer.Argument(default="/"),
    env: Optional[str] = ENV_OPTION,
):
    ensure_env(env)
    volume = _NetworkFileSystem.from_name(volume_name)
    try:
        entries = await volume.listdir(path)
    except GRPCError as exc:
        if exc.status in (Status.INVALID_ARGUMENT, Status.NOT_FOUND):
            raise UsageError(exc.message)
        raise

    if sys.stdout.isatty():
        console = make_console()
        console.print(f"Directory listing of '{path}' in '{volume_name}'")
        table = Table()

        table.add_column("filename")
        table.add_column("type")

        for entry in entries:
            filetype = "dir" if entry.type == api_pb2.FileEntry.FileType.DIRECTORY else "file"
            table.add_row(entry.path, filetype)
        console.print(table)
    else:
        for entry in entries:
            print(entry.path)


@nfs_cli.command(
    name="put",
    help="""Upload a file or directory to a network file system.

Remote parent directories will be created as needed.

Ending the REMOTE_PATH with a forward slash (/), it's assumed to be a directory and the file
will be uploaded with its current name under that directory.
""",
    rich_help_panel="File operations",
)
@synchronizer.create_blocking
async def put(
    volume_name: str,
    local_path: str,
    remote_path: str = typer.Argument(default="/"),
    env: Optional[str] = ENV_OPTION,
):
    ensure_env(env)
    volume = _NetworkFileSystem.from_name(volume_name)
    if remote_path.endswith("/"):
        remote_path = remote_path + os.path.basename(local_path)
    console = make_console()

    if Path(local_path).is_dir():
        progress_handler = ProgressHandler(type="upload", console=console)
        with progress_handler.live:
            await volume.add_local_dir(local_path, remote_path, progress_cb=progress_handler.progress)
            progress_handler.progress(complete=True)
        console.print(OutputManager.step_completed(f"Uploaded directory '{local_path}' to '{remote_path}'"))

    elif "*" in local_path:
        raise UsageError("Glob uploads are currently not supported")
    else:
        progress_handler = ProgressHandler(type="upload", console=console)
        with progress_handler.live:
            written_bytes = await volume.add_local_file(local_path, remote_path, progress_cb=progress_handler.progress)
            progress_handler.progress(complete=True)
        console.print(
            OutputManager.step_completed(
                f"Uploaded file '{local_path}' to '{remote_path}' ({written_bytes} bytes written)"
            )
        )


class CliError(Exception):
    def __init__(self, message):
        self.message = message


@nfs_cli.command(name="get", rich_help_panel="File operations")
@synchronizer.create_blocking
async def get(
    volume_name: str,
    remote_path: str,
    local_destination: str = typer.Argument("."),
    force: bool = False,
    env: Optional[str] = ENV_OPTION,
):
    """Download a file from a network file system.

    Specifying a glob pattern (using any `*` or `**` patterns) as the `remote_path` will download
    all matching files, preserving their directory structure.

    For example, to download an entire network file system into `dump_volume`:

    ```
    modal nfs get <volume-name> "**" dump_volume
    ```

    Use "-" as LOCAL_DESTINATION to write file contents to standard output.
    """
    ensure_env(env)
    destination = Path(local_destination)
    volume = _NetworkFileSystem.from_name(volume_name)
    console = make_console()
    progress_handler = ProgressHandler(type="download", console=console)
    with progress_handler.live:
        await _volume_download(volume, remote_path, destination, force, progress_cb=progress_handler.progress)
    console.print(OutputManager.step_completed("Finished downloading files to local!"))


@nfs_cli.command(
    name="rm", help="Delete a file or directory from a network file system.", rich_help_panel="File operations"
)
@synchronizer.create_blocking
async def rm(
    volume_name: str,
    remote_path: str,
    recursive: bool = typer.Option(False, "-r", "--recursive", help="Delete directory recursively"),
    env: Optional[str] = ENV_OPTION,
):
    ensure_env(env)
    volume = _NetworkFileSystem.from_name(volume_name)
    console = make_console()
    try:
        await volume.remove_file(remote_path, recursive=recursive)
        console.print(OutputManager.step_completed(f"{remote_path} was deleted successfully!"))

    except GRPCError as exc:
        if exc.status in (Status.NOT_FOUND, Status.INVALID_ARGUMENT):
            raise UsageError(exc.message)
        raise


@nfs_cli.command(
    name="delete",
    help="Delete a named, persistent modal.NetworkFileSystem.",
    rich_help_panel="Management",
)
@synchronizer.create_blocking
async def delete(
    nfs_name: str = Argument(help="Name of the modal.NetworkFileSystem to be deleted. Case sensitive"),
    yes: bool = YES_OPTION,
    env: Optional[str] = ENV_OPTION,
):
    # Lookup first to validate the name, even though delete is a staticmethod
    await _NetworkFileSystem.from_name(nfs_name, environment_name=env).hydrate()
    if not yes:
        typer.confirm(
            f"Are you sure you want to irrevocably delete the modal.NetworkFileSystem '{nfs_name}'?",
            default=False,
            abort=True,
        )

    await _NetworkFileSystem.delete(nfs_name, environment_name=env)



================================================
FILE: modal/cli/profile.py
================================================
# Copyright Modal Labs 2022

import asyncio
import os
from typing import Optional

import typer
from rich.json import JSON
from rich.table import Table

from modal._output import make_console
from modal._utils.async_utils import synchronizer
from modal.config import Config, _lookup_workspace, _profile, config_profiles, config_set_active_profile
from modal.exception import AuthError

profile_cli = typer.Typer(name="profile", help="Switch between Modal profiles.", no_args_is_help=True)


@profile_cli.command(help="Change the active Modal profile.")
def activate(profile: str = typer.Argument(..., help="Modal profile to activate.")):
    config_set_active_profile(profile)
    typer.echo(f"Active profile: {profile}")


@profile_cli.command(help="Print the currently active Modal profile.")
def current():
    typer.echo(_profile)


@profile_cli.command(name="list", help="Show all Modal profiles and highlight the active one.")
@synchronizer.create_blocking
async def list_(json: Optional[bool] = False):
    config = Config()
    profiles = config_profiles()
    lookup_coros = [
        _lookup_workspace(
            config.get("server_url", profile),
            config.get("token_id", profile, use_env=False),
            config.get("token_secret", profile, use_env=False),
        )
        for profile in profiles
    ]
    responses = await asyncio.gather(*lookup_coros, return_exceptions=True)

    rows = []
    for profile, resp in zip(profiles, responses):
        active = profile == _profile
        if isinstance(resp, AuthError):
            workspace = "Unknown (authentication failure)"
        elif isinstance(resp, TimeoutError):
            workspace = "Unknown (timed out)"
        elif isinstance(resp, Exception):
            # Catch-all for other exceptions, like incorrect server url
            workspace = "Unknown (profile misconfigured)"
        else:
            assert hasattr(resp, "username")
            workspace = resp.username
        content = ["â€¢" if active else "", profile, workspace]
        rows.append((active, content))

    env_based_workspace: Optional[str] = None
    if "MODAL_TOKEN_ID" in os.environ:
        try:
            env_based_resp = await _lookup_workspace(
                config.get("server_url", _profile),
                os.environ.get("MODAL_TOKEN_ID"),
                os.environ.get("MODAL_TOKEN_SECRET"),
            )
            env_based_workspace = env_based_resp.username
        except AuthError:
            env_based_workspace = "Unknown (authentication failure)"

    console = make_console()
    highlight = "bold green" if env_based_workspace is None else "yellow"
    if json:
        json_data = []
        for active, content in rows:
            json_data.append({"name": content[1], "workspace": content[2], "active": active})
        console.print(JSON.from_data(json_data))
    else:
        table = Table(" ", "Profile", "Workspace")
        for active, content in rows:
            table.add_row(*content, style=highlight if active else "dim")
        console.print(table)

    if env_based_workspace is not None:
        console.print(
            f"Using [bold]{env_based_workspace}[/bold] workspace based on environment variables", style="yellow"
        )



================================================
FILE: modal/cli/queues.py
================================================
# Copyright Modal Labs 2024
from datetime import datetime
from typing import Optional

import typer
from typer import Argument, Option, Typer

from modal._output import make_console
from modal._resolver import Resolver
from modal._utils.async_utils import synchronizer
from modal._utils.grpc_utils import retry_transient_errors
from modal._utils.time_utils import timestamp_to_localized_str
from modal.cli.utils import ENV_OPTION, YES_OPTION, display_table
from modal.client import _Client
from modal.environments import ensure_env
from modal.queue import _Queue
from modal_proto import api_pb2

queue_cli = Typer(
    name="queue",
    no_args_is_help=True,
    help="Manage `modal.Queue` objects and inspect their contents.",
)

PARTITION_OPTION = Option(
    None,
    "-p",
    "--partition",
    help="Name of the partition to use, otherwise use the default (anonymous) partition.",
)


@queue_cli.command(name="create", rich_help_panel="Management")
@synchronizer.create_blocking
async def create(name: str, *, env: Optional[str] = ENV_OPTION):
    """Create a named Queue.

    Note: This is a no-op when the Queue already exists.
    """
    q = _Queue.from_name(name, environment_name=env, create_if_missing=True)
    client = await _Client.from_env()
    resolver = Resolver(client=client)
    await resolver.load(q)


@queue_cli.command(name="delete", rich_help_panel="Management")
@synchronizer.create_blocking
async def delete(
    name: str,
    *,
    allow_missing: bool = Option(False, "--allow-missing", help="Don't error if the Queue doesn't exist."),
    yes: bool = YES_OPTION,
    env: Optional[str] = ENV_OPTION,
):
    """Delete a named Queue and all of its data."""
    env = ensure_env(env)
    if not yes:
        typer.confirm(
            f"Are you sure you want to irrevocably delete the modal.Queue '{name}'?",
            default=False,
            abort=True,
        )
    await _Queue.objects.delete(name, environment_name=env, allow_missing=allow_missing)


@queue_cli.command(name="list", rich_help_panel="Management")
@synchronizer.create_blocking
async def list_(*, json: bool = False, env: Optional[str] = ENV_OPTION):
    """List all named Queues."""
    env = ensure_env(env)
    client = await _Client.from_env()
    max_total_size = 100_000  # Limit on the *Queue size* that we report

    items: list[api_pb2.QueueListResponse.QueueInfo] = []

    # Note that we need to continue using the gRPC API directly here rather than using Queue.objects.list.
    # There is some metadata that historically appears in the CLI output (num_partitions, total_size) that
    # doesn't make sense to transmit as hydration metadata, because the values can change over time and
    # the metadata retrieved at hydration time could get stale. Alternatively, we could rewrite this using
    # only public API by sequentially retrieving the queues and then querying their dynamic metadata, but
    # that would require multiple round trips and would add lag to the CLI.
    async def retrieve_page(created_before: float) -> bool:
        max_page_size = 100
        pagination = api_pb2.ListPagination(max_objects=max_page_size, created_before=created_before)
        req = api_pb2.QueueListRequest(environment_name=env, pagination=pagination, total_size_limit=max_total_size)
        resp = await retry_transient_errors(client.stub.QueueList, req)
        items.extend(resp.queues)
        return len(resp.queues) < max_page_size

    finished = await retrieve_page(datetime.now().timestamp())
    while True:
        if finished:
            break
        finished = await retrieve_page(items[-1].metadata.creation_info.created_at)

    queues = [_Queue._new_hydrated(item.queue_id, client, item.metadata, is_another_app=True) for item in items]

    rows = []
    for obj, resp_data in zip(queues, items):
        info = await obj.info()
        rows.append(
            (
                obj.name,
                timestamp_to_localized_str(info.created_at.timestamp(), json),
                info.created_by,
                str(resp_data.num_partitions),
                str(resp_data.total_size) if resp_data.total_size <= max_total_size else f">{max_total_size}",
            )
        )
    display_table(["Name", "Created at", "Created by", "Partitions", "Total size"], rows, json)


@queue_cli.command(name="clear", rich_help_panel="Management")
@synchronizer.create_blocking
async def clear(
    name: str,
    partition: Optional[str] = PARTITION_OPTION,
    all: bool = Option(False, "-a", "--all", help="Clear the contents of all partitions."),
    yes: bool = YES_OPTION,
    *,
    env: Optional[str] = ENV_OPTION,
):
    """Clear the contents of a queue by removing all of its data."""
    q = _Queue.from_name(name, environment_name=env)
    if not yes:
        typer.confirm(
            f"Are you sure you want to irrevocably delete the contents of modal.Queue '{name}'?",
            default=False,
            abort=True,
        )
    await q.clear(partition=partition, all=all)


@queue_cli.command(name="peek", rich_help_panel="Inspection")
@synchronizer.create_blocking
async def peek(
    name: str, n: int = Argument(1), partition: Optional[str] = PARTITION_OPTION, *, env: Optional[str] = ENV_OPTION
):
    """Print the next N items in the queue or queue partition (without removal)."""
    q = _Queue.from_name(name, environment_name=env)
    console = make_console()
    i = 0
    async for item in q.iterate(partition=partition):
        console.print(item)
        i += 1
        if i >= n:
            break


@queue_cli.command(name="len", rich_help_panel="Inspection")
@synchronizer.create_blocking
async def len_(
    name: str,
    partition: Optional[str] = PARTITION_OPTION,
    total: bool = Option(False, "-t", "--total", help="Compute the sum of the queue lengths across all partitions"),
    *,
    env: Optional[str] = ENV_OPTION,
):
    """Print the length of a queue partition or the total length of all partitions."""
    q = _Queue.from_name(name, environment_name=env)
    console = make_console()
    console.print(await q.len(partition=partition, total=total))



================================================
FILE: modal/cli/run.py
================================================
# Copyright Modal Labs 2022
import asyncio
import functools
import inspect
import platform
import re
import shlex
import sys
import time
import typing
from dataclasses import dataclass
from functools import partial
from typing import Any, Callable, Optional

import click
import typer
from click import ClickException
from typing_extensions import TypedDict

from .._functions import _FunctionSpec
from ..app import App, LocalEntrypoint
from ..cls import _get_class_constructor_signature
from ..config import config
from ..environments import ensure_env
from ..exception import ExecutionError, InvalidError, NotFoundError, _CliUserExecutionError
from ..functions import Function
from ..image import Image
from ..output import enable_output
from ..runner import deploy_app, interactive_shell, run_app
from ..secret import Secret
from ..serving import serve_app
from ..volume import Volume
from .import_refs import (
    CLICommand,
    MethodReference,
    _get_runnable_app,
    import_and_filter,
    import_app_from_ref,
    parse_import_ref,
)
from .utils import ENV_OPTION, ENV_OPTION_HELP, is_tty, stream_app_logs


class ParameterMetadata(TypedDict):
    name: str
    default: Any
    annotation: Any
    type_hint: Any  # same as annotation but evaluated by typing.get_type_hints
    kind: Any


class AnyParamType(click.ParamType):
    name = "any"

    def convert(self, value, param, ctx):
        return value


option_parsers = {
    "str": str,
    "int": int,
    "float": float,
    "bool": bool,
    "datetime.datetime": click.DateTime(),
    "Any": AnyParamType(),
}


class NoParserAvailable(InvalidError):
    pass


@dataclass
class CliRunnableSignature:
    parameters: dict[str, ParameterMetadata]
    has_variadic_args: bool


def safe_get_type_hints(func_or_cls: typing.Union[Callable[..., Any], type]) -> dict[str, type]:
    try:
        return typing.get_type_hints(func_or_cls)
    except Exception as exc:
        # E.g., if entrypoint type hints cannot be evaluated by local Python runtime
        msg = "Unable to generate command line interface for app entrypoint due to unparseable type hints:\n" + str(exc)
        raise ExecutionError(msg) from exc


def _get_cli_runnable_signature(sig: inspect.Signature, type_hints: dict[str, type]) -> CliRunnableSignature:
    has_variadic_args = False
    signature: dict[str, ParameterMetadata] = {}
    for param in sig.parameters.values():
        if param.kind == inspect.Parameter.VAR_POSITIONAL:
            has_variadic_args = True
        else:
            signature[param.name] = {
                "name": param.name,
                "default": param.default,
                "annotation": param.annotation,
                "type_hint": type_hints.get(param.name, "Any"),
                "kind": param.kind,
            }

    if has_variadic_args and len(signature) > 0:
        raise InvalidError("Functions with variable-length positional arguments (*args) cannot have other parameters.")

    return CliRunnableSignature(signature, has_variadic_args)


def _get_param_type_as_str(annot: Any) -> str:
    """Return annotation as a string, handling various spellings for optional types."""
    annot_str = str(annot)
    annot_patterns = [
        r"typing\.Optional\[([\w.]+)\]",
        r"typing\.Union\[([\w.]+), NoneType\]",
        r"([\w.]+) \| None",
        r"<class '([\w\.]+)'>",
    ]
    for pat in annot_patterns:
        m = re.match(pat, annot_str)
        if m is not None:
            return m.group(1)
    return annot_str


def _add_click_options(func, parameters: dict[str, ParameterMetadata]):
    """Adds @click.option based on function signature

    Kind of like typer, but using options instead of positional arguments
    """
    for param in parameters.values():
        param_type_str = _get_param_type_as_str(param["type_hint"])
        param_name = param["name"].replace("_", "-")
        cli_name = "--" + param_name
        if param_type_str == "bool":
            cli_name += "/--no-" + param_name

        parser = option_parsers.get(param_type_str)
        if parser is None:
            msg = f"Parameter `{param_name}` has unparseable annotation: {param['annotation']!r}"
            raise NoParserAvailable(msg)
        kwargs: Any = {
            "type": parser,
        }
        if param["default"] is not inspect.Signature.empty:
            kwargs["default"] = param["default"]
        else:
            kwargs["required"] = True

        click.option(cli_name, **kwargs)(func)
    return func


def _get_clean_app_description(func_ref: str) -> str:
    # If possible, consider the 'ref' argument the start of the app's args. Everything
    # before it Modal CLI cruft (eg. `modal run --detach`).
    try:
        func_ref_arg_idx = sys.argv.index(func_ref)
        return " ".join(sys.argv[func_ref_arg_idx:])
    except ValueError:
        return " ".join(sys.argv)


def _write_local_result(result_path: str, res: Any):
    if isinstance(res, str):
        mode = "wt"
    elif isinstance(res, bytes):
        mode = "wb"
    else:
        res_type = type(res).__name__
        raise InvalidError(f"Function must return str or bytes when using `--write-result`; got {res_type}.")
    with open(result_path, mode) as fid:
        fid.write(res)


def _validate_interactive_quiet_params(ctx):
    interactive = ctx.obj["interactive"]
    show_progress = ctx.obj["show_progress"]

    if not show_progress and interactive:
        raise InvalidError("To use interactive mode, remove the --quiet flag")


def _make_click_function(app, signature: CliRunnableSignature, inner: Callable[[tuple[str, ...], dict[str, Any]], Any]):
    @click.pass_context
    def f(ctx, **kwargs):
        if signature.has_variadic_args:
            assert len(kwargs) == 0
            args = ctx.args
        else:
            args = ()

        _validate_interactive_quiet_params(ctx)

        show_progress: bool = ctx.obj["show_progress"]
        with enable_output(show_progress):
            with run_app(
                app,
                detach=ctx.obj["detach"],
                environment_name=ctx.obj["env"],
                interactive=ctx.obj["interactive"],
            ):
                res = inner(args, kwargs)

            if result_path := ctx.obj["result_path"]:
                _write_local_result(result_path, res)

    return f


def _get_click_command_for_function(app: App, function: Function, ctx: click.Context):
    if function.is_generator:
        raise InvalidError("`modal run` is not supported for generator functions")

    sig: inspect.Signature = inspect.signature(function.info.raw_f)
    type_hints = safe_get_type_hints(function.info.raw_f)
    signature: CliRunnableSignature = _get_cli_runnable_signature(sig, type_hints)

    def _inner(args, click_kwargs):
        if ctx.obj["detach"]:
            return function.spawn(*args, **click_kwargs).get()
        else:
            return function.remote(*args, **click_kwargs)

    f = _make_click_function(app, signature, _inner)

    with_click_options = _add_click_options(f, signature.parameters)

    if signature.has_variadic_args:
        return click.command(context_settings={"ignore_unknown_options": True, "allow_extra_args": True})(
            with_click_options
        )
    else:
        return click.command(with_click_options)


def _get_click_command_for_cls(app: App, method_ref: MethodReference, ctx: click.Context):
    parameters: dict[str, ParameterMetadata]
    cls = method_ref.cls
    method_name = method_ref.method_name

    user_cls = cls._get_user_cls()
    type_hints = safe_get_type_hints(user_cls)
    sig: inspect.Signature = _get_class_constructor_signature(user_cls)
    cls_signature: CliRunnableSignature = _get_cli_runnable_signature(sig, type_hints)

    if cls_signature.has_variadic_args:
        raise InvalidError("Modal classes cannot have variable-length positional arguments (*args).")

    partial_functions = cls._get_partial_functions()

    if method_name in ("*", ""):
        # auto infer method name - not sure if we have to support this...
        method_names = list(partial_functions.keys())
        if len(method_names) == 1:
            method_name = method_names[0]
        else:
            raise click.UsageError(
                f"Please specify a specific method of {cls._get_name()} to run, e.g. `modal run foo.py::MyClass.bar`"  # noqa: E501
            )

    partial_function = partial_functions[method_name]
    raw_f = partial_function._get_raw_f()
    sig_without_self = inspect.signature(functools.partial(raw_f, None))
    fun_signature = _get_cli_runnable_signature(sig_without_self, safe_get_type_hints(raw_f))

    # TODO(erikbern): assert there's no overlap?
    parameters = dict(**cls_signature.parameters, **fun_signature.parameters)  # Pool all arguments

    def _inner(args, click_kwargs):
        # unpool class and method arguments
        # TODO(erikbern): this code is a bit hacky
        cls_kwargs = {k: click_kwargs[k] for k in cls_signature.parameters}
        fun_kwargs = {k: click_kwargs[k] for k in fun_signature.parameters}

        instance = cls(**cls_kwargs)
        method: Function = getattr(instance, method_name)
        if ctx.obj["detach"]:
            return method.spawn(*args, **fun_kwargs).get()
        else:
            return method.remote(*args, **fun_kwargs)

    f = _make_click_function(app, fun_signature, _inner)
    with_click_options = _add_click_options(f, parameters)

    if fun_signature.has_variadic_args:
        return click.command(context_settings={"ignore_unknown_options": True, "allow_extra_args": True})(
            with_click_options
        )
    else:
        return click.command(with_click_options)


def _get_click_command_for_local_entrypoint(app: App, entrypoint: LocalEntrypoint):
    func = entrypoint.info.raw_f
    isasync = inspect.iscoroutinefunction(func)

    signature = _get_cli_runnable_signature(inspect.signature(func), safe_get_type_hints(func))

    @click.pass_context
    def f(ctx, *args, **kwargs):
        if ctx.obj["detach"]:
            print(
                "Note that running a local entrypoint in detached mode only keeps the last "
                "triggered Modal function alive after the parent process has been killed or disconnected."
            )

        if signature.has_variadic_args:
            assert len(args) == 0 and len(kwargs) == 0
            args = ctx.args

        _validate_interactive_quiet_params(ctx)

        show_progress: bool = ctx.obj["show_progress"]
        with enable_output(show_progress):
            with run_app(
                app,
                detach=ctx.obj["detach"],
                environment_name=ctx.obj["env"],
                interactive=ctx.obj["interactive"],
            ):
                try:
                    if isasync:
                        res = asyncio.run(func(*args, **kwargs))
                    else:
                        res = func(*args, **kwargs)
                except Exception as exc:
                    raise _CliUserExecutionError(inspect.getsourcefile(func)) from exc

            if result_path := ctx.obj["result_path"]:
                _write_local_result(result_path, res)

    with_click_options = _add_click_options(f, signature.parameters)

    if signature.has_variadic_args:
        return click.command(context_settings={"ignore_unknown_options": True, "allow_extra_args": True})(
            with_click_options
        )
    else:
        return click.command(with_click_options)


def _get_runnable_list(all_usable_commands: list[CLICommand]) -> str:
    usable_command_lines = []
    for cmd in all_usable_commands:
        cmd_names = " / ".join(cmd.names)
        usable_command_lines.append(cmd_names)

    return "\n".join(usable_command_lines)


class RunGroup(click.Group):
    def get_command(self, ctx, func_ref):
        # note: get_command here is run before the "group logic" in the `run` logic below
        # so to ensure that `env` has been globally populated before user code is loaded, it
        # needs to be handled here, and not in the `run` logic below
        ctx.ensure_object(dict)
        ctx.obj["env"] = ensure_env(ctx.params["env"])

        import_ref = parse_import_ref(func_ref, use_module_mode=ctx.params["m"])
        runnable, all_usable_commands = import_and_filter(
            import_ref, base_cmd="modal run", accept_local_entrypoint=True, accept_webhook=False
        )
        if not runnable:
            help_header = (
                "Specify a Modal Function or local entrypoint to run. E.g.\n"
                f"> modal run {import_ref.file_or_module}::my_function [..args]"
            )

            if all_usable_commands:
                help_footer = f"'{import_ref.file_or_module}' has the following functions and local entrypoints:\n"
                help_footer += _get_runnable_list(all_usable_commands)
            else:
                help_footer = f"'{import_ref.file_or_module}' has no functions or local entrypoints."

            raise ClickException(f"{help_header}\n\n{help_footer}")

        app = _get_runnable_app(runnable)

        if app.description is None:
            app.set_description(_get_clean_app_description(func_ref))

        if isinstance(runnable, LocalEntrypoint):
            click_command = _get_click_command_for_local_entrypoint(app, runnable)
        elif isinstance(runnable, Function):
            click_command = _get_click_command_for_function(app, runnable, ctx)
        elif isinstance(runnable, MethodReference):
            click_command = _get_click_command_for_cls(app, runnable, ctx)
        else:
            # This should be unreachable...
            raise ValueError(f"{runnable} is neither function, local entrypoint or class/method")
        return click_command


@click.group(
    cls=RunGroup,
    subcommand_metavar="FUNC_REF",
)
@click.option("-w", "--write-result", help="Write return value (which must be str or bytes) to this local path.")
@click.option("-q", "--quiet", is_flag=True, help="Don't show Modal progress indicators.")
@click.option("-d", "--detach", is_flag=True, help="Don't stop the app if the local process dies or disconnects.")
@click.option("-i", "--interactive", is_flag=True, help="Run the app in interactive mode.")
@click.option("-e", "--env", help=ENV_OPTION_HELP, default=None)
@click.option("-m", is_flag=True, help="Interpret argument as a Python module path instead of a file/script path")
@click.pass_context
def run(ctx, write_result, detach, quiet, interactive, env, m):
    """Run a Modal function or local entrypoint.

    `FUNC_REF` should be of the format `{file or module}::{function name}`.
    Alternatively, you can refer to the function via the app:

    `{file or module}::{app variable name}.{function name}`

    **Examples:**

    To run the hello_world function (or local entrypoint) in my_app.py:

    ```
    modal run my_app.py::hello_world
    ```

    If your module only has a single app and your app has a
    single local entrypoint (or single function), you can omit the app and
    function parts:

    ```
    modal run my_app.py
    ```

    Instead of pointing to a file, you can also use the Python module path, which
    by default will ensure that your remote functions will use the same module
    names as they do locally.

    ```
    modal run -m my_project.my_app
    ```
    """
    ctx.ensure_object(dict)
    ctx.obj["result_path"] = write_result
    ctx.obj["detach"] = detach  # if subcommand would be a click command...
    ctx.obj["show_progress"] = False if quiet else True
    ctx.obj["interactive"] = interactive


def deploy(
    app_ref: str = typer.Argument(..., help="Path to a Python file with an app to deploy"),
    name: str = typer.Option("", help="Name of the deployment."),
    env: str = ENV_OPTION,
    stream_logs: bool = typer.Option(False, help="Stream logs from the app upon deployment."),
    tag: str = typer.Option("", help="Tag the deployment with a version."),
    use_module_mode: bool = typer.Option(
        False, "-m", help="Interpret argument as a Python module path instead of a file/script path"
    ),
):
    """Deploy a Modal application.

    **Usage:**
    modal deploy my_script.py
    modal deploy -m my_package.my_mod
    """
    # this ensures that lookups without environment specification use the same env as specified
    env = ensure_env(env)

    import_ref = parse_import_ref(app_ref, use_module_mode=use_module_mode)
    app = import_app_from_ref(import_ref, base_cmd="modal deploy")

    name = name or app.name or ""
    if not name:
        raise ExecutionError(
            "You need to either supply an explicit deployment name on the command line "
            "or have a name set on the App.\n"
            "\n"
            "Examples:\n"
            'app = modal.App("some-name")\n'
            "modal deploy ... --name=some-name"
        )

    with enable_output():
        res = deploy_app(app, name=name, environment_name=env or "", tag=tag)

    if stream_logs:
        stream_app_logs(app_id=res.app_id, app_logs_url=res.app_logs_url)


def serve(
    app_ref: str = typer.Argument(..., help="Path to a Python file with an app."),
    timeout: Optional[float] = None,
    env: str = ENV_OPTION,
    use_module_mode: bool = typer.Option(
        False, "-m", help="Interpret argument as a Python module path instead of a file/script path"
    ),
):
    """Run a web endpoint(s) associated with a Modal app and hot-reload code.

    **Examples:**

    ```
    modal serve hello_world.py
    ```
    """
    env = ensure_env(env)
    import_ref = parse_import_ref(app_ref, use_module_mode=use_module_mode)
    app = import_app_from_ref(import_ref, base_cmd="modal serve")
    if app.description is None:
        app.set_description(_get_clean_app_description(app_ref))

    with enable_output():
        with serve_app(app, import_ref, environment_name=env):
            if timeout is None:
                timeout = config["serve_timeout"]
            if timeout is None:
                timeout = float("inf")
            while timeout > 0:
                t = min(timeout, 3600)
                time.sleep(t)
                timeout -= t


def shell(
    ref: Optional[str] = typer.Argument(
        default=None,
        help=(
            "ID of running container or Sandbox, or path to a Python file containing an App."
            " Can also include a Function specifier, like `module.py::func`, if the file defines multiple Functions."
        ),
    ),
    cmd: str = typer.Option("/bin/bash", "-c", "--cmd", help="Command to run inside the Modal image."),
    env: str = ENV_OPTION,
    image: Optional[str] = typer.Option(
        default=None, help="Container image tag for inside the shell (if not using REF)."
    ),
    add_python: Optional[str] = typer.Option(default=None, help="Add Python to the image (if not using REF)."),
    volume: Optional[list[str]] = typer.Option(
        default=None,
        help=(
            "Name of a `modal.Volume` to mount inside the shell at `/mnt/{name}` (if not using REF)."
            " Can be used multiple times."
        ),
    ),
    secret: Optional[list[str]] = typer.Option(
        default=None,
        help=("Name of a `modal.Secret` to mount inside the shell (if not using REF). Can be used multiple times."),
    ),
    cpu: Optional[int] = typer.Option(default=None, help="Number of CPUs to allocate to the shell (if not using REF)."),
    memory: Optional[int] = typer.Option(
        default=None, help="Memory to allocate for the shell, in MiB (if not using REF)."
    ),
    gpu: Optional[str] = typer.Option(
        default=None,
        help="GPUs to request for the shell, if any. Examples are `any`, `a10g`, `a100:4` (if not using REF).",
    ),
    cloud: Optional[str] = typer.Option(
        default=None,
        help=(
            "Cloud provider to run the shell on. Possible values are `aws`, `gcp`, `oci`, `auto` (if not using REF)."
        ),
    ),
    region: Optional[str] = typer.Option(
        default=None,
        help=(
            "Region(s) to run the container on. "
            "Can be a single region or a comma-separated list to choose from (if not using REF)."
        ),
    ),
    pty: Optional[bool] = typer.Option(default=None, help="Run the command using a PTY."),
    use_module_mode: bool = typer.Option(
        False, "-m", help="Interpret argument as a Python module path instead of a file/script path"
    ),
):
    """Run a command or interactive shell inside a Modal container.

    **Examples:**

    Start an interactive shell inside the default Debian-based image:

    ```
    modal shell
    ```

    Start an interactive shell with the spec for `my_function` in your App
    (uses the same image, volumes, mounts, etc.):

    ```
    modal shell hello_world.py::my_function
    ```

    Or, if you're using a [modal.Cls](https://modal.com/docs/reference/modal.Cls)
    you can refer to a `@modal.method` directly:

    ```
    modal shell hello_world.py::MyClass.my_method
    ```

    Start a `python` shell:

    ```
    modal shell hello_world.py --cmd=python
    ```

    Run a command with your function's spec and pipe the output to a file:

    ```
    modal shell hello_world.py -c 'uv pip list' > env.txt
    ```

    Connect to a running Sandbox by ID:

    ```
    modal shell sb-abc123xyz
    ```
    """
    env = ensure_env(env)

    if pty is None:
        pty = is_tty()

    if platform.system() == "Windows":
        raise InvalidError("`modal shell` is currently not supported on Windows")

    app = App("modal shell")

    if ref is not None:
        # `modal shell` with a sandbox ID gets the task_id, that's then handled by the `ta-*` flow below.
        if ref.startswith("sb-") and len(ref[3:]) > 0 and ref[3:].isalnum():
            from ..sandbox import Sandbox

            try:
                sandbox = Sandbox.from_id(ref)
                task_id = sandbox._get_task_id()
                ref = task_id
            except NotFoundError as e:
                raise ClickException(f"Sandbox '{ref}' not found")
            except Exception as e:
                raise ClickException(f"Error connecting to sandbox '{ref}': {str(e)}")

        # `modal shell` with a container ID is a special case, alias for `modal container exec`.
        if ref.startswith("ta-") and len(ref[3:]) > 0 and ref[3:].isalnum():
            from .container import exec

            exec(container_id=ref, command=shlex.split(cmd), pty=pty)
            return

        import_ref = parse_import_ref(ref, use_module_mode=use_module_mode)
        runnable, all_usable_commands = import_and_filter(
            import_ref, base_cmd="modal shell", accept_local_entrypoint=False, accept_webhook=True
        )
        if not runnable:
            help_header = (
                "Specify a Modal function to start a shell session for. E.g.\n"
                f"> modal shell {import_ref.file_or_module}::my_function"
            )

            if all_usable_commands:
                help_footer = f"The selected module '{import_ref.file_or_module}' has the following choices:\n\n"
                help_footer += _get_runnable_list(all_usable_commands)
            else:
                help_footer = f"The selected module '{import_ref.file_or_module}' has no Modal functions or classes."

            raise ClickException(f"{help_header}\n\n{help_footer}")

        function_spec: _FunctionSpec
        if isinstance(runnable, MethodReference):
            # TODO: let users specify a class instead of a method, since they use the same environment
            class_service_function = runnable.cls._get_class_service_function()
            function_spec = class_service_function.spec
        elif isinstance(runnable, Function):
            function_spec = runnable.spec
        else:
            raise ValueError("Referenced entity is not a Modal function or class")

        start_shell = partial(
            interactive_shell,
            image=function_spec.image,
            mounts=function_spec.mounts,
            secrets=function_spec.secrets,
            network_file_systems=function_spec.network_file_systems,
            gpu=function_spec.gpus,
            cloud=function_spec.cloud,
            cpu=function_spec.cpu,
            memory=function_spec.memory,
            volumes=function_spec.volumes,
            region=function_spec.scheduler_placement.proto.regions if function_spec.scheduler_placement else None,
            pty=pty,
            proxy=function_spec.proxy,
        )
    else:
        modal_image = Image.from_registry(image, add_python=add_python) if image else None
        volumes = {} if volume is None else {f"/mnt/{vol}": Volume.from_name(vol) for vol in volume}
        secrets = [] if secret is None else [Secret.from_name(s) for s in secret]
        start_shell = partial(
            interactive_shell,
            image=modal_image,
            cpu=cpu,
            memory=memory,
            gpu=gpu,
            cloud=cloud,
            volumes=volumes,
            secrets=secrets,
            region=region.split(",") if region else [],
            pty=pty,
        )

    # NB: invoking under bash makes --cmd a lot more flexible.
    cmds = shlex.split(f'/bin/bash -c "{cmd}"')
    start_shell(app, cmds=cmds, environment_name=env, timeout=3600)



================================================
FILE: modal/cli/secret.py
================================================
# Copyright Modal Labs 2022
import json
import os
import platform
import subprocess
from datetime import datetime
from pathlib import Path
from tempfile import NamedTemporaryFile
from typing import Optional

import click
import typer
from rich.syntax import Syntax
from typer import Argument, Option

from modal._output import make_console
from modal._utils.async_utils import synchronizer
from modal._utils.grpc_utils import retry_transient_errors
from modal._utils.time_utils import timestamp_to_localized_str
from modal.cli.utils import ENV_OPTION, YES_OPTION, display_table
from modal.client import _Client
from modal.environments import ensure_env
from modal.secret import _Secret
from modal_proto import api_pb2

secret_cli = typer.Typer(name="secret", help="Manage secrets.", no_args_is_help=True)


@secret_cli.command("list", help="List your published secrets.")
@synchronizer.create_blocking
async def list_(env: Optional[str] = ENV_OPTION, json: bool = False):
    env = ensure_env(env)
    client = await _Client.from_env()

    items: list[api_pb2.SecretListItem] = []

    # Note that we need to continue using the gRPC API directly here rather than using Secret.objects.list.
    # There is some metadata that historically appears in the CLI output (last_used_at) that
    # doesn't make sense to transmit as hydration metadata, because the value can change over time and
    # the metadata retrieved at hydration time could get stale. Alternatively, we could rewrite this using
    # only public API by sequentially retrieving the secrets and then querying their dynamic metadata, but
    # that would require multiple round trips and would add lag to the CLI.
    async def retrieve_page(created_before: float) -> bool:
        max_page_size = 100
        pagination = api_pb2.ListPagination(max_objects=max_page_size, created_before=created_before)
        req = api_pb2.SecretListRequest(environment_name=env, pagination=pagination)
        resp = await retry_transient_errors(client.stub.SecretList, req)
        items.extend(resp.items)
        return len(resp.items) < max_page_size

    finished = await retrieve_page(datetime.now().timestamp())
    while True:
        if finished:
            break
        finished = await retrieve_page(items[-1].metadata.creation_info.created_at)

    secrets = [_Secret._new_hydrated(item.secret_id, client, item.metadata, is_another_app=True) for item in items]

    rows = []
    for obj, resp_data in zip(secrets, items):
        info = await obj.info()
        rows.append(
            [
                obj.name,
                timestamp_to_localized_str(info.created_at.timestamp(), json),
                info.created_by,
                timestamp_to_localized_str(resp_data.last_used_at, json) if resp_data.last_used_at else "-",
            ]
        )

    env_part = f" in environment '{env}'" if env else ""
    column_names = ["Name", "Created at", "Created by", "Last used at"]
    display_table(column_names, rows, json, title=f"Secrets{env_part}")


@secret_cli.command("create", help="Create a new secret.")
@synchronizer.create_blocking
async def create(
    secret_name: str,
    keyvalues: Optional[list[str]] = typer.Argument(default=None, help="Space-separated KEY=VALUE items."),
    env: Optional[str] = ENV_OPTION,
    from_dotenv: Optional[Path] = typer.Option(default=None, help="Path to a .env file to load secrets from."),
    from_json: Optional[Path] = typer.Option(default=None, help="Path to a JSON file to load secrets from."),
    force: bool = typer.Option(False, "--force", help="Overwrite the secret if it already exists."),
):
    env = ensure_env(env)
    env_dict = {}

    for arg in keyvalues or []:
        if "=" in arg:
            key, value = arg.split("=", 1)
            if value == "-":
                value = get_text_from_editor(key)
            env_dict[key] = value
        else:
            raise click.UsageError(
                """Each item should be of the form <KEY>=VALUE. To enter secrets using your $EDITOR, use `<KEY>=-`. To
enter secrets from environment variables, use `<KEY>="$ENV_VAR"`.

E.g.

modal secret create my-credentials username=john password=-
modal secret create my-credentials username=john password="$PASSWORD"
"""
            )

    if from_dotenv:
        if not from_dotenv.is_file():
            raise click.UsageError(f"Could not read .env file at {from_dotenv}")

        try:
            from dotenv import dotenv_values
        except ImportError:
            raise ImportError(
                "Need the `python-dotenv` package installed. You can install it by running `pip install python-dotenv`."
            )

        try:
            env_dict.update(dotenv_values(from_dotenv))
        except Exception as e:
            raise click.UsageError(f"Could not parse .env file at {from_dotenv}: {e}")

    if from_json:
        if not from_json.is_file():
            raise click.UsageError(f"Could not read JSON file at {from_json}")

        try:
            with from_json.open("r") as f:
                env_dict.update(json.load(f))
        except Exception as e:
            raise click.UsageError(f"Could not parse JSON file at {from_json}: {e}")

    if not env_dict:
        raise click.UsageError("You need to specify at least one key for your secret")

    for k, v in env_dict.items():
        if not isinstance(k, str) or not k:
            raise click.UsageError(f"Invalid key: '{k}'")
        if not isinstance(v, str):
            raise click.UsageError(f"Non-string value for secret '{k}'")

    # Create secret
    if force:
        # TODO migrate this path once we support Secret.update()?
        await _Secret._create_deployed(secret_name, env_dict, overwrite=force)
    else:
        await _Secret.objects.create(secret_name, env_dict)

    # Print code sample
    console = make_console()
    env_var_code = "\n    ".join(f'os.getenv("{name}")' for name in env_dict.keys()) if env_dict else "..."
    example_code = f"""
@app.function(secrets=[modal.Secret.from_name("{secret_name}")])
def some_function():
    {env_var_code}
"""
    plural_s = "s" if len(env_dict) > 1 else ""
    console.print(
        f"""Created a new secret '{secret_name}' with the key{plural_s} {", ".join(repr(k) for k in env_dict.keys())}"""
    )
    console.print("\nUse it in your Modal app:\n")
    console.print(Syntax(example_code, "python"))


@secret_cli.command("delete", help="Delete a named Secret.")
@synchronizer.create_blocking
async def delete(
    name: str = Argument(help="Name of the modal.Secret to be deleted. Case sensitive"),
    *,
    allow_missing: bool = Option(False, "--allow-missing", help="Don't error if the Secret doesn't exist."),
    yes: bool = YES_OPTION,
    env: Optional[str] = ENV_OPTION,
):
    env = ensure_env(env)
    if not yes:
        typer.confirm(
            f"Are you sure you want to irrevocably delete the modal.Secret '{name}'?",
            default=False,
            abort=True,
        )
    await _Secret.objects.delete(name, environment_name=env, allow_missing=allow_missing)


def get_text_from_editor(key) -> str:
    with NamedTemporaryFile("w+", prefix="secret_buffer", suffix=".txt") as bufferfile:
        if platform.system() != "Windows":
            editor = os.getenv("EDITOR", "vi")
            input(f"Pressing enter will open an external editor ({editor}) for editing '{key}'...")
            status_code = subprocess.call([editor, bufferfile.name])
        else:
            # not tested, but according to https://stackoverflow.com/questions/1442841/lauch-default-editor-like-webbrowser-module
            # this should open an editor on Windows...
            input("Pressing enter will open an external editor to allow you to edit the secret value...")
            status_code = os.system(bufferfile.name)

        if status_code != 0:
            raise ValueError(
                "Something went wrong with the external editor. "
                "Try again, or use '--' as the value to pass input through stdin instead"
            )

        bufferfile.seek(0)
        return bufferfile.read()



================================================
FILE: modal/cli/token.py
================================================
# Copyright Modal Labs 2022
import getpass
from typing import Optional

import typer

from modal._utils.async_utils import synchronizer
from modal.token_flow import _new_token, _set_token

token_cli = typer.Typer(name="token", help="Manage tokens.", no_args_is_help=True)

profile_option = typer.Option(
    None,
    help=(
        "Modal profile to set credentials for. If unspecified "
        "(and MODAL_PROFILE environment variable is not set), "
        "uses the workspace name associated with the credentials."
    ),
)
activate_option = typer.Option(
    True,
    help="Activate the profile containing this token after creation.",
)

verify_option = typer.Option(
    True,
    help="Make a test request to verify the new credentials.",
)


@token_cli.command(name="set")
@synchronizer.create_blocking
async def set(
    token_id: Optional[str] = typer.Option(None, help="Account token ID."),
    token_secret: Optional[str] = typer.Option(None, help="Account token secret."),
    profile: Optional[str] = profile_option,
    activate: bool = activate_option,
    verify: bool = verify_option,
):
    """Set account credentials for connecting to Modal.

    If the credentials are not provided on the command line, you will be prompted to enter them.
    """
    if token_id is None:
        token_id = getpass.getpass("Token ID:")
    if token_secret is None:
        token_secret = getpass.getpass("Token secret:")
    await _set_token(token_id, token_secret, profile=profile, activate=activate, verify=verify)


@token_cli.command(name="new")
@synchronizer.create_blocking
async def new(
    profile: Optional[str] = profile_option,
    activate: bool = activate_option,
    verify: bool = verify_option,
    source: Optional[str] = None,
):
    """Create a new token by using an authenticated web session."""
    await _new_token(profile=profile, activate=activate, verify=verify, source=source)



================================================
FILE: modal/cli/utils.py
================================================
# Copyright Modal Labs 2022
import asyncio
from collections.abc import Sequence
from json import dumps
from typing import Optional, Union

import typer
from click import UsageError
from grpclib import GRPCError, Status
from rich.table import Column, Table
from rich.text import Text

from modal_proto import api_pb2

from .._output import OutputManager, get_app_logs_loop, make_console
from .._utils.async_utils import synchronizer
from ..client import _Client
from ..environments import ensure_env
from ..exception import NotFoundError


@synchronizer.create_blocking
async def stream_app_logs(
    app_id: Optional[str] = None,
    task_id: Optional[str] = None,
    app_logs_url: Optional[str] = None,
    show_timestamps: bool = False,
):
    client = await _Client.from_env()
    output_mgr = OutputManager(status_spinner_text=f"Tailing logs for {app_id}", show_timestamps=show_timestamps)
    try:
        with output_mgr.show_status_spinner():
            await get_app_logs_loop(client, output_mgr, app_id=app_id, task_id=task_id, app_logs_url=app_logs_url)
    except asyncio.CancelledError:
        pass
    except GRPCError as exc:
        if exc.status in (Status.INVALID_ARGUMENT, Status.NOT_FOUND):
            raise UsageError(exc.message)
        else:
            raise
    except KeyboardInterrupt:
        pass


@synchronizer.create_blocking
async def get_app_id_from_name(name: str, env: Optional[str], client: Optional[_Client] = None) -> str:
    if client is None:
        client = await _Client.from_env()
    env_name = ensure_env(env)
    request = api_pb2.AppGetByDeploymentNameRequest(name=name, environment_name=env_name)
    try:
        resp = await client.stub.AppGetByDeploymentName(request)
    except GRPCError as exc:
        if exc.status in (Status.INVALID_ARGUMENT, Status.NOT_FOUND):
            raise UsageError(exc.message or "")
        raise
    if not resp.app_id:
        env_comment = f" in the '{env_name}' environment" if env_name else ""
        raise NotFoundError(f"Could not find a deployed app named '{name}'{env_comment}.")
    return resp.app_id


def _plain(text: Union[Text, str]) -> str:
    return text.plain if isinstance(text, Text) else text


def is_tty() -> bool:
    return make_console().is_terminal


def display_table(
    columns: Sequence[Union[Column, str]],
    rows: Sequence[Sequence[Union[Text, str]]],
    json: bool = False,
    title: str = "",
):
    def col_to_str(col: Union[Column, str]) -> str:
        return str(col.header) if isinstance(col, Column) else col

    console = make_console()
    if json:
        json_data = [{col_to_str(col): _plain(row[i]) for i, col in enumerate(columns)} for row in rows]
        console.print_json(dumps(json_data))
    else:
        table = Table(*columns, title=title)
        for row in rows:
            table.add_row(*row)
        console.print(table)


ENV_OPTION_HELP = """Environment to interact with.

If not specified, Modal will use the default environment of your current profile, or the `MODAL_ENVIRONMENT` variable.
Otherwise, raises an error if the workspace has multiple environments.
"""
ENV_OPTION = typer.Option(None, "-e", "--env", help=ENV_OPTION_HELP)

YES_OPTION = typer.Option(False, "-y", "--yes", help="Run without pausing for confirmation.")



================================================
FILE: modal/cli/volume.py
================================================
# Copyright Modal Labs 2022
import os
import sys
from pathlib import Path
from typing import Optional

import typer
from click import UsageError
from grpclib import GRPCError, Status
from rich.syntax import Syntax
from typer import Argument, Option, Typer

import modal
from modal._output import OutputManager, ProgressHandler, make_console
from modal._utils.async_utils import synchronizer
from modal._utils.time_utils import timestamp_to_localized_str
from modal.cli._download import _volume_download
from modal.cli.utils import ENV_OPTION, YES_OPTION, display_table
from modal.environments import ensure_env
from modal.volume import _AbstractVolumeUploadContextManager, _Volume
from modal_proto import api_pb2

volume_cli = Typer(
    name="volume",
    no_args_is_help=True,
    help="""
    Read and edit `modal.Volume` volumes.

    Note: users of `modal.NetworkFileSystem` should use the `modal nfs` command instead.
    """,
)


def humanize_filesize(value: int) -> str:
    if value < 0:
        raise ValueError("value should be >= 0")
    suffix = (" KiB", " MiB", " GiB", " TiB", " PiB", " EiB", " ZiB")
    format = "%.1f"
    base = 1024
    bytes_ = float(value)
    if bytes_ < base:
        return f"{bytes_:0.0f} B"
    for i, s in enumerate(suffix):
        unit = base ** (i + 2)
        if bytes_ < unit:
            break
    return format % (base * bytes_ / unit) + s


@volume_cli.command(name="create", help="Create a named, persistent modal.Volume.", rich_help_panel="Management")
def create(
    name: str,
    env: Optional[str] = ENV_OPTION,
    version: Optional[int] = Option(default=None, help="VolumeFS version. (Experimental)"),
):
    env_name = ensure_env(env)
    modal.Volume.objects.create(name, environment_name=env, version=version)
    usage_code = f"""
@app.function(volumes={{"/my_vol": modal.Volume.from_name("{name}")}})
def some_func():
    os.listdir("/my_vol")
"""

    console = make_console()
    console.print(f"Created Volume '{name}' in environment '{env_name}'. \n\nCode example:\n")
    usage = Syntax(usage_code, "python")
    console.print(usage)


@volume_cli.command(name="get", rich_help_panel="File operations")
@synchronizer.create_blocking
async def get(
    volume_name: str,
    remote_path: str,
    local_destination: str = Argument("."),
    force: bool = False,
    env: Optional[str] = ENV_OPTION,
):
    """Download files from a modal.Volume object.

    If a folder is passed for REMOTE_PATH, the contents of the folder will be downloaded
    recursively, including all subdirectories.

    **Example**

    ```
    modal volume get <volume_name> logs/april-12-1.txt
    modal volume get <volume_name> / volume_data_dump
    ```

    Use "-" as LOCAL_DESTINATION to write file contents to standard output.
    """
    ensure_env(env)
    destination = Path(local_destination)
    volume = _Volume.from_name(volume_name, environment_name=env)
    console = make_console()
    progress_handler = ProgressHandler(type="download", console=console)
    with progress_handler.live:
        await _volume_download(volume, remote_path, destination, force, progress_cb=progress_handler.progress)
    console.print(OutputManager.step_completed("Finished downloading files to local!"))


@volume_cli.command(
    name="list",
    help="List the details of all modal.Volume volumes in an Environment.",
    rich_help_panel="Management",
)
@synchronizer.create_blocking
async def list_(env: Optional[str] = ENV_OPTION, json: Optional[bool] = False):
    env = ensure_env(env)
    volumes = await _Volume.objects.list(environment_name=env)
    rows = []
    for obj in volumes:
        info = await obj.info()
        rows.append((info.name, timestamp_to_localized_str(info.created_at.timestamp(), json), info.created_by))

    display_table(["Name", "Created at", "Created by"], rows, json)


@volume_cli.command(
    name="ls",
    help="List files and directories in a modal.Volume volume.",
    rich_help_panel="File operations",
)
@synchronizer.create_blocking
async def ls(
    volume_name: str,
    path: str = Argument(default="/"),
    json: bool = False,
    env: Optional[str] = ENV_OPTION,
):
    ensure_env(env)
    vol = _Volume.from_name(volume_name, environment_name=env)

    try:
        entries = await vol.listdir(path)
    except GRPCError as exc:
        if exc.status in (Status.INVALID_ARGUMENT, Status.NOT_FOUND):
            raise UsageError(exc.message)
        raise

    if not json and not sys.stdout.isatty():
        # Legacy behavior -- I am not sure why exactly we did this originally but I don't want to break it
        for entry in entries:
            print(entry.path)
    else:
        rows = []
        for entry in entries:
            if entry.type == api_pb2.FileEntry.FileType.DIRECTORY:
                filetype = "dir"
            elif entry.type == api_pb2.FileEntry.FileType.SYMLINK:
                filetype = "link"
            elif entry.type == api_pb2.FileEntry.FileType.FIFO:
                filetype = "fifo"
            elif entry.type == api_pb2.FileEntry.FileType.SOCKET:
                filetype = "socket"
            else:
                filetype = "file"
            rows.append(
                (
                    entry.path.encode("unicode_escape").decode("utf-8"),
                    filetype,
                    timestamp_to_localized_str(entry.mtime, False),
                    humanize_filesize(entry.size),
                )
            )
        columns = ["Filename", "Type", "Created/Modified", "Size"]
        title = f"Directory listing of '{path}' in '{volume_name}'"
        display_table(columns, rows, json, title)


@volume_cli.command(
    name="put",
    help="""Upload a file or directory to a modal.Volume.

Remote parent directories will be created as needed.

Ending the REMOTE_PATH with a forward slash (/), it's assumed to be a directory
and the file will be uploaded with its current name under that directory.
""",
    rich_help_panel="File operations",
)
@synchronizer.create_blocking
async def put(
    volume_name: str,
    local_path: str = Argument(),
    remote_path: str = Argument(default="/"),
    force: bool = Option(False, "-f", "--force", help="Overwrite existing files."),
    env: Optional[str] = ENV_OPTION,
):
    ensure_env(env)
    vol = await _Volume.from_name(volume_name, environment_name=env).hydrate()

    if remote_path.endswith("/"):
        remote_path = remote_path + os.path.basename(local_path)
    console = make_console()
    progress_handler = ProgressHandler(type="upload", console=console)

    if Path(local_path).is_dir():
        with progress_handler.live:
            try:
                async with _AbstractVolumeUploadContextManager.resolve(
                    vol._metadata.version,
                    vol.object_id,
                    vol._client,
                    progress_cb=progress_handler.progress,
                    force=force,
                ) as batch:
                    batch.put_directory(local_path, remote_path)
            except FileExistsError as exc:
                raise UsageError(str(exc))
        console.print(OutputManager.step_completed(f"Uploaded directory '{local_path}' to '{remote_path}'"))
    elif "*" in local_path:
        raise UsageError("Glob uploads are currently not supported")
    else:
        with progress_handler.live:
            try:
                async with _AbstractVolumeUploadContextManager.resolve(
                    vol._metadata.version,
                    vol.object_id,
                    vol._client,
                    progress_cb=progress_handler.progress,
                    force=force,
                ) as batch:
                    batch.put_file(local_path, remote_path)

            except FileExistsError as exc:
                raise UsageError(str(exc))
        console.print(OutputManager.step_completed(f"Uploaded file '{local_path}' to '{remote_path}'"))


@volume_cli.command(
    name="rm", help="Delete a file or directory from a modal.Volume.", rich_help_panel="File operations"
)
@synchronizer.create_blocking
async def rm(
    volume_name: str,
    remote_path: str,
    recursive: bool = Option(False, "-r", "--recursive", help="Delete directory recursively"),
    env: Optional[str] = ENV_OPTION,
):
    ensure_env(env)
    volume = _Volume.from_name(volume_name, environment_name=env)
    console = make_console()
    try:
        await volume.remove_file(remote_path, recursive=recursive)
        console.print(OutputManager.step_completed(f"{remote_path} was deleted successfully!"))
    except GRPCError as exc:
        if exc.status in (Status.NOT_FOUND, Status.INVALID_ARGUMENT):
            raise UsageError(exc.message)
        raise


@volume_cli.command(
    name="cp",
    help=(
        "Copy within a modal.Volume. "
        "Copy source file to destination file or multiple source files to destination directory."
    ),
    rich_help_panel="File operations",
)
@synchronizer.create_blocking
async def cp(
    volume_name: str,
    paths: list[str],  # accepts multiple paths, last path is treated as destination path
    recursive: bool = Option(False, "-r", "--recursive", help="Copy directories recursively"),
    env: Optional[str] = ENV_OPTION,
):
    ensure_env(env)
    volume = _Volume.from_name(volume_name, environment_name=env)
    *src_paths, dst_path = paths
    await volume.copy_files(src_paths, dst_path, recursive)


@volume_cli.command(
    name="delete",
    help="Delete a named Volume and all of its data.",
    rich_help_panel="Management",
)
@synchronizer.create_blocking
async def delete(
    name: str = Argument(help="Name of the modal.Volume to be deleted. Case sensitive"),
    *,
    allow_missing: bool = Option(False, "--allow-missing", help="Don't error if the Volume doesn't exist."),
    yes: bool = YES_OPTION,
    env: Optional[str] = ENV_OPTION,
):
    env = ensure_env(env)
    if not yes:
        typer.confirm(
            f"Are you sure you want to irrevocably delete the modal.Volume '{name}'?",
            default=False,
            abort=True,
        )

    await _Volume.objects.delete(name, environment_name=env, allow_missing=allow_missing)


@volume_cli.command(
    name="rename",
    help="Rename a modal.Volume.",
    rich_help_panel="Management",
)
@synchronizer.create_blocking
async def rename(
    old_name: str,
    new_name: str,
    yes: bool = YES_OPTION,
    env: Optional[str] = ENV_OPTION,
):
    if not yes:
        typer.confirm(
            f"Are you sure you want rename the modal.Volume '{old_name}'? This may break any Apps currently using it.",
            default=False,
            abort=True,
        )

    await _Volume.rename(old_name, new_name, environment_name=env)



================================================
FILE: modal/cli/programs/__init__.py
================================================
# Copyright Modal Labs 2023



================================================
FILE: modal/cli/programs/launch_instance_ssh.py
================================================
# Copyright Modal Labs 2023
# type: ignore
import json
import os
import sys
from typing import Any

import rich
import rich.panel
import rich.rule

import modal
import modal.experimental

# Passed by `modal launch` locally via CLI.
args: dict[str, Any] = json.loads(os.environ.get("MODAL_LAUNCH_ARGS", "{}"))

app = modal.App()

image: modal.Image
if args.get("image"):
    image = modal.Image.from_registry(args.get("image"))
else:
    # Must be set to the same image builder version as the notebook base image.
    os.environ["MODAL_IMAGE_BUILDER_VERSION"] = "2024.10"
    image = modal.experimental.notebook_base_image(python_version="3.12")

volume = (
    modal.Volume.from_name(
        args.get("volume"),
        create_if_missing=True,
    )
    if args.get("volume")
    else None
)
volumes = {"/workspace": volume} if volume else {}


startup_script = """
set -eu
mkdir -p /run/sshd

# Check if sshd is installed, install if not
test -x /usr/sbin/sshd || (apt-get update && apt-get install -y openssh-server)

# Change default working directory to /workspace
echo "cd /workspace" >> /root/.profile

mkdir -p /root/.ssh
echo "$SSH_PUBLIC_KEY" >> /root/.ssh/authorized_keys
/usr/sbin/sshd -D -e
"""


@app.local_entrypoint()
def main():
    if not os.environ.get("SSH_PUBLIC_KEY"):
        raise ValueError("SSH_PUBLIC_KEY environment variable is not set")

    sb = modal.Sandbox.create(
        *("sh", "-c", startup_script),
        app=app,
        image=image,
        cpu=args.get("cpu"),
        memory=args.get("memory"),
        gpu=args.get("gpu"),
        timeout=args.get("timeout"),
        volumes=volumes,
        unencrypted_ports=[22],  # Forward SSH port
        secrets=[modal.Secret.from_dict({"SSH_PUBLIC_KEY": os.environ.get("SSH_PUBLIC_KEY")})],
    )
    hostname, port = sb.tunnels()[22].tcp_socket
    connection_cmd = f"ssh -A -p {port} root@{hostname}"

    rich.print(
        rich.rule.Rule(style="yellow"),
        rich.panel.Panel(
            f"""Your instance is ready! You can SSH into it using the following command:

  [dim gray]>[/dim gray] [bold cyan]{connection_cmd}[/bold cyan]

[italic]Details:[/italic]
  â€¢ Name:    [magenta]{app.description}[/magenta]
  â€¢ CPU:     [yellow]{args.get("cpu")} cores[/yellow]
  â€¢ Memory:  [yellow]{args.get("memory")} MiB[/yellow]
  â€¢ Timeout: [yellow]{args.get("timeout")} seconds[/yellow]
  â€¢ GPU:     [green]{(args.get("gpu") or "N/A").upper()}[/green]""",
            title="SSH Connection",
            expand=False,
        ),
        rich.rule.Rule(style="yellow"),
    )

    sys.exit(0)  # Exit immediately to prevent "Timed out waiting for final apps log."



================================================
FILE: modal/cli/programs/run_jupyter.py
================================================
# Copyright Modal Labs 2023
# type: ignore
import json
import os
import secrets
import socket
import subprocess
import threading
import time
import webbrowser
from typing import Any

from modal import App, Image, Queue, Secret, Volume, forward

# Passed by `modal launch` locally via CLI, plumbed to remote runner through secrets.
args: dict[str, Any] = json.loads(os.environ.get("MODAL_LAUNCH_ARGS", "{}"))

app = App()

image = Image.from_registry(args.get("image"), add_python=args.get("add_python")).pip_install("jupyterlab")

if args.get("mount"):
    image = image.add_local_dir(
        args.get("mount"),
        remote_path="/root/lab/mount",
    )

volume = (
    Volume.from_name(
        args.get("volume"),
        create_if_missing=True,
    )
    if args.get("volume")
    else None
)
volumes = {"/root/lab/volume": volume} if volume else {}


def wait_for_port(url: str, q: Queue):
    start_time = time.monotonic()
    while True:
        try:
            with socket.create_connection(("localhost", 8888), timeout=30.0):
                break
        except OSError as exc:
            time.sleep(0.01)
            if time.monotonic() - start_time >= 30.0:
                raise TimeoutError("Waited too long for port 8888 to accept connections") from exc
    q.put(url)


@app.function(
    image=image,
    cpu=args.get("cpu"),
    memory=args.get("memory"),
    gpu=args.get("gpu"),
    timeout=args.get("timeout", 3600),
    secrets=[Secret.from_dict({"MODAL_LAUNCH_ARGS": json.dumps(args)})],
    volumes=volumes,
    max_containers=1 if volume else None,
)
def run_jupyter(q: Queue):
    os.makedirs("/root/lab", exist_ok=True)
    token = secrets.token_urlsafe(13)
    with forward(8888) as tunnel:
        url = tunnel.url + "/?token=" + token
        threading.Thread(target=wait_for_port, args=(url, q)).start()
        print("\nJupyter on Modal, opening in browser...")
        print(f"   -> {url}\n")
        subprocess.run(
            [
                "jupyter",
                "lab",
                "--no-browser",
                "--allow-root",
                "--ip=0.0.0.0",
                "--port=8888",
                "--notebook-dir=/root/lab",
                "--LabApp.allow_origin='*'",
                "--LabApp.allow_remote_access=1",
            ],
            env={**os.environ, "JUPYTER_TOKEN": token, "SHELL": "/bin/bash"},
            stderr=subprocess.DEVNULL,
        )
    q.put("done")


@app.local_entrypoint()
def main():
    with Queue.ephemeral() as q:
        run_jupyter.spawn(q)
        url = q.get()
        time.sleep(1)  # Give Jupyter a chance to start up
        webbrowser.open(url)
        assert q.get() == "done"



================================================
FILE: modal/cli/programs/run_marimo.py
================================================
# Copyright Modal Labs 2025
# type: ignore
import json
import os
import secrets
import socket
import subprocess
import threading
import time
import webbrowser
from typing import Any

from modal import App, Image, Queue, Secret, Volume, forward

# Args injected by `modal launch` CLI.
args: dict[str, Any] = json.loads(os.environ.get("MODAL_LAUNCH_ARGS", "{}"))

app = App()

image = Image.from_registry(args.get("image"), add_python=args.get("add_python")).uv_pip_install("marimo")

# Optional host-filesystem mount (read-only snapshot of your project, useful for editing)
if args.get("mount"):
    image = image.add_local_dir(args["mount"], remote_path="/root/marimo/mount")

# Optional persistent Modal volume
volume = Volume.from_name(args["volume"], create_if_missing=True) if args.get("volume") else None
volumes = {"/root/marimo/volume": volume} if volume else {}


def _wait_for_port(url: str, q: Queue) -> None:
    start = time.monotonic()
    while True:
        try:
            with socket.create_connection(("localhost", 8888), timeout=30):
                break
        except OSError as exc:
            if time.monotonic() - start > 30:
                raise TimeoutError("marimo server did not start within 30 s") from exc
            time.sleep(0.05)
    q.put(url)


@app.function(
    image=image,
    cpu=args.get("cpu"),
    memory=args.get("memory"),
    gpu=args.get("gpu"),
    timeout=args.get("timeout", 3600),
    secrets=[Secret.from_dict({"MODAL_LAUNCH_ARGS": json.dumps(args)})],
    volumes=volumes,
    max_containers=1 if volume else None,
)
def run_marimo(q: Queue):
    os.makedirs("/root/marimo", exist_ok=True)

    # marimo supports token-based auth; generate one so only you can connect
    token = secrets.token_urlsafe(12)

    with forward(8888) as tunnel:
        url = f"{tunnel.url}/?access_token={token}"
        threading.Thread(target=_wait_for_port, args=(url, q), daemon=True).start()

        print("\nmarimo on Modal, opening in browser â€¦")
        print(f"   -> {url}\n")

        # Launch the headless edit server
        subprocess.run(
            [
                "marimo",
                "edit",
                "--headless",  # don't open browser in container
                "--host",
                "0.0.0.0",  # bind all interfaces
                "--port",
                "8888",
                "--token-password",
                token,  # enable session-based auth
                "--skip-update-check",
                "/root/marimo",  # workspace directory
            ],
            env={**os.environ, "SHELL": "/bin/bash"},
        )

    q.put("done")


@app.local_entrypoint()
def main():
    with Queue.ephemeral() as q:
        run_marimo.spawn(q)
        url = q.get()  # first message = connect URL
        time.sleep(1)  # give server a heartbeat
        webbrowser.open(url)
        assert q.get() == "done"



================================================
FILE: modal/cli/programs/vscode.py
================================================
# Copyright Modal Labs 2023
# type: ignore
import json
import os
import secrets
import socket
import subprocess
import threading
import time
import webbrowser
from typing import Any

from modal import App, Image, Queue, Secret, Volume, forward

# Passed by `modal launch` locally via CLI, plumbed to remote runner through secrets.
args: dict[str, Any] = json.loads(os.environ.get("MODAL_LAUNCH_ARGS", "{}"))

CODE_SERVER_INSTALLER = "https://code-server.dev/install.sh"
CODE_SERVER_ENTRYPOINT = (
    "https://raw.githubusercontent.com/coder/code-server/refs/tags/v4.96.1/ci/release-image/entrypoint.sh"
)
FIXUD_INSTALLER = "https://github.com/boxboat/fixuid/releases/download/v0.6.0/fixuid-0.6.0-linux-$ARCH.tar.gz"


app = App()
image = (
    Image.from_registry(args.get("image"), add_python="3.11")
    .apt_install("curl", "dumb-init", "git", "git-lfs")
    .run_commands(
        f"curl -fsSL {CODE_SERVER_INSTALLER} | sh",
        f"curl -fsSL {CODE_SERVER_ENTRYPOINT}  > /code-server.sh",
        "chmod u+x /code-server.sh",
    )
    .run_commands(
        'ARCH="$(dpkg --print-architecture)"'
        f' && curl -fsSL "{FIXUD_INSTALLER}" | tar -C /usr/local/bin -xzf - '
        " && chown root:root /usr/local/bin/fixuid"
        " && chmod 4755 /usr/local/bin/fixuid"
        " && mkdir -p /etc/fixuid"
        ' && echo "user: root" >> /etc/fixuid/config.yml'
        ' && echo "group: root" >> /etc/fixuid/config.yml'
    )
    .run_commands("mkdir /home/coder")
    .env({"ENTRYPOINTD": ""})
)

if args.get("mount"):
    image = image.add_local_dir(
        args.get("mount"),
        remote_path="/home/coder/mount",
    )

volume = (
    Volume.from_name(
        args.get("volume"),
        create_if_missing=True,
    )
    if args.get("volume")
    else None
)
volumes = {"/home/coder/volume": volume} if volume else {}


def wait_for_port(data: tuple[str, str], q: Queue):
    start_time = time.monotonic()
    while True:
        try:
            with socket.create_connection(("localhost", 8080), timeout=30.0):
                break
        except OSError as exc:
            time.sleep(0.01)
            if time.monotonic() - start_time >= 30.0:
                raise TimeoutError("Waited too long for port 8080 to accept connections") from exc
    q.put(data)


@app.function(
    image=image,
    cpu=args.get("cpu"),
    memory=args.get("memory"),
    gpu=args.get("gpu"),
    timeout=args.get("timeout", 3600),
    secrets=[Secret.from_dict({"MODAL_LAUNCH_ARGS": json.dumps(args)})],
    volumes=volumes,
    max_containers=1 if volume else None,
)
def run_vscode(q: Queue):
    os.chdir("/home/coder")
    token = secrets.token_urlsafe(13)
    with forward(8080) as tunnel:
        url = tunnel.url
        print("\nVS Code on Modal, opening in browser...")
        print(f"   -> {url}")
        print(f"   -> password: {token}\n")
        threading.Thread(target=wait_for_port, args=((url, token), q)).start()
        subprocess.run(
            ["/code-server.sh", "--bind-addr", "0.0.0.0:8080", "."],
            env={**os.environ, "SHELL": "/bin/bash", "PASSWORD": token},
        )
    q.put("done")


@app.local_entrypoint()
def main():
    with Queue.ephemeral() as q:
        run_vscode.spawn(q)
        url, token = q.get()
        time.sleep(1)  # Give VS Code a chance to start up
        webbrowser.open(url)
        assert q.get() == "done"



================================================
FILE: modal/experimental/__init__.py
================================================
# Copyright Modal Labs 2025
import os
import shlex
from dataclasses import dataclass
from pathlib import Path
from typing import Literal, Optional, Union

from modal_proto import api_pb2

from .._clustered_functions import ClusterInfo, get_cluster_info as _get_cluster_info
from .._functions import _Function
from .._object import _get_environment_name
from .._partial_function import _clustered
from .._runtime.container_io_manager import _ContainerIOManager
from .._utils.async_utils import synchronize_api, synchronizer
from .._utils.grpc_utils import retry_transient_errors
from ..app import _App
from ..client import _Client
from ..cls import _Cls
from ..exception import InvalidError
from ..image import DockerfileSpec, ImageBuilderVersion, _Image, _ImageRegistryConfig
from ..secret import _Secret
from .flash import flash_forward, flash_get_containers, flash_prometheus_autoscaler  # noqa: F401


def stop_fetching_inputs():
    """Don't fetch any more inputs from the server, after the current one.
    The container will exit gracefully after the current input is processed."""
    _ContainerIOManager.stop_fetching_inputs()


def get_local_input_concurrency():
    """Get the container's local input concurrency.
    If recently reduced to particular value, it can return a larger number than
    set due to in-progress inputs."""
    return _ContainerIOManager.get_input_concurrency()


def set_local_input_concurrency(concurrency: int):
    """Set the container's local input concurrency. Dynamic concurrency will be disabled.
    When setting to a smaller value, this method will not interrupt in-progress inputs.
    """
    _ContainerIOManager.set_input_concurrency(concurrency)


def get_cluster_info() -> ClusterInfo:
    return _get_cluster_info()


clustered = synchronize_api(_clustered, target_module=__name__)


@dataclass
class AppInfo:
    app_id: str
    name: str
    containers: int


@synchronizer.create_blocking
async def list_deployed_apps(environment_name: str = "", client: Optional[_Client] = None) -> list[AppInfo]:
    """List deployed Apps along with the number of containers currently running."""
    # This function exists to provide backwards compatibility for some users who had been
    # calling into the private function that previously backed the `modal app list` CLI command.
    # We plan to add more Python API for exposing this sort of information, but we haven't
    # settled on a design we're happy with yet. In the meantime, this function will continue
    # to support existing codebases. It's likely that the final API will be different
    # (e.g. more oriented around the App object). This function should be gracefully deprecated
    # one the new API is released.
    client = client or await _Client.from_env()

    resp: api_pb2.AppListResponse = await client.stub.AppList(
        api_pb2.AppListRequest(environment_name=_get_environment_name(environment_name))
    )

    app_infos = []
    for app_stats in resp.apps:
        if app_stats.state == api_pb2.APP_STATE_DEPLOYED:
            app_infos.append(
                AppInfo(
                    app_id=app_stats.app_id,
                    name=app_stats.description,
                    containers=app_stats.n_running_tasks,
                )
            )
    return app_infos


@synchronizer.create_blocking
async def get_app_objects(
    app_name: str, *, environment_name: Optional[str] = None, client: Optional[_Client] = None
) -> dict[str, Union[_Function, _Cls]]:
    """Experimental interface for retrieving a dictionary of the Functions / Clses in an App.

    The return value is a dictionary mapping names to unhydrated Function or Cls objects.

    We plan to support this functionality through a stable API in the future. It's likely that
    the stable API will look different (it will probably be a method on the App object itself).

    """
    # This is implemented through a somewhat odd mixture of internal RPCs and public APIs.
    # While AppGetLayout provides the object ID and metadata for each object in the App, it's
    # currently somewhere between very awkward and impossible to hydrate a modal.Cls with just
    # that information, since the "class service function" needs to be loaded first
    # (and it's not always possible to do that without knowledge of the parameterization).
    # So instead we just use AppGetLayout to retrieve the names of the Functions / Clsices on
    # the App and then use the public .from_name constructors to return unhydrated handles.

    # Additionally, since we need to know the environment name to use `.from_name`, and the App's
    # environment name isn't stored anywhere on the App (and cannot be retrieved via an RPC), the
    # experimental function is parameterized by an App name while the stable API would instead
    # be a method on the App itself.

    if client is None:
        client = await _Client.from_env()

    app = await _App.lookup(app_name, environment_name=environment_name, client=client)
    req = api_pb2.AppGetLayoutRequest(app_id=app.app_id)
    app_layout_resp = await retry_transient_errors(client.stub.AppGetLayout, req)

    app_objects: dict[str, Union[_Function, _Cls]] = {}

    for cls_name in app_layout_resp.app_layout.class_ids:
        app_objects[cls_name] = _Cls.from_name(app_name, cls_name, environment_name=environment_name)

    for func_name in app_layout_resp.app_layout.function_ids:
        if func_name.endswith(".*"):
            continue  # TODO explain
        app_objects[func_name] = _Function.from_name(app_name, func_name, environment_name=environment_name)

    return app_objects


@synchronizer.create_blocking
async def raw_dockerfile_image(
    path: Union[str, Path],
    force_build: bool = False,
) -> _Image:
    """
    Build a Modal Image from a local Dockerfile recipe without any changes.

    Unlike for `modal.Image.from_dockerfile`, the provided recipe will not be embellished with
    steps to install dependencies for the Modal client package. As a consequence, the resulting
    Image cannot be used with a modal Function unless those dependencies are already included
    as part of the base Dockerfile recipe or are added in a subsequent layer. The Image _can_ be
    directly used with a modal Sandbox, which does not need the Modal client.

    We expect to support this experimental function until the `2025.04` Modal Image Builder is
    stable, at which point Modal Image recipes will no longer install the client dependencies
    by default. At that point, users can upgrade their Image Builder Version and migrate to
    `modal.Image.from_dockerfile` for usecases supported by this function.

    """

    def build_dockerfile(version: ImageBuilderVersion) -> DockerfileSpec:
        with open(os.path.expanduser(path)) as f:
            commands = f.read().split("\n")
        return DockerfileSpec(commands=commands, context_files={})

    return _Image._from_args(
        dockerfile_function=build_dockerfile,
        force_build=force_build,
    )


@synchronizer.create_blocking
async def raw_registry_image(
    tag: str,
    registry_secret: Optional[_Secret] = None,
    credential_type: Literal["static", "aws", "gcp", None] = None,
    force_build: bool = False,
) -> _Image:
    """
    Build a Modal Image from a public or private image registry without any changes.

    Unlike for `modal.Image.from_registry`, the provided recipe will not be embellished with
    steps to install dependencies for the Modal client package. As a consequence, the resulting
    Image cannot be used with a modal Function unless those dependencies are already included
    as part of the registry Image or are added in a subsequent layer. The Image _can_ be
    directly used with a modal Sandbox, which does not need the Modal client.

    We expect to support this experimental function until the `2025.04` Modal Image Builder is
    stable, at which point Modal Image recipes will no longer install the client dependencies
    by default. At that point, users can upgrade their Image Builder Version and migrate to
    `modal.Image.from_registry` for usecases supported by this function.

    """

    def build_dockerfile(version: ImageBuilderVersion) -> DockerfileSpec:
        commands = [f"FROM {tag}"]
        return DockerfileSpec(commands=commands, context_files={})

    if registry_secret:
        if credential_type is None:
            raise InvalidError("credential_type must be provided when using a registry_secret")
        elif credential_type == "static":
            auth_type = api_pb2.REGISTRY_AUTH_TYPE_STATIC_CREDS
        elif credential_type == "aws":
            auth_type = api_pb2.REGISTRY_AUTH_TYPE_AWS
        elif credential_type == "gcp":
            auth_type = api_pb2.REGISTRY_AUTH_TYPE_GCP
        else:
            raise InvalidError(f"Invalid credential_type: {credential_type!r}")
        registry_config = _ImageRegistryConfig(auth_type, registry_secret)
    else:
        registry_config = None

    return _Image._from_args(
        dockerfile_function=build_dockerfile,
        image_registry_config=registry_config,
        force_build=force_build,
    )


def _install_cuda_command() -> str:
    """Command to install CUDA Toolkit (nvcc) inside a container."""
    arch = "x86_64"  # instruction set architecture for the CPU, all Modal machines are x86_64
    distro = "debian12"  # the distribution and version number of our OS (GNU/Linux)
    filename = "cuda-keyring_1.1-1_all.deb"  # NVIDIA signing key file
    cuda_keyring_url = f"https://developer.download.nvidia.com/compute/cuda/repos/{distro}/{arch}/{filename}"

    major, minor = 12, 8
    max_cuda_version = f"{major}-{minor}"

    return (
        f"wget {cuda_keyring_url} && "
        + f"dpkg -i {filename} && "
        + f"rm -f {filename} && "
        + f"apt-get update && apt-get install -y cuda-nvcc-{max_cuda_version}"
    )


@synchronizer.create_blocking
async def notebook_base_image(*, python_version: Optional[str] = None, force_build: bool = False) -> _Image:
    """Default image used for Modal notebook kernels, with common libraries.

    This can be used to bootstrap development workflows quickly. We don't
    recommend using this image for production Modal Functions though, as it may
    change at any time in the future.
    """
    # Include several common packages, as well as kernelshim dependencies (except 'modal').
    # These packages aren't pinned, so they may change over time with builds.
    #
    # We plan to use `--exclude-newer` in the future, with date-specific image builds.
    base_image = _Image.debian_slim(python_version=python_version)

    environment_packages: list[str] = [
        "accelerate",
        "aiohttp",
        "altair",
        "anthropic",
        "asyncpg",
        "beautifulsoup4",
        "bokeh",
        "boto3[crt]",
        "click",
        "diffusers[torch,flax]",
        "dm-sonnet",
        "flax",
        "ftfy",
        "h5py",
        "urllib3",
        "httpx",
        "huggingface-hub",
        "ipywidgets",
        "jax[cuda12]",
        "keras",
        "matplotlib",
        "nbformat",
        "numba",
        "numpy",
        "openai",
        "optax",
        "pandas",
        "plotly[express]",
        "polars",
        "psycopg2",
        "requests",
        "safetensors",
        "scikit-image",
        "scikit-learn",
        "scipy",
        "seaborn",
        "sentencepiece",
        "sqlalchemy",
        "statsmodels",
        "sympy",
        "tabulate",
        "tensorboard",
        "toml",
        "transformers",
        "triton",
        "typer",
        "vega-datasets",
        "watchfiles",
        "websockets",
    ]

    # Kernelshim dependencies. (see NOTEBOOK_KERNELSHIM_DEPENDENCIES)
    kernelshim_packages: list[str] = [
        "authlib>=1.3",
        "basedpyright>=1.28",
        "fastapi>=0.100",
        "ipykernel>=6",
        "pydantic>=2",
        "pyzmq>=26",
        "ruff>=0.11",
        "uvicorn>=0.32",
    ]

    commands: list[str] = [
        "apt-get update",
        "apt-get install -y "
        + "libpq-dev pkg-config cmake git curl wget unzip zip libsqlite3-dev openssh-server vim ffmpeg",
        _install_cuda_command(),
        # Install uv since it's faster than pip for installing packages.
        "pip install uv",
        # https://github.com/astral-sh/uv/issues/11480
        "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu129",
        f"uv pip install --system {shlex.join(sorted(environment_packages))}",
        f"uv pip install --system {shlex.join(sorted(kernelshim_packages))}",
    ]

    def build_dockerfile(version: ImageBuilderVersion) -> DockerfileSpec:
        return DockerfileSpec(
            commands=[
                "FROM base",
                *(f"RUN {cmd}" for cmd in commands),
                "ENV PATH=/usr/local/cuda/bin:$PATH",
            ],
            context_files={},
        )

    return _Image._from_args(
        base_images={"base": base_image},
        dockerfile_function=build_dockerfile,
        force_build=force_build,
        _namespace=api_pb2.DEPLOYMENT_NAMESPACE_GLOBAL,
    )


@synchronizer.create_blocking
async def image_delete(
    image_id: str,
    *,
    client: Optional[_Client] = None,
) -> None:
    """Delete an Image by its ID.

    Deletion is irreversible and will prevent Apps from using the Image.

    This is an experimental interface for a feature that we will be adding to
    the main Image class. The stable form of this interface may look different.

    Note: When building an Image, each chained method call will create an
    intermediate Image layer, each with its own ID. Deleting an Image will not
    delete any of its intermediate layers, only the image identified by the
    provided ID.
    """
    if client is None:
        client = await _Client.from_env()

    req = api_pb2.ImageDeleteRequest(image_id=image_id)
    await retry_transient_errors(client.stub.ImageDelete, req)



================================================
FILE: modal/experimental/flash.py
================================================
# Copyright Modal Labs 2025
import asyncio
import math
import os
import subprocess
import sys
import time
import traceback
from collections import defaultdict
from typing import Any, Optional
from urllib.parse import urlparse

from modal.cls import _Cls
from modal.dict import _Dict
from modal_proto import api_pb2

from .._tunnel import _forward as _forward_tunnel
from .._utils.async_utils import synchronize_api, synchronizer
from .._utils.grpc_utils import retry_transient_errors
from ..client import _Client
from ..config import logger
from ..exception import InvalidError

_MAX_FAILURES = 10


class _FlashManager:
    def __init__(
        self,
        client: _Client,
        port: int,
        process: Optional[subprocess.Popen] = None,
        health_check_url: Optional[str] = None,
    ):
        self.client = client
        self.port = port
        # Health check is not currently being used
        self.health_check_url = health_check_url
        self.process = process
        self.tunnel_manager = _forward_tunnel(port, client=client)
        self.stopped = False
        self.num_failures = 0
        self.task_id = os.environ["MODAL_TASK_ID"]

    async def is_port_connection_healthy(
        self, process: Optional[subprocess.Popen], timeout: float = 0.5
    ) -> tuple[bool, Optional[Exception]]:
        import socket

        start_time = time.monotonic()

        while time.monotonic() - start_time < timeout:
            try:
                if process is not None and process.poll() is not None:
                    return False, Exception(f"Process {process.pid} exited with code {process.returncode}")
                with socket.create_connection(("localhost", self.port), timeout=0.5):
                    return True, None
            except (ConnectionRefusedError, OSError):
                await asyncio.sleep(0.1)

        return False, Exception(f"Waited too long for port {self.port} to start accepting connections")

    async def _start(self):
        self.tunnel = await self.tunnel_manager.__aenter__()
        parsed_url = urlparse(self.tunnel.url)
        host = parsed_url.hostname
        port = parsed_url.port or 443

        self.heartbeat_task = asyncio.create_task(self._run_heartbeat(host, port))
        self.drain_task = asyncio.create_task(self._drain_container())

    async def _drain_container(self):
        """
        Background task that checks if we've encountered too many failures and drains the container if so.
        """
        while True:
            try:
                # Check if the container should be drained (e.g., too many failures)
                if self.num_failures > _MAX_FAILURES:
                    logger.warning(
                        f"[Modal Flash] Draining task {self.task_id} on {self.tunnel.url} due to too many failures."
                    )
                    await self.stop()
                    # handle close upon container exit

                    if self.task_id:
                        await self.client.stub.ContainerStop(api_pb2.ContainerStopRequest(task_id=self.task_id))
                    return
            except asyncio.CancelledError:
                logger.warning("[Modal Flash] Shutting down...")
                return
            except Exception as e:
                logger.error(f"[Modal Flash] Error draining container: {e}")
                await asyncio.sleep(1)

            try:
                await asyncio.sleep(1)
            except asyncio.CancelledError:
                logger.warning("[Modal Flash] Shutting down...")
                return

    async def _run_heartbeat(self, host: str, port: int):
        first_registration = True
        while True:
            try:
                port_check_resp, port_check_error = await self.is_port_connection_healthy(process=self.process)
                if port_check_resp:
                    resp = await self.client.stub.FlashContainerRegister(
                        api_pb2.FlashContainerRegisterRequest(
                            priority=10,
                            weight=5,
                            host=host,
                            port=port,
                        ),
                        timeout=10,
                    )
                    self.num_failures = 0
                    if first_registration:
                        logger.warning(
                            f"[Modal Flash] Listening at {resp.url} over {self.tunnel.url} for task_id {self.task_id}"
                        )
                        first_registration = False
                else:
                    logger.error(
                        f"[Modal Flash] Deregistering container {self.task_id} on {self.tunnel.url} "
                        f"due to error: {port_check_error}, num_failures: {self.num_failures}"
                    )
                    self.num_failures += 1
                    await retry_transient_errors(
                        self.client.stub.FlashContainerDeregister,
                        api_pb2.FlashContainerDeregisterRequest(),
                    )
            except asyncio.CancelledError:
                logger.warning("[Modal Flash] Shutting down...")
                break
            except Exception as e:
                logger.error(f"[Modal Flash] Heartbeat failed: {e}")

            try:
                await asyncio.sleep(1)
            except asyncio.CancelledError:
                logger.warning("[Modal Flash] Shutting down...")
                break

    def get_container_url(self):
        # WARNING: Try not to use this method; we aren't sure if we will keep it.
        return self.tunnel.url

    async def stop(self):
        self.heartbeat_task.cancel()
        await retry_transient_errors(
            self.client.stub.FlashContainerDeregister,
            api_pb2.FlashContainerDeregisterRequest(),
        )

        self.stopped = True
        logger.warning(f"[Modal Flash] No longer accepting new requests on {self.tunnel.url}.")

        # NOTE(gongy): We skip calling TunnelStop to avoid interrupting in-flight requests.
        # It is up to the user to wait after calling .stop() to drain in-flight requests.

    async def close(self):
        if not self.stopped:
            await self.stop()

        logger.warning(f"[Modal Flash] Closing tunnel on {self.tunnel.url}.")
        await self.tunnel_manager.__aexit__(*sys.exc_info())


FlashManager = synchronize_api(_FlashManager)


@synchronizer.create_blocking
async def flash_forward(
    port: int,
    process: Optional[subprocess.Popen] = None,
    health_check_url: Optional[str] = None,
) -> _FlashManager:
    """
    Forward a port to the Modal Flash service, exposing that port as a stable web endpoint.
    This is a highly experimental method that can break or be removed at any time without warning.
    Do not use this method unless explicitly instructed to do so by Modal support.
    """
    client = await _Client.from_env()

    manager = _FlashManager(client, port, process=process, health_check_url=health_check_url)
    await manager._start()
    return manager


class _FlashPrometheusAutoscaler:
    _max_window_seconds = 60 * 60

    def __init__(
        self,
        client: _Client,
        app_name: str,
        cls_name: str,
        metrics_endpoint: str,
        target_metric: str,
        target_metric_value: float,
        min_containers: Optional[int],
        max_containers: Optional[int],
        buffer_containers: Optional[int],
        scale_up_tolerance: float,
        scale_down_tolerance: float,
        scale_up_stabilization_window_seconds: int,
        scale_down_stabilization_window_seconds: int,
        autoscaling_interval_seconds: int,
    ):
        import aiohttp

        if scale_up_stabilization_window_seconds > self._max_window_seconds:
            raise InvalidError(
                f"scale_up_stabilization_window_seconds must be less than or equal to {self._max_window_seconds}"
            )
        if scale_down_stabilization_window_seconds > self._max_window_seconds:
            raise InvalidError(
                f"scale_down_stabilization_window_seconds must be less than or equal to {self._max_window_seconds}"
            )
        if target_metric_value <= 0:
            raise InvalidError("target_metric_value must be greater than 0")

        self.client = client
        self.app_name = app_name
        self.cls_name = cls_name
        self.metrics_endpoint = metrics_endpoint
        self.target_metric = target_metric
        self.target_metric_value = target_metric_value
        self.min_containers = min_containers
        self.max_containers = max_containers
        self.buffer_containers = buffer_containers
        self.scale_up_tolerance = scale_up_tolerance
        self.scale_down_tolerance = scale_down_tolerance
        self.scale_up_stabilization_window_seconds = scale_up_stabilization_window_seconds
        self.scale_down_stabilization_window_seconds = scale_down_stabilization_window_seconds
        self.autoscaling_interval_seconds = autoscaling_interval_seconds

        FlashClass = _Cls.from_name(app_name, cls_name)
        self.fn = FlashClass._class_service_function
        self.cls = FlashClass()

        self.http_client = aiohttp.ClientSession()
        self.autoscaling_decisions_dict = _Dict.from_name(
            f"{app_name}-{cls_name}-autoscaling-decisions",
            create_if_missing=True,
        )

        self.autoscaler_thread = None

    async def start(self):
        await self.fn.hydrate(client=self.client)
        self.autoscaler_thread = asyncio.create_task(self._run_autoscaler_loop())

    async def _run_autoscaler_loop(self):
        while True:
            try:
                autoscaling_time = time.time()

                current_replicas = await self.autoscaling_decisions_dict.get("current_replicas", 0)
                autoscaling_decisions = await self.autoscaling_decisions_dict.get("autoscaling_decisions", [])
                if not isinstance(current_replicas, int):
                    logger.warning(f"[Modal Flash] Invalid item in autoscaling decisions: {current_replicas}")
                    current_replicas = 0
                if not isinstance(autoscaling_decisions, list):
                    logger.warning(f"[Modal Flash] Invalid item in autoscaling decisions: {autoscaling_decisions}")
                    autoscaling_decisions = []
                for item in autoscaling_decisions:
                    if (
                        not isinstance(item, tuple)
                        or len(item) != 2
                        or not isinstance(item[0], float)
                        or not isinstance(item[1], int)
                    ):
                        logger.warning(f"[Modal Flash] Invalid item in autoscaling decisions: {item}")
                        autoscaling_decisions = []
                        break

                autoscaling_decisions = [
                    (timestamp, decision)
                    for timestamp, decision in autoscaling_decisions
                    if timestamp >= autoscaling_time - self._max_window_seconds
                ]

                current_target_containers = await self._compute_target_containers(current_replicas=current_replicas)
                autoscaling_decisions.append((autoscaling_time, current_target_containers))

                actual_target_containers = self._make_scaling_decision(
                    current_replicas,
                    autoscaling_decisions,
                    scale_up_stabilization_window_seconds=self.scale_up_stabilization_window_seconds,
                    scale_down_stabilization_window_seconds=self.scale_down_stabilization_window_seconds,
                    min_containers=self.min_containers,
                    max_containers=self.max_containers,
                    buffer_containers=self.buffer_containers,
                )

                logger.warning(
                    f"[Modal Flash] Scaling to {actual_target_containers=} containers. "
                    f" Autoscaling decision made in {time.time() - autoscaling_time} seconds."
                )

                await self.autoscaling_decisions_dict.put(
                    "autoscaling_decisions",
                    autoscaling_decisions,
                )
                await self.autoscaling_decisions_dict.put("current_replicas", actual_target_containers)

                await self._set_target_slots(actual_target_containers)

                if time.time() - autoscaling_time < self.autoscaling_interval_seconds:
                    await asyncio.sleep(self.autoscaling_interval_seconds - (time.time() - autoscaling_time))
            except asyncio.CancelledError:
                logger.warning("[Modal Flash] Shutting down autoscaler...")
                await self.http_client.close()
                break
            except Exception as e:
                logger.error(f"[Modal Flash] Error in autoscaler: {e}")
                logger.error(traceback.format_exc())
                await asyncio.sleep(self.autoscaling_interval_seconds)

    async def _compute_target_containers(self, current_replicas: int) -> int:
        """
        Gets internal metrics from container to autoscale up or down.
        """
        containers = await self._get_all_containers()
        if len(containers) > current_replicas:
            logger.info(
                f"[Modal Flash] Current replicas {current_replicas} is less than the number of containers "
                f"{len(containers)}. Setting current_replicas = num_containers."
            )
            current_replicas = len(containers)

        if current_replicas == 0:
            return 1

        # Get metrics based on autoscaler type (prometheus or internal)
        sum_metric, n_containers_with_metrics = await self._get_scaling_info(containers)

        desired_replicas = self._calculate_desired_replicas(
            n_current_replicas=current_replicas,
            sum_metric=sum_metric,
            n_containers_with_metrics=n_containers_with_metrics,
            n_total_containers=len(containers),
            target_metric_value=self.target_metric_value,
        )

        return max(1, desired_replicas)

    def _calculate_desired_replicas(
        self,
        n_current_replicas: int,
        sum_metric: float,
        n_containers_with_metrics: int,
        n_total_containers: int,
        target_metric_value: float,
    ) -> int:
        """
        Calculate the desired number of replicas to autoscale to.
        """
        buffer_containers = self.buffer_containers or 0

        # n_containers_missing = number of unhealthy containers + number of containers not registered in flash dns
        n_containers_missing_metric = n_current_replicas - n_containers_with_metrics
        # n_containers_unhealthy = number of dns registered containers that are not emitting metrics
        n_containers_unhealthy = n_total_containers - n_containers_with_metrics

        # Max is used to handle case when buffer_containers are first initialized.
        num_provisioned_containers = max(n_current_replicas - buffer_containers, 1)

        # Scale up assuming that every unhealthy container is at 1.5 x (1 + scale_up_tolerance) the target metric value.
        # This way if all containers are unhealthy, we will increase our number of containers.
        scale_up_target_metric_value = (
            sum_metric + 1.5 * (1 + self.scale_up_tolerance) * n_containers_unhealthy * target_metric_value
        ) / (num_provisioned_containers)

        # Scale down assuming that every container (including cold starting containers) are at the target metric value.
        # The denominator is just num_provisioned_containers because we don't want to account for the buffer containers.
        scale_down_target_metric_value = (sum_metric + n_containers_missing_metric * target_metric_value) / (
            num_provisioned_containers
        )

        scale_up_ratio = scale_up_target_metric_value / target_metric_value
        scale_down_ratio = scale_down_target_metric_value / target_metric_value

        desired_replicas = num_provisioned_containers
        if scale_up_ratio > 1 + self.scale_up_tolerance:
            desired_replicas = math.ceil(desired_replicas * scale_up_ratio)
        elif scale_down_ratio < 1 - self.scale_down_tolerance:
            desired_replicas = math.ceil(desired_replicas * scale_down_ratio)

        logger.warning(
            f"[Modal Flash] Current replicas: {n_current_replicas}, "
            f"target metric: {self.target_metric}"
            f"target metric value: {target_metric_value}, "
            f"current sum of metric values: {sum_metric}, "
            f"number of containers with metrics: {n_containers_with_metrics}, "
            f"number of containers unhealthy: {n_containers_unhealthy}, "
            f"number of containers missing metric (includes unhealthy): {n_containers_missing_metric}, "
            f"number of provisioned containers: {num_provisioned_containers}, "
            f"scale up ratio: {scale_up_ratio}, "
            f"scale down ratio: {scale_down_ratio}, "
            f"desired replicas: {desired_replicas}"
        )

        return desired_replicas

    async def _get_scaling_info(self, containers) -> tuple[float, int]:
        """Get metrics using either internal container metrics API or prometheus HTTP endpoints."""
        if self.metrics_endpoint == "internal":
            container_metrics_results = await asyncio.gather(
                *[self._get_container_metrics(container.task_id) for container in containers]
            )
            container_metrics_list = []
            for container_metric in container_metrics_results:
                if container_metric is None:
                    continue
                container_metrics_list.append(getattr(container_metric.metrics, self.target_metric))

            sum_metric = sum(container_metrics_list)
            n_containers_with_metrics = len(container_metrics_list)
        else:
            sum_metric = 0
            n_containers_with_metrics = 0

            container_metrics_list = await asyncio.gather(
                *[
                    self._get_metrics(f"https://{container.host}:{container.port}/{self.metrics_endpoint}")
                    for container in containers
                ]
            )

            for container_metrics in container_metrics_list:
                if (
                    container_metrics is None
                    or self.target_metric not in container_metrics
                    or len(container_metrics[self.target_metric]) == 0
                ):
                    continue
                sum_metric += container_metrics[self.target_metric][0].value
                n_containers_with_metrics += 1

        return sum_metric, n_containers_with_metrics

    async def _get_metrics(self, url: str) -> Optional[dict[str, list[Any]]]:  # technically any should be Sample
        from prometheus_client.parser import Sample, text_string_to_metric_families

        # Fetch the metrics from the endpoint
        try:
            response = await self.http_client.get(url, timeout=3)
            response.raise_for_status()
        except asyncio.TimeoutError:
            logger.warning(f"[Modal Flash] Timeout getting metrics from {url}")
            return None
        except Exception as e:
            logger.warning(f"[Modal Flash] Error getting metrics from {url}: {e}")
            return None

        # Read body with timeout/error handling and parse Prometheus metrics
        try:
            text_body = await response.text()
        except asyncio.TimeoutError:
            logger.warning(f"[Modal Flash] Timeout reading metrics body from {url}")
            return None
        except Exception as e:
            logger.warning(f"[Modal Flash] Error reading metrics body from {url}: {e}")
            return None

        # Parse the text-based Prometheus metrics format
        metrics: dict[str, list[Sample]] = defaultdict(list)
        for family in text_string_to_metric_families(text_body):
            for sample in family.samples:
                metrics[sample.name] += [sample]

        return metrics

    async def _get_container_metrics(self, container_id: str) -> Optional[api_pb2.TaskGetAutoscalingMetricsResponse]:
        req = api_pb2.TaskGetAutoscalingMetricsRequest(task_id=container_id)
        try:
            resp = await retry_transient_errors(self.client.stub.TaskGetAutoscalingMetrics, req)
            return resp
        except Exception as e:
            logger.warning(f"[Modal Flash] Error getting metrics for container {container_id}: {e}")
            return None

    async def _get_all_containers(self):
        req = api_pb2.FlashContainerListRequest(function_id=self.fn.object_id)
        resp = await retry_transient_errors(self.client.stub.FlashContainerList, req)
        return resp.containers

    async def _set_target_slots(self, target_slots: int):
        req = api_pb2.FlashSetTargetSlotsMetricsRequest(function_id=self.fn.object_id, target_slots=target_slots)
        await retry_transient_errors(self.client.stub.FlashSetTargetSlotsMetrics, req)
        return

    def _make_scaling_decision(
        self,
        current_replicas: int,
        autoscaling_decisions: list[tuple[float, int]],
        scale_up_stabilization_window_seconds: int = 0,
        scale_down_stabilization_window_seconds: int = 60 * 5,
        min_containers: Optional[int] = None,
        max_containers: Optional[int] = None,
        buffer_containers: Optional[int] = None,
    ) -> int:
        """
        Return the target number of containers following (simplified) Kubernetes HPA
        stabilization-window semantics.

        Args:
            current_replicas: Current number of running Pods/containers.
            autoscaling_decisions: List of (timestamp, desired_replicas) pairs, where
                                   timestamp is a UNIX epoch float (seconds).
                                   The list *must* contain at least one entry and should
                                   already include the most-recent measurement.
            scale_up_stabilization_window_seconds: 0 disables the up-window.
            scale_down_stabilization_window_seconds: 0 disables the down-window.
            min_containers / max_containers: Clamp the final decision to this range.

        Returns:
            The target number of containers.
        """

        if not autoscaling_decisions:
            # Without data we canâ€™t make a new decision â€“ stay where we are.
            return current_replicas

        # Sort just once in case the caller didnâ€™t: newest record is last.
        autoscaling_decisions.sort(key=lambda rec: rec[0])
        now_ts, latest_desired = autoscaling_decisions[-1]

        if latest_desired > current_replicas:
            # ---- SCALE-UP path ----
            window_start = now_ts - scale_up_stabilization_window_seconds
            # Consider only records *inside* the window.
            desired_candidates = [desired for ts, desired in autoscaling_decisions if ts >= window_start]
            # Use the *minimum* so that any temporary dip blocks the scale-up.
            candidate = min(desired_candidates) if desired_candidates else latest_desired
            new_replicas = max(current_replicas, candidate)  # never scale *down* here
        elif latest_desired < current_replicas:
            # ---- SCALE-DOWN path ----
            window_start = now_ts - scale_down_stabilization_window_seconds
            desired_candidates = [desired for ts, desired in autoscaling_decisions if ts >= window_start]
            # Use the *maximum* so that any temporary spike blocks the scale-down.
            candidate = max(desired_candidates) if desired_candidates else latest_desired
            new_replicas = min(current_replicas, candidate)  # never scale *up* here
        else:
            # No change requested.
            new_replicas = current_replicas

        # Clamp to [min_containers, max_containers].
        if min_containers is not None:
            new_replicas = max(min_containers, new_replicas)
        if max_containers is not None:
            new_replicas = min(max_containers, new_replicas)

        if buffer_containers is not None:
            new_replicas += buffer_containers

        return new_replicas

    async def stop(self):
        self.autoscaler_thread.cancel()
        await self.autoscaler_thread


FlashPrometheusAutoscaler = synchronize_api(_FlashPrometheusAutoscaler)


@synchronizer.create_blocking
async def flash_prometheus_autoscaler(
    app_name: str,
    cls_name: str,
    # Endpoint to fetch metrics from. Must be in Prometheus format. Example: "/metrics"
    # If metrics_endpoint is "internal", we will use containers' internal metrics to autoscale instead.
    metrics_endpoint: str,
    # Target metric to autoscale on. Example: "vllm:num_requests_running"
    # If metrics_endpoint is "internal", target_metrics options are: [cpu_usage_percent, memory_usage_percent]
    target_metric: str,
    # Target metric value. Example: 25
    # If metrics_endpoint is "internal", target_metric_value is a percentage value between 0.1 and 1.0 (inclusive),
    # indicating container's usage of that metric.
    target_metric_value: float,
    min_containers: Optional[int] = None,
    max_containers: Optional[int] = None,
    # Corresponds to https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#tolerance
    scale_up_tolerance: float = 0.1,
    # Corresponds to https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#tolerance
    scale_down_tolerance: float = 0.1,
    # Corresponds to https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#stabilization-window
    scale_up_stabilization_window_seconds: int = 0,
    # Corresponds to https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#stabilization-window
    scale_down_stabilization_window_seconds: int = 300,
    # How often to make autoscaling decisions.
    # Corresponds to --horizontal-pod-autoscaler-sync-period in Kubernetes.
    autoscaling_interval_seconds: int = 15,
    # Whether to include overprovisioned containers in the scale up calculation.
    buffer_containers: Optional[int] = None,
) -> _FlashPrometheusAutoscaler:
    """
    Autoscale a Flash service based on containers' Prometheus metrics.

    The package `prometheus_client` is required to use this method.

    This is a highly experimental method that can break or be removed at any time without warning.
    Do not use this method unless explicitly instructed to do so by Modal support.
    """

    try:
        import prometheus_client  # noqa: F401
    except ImportError:
        raise ImportError("The package `prometheus_client` is required to use this method.")

    client = await _Client.from_env()
    autoscaler = _FlashPrometheusAutoscaler(
        client=client,
        app_name=app_name,
        cls_name=cls_name,
        metrics_endpoint=metrics_endpoint,
        target_metric=target_metric,
        target_metric_value=target_metric_value,
        min_containers=min_containers,
        max_containers=max_containers,
        buffer_containers=buffer_containers,
        scale_up_tolerance=scale_up_tolerance,
        scale_down_tolerance=scale_down_tolerance,
        scale_up_stabilization_window_seconds=scale_up_stabilization_window_seconds,
        scale_down_stabilization_window_seconds=scale_down_stabilization_window_seconds,
        autoscaling_interval_seconds=autoscaling_interval_seconds,
    )
    await autoscaler.start()
    return autoscaler


@synchronizer.create_blocking
async def flash_get_containers(app_name: str, cls_name: str) -> list[dict[str, Any]]:
    """
    Return a list of flash containers for a deployed Flash service.

    This is a highly experimental method that can break or be removed at any time without warning.
    Do not use this method unless explicitly instructed to do so by Modal support.
    """
    client = await _Client.from_env()
    fn = _Cls.from_name(app_name, cls_name)._class_service_function
    assert fn is not None
    await fn.hydrate(client=client)
    req = api_pb2.FlashContainerListRequest(function_id=fn.object_id)
    resp = await retry_transient_errors(client.stub.FlashContainerList, req)
    return resp.containers



================================================
FILE: modal/experimental/ipython.py
================================================
# Copyright Modal Labs 2025

"""This module provides Jupyter/IPython extensions for Modal.

Use in a notebook with `%load_ext modal.experimental.ipython`.
"""

from IPython.core.magic import Magics, line_magic, magics_class

from ..cls import Cls
from ..exception import NotFoundError
from ..functions import Function


@magics_class
class ModalMagics(Magics):
    @line_magic
    def modal(self, line):
        """Lookup a deployed Modal Function or Class.

        **Example:**

        ```python notest
        %modal from my-app import my_function, MyClass as Foo

        # Now you can call my_function() and Foo from your notebook.
        my_function.remote()
        Foo().my_method.remote()
        ```
        """
        line = line.strip()
        if not line.startswith("from "):
            print("Invalid syntax. Use: %modal from [env/]<app> import <function|Class>[, <function|Class> [as alias]]")
            return

        # Remove the initial "from "
        line_without_from = line[5:]
        env_app_part, sep, import_part = line_without_from.partition(" import ")
        if not sep:
            print("Invalid syntax. Missing 'import' keyword.")
            return

        # Parse environment and app from "[env/]app"
        environment: str | None
        if "/" not in env_app_part:
            environment, app = None, env_app_part
        else:
            environment, app = env_app_part.split("/", 1)

        # Parse the import items (multiple imports separated by commas)
        import_items = [item.strip() for item in import_part.split(",")]
        for item in import_items:
            if not item:
                continue
            parts = item.split()
            # Expect either "Model" or "Model as alias"
            if len(parts) == 0:
                continue
            model_name = parts[0]
            alias = model_name
            if len(parts) == 3 and parts[1] == "as":
                alias = parts[2]
            elif len(parts) > 1:
                print(f"Invalid syntax in import item: {item!r}. Expected format: <function|Class> [as alias]")
                return

            # Try to load using Function; if not found, fallback to Cls
            try:
                obj: Function | Cls = Function.from_name(app, model_name, environment_name=environment)
                obj.hydrate()
            except NotFoundError:
                obj = Cls.from_name(app, model_name, environment_name=environment)
                obj.hydrate()

            # Set the loaded object in the notebook namespace
            self.shell.user_ns[alias] = obj  # type: ignore
            if environment:
                print(f"Loaded {alias!r} from environment {environment!r} and app {app!r}.")
            else:
                print(f"Loaded {alias!r} from app {app!r}.")


def load_ipython_extension(ipython):
    ipython.register_magics(ModalMagics)



================================================
FILE: modal_docs/__init__.py
================================================
# Copyright Modal Labs 2023



================================================
FILE: modal_docs/gen_cli_docs.py
================================================
# Copyright Modal Labs 2023
import inspect
import sys
from pathlib import Path
from typing import Optional, cast

from click import Command, Context, Group

from modal.cli.entry_point import entrypoint_cli


# Adapted from typer_cli, since it's incompatible with the latest version of typer
# (see https://github.com/tiangolo/typer-cli/issues/50)
def get_docs_for_click(
    obj: Command,
    ctx: Context,
    *,
    indent: int = 0,
    name: str = "",
    call_prefix: str = "",
) -> str:
    docs = "#" * (1 + indent)
    command_name = name or obj.name
    if call_prefix:
        command_name = f"{call_prefix} {command_name}"
    title = f"`{command_name}`" if command_name else "CLI"
    docs += f" {title}\n\n"
    if obj.help:
        docs += f"{inspect.cleandoc(obj.help)}\n\n"
    usage_pieces = obj.collect_usage_pieces(ctx)
    if usage_pieces:
        docs += "**Usage**:\n\n"
        docs += "```shell\n"
        if command_name:
            docs += f"{command_name} "
        docs += f"{' '.join(usage_pieces)}\n"
        docs += "```\n\n"
    args = []
    opts = []
    for param in obj.get_params(ctx):
        rv = param.get_help_record(ctx)
        if rv is not None:
            if getattr(param, "hidden", False):
                continue
            if param.param_type_name == "argument":
                args.append(rv)
            elif param.param_type_name == "option":
                opts.append(rv)
    if args:
        docs += "**Arguments**:\n\n"
        for arg_name, arg_help in args:
            docs += f"* `{arg_name}`"
            if arg_help:
                docs += f": {arg_help}"
            docs += "\n"
        docs += "\n"
    if opts:
        docs += "**Options**:\n\n"
        for opt_name, opt_help in opts:
            docs += f"* `{opt_name}`"
            if opt_help:
                docs += f": {opt_help}"
            docs += "\n"
        docs += "\n"
    if obj.epilog:
        docs += f"{obj.epilog}\n\n"
    if isinstance(obj, Group):
        group: Group = cast(Group, obj)
        commands = group.list_commands(ctx)
        if commands:
            docs += "**Commands**:\n\n"
            for command in commands:
                command_obj = group.get_command(ctx, command)
                assert command_obj
                if command_obj.hidden:
                    continue
                docs += f"* `{command_obj.name}`"
                command_help = command_obj.get_short_help_str(limit=250)
                if command_help:
                    docs += f": {command_help}"
                docs += "\n"
            docs += "\n"
        for command in commands:
            command_obj = group.get_command(ctx, command)
            if command_obj.hidden:
                continue
            assert command_obj
            use_prefix = ""
            if command_name:
                use_prefix += f"{command_name}"
            docs += get_docs_for_click(obj=command_obj, ctx=ctx, indent=indent + 1, call_prefix=use_prefix)
    return docs


def run(output_dirname: Optional[str]) -> None:
    entrypoint: Group = cast(Group, entrypoint_cli)
    ctx = Context(entrypoint)
    commands = entrypoint.list_commands(ctx)

    for command in commands:
        command_obj = entrypoint.get_command(ctx, command)
        if command_obj.hidden:
            continue
        docs = get_docs_for_click(obj=command_obj, ctx=ctx, call_prefix="modal")

        if output_dirname:
            output_dir = Path(output_dirname)
            output_dir.mkdir(parents=True, exist_ok=True)
            output_file = output_dir / f"{command}.md"
            print("Writing to", output_file)
            output_file.write_text(docs)
        else:
            print(docs)


if __name__ == "__main__":
    run(None if len(sys.argv) <= 1 else sys.argv[1])



================================================
FILE: modal_docs/gen_reference_docs.py
================================================
# Copyright Modal Labs 2023
import importlib
import inspect
import json
import os
import sys
import warnings
from typing import NamedTuple

from synchronicity.synchronizer import FunctionWithAio

from .mdmd.mdmd import (
    Category,
    class_str,
    default_filter,
    function_str,
    module_items,
    module_str,
    object_is_private,
    package_filter,
)


class DocItem(NamedTuple):
    label: str
    category: Category
    document: str
    in_sidebar: bool = True


def validate_doc_item(docitem: DocItem) -> DocItem:
    # Check that unwanted strings aren't leaking into our docs.
    bad_strings = [
        # Presence of a to-do inside a `DocItem` usually indicates it's been
        # placed inside a function signature definition or right underneath it, before the body.
        # Fix by moving the to-do into the body or above the signature.
        "TODO:"
    ]
    for line in docitem.document.splitlines():
        for bad_str in bad_strings:
            if bad_str in line:
                msg = f"Found unwanted string '{bad_str}' in content for item '{docitem.label}'. Problem line: {line}"
                raise ValueError(msg)
    return docitem


def run(output_dir: str = None):
    """Generate Modal docs."""
    import modal

    ordered_doc_items: list[DocItem] = []
    documented_items = set()

    def filter_non_aio(module, name):
        return not name.lower().startswith("aio")

    def filter_already_documented(module, name):
        item = getattr(module, name)
        try:
            if item in documented_items:
                return False
        except TypeError:  # unhashable stuff
            print(f"Warning: could not document item {name}: {item}:")
            return False
        documented_items.add(item)
        return True

    def modal_default_filter(module, name):
        return default_filter(module, name) and filter_non_aio(module, name) and filter_already_documented(module, name)

    def top_level_filter(module, name):
        item = getattr(module, name)
        if object_is_private(name, item) or inspect.ismodule(item):
            return False
        return package_filter("modal") and filter_already_documented(module, name) and filter_non_aio(module, name)

    base_title_level = "#"
    forced_module_docs = [
        ("modal.call_graph", "modal.call_graph"),
        ("modal.container_process", "modal.container_process"),
        ("modal.gpu", "modal.gpu"),
        ("modal.io_streams", "modal.io_streams"),
        ("modal.file_io", "modal.file_io"),
    ]
    # These aren't defined in `modal`, but should still be documented as top-level entries.
    forced_members: set[str] = set()
    # These are excluded from the sidebar, typically to 'soft release' some documentation.
    sidebar_excluded: set[str] = set()

    for title, modulepath in forced_module_docs:
        module = importlib.import_module(modulepath)
        document = module_str(modulepath, module, title_level=base_title_level, filter_items=modal_default_filter)
        if document:
            ordered_doc_items.append(
                validate_doc_item(
                    DocItem(
                        label=title,
                        category=Category.MODULE,
                        document=document,
                        in_sidebar=title not in sidebar_excluded,
                    )
                )
            )

    def f(module, member_name):
        return top_level_filter(module, member_name) or (member_name in forced_members)

    # now add all remaining top level modal.X entries
    for qual_name, item_name, item in module_items(modal, filter_items=f):
        if object_is_private(item_name, item):
            continue  # skip stuff that's part of explicit `handle_objects` above

        title = f"modal.{item_name}"
        if inspect.isclass(item):
            content = f"{base_title_level} {qual_name}\n\n" + class_str(item_name, item, base_title_level)
            category = Category.CLASS
        elif inspect.isroutine(item) or isinstance(item, FunctionWithAio):
            content = f"{base_title_level} {qual_name}\n\n" + function_str(item_name, item)
            category = Category.FUNCTION
        elif inspect.ismodule(item):
            continue  # skipping imported modules
        else:
            warnings.warn(f"Not sure how to document: {item_name} ({item})")
            continue
        ordered_doc_items.append(
            validate_doc_item(
                DocItem(
                    label=title,
                    category=category,
                    document=content,
                    in_sidebar=title not in sidebar_excluded,
                )
            )
        )
    ordered_doc_items.sort()

    for modulepath in ["modal.exception", "modal.config"]:
        module = importlib.import_module(modulepath)
        document = module_str(modulepath, module, title_level=base_title_level, filter_items=modal_default_filter)
        ordered_doc_items.append(
            DocItem(
                label=modulepath,
                category=Category.MODULE,
                document=document,
            )
        )

    # TODO: add some way of documenting our .aio sub-methods

    make_markdown_docs(
        ordered_doc_items,
        output_dir,
    )


def make_markdown_docs(items: list[DocItem], output_dir: str = None):
    def _write_file(rel_path: str, data: str):
        if output_dir is None:
            print(f"<<< {rel_path}")
            print(data)
            print(f">>> {rel_path}")
            return

        filename = os.path.join(output_dir, rel_path)
        print("Writing to", filename)
        os.makedirs(os.path.dirname(filename), exist_ok=True)
        with open(filename, "w") as fp:
            fp.write(data)

    sidebar_items = []
    for item in items:
        if item.in_sidebar:
            sidebar_items.append(
                {
                    "label": item.label,
                    "category": item.category.value,
                }
            )
        _write_file(f"{item.label}.md", item.document)

    sidebar_data = {"items": sidebar_items}
    _write_file("sidebar.json", json.dumps(sidebar_data))


if __name__ == "__main__":
    # running this module outputs docs to stdout for inspection, useful for debugging
    run(None if len(sys.argv) <= 1 else sys.argv[1])



================================================
FILE: modal_docs/mdmd/__init__.py
================================================
# Copyright Modal Labs 2023



================================================
FILE: modal_docs/mdmd/mdmd.py
================================================
# Copyright Modal Labs 2023
"""mdmd - MoDal MarkDown"""

import inspect
import warnings
from enum import Enum, EnumMeta
from types import ModuleType
from typing import Callable, Optional

import synchronicity.synchronizer

from .signatures import get_signature


def format_docstring(docstring: Optional[str]) -> str:
    if docstring is None:
        docstring = ""
    else:
        docstring = inspect.cleandoc(docstring)

    docstring = "\n".join(l for l in docstring.split("\n") if "mdmd:line-hidden" not in l)

    if docstring and not docstring.endswith("\n"):
        docstring += "\n"

    return docstring


def function_str(name: str, func) -> str:
    signature = get_signature(name, func)
    signature = "\n".join(l for l in signature.split("\n") if "mdmd:line-hidden" not in l)
    decl = f"""```python
{signature}
```\n\n"""
    docstring = format_docstring(func.__doc__)
    return decl + docstring


def class_str(name, obj, title_level="##"):
    def qual_name(cls):
        if cls.__module__ == "builtins":
            return cls.__name__
        return f"{cls.__module__}.{cls.__name__}"

    bases = [qual_name(b) for b in obj.__bases__]
    bases_str = f"({', '.join(bases)})" if bases else ""
    decl = f"""```python
class {name}{bases_str}
```\n\n"""
    parts = [decl]
    docstring = format_docstring(obj.__doc__)

    if isinstance(obj, EnumMeta) and not docstring:
        # Python 3.11 removed the docstring from enums
        docstring = "An enumeration.\n"

    if docstring:
        parts.append(docstring + "\n")

    if isinstance(obj, EnumMeta):
        enum_vals = "\n".join(f"* `{k}`" for k in obj.__members__.keys())
        parts.append(f"The possible values are:\n\n{enum_vals}\n")

    else:
        init = inspect.unwrap(obj.__init__)

        if (inspect.isfunction(init) or inspect.ismethod(init)) and not object_is_private("constructor", init):
            parts.append(function_str("__init__", init))

    member_title_level = title_level + "#"

    entries = {}

    def rec_update_attributes(cls):
        # first bases, then class itself
        for base_cls in cls.__bases__:
            rec_update_attributes(base_cls)
        entries.update(cls.__dict__)

    rec_update_attributes(obj)

    for member_name, member in entries.items():
        if isinstance(member, synchronicity.synchronizer.classproperty):
            member_obj = getattr(obj, member_name)
            if not inspect.isclass(member_obj):
                # A little hacky; right now we are only using classproperty for the .objects manager classes
                # I'm adding this constraint to avoid refactoring this to support more recursive calling
                print(f"* Skipping {member_name}; we currnetly assume classproperty is a class")
                continue
            parts.append(f"{member_title_level} {member_name}\n\n")
            parts.append(class_str(member_name, member_obj, title_level=title_level + "#"))
            continue
        elif isinstance(member, classmethod) or isinstance(member, staticmethod):
            # get the original function definition instead of the descriptor object
            member = getattr(obj, member_name)
        elif isinstance(member, property):
            member = member.fget
        elif isinstance(member, (synchronicity.synchronizer.FunctionWithAio, synchronicity.synchronizer.MethodWithAio)):
            member = member._func

        if object_is_private(member_name, member):
            continue

        if callable(member):
            parts.append(f"{member_title_level} {member_name}\n\n")
            parts.append(function_str(member_name, member))

    return "".join(parts)


def module_str(header, module, title_level="#", filter_items: Callable[[ModuleType, str], bool] = None):
    header = [f"{title_level} {header}\n\n"]
    docstring = format_docstring(module.__doc__)
    if docstring:
        header.append(docstring + "\n")

    object_docs = []
    member_title_level = title_level + "#"
    for qual_name, name, item in module_items(module, filter_items):
        try:
            if hasattr(item, "__wrapped__"):
                item = item.__wrapped__
        except KeyError:
            pass
        except:
            print("failed on", qual_name, name, item)
            raise
        if inspect.isclass(item):
            classdoc = class_str(name, item, title_level=member_title_level)
            object_docs.append(f"{member_title_level} {qual_name}\n\n")
            object_docs.append(classdoc)
        elif callable(item):
            funcdoc = function_str(name, item)
            object_docs.append(f"{member_title_level} {qual_name}\n\n")
            object_docs.append(funcdoc)
        else:
            item_doc = getattr(module, f"__doc__{name}", None)
            if item_doc:
                # variable documentation
                object_docs.append(f"{member_title_level} {qual_name}\n\n")
                object_docs.append(item_doc)
            else:
                warnings.warn(f"Not sure how to document: {name} ({item}")

    if object_docs:
        return "".join(header + object_docs)
    return ""


def object_is_private(name, obj):
    docstring = inspect.getdoc(obj)
    if docstring is None:
        docstring = ""
    module = getattr(obj, "__module__", None)  # obj is class
    if not module:
        cls = getattr(obj, "__class__", None)  # obj is instance
        if cls:
            module = getattr(cls, "__module__", None)
    if module == "builtins":
        return True

    if docstring.lstrip().startswith("mdmd:hidden") or name.startswith("_"):
        return True

    return False


def default_filter(module, item_name):
    """Include non-private objects defined in the module itself"""
    item = getattr(module, item_name)
    if object_is_private(item_name, item) or inspect.ismodule(item):
        return False
    member_module = getattr(item, "__module__", type(item).__module__)
    return member_module == module.__name__


def package_filter(module_prefix: str):
    """Include non-private objects defined in any module with the prefix `module_prefix`"""

    def return_filter(module, item_name):
        item = getattr(module, item_name)
        if object_is_private(item_name, item) or inspect.ismodule(item):
            return False
        member_module = getattr(item, "__module__", type(item).__module__)
        return member_module.startswith(module_prefix)

    return return_filter


def module_items(module, filter_items: Callable[[ModuleType, str], bool] = None):
    """Returns filtered members of module"""
    if filter_items is None:
        # default filter is to only include classes and functions declared (or whose type is declared) in the file
        filter_items = default_filter

    for member_name, member in inspect.getmembers(module):
        # only modal items
        if not filter_items(module, member_name):
            continue

        qual_name = f"{module.__name__}.{member_name}"
        yield qual_name, member_name, member


class Category(Enum):
    FUNCTION = "function"
    CLASS = "class"
    MODULE = "module"



================================================
FILE: modal_docs/mdmd/signatures.py
================================================
# Copyright Modal Labs 2023
import ast
import inspect
import re
import textwrap
import warnings

from synchronicity.synchronizer import FunctionWithAio


def _signature_from_ast(func) -> tuple[str, str]:
    """Get function signature, including decorators and comments, from source code

    Traverses functools.wraps-wrappings to get source of underlying function.

    Has the advantage over inspect.signature that it can get decorators, default arguments and comments verbatim
    from the function definition.
    """
    src = inspect.getsource(func)
    src = textwrap.dedent(src)

    def get_source_segment(src, fromline, fromcol, toline, tocol) -> str:
        lines = src.split("\n")
        lines = lines[fromline - 1 : toline]
        lines[-1] = lines[-1][:tocol]
        lines[0] = lines[0][fromcol:]
        return "\n".join(lines)

    tree = ast.parse(src)
    func_def = list(ast.iter_child_nodes(tree))[0]
    assert isinstance(func_def, (ast.FunctionDef, ast.AsyncFunctionDef))
    decorator_starts = [(item.lineno, item.col_offset - 1) for item in func_def.decorator_list]
    declaration_start = min([(func_def.lineno, func_def.col_offset)] + decorator_starts)
    body_start = min((item.lineno, item.col_offset) for item in func_def.body)

    return (
        func_def.name,
        get_source_segment(src, declaration_start[0], declaration_start[1], body_start[0], body_start[1] - 1).strip(),
    )


def get_signature(name, callable) -> str:
    """A problem with using *only* this method is that the wrapping method signature will not be respected.
    TODO: use source parsing *only* to extract default arguments, comments (and possibly decorators) and "merge"
          that definition with the outer-most definition."""

    if not (inspect.isfunction(callable) or inspect.ismethod(callable) or isinstance(callable, FunctionWithAio)):
        assert hasattr(callable, "__call__")
        callable = callable.__call__

    try:
        original_name, definition_source = _signature_from_ast(callable)
    except Exception:
        warnings.warn(f"Could not get source signature for {name}. Using fallback.")
        original_name = name
        definition_source = f"def {name}{inspect.signature(callable)}"

    if original_name != name:
        # ugly name and definition replacement hack when needed
        definition_source = definition_source.replace(f"def {original_name}", f"def {name}")

    if (
        "async def" in definition_source
        and not inspect.iscoroutinefunction(callable)
        and not inspect.isasyncgenfunction(callable)
    ):
        # hack to "reset" signature to a blocking one if the underlying source definition is async
        # but the wrapper function isn't (like when synchronicity wraps an async function as a blocking one)
        definition_source = definition_source.replace("async def", "def")
        definition_source = definition_source.replace("asynccontextmanager", "contextmanager")
        definition_source = definition_source.replace("AsyncIterator", "Iterator")

    # remove any synchronicity-internal decorators
    definition_source, _ = re.subn(r"^\s*@synchronizer\..*\n", "", definition_source)

    return definition_source



================================================
FILE: modal_global_objects/__init__.py
================================================
# Copyright Modal Labs 2022



================================================
FILE: modal_global_objects/images/__init__.py
================================================
# Copyright Modal Labs 2024



================================================
FILE: modal_global_objects/images/base_images.py
================================================
# Copyright Modal Labs 2022
import os
import sys
from typing import Any, cast

import rich
from rich.table import Table

import modal
from modal.experimental import notebook_base_image
from modal.image import SUPPORTED_PYTHON_SERIES, ImageBuilderVersion


def dummy():
    pass


if __name__ == "__main__":
    _, name = sys.argv

    constructor: Any  # todo: notebook_base_image has messed up type inference
    if name in ("debian_slim", "micromamba"):
        constructor = getattr(modal.Image, name)
    elif name == "notebook":
        constructor = notebook_base_image
    else:
        raise ValueError(f"Unknown base image type: {name}")

    builder_version = os.environ.get("MODAL_IMAGE_BUILDER_VERSION")
    assert builder_version, "Script requires MODAL_IMAGE_BUILDER_VERSION environment variable"
    python_versions = SUPPORTED_PYTHON_SERIES[cast(ImageBuilderVersion, builder_version)]

    app = modal.App(f"build-{name.replace('_', '-')}-image")
    images_map: dict[str, modal.Image] = {}
    for v in python_versions:
        image = constructor(python_version=v)
        images_map[v] = image
        app.function(image=image, name=f"{v}", serialized=True)(dummy)

    with modal.enable_output():
        with app.run():
            pass

    table = Table(title=f"Images for {name} ({builder_version})")
    table.add_column("Python version")
    table.add_column("Image ID")

    for v, image in images_map.items():
        table.add_row(v, image.object_id)

    rich.print()
    rich.print(table)



================================================
FILE: modal_global_objects/mounts/__init__.py
================================================
# Copyright Modal Labs 2024



================================================
FILE: modal_global_objects/mounts/modal_client_dependencies.py
================================================
# Copyright Modal Labs 2025
import argparse
import asyncio

from modal.mount import _create_client_dependency_mounts


def main():
    parser = argparse.ArgumentParser(description="Create client dependency mounts")
    parser.add_argument(
        "--dry-run", action="store_true", default=False, help="Run in dry-run mode without making actual changes"
    )
    args = parser.parse_args()

    loop = asyncio.get_event_loop()
    loop.run_until_complete(_create_client_dependency_mounts(dry_run=args.dry_run))


if __name__ == "__main__":
    main()



================================================
FILE: modal_global_objects/mounts/modal_client_package.py
================================================
# Copyright Modal Labs 2022
from modal.config import config
from modal.exception import NotFoundError
from modal.mount import (
    Mount,
    client_mount_name,
    create_client_mount,
)
from modal_proto import api_pb2


def publish_client_mount(client):
    mount = create_client_mount()
    name = client_mount_name()
    profile_environment = config.get("environment")
    try:
        Mount.from_name(name, namespace=api_pb2.DEPLOYMENT_NAMESPACE_GLOBAL).hydrate(client)
        print(f"âž– Found existing mount {name} in global namespace.")
    except NotFoundError:
        mount._deploy(
            name,
            api_pb2.DEPLOYMENT_NAMESPACE_GLOBAL,
            client=client,
            environment_name=profile_environment,
        )
        print(f"âœ… Deployed client mount {name} to global namespace.")


def main(client=None):
    publish_client_mount(client)


if __name__ == "__main__":
    main()



================================================
FILE: modal_global_objects/mounts/python_standalone.py
================================================
# Copyright Modal Labs 2022
import shutil
import tempfile
import urllib.request

from modal.config import config
from modal.exception import NotFoundError
from modal.mount import (
    PYTHON_STANDALONE_VERSIONS,
    Mount,
    python_standalone_mount_name,
)
from modal_proto import api_pb2


def publish_python_standalone_mount(client, version: str) -> None:
    release, full_version = PYTHON_STANDALONE_VERSIONS[version]

    libc = "gnu"
    arch = "x86_64_v3"
    url = (
        "https://github.com/indygreg/python-build-standalone/releases/download"
        + f"/{release}/cpython-{full_version}+{release}-{arch}-unknown-linux-gnu-install_only.tar.gz"
    )

    profile_environment = config.get("environment")
    mount_name = python_standalone_mount_name(f"{version}-{libc}")
    try:
        Mount.from_name(mount_name, namespace=api_pb2.DEPLOYMENT_NAMESPACE_GLOBAL).hydrate(client)
        print(f"âž– Found existing mount {mount_name} in global namespace.")
    except NotFoundError:
        print(f"ðŸ“¦ Unpacking python-build-standalone for {version}-{libc}.")
        with tempfile.TemporaryDirectory() as d:
            urllib.request.urlretrieve(url, f"{d}/cpython.tar.gz")
            shutil.unpack_archive(f"{d}/cpython.tar.gz", d)
            print(f"ðŸŒ Downloaded and unpacked archive to {d}.")
            python_mount = Mount._from_local_dir(f"{d}/python")
            python_mount._deploy(
                mount_name,
                api_pb2.DEPLOYMENT_NAMESPACE_GLOBAL,
                client=client,
                environment_name=profile_environment,
            )
            print(f"âœ… Deployed mount {mount_name} to global namespace.")


def main(client=None):
    for version in PYTHON_STANDALONE_VERSIONS:
        publish_python_standalone_mount(client, version)


if __name__ == "__main__":
    main()



================================================
FILE: modal_proto/__init__.py
================================================
# Copyright Modal Labs 2024



================================================
FILE: modal_proto/options.proto
================================================
// Defines custom options used internally at Modal.
// Custom options must be in the range 50000-99999.
// Reference: https://protobuf.dev/programming-guides/proto2/#customoptions
syntax = "proto3";

option go_package = "github.com/modal-labs/modal/go/proto";

import "google/protobuf/descriptor.proto";

package modal.options;

extend google.protobuf.FieldOptions {
  optional bool audit_target_attr = 50000;
}

extend google.protobuf.MethodOptions {
  optional string audit_event_name = 50000;
  optional string audit_event_description = 50001;
}



================================================
FILE: modal_proto/py.typed
================================================
[Empty file]


================================================
FILE: modal_proto/sandbox_router.proto
================================================
syntax = "proto3";

import "modal_proto/api.proto";

package modal.sandbox_router;

enum SandboxExecStderrConfig {
  // The output will be discarded.
  SANDBOX_EXEC_STDERR_CONFIG_DEVNULL = 0;
  // The output will be streamed to the client.
  SANDBOX_EXEC_STDERR_CONFIG_PIPE = 1;
  // A special value that can be used to indicate that the stderr stream should
  // be merged with the stdout stream.
  SANDBOX_EXEC_STDERR_CONFIG_STDOUT = 2;
}

enum SandboxExecStdioFileDescriptor {
  // Read from stdout.
  SANDBOX_EXEC_STDIO_FILE_DESCRIPTOR_STDOUT = 0;
  // Read from stderr.
  SANDBOX_EXEC_STDIO_FILE_DESCRIPTOR_STDERR = 1;
}

enum SandboxExecStdoutConfig {
  // The output will be discarded.
  SANDBOX_EXEC_STDOUT_CONFIG_DEVNULL = 0;
  // The output will be streamed to the client.
  SANDBOX_EXEC_STDOUT_CONFIG_PIPE = 1;
}

message SandboxExecStartRequest {
  // The task ID of the sandbox to execute the command in.
  string task_id = 1;
  // Execution ID. This ID will be used to identify the execution for other
  // requests and ensure exec commands are idempotent.
  //
  // TODO(saltzm): Could instead have a separate idempotency key from the exec_id
  // like present day, and have the server generate the exec_id and return it in
  // the ExecStartResponse.
  string exec_id = 2;
  // Command arguments to execute.
  repeated string command_args= 3;
  // Configures how the stdout of the command will be handled.
  SandboxExecStdoutConfig stdout_config = 4;
  // Configures how the stderr of the command will be handled.
  SandboxExecStderrConfig stderr_config = 5;
  // Timeout in seconds for the exec'd command to exit. If the command does not
  // exit within this duration, the command will be killed. This is NOT the
  // timeout for the ExecStartRequest RPC to complete.
  optional uint32 timeout_secs = 6;
  // Working directory for the command.
  optional string workdir = 7;
  // Secret IDs to mount into the sandbox.
  repeated string secret_ids = 8;
  // PTY info for the command.
  optional modal.client.PTYInfo pty_info = 9;
  // Enable debugging capabilities on the container runtime. Used only for
  // internal debugging.
  bool runtime_debug = 10;
}

message SandboxExecStartResponse { }

message SandboxExecStdinWriteRequest {
  // The task ID of the sandbox running the exec'd command.
  string task_id = 1;
  // The execution ID of the command to write to.
  string exec_id = 2;
  // The offset to start writing to. This is used to resume writing from the
  // last write position if the connection is closed and reopened.
  uint64 offset = 3;
  bytes data = 4;
  // If true, close the stdin stream after writing any provided data.
  // This signals EOF to the exec'd process.
  bool eof = 5;
}

message SandboxExecStdinWriteResponse { }


message SandboxExecStdioReadRequest {
  // The task ID of the sandbox running the exec'd command.
  string task_id = 1;
  // The execution ID of the command to read from.
  string exec_id = 2;
  // The offset to start reading from. This is used to resume reading from the
  // last read position if the connection is closed and reopened.
  uint64 offset = 3;
  // Which file descriptor to read from.
  SandboxExecStdioFileDescriptor file_descriptor = 4;
}

message SandboxExecStdioReadResponse {
  // The data read from the file descriptor.
  bytes data = 1;
}

message SandboxExecWaitRequest {
  // The task ID of the sandbox running the exec'd command.
  string task_id = 1;
  // The execution ID of the command to wait on. 
  string exec_id = 2;
}

message SandboxExecWaitResponse {
  oneof exit_status {
    // The exit code of the command.
    int32 code = 3;
    // The signal that terminated the command.
    int32 signal = 4;
  }
  // TODO(saltzm): Give a way for the user to distinguish between normal exit
  // and termination by Modal (due to sandbox timeout, exec exceeded deadline, etc.)
}

service SandboxRouter {
  // Execute a command in the sandbox.
  rpc SandboxExecStart(SandboxExecStartRequest) returns (SandboxExecStartResponse);
 // Write to the stdin stream of an exec'd command.
  rpc SandboxExecStdinWrite(SandboxExecStdinWriteRequest) returns (SandboxExecStdinWriteResponse);
  // Get a stream of output from the stdout or stderr stream of an exec'd command.
  rpc SandboxExecStdioRead(SandboxExecStdioReadRequest) returns (stream SandboxExecStdioReadResponse);
  // Wait for an exec'd command to exit and return the exit code.
  rpc SandboxExecWait(SandboxExecWaitRequest) returns (SandboxExecWaitResponse);
}



================================================
FILE: modal_version/__init__.py
================================================
# Copyright Modal Labs 2025
"""Supplies the current version of the modal client library."""

__version__ = "1.1.5.dev75"



================================================
FILE: modal_version/__main__.py
================================================
# Copyright Modal Labs 2024
from . import __version__

if __name__ == "__main__":
    print(__version__)



================================================
FILE: protoc_plugin/plugin.py
================================================
#!/usr/bin/env python
# Copyright Modal Labs 2024
# built by modifying grpclib.plugin.main, see https://github.com/vmagamedov/grpclib
# original: Copyright (c) 2019  , Vladimir Magamedov
import os
import sys
from collections import deque
from collections.abc import Collection, Iterator
from contextlib import contextmanager
from pathlib import Path
from typing import Any, Deque, NamedTuple, Optional

from google.protobuf.compiler.plugin_pb2 import CodeGeneratorRequest, CodeGeneratorResponse
from google.protobuf.descriptor_pb2 import DescriptorProto, FileDescriptorProto
from grpclib import const

_CARDINALITY = {
    (False, False): const.Cardinality.UNARY_UNARY,
    (True, False): const.Cardinality.STREAM_UNARY,
    (False, True): const.Cardinality.UNARY_STREAM,
    (True, True): const.Cardinality.STREAM_STREAM,
}


class Method(NamedTuple):
    name: str
    cardinality: const.Cardinality
    request_type: str
    reply_type: str


class Service(NamedTuple):
    name: str
    methods: list[Method]


class Buffer:
    def __init__(self) -> None:
        self._lines: list[str] = []
        self._indent = 0

    def add(self, string: str, *args: Any, **kwargs: Any) -> None:
        line = " " * self._indent * 4 + string.format(*args, **kwargs)
        self._lines.append(line.rstrip(" "))

    @contextmanager
    def indent(self) -> Iterator[None]:
        self._indent += 1
        try:
            yield
        finally:
            self._indent -= 1

    def content(self) -> str:
        return "\n".join(self._lines) + "\n"


def render(
    proto_file: str,
    imports: Collection[str],
    services: Collection[Service],
    grpclib_module: str,
) -> str:
    buf = Buffer()
    buf.add("# Generated by the Modal Protocol Buffers compiler. DO NOT EDIT!")
    buf.add("# source: {}", proto_file)
    buf.add("# plugin: {}", __name__)
    if not services:
        return buf.content()

    buf.add("")
    for mod in imports:
        buf.add("import {}", mod)

    buf.add("import typing")
    buf.add("if typing.TYPE_CHECKING:")
    with buf.indent():
        buf.add("import modal.client")

    for service in services:
        buf.add("")
        buf.add("")
        grpclib_stub_name = f"{service.name}Stub"
        buf.add("class {}Modal:", service.name)
        with buf.indent():
            buf.add("@classmethod")
            buf.add("async def _create(cls, client: 'modal.client._Client', server_url: str):")
            with buf.indent():
                buf.add("channel = await client._get_channel(server_url)")
                buf.add(f"grpclib_stub = {grpclib_module}.{grpclib_stub_name}(channel)")
                buf.add("return cls(grpclib_stub, client, server_url)")

            buf.add("")
            buf.add("")
            buf.add(
                f"def __init__(self, grpclib_stub: {grpclib_module}.{grpclib_stub_name}, "
                + """client: "modal.client._Client", server_url: str) -> None:"""
            )
            with buf.indent():
                if len(service.methods) == 0:
                    buf.add("pass")
                for method in service.methods:
                    name, cardinality, request_type, reply_type = method
                    wrapper_cls: str
                    if cardinality is const.Cardinality.UNARY_UNARY:
                        wrapper_cls = "modal.client.UnaryUnaryWrapper"
                    elif cardinality is const.Cardinality.UNARY_STREAM:
                        wrapper_cls = "modal.client.UnaryStreamWrapper"
                    # elif cardinality is const.Cardinality.STREAM_UNARY:
                    #     wrapper_cls = StreamUnaryWrapper
                    # elif cardinality is const.Cardinality.STREAM_STREAM:
                    #     wrapper_cls = StreamStreamWrapper
                    else:
                        raise TypeError(cardinality)

                    original_method = f"grpclib_stub.{name}"
                    buf.add(f"self.{name} = {wrapper_cls}({original_method}, client, server_url)")

    return buf.content()


def _get_proto(request: CodeGeneratorRequest, name: str) -> FileDescriptorProto:
    return next(f for f in request.proto_file if f.name == name)


def _strip_proto(proto_file_path: str) -> str:
    for suffix in [".protodevel", ".proto"]:
        if proto_file_path.endswith(suffix):
            return proto_file_path[: -len(suffix)]

    return proto_file_path


def _base_module_name(proto_file_path: str) -> str:
    basename = _strip_proto(proto_file_path)
    return basename.replace("-", "_").replace("/", ".")


def _proto2pb2_module_name(proto_file_path: str) -> str:
    return _base_module_name(proto_file_path) + "_pb2"


def _proto2grpc_module_name(proto_file_path: str) -> str:
    return _base_module_name(proto_file_path) + "_grpc"


def _type_names(
    proto_file: FileDescriptorProto,
    message_type: DescriptorProto,
    parents: Optional[Deque[str]] = None,
) -> Iterator[tuple[str, str]]:
    if parents is None:
        parents = deque()

    proto_name_parts = [""]
    if proto_file.package:
        proto_name_parts.append(proto_file.package)
    proto_name_parts.extend(parents)
    proto_name_parts.append(message_type.name)

    py_name_parts = [_proto2pb2_module_name(proto_file.name)]
    py_name_parts.extend(parents)
    py_name_parts.append(message_type.name)

    yield ".".join(proto_name_parts), ".".join(py_name_parts)

    parents.append(message_type.name)
    for nested in message_type.nested_type:
        yield from _type_names(proto_file, nested, parents=parents)
    parents.pop()


def main() -> None:
    with os.fdopen(sys.stdin.fileno(), "rb") as inp:
        request = CodeGeneratorRequest.FromString(inp.read())

    types_map: dict[str, str] = {}
    for pf in request.proto_file:
        for mt in pf.message_type:
            types_map.update(_type_names(pf, mt))

    response = CodeGeneratorResponse()

    # See https://github.com/protocolbuffers/protobuf/blob/v3.12.0/docs/implementing_proto3_presence.md  # noqa
    if hasattr(CodeGeneratorResponse, "Feature"):
        response.supported_features = CodeGeneratorResponse.FEATURE_PROTO3_OPTIONAL

    for file_to_generate in request.file_to_generate:
        proto_file = _get_proto(request, file_to_generate)
        module_name = _proto2grpc_module_name(file_to_generate)
        grpclib_module_path = Path(module_name.replace(".", "/") + ".py")

        imports = ["modal._utils.grpc_utils", module_name]

        services = []
        for service in proto_file.service:
            methods = []
            for method in service.method:
                cardinality = _CARDINALITY[(method.client_streaming, method.server_streaming)]
                methods.append(
                    Method(
                        name=method.name,
                        cardinality=cardinality,
                        request_type=types_map[method.input_type],
                        reply_type=types_map[method.output_type],
                    )
                )
            services.append(Service(name=service.name, methods=methods))

        file = response.file.add()

        file.name = str(grpclib_module_path.with_name("modal_" + grpclib_module_path.name))
        file.content = render(
            proto_file=proto_file.name, imports=imports, services=services, grpclib_module=module_name
        )

    with os.fdopen(sys.stdout.fileno(), "wb") as out:
        out.write(response.SerializeToString())


if __name__ == "__main__":
    main()



================================================
FILE: test/__init__.py
================================================
# Copyright Modal Labs 2022



================================================
FILE: test/aio_test.py
================================================
# Copyright Modal Labs 2023
import pytest


@pytest.mark.asyncio
async def test_new(servicer, client):
    from modal import App

    app = App()

    async with app.run(client=client):
        pass



================================================
FILE: test/app_composition_test.py
================================================
# Copyright Modal Labs 2024
from test.helpers import deploy_app_externally


def test_app_composition_includes_all_functions(servicer, credentials, supports_dir, monkeypatch, client):
    print(deploy_app_externally(servicer, credentials, "multifile_project.main", cwd=supports_dir))
    assert servicer.n_functions == 5
    assert {
        "/root/multifile_project/__init__.py",
        "/root/multifile_project/main.py",
        "/root/multifile_project/a.py",
        "/root/multifile_project/b.py",
        "/root/multifile_project/c.py",
    } == set(servicer.files_name2sha.keys())
    assert len(servicer.secrets) == 1  # secret from B should be included
    assert servicer.n_mounts == 1  # mounts should not be duplicated, and the automount for the package includes all



================================================
FILE: test/app_test.py
================================================
# Copyright Modal Labs 2022
import asyncio
import logging
import pytest
import re
import sys
import time

from grpclib import GRPCError, Status

from modal import App, Image, Secret, Volume, enable_output, fastapi_endpoint, web_endpoint
from modal._partial_function import _parse_custom_domains
from modal._utils.async_utils import synchronizer
from modal.exception import DeprecationError, ExecutionError, InvalidError, NotFoundError
from modal.runner import run_app
from modal_proto import api_pb2

from .supports import module_1, module_2


def square(x):
    return x**2


@pytest.mark.asyncio
async def test_redeploy(servicer, client):
    app = App(image=Image.debian_slim().pip_install("pandas"))
    app.function()(square)

    # Deploy app
    await app.deploy.aio(name="my-app", client=client)
    assert app.app_id == "ap-1"
    assert servicer.app_objects["ap-1"]["square"] == "fu-1"
    assert servicer.app_state_history[app.app_id] == [api_pb2.APP_STATE_INITIALIZING, api_pb2.APP_STATE_DEPLOYED]

    # Redeploy, make sure all ids are the same
    await app.deploy.aio(name="my-app", client=client)
    assert app.app_id == "ap-1"
    assert servicer.app_objects["ap-1"]["square"] == "fu-1"
    assert servicer.app_state_history[app.app_id] == [
        api_pb2.APP_STATE_INITIALIZING,
        api_pb2.APP_STATE_DEPLOYED,
        api_pb2.APP_STATE_DEPLOYED,
    ]

    # Deploy to a different name, ids should change
    await app.deploy.aio(name="my-app-xyz", client=client)
    assert app.app_id == "ap-2"
    assert servicer.app_objects["ap-2"]["square"] == "fu-2"
    assert servicer.app_state_history[app.app_id] == [api_pb2.APP_STATE_INITIALIZING, api_pb2.APP_STATE_DEPLOYED]


def dummy():
    pass


# Should exit without waiting for the "logs_timeout" grace period.
@pytest.mark.timeout(5)
def test_create_object_internal_exception(servicer, client):
    servicer.function_create_error = GRPCError(Status.INTERNAL, "Function create failed")

    app = App()
    app.function()(dummy)

    with servicer.intercept() as ctx:
        with pytest.raises(GRPCError) as excinfo:
            with enable_output():  # this activates the log streaming loop, which could potentially hold up context exit
                with app.run(client=client):
                    pass

    assert len(ctx.get_requests("FunctionCreate")) == 4  # some retries are applied to internal errors
    assert excinfo.value.status == Status.INTERNAL
    assert len(ctx.get_requests("AppClientDisconnect")) == 1


@pytest.mark.timeout(5)
def test_create_object_invalid_exception(servicer, client):
    servicer.function_create_error = GRPCError(Status.INVALID_ARGUMENT, "something was invalid")

    app = App()
    app.function()(dummy)

    with servicer.intercept() as ctx:
        with pytest.raises(InvalidError, match="something was invalid"):  # error should be converted
            with enable_output():  # this activates the log streaming loop, which could potentially hold up context exit
                with app.run(client=client):
                    pass
    assert len(ctx.get_requests("FunctionCreate")) == 1  # no retries on an invalid request
    assert len(ctx.get_requests("AppClientDisconnect")) == 1


def test_deploy_falls_back_to_app_name(servicer, client):
    named_app = App(name="foo_app")
    named_app.deploy(client=client)
    app_names = {app_name for (_, app_name) in servicer.deployed_apps}
    assert "foo_app" in app_names


def test_deploy_uses_deployment_name_if_specified(servicer, client):
    named_app = App(name="foo_app")
    named_app.deploy(name="bar_app", client=client)
    app_names = {app_name for (_, app_name) in servicer.deployed_apps}
    assert "bar_app" in app_names
    assert "foo_app" not in app_names


def test_run_function_without_app_error():
    app = App()
    dummy_modal = app.function()(dummy)

    with pytest.raises(ExecutionError) as excinfo:
        dummy_modal.remote()

    assert "hydrated" in str(excinfo.value)


def test_missing_attr():
    """Trying to call a non-existent function on the App should produce
    an understandable error message."""

    app = App()
    with pytest.raises(AttributeError):
        app.fun()  # type: ignore


def test_same_function_name(caplog):
    app = App()

    # Add first function
    with caplog.at_level(logging.WARNING):
        app.function()(module_1.square)
    assert len(caplog.records) == 0

    # Add second function: check warning
    with caplog.at_level(logging.WARNING):
        app.function()(module_2.square)
    assert len(caplog.records) == 1
    assert "module_1" in caplog.text
    assert "module_2" in caplog.text
    assert "square" in caplog.text


def test_run_state(client, servicer):
    app = App()
    with app.run(client=client):
        assert servicer.app_state_history[app.app_id] == [api_pb2.APP_STATE_INITIALIZING, api_pb2.APP_STATE_EPHEMERAL]


def test_deploy_state(client, servicer):
    app = App()
    app.deploy(name="foobar", client=client)
    assert servicer.app_state_history[app.app_id] == [api_pb2.APP_STATE_INITIALIZING, api_pb2.APP_STATE_DEPLOYED]


def test_detach_state(client, servicer):
    app = App()
    with app.run(client=client, detach=True):
        assert servicer.app_state_history[app.app_id] == [api_pb2.APP_STATE_INITIALIZING, api_pb2.APP_STATE_DETACHED]


@pytest.mark.asyncio
async def test_grpc_protocol(client, servicer):
    app = App()
    async with app.run(client=client):
        await asyncio.sleep(0.01)  # wait for heartbeat
    assert len(servicer.requests) == 3
    assert isinstance(servicer.requests[0], api_pb2.AppCreateRequest)
    assert isinstance(servicer.requests[1], api_pb2.AppHeartbeatRequest)
    assert isinstance(servicer.requests[2], api_pb2.AppClientDisconnectRequest)


async def web1(x):
    return {"square": x**2}


async def web2(x):
    return {"cube": x**3}


def test_registered_fastapi_endpoints(client, servicer):
    app = App()
    app.function()(square)
    app.function()(fastapi_endpoint()(web1))
    app.function()(fastapi_endpoint()(web2))

    @app.cls(serialized=True)
    class Cls:
        @fastapi_endpoint()
        def web3(self):
            pass

    assert app.registered_web_endpoints == ["web1", "web2", "Cls.web3"]


def test_registered_legacy_web_endpoints(client, servicer):
    with pytest.warns(DeprecationError, match="fastapi_endpoint"):
        app = App()
        app.function()(square)
        app.function()(web_endpoint()(web1))
        app.function()(web_endpoint()(web2))

        @app.cls(serialized=True)
        class Cls:
            @web_endpoint()
            def cls_web_endpoint(self):
                pass

    assert app.registered_web_endpoints == ["web1", "web2", "Cls.cls_web_endpoint"]


def test_init_types():
    with pytest.raises(InvalidError):
        # singular secret to plural argument
        App(secrets=Secret.from_dict())  # type: ignore
    with pytest.raises(InvalidError):
        # not a Secret Object
        App(secrets=[{"foo": "bar"}])  # type: ignore
    with pytest.raises(InvalidError):
        # should be an Image
        App(image=Secret.from_dict())  # type: ignore

    App(
        image=Image.debian_slim().pip_install("pandas"),
        secrets=[Secret.from_dict()],
    )


def test_set_image_on_app_as_attribute():
    # TODO: do we want to deprecate this syntax? It's kind of random for image to
    #     have a reserved name in the blueprint, and being the only of the construction
    #     arguments that can be set on the instance after construction
    custom_img = Image.debian_slim().apt_install("emacs")
    app = App(image=custom_img)
    assert app._get_default_image() == custom_img


def test_redeploy_delete_objects(servicer, client):
    # Deploy an app with objects d1 and d2
    app = App()
    app.function(name="d1", serialized=True)(dummy)
    app.function(name="d2", serialized=True)(dummy)
    app.deploy(name="xyz", client=client)

    # Check objects
    assert set(servicer.app_objects[app.app_id].keys()) == {"d1", "d2"}

    # Deploy an app with objects d2 and d3
    app = App()
    app.function(name="d2", serialized=True)(dummy)
    app.function(name="d3", serialized=True)(dummy)
    app.deploy(name="xyz", client=client)

    # Make sure d1 is deleted
    assert set(servicer.app_objects[app.app_id].keys()) == {"d2", "d3"}


@pytest.mark.asyncio
async def test_unhydrate(servicer, client):
    app = App()

    f = app.function()(dummy)

    assert not f.is_hydrated
    async with app.run(client=client):
        assert f.is_hydrated

    # After app finishes, it should unhydrate
    assert not f.is_hydrated


def test_keyboard_interrupt(servicer, client):
    app = App()
    app.function()(square)
    with app.run(client=client):
        # The exit handler should catch this interrupt and exit gracefully
        raise KeyboardInterrupt()


def test_function_image_positional():
    app = App()
    image = Image.debian_slim()

    with pytest.raises(InvalidError) as excinfo:

        @app.function(image)  # type: ignore
        def f():
            pass

    assert "function(image=image)" in str(excinfo.value)


def test_function_decorator_on_class():
    app = App()
    with pytest.raises(TypeError, match="cannot be used on a class"):

        @app.function()
        class Foo:
            pass


@pytest.mark.asyncio
async def test_deploy_disconnect(servicer, client):
    app = App()
    app.function(secrets=[Secret.from_name("nonexistent-secret")])(square)

    with pytest.raises(NotFoundError):
        await app.deploy.aio(name="my-app", client=client)

    assert servicer.app_state_history["ap-1"] == [
        api_pb2.APP_STATE_INITIALIZING,
        api_pb2.APP_STATE_STOPPED,
    ]


def test_parse_custom_domains():
    assert len(_parse_custom_domains(None)) == 0
    assert len(_parse_custom_domains(["foo.com", "bar.com"])) == 2
    with pytest.raises(AssertionError):
        assert _parse_custom_domains("foo.com")


def test_hydrated_other_app_object_gets_referenced(servicer, client):
    app = App("my-app")
    with servicer.intercept() as ctx:
        with Volume.ephemeral(client=client) as vol:
            app.function(volumes={"/vol": vol})(dummy)  # implicitly load vol
            app.deploy(client=client)
            function_create_req: api_pb2.FunctionCreateRequest = ctx.pop_request("FunctionCreate")
            assert vol.object_id in {obj.object_id for obj in function_create_req.function.object_dependencies}


def test_hasattr():
    app = App()
    assert not hasattr(app, "xyz")


def test_app(client):
    app = App()
    square_modal = app.function()(square)

    with app.run(client=client):
        square_modal.remote(42)


def test_non_string_app_name():
    with pytest.raises(InvalidError, match="Must be string"):
        App(Image.debian_slim())  # type: ignore


def test_app_logs(servicer, client):
    app = App()
    f = app.function()(dummy)

    with app.run(client=client):
        f.remote()

    logs = [data for data in app._logs(client=client)]
    assert logs == ["hello, world (1)\n"]


def test_app_interactive(servicer, client, capsys):
    app = App()

    async def app_logs_pty(servicer, stream):
        await stream.recv_message()

        # Enable PTY
        await stream.send_message(api_pb2.TaskLogsBatch(pty_exec_id="ta-123"))

        # Send some data (should be written raw to stdout)
        log = api_pb2.TaskLogs(data="some data\n", file_descriptor=api_pb2.FILE_DESCRIPTOR_STDOUT)
        await stream.send_message(api_pb2.TaskLogsBatch(entry_id="xyz", items=[log]))

        # Send an EOF
        await stream.send_message(api_pb2.TaskLogsBatch(eof=True, task_id="ta-123"))

        # Terminate app
        await stream.send_message(api_pb2.TaskLogsBatch(app_done=True))

    with servicer.intercept() as ctx:
        ctx.set_responder("AppGetLogs", app_logs_pty)

        with enable_output():
            with app.run(client=client):
                time.sleep(0.1)

    captured = capsys.readouterr()
    assert captured.out.endswith("\nsome data\n\r")


def test_app_interactive_no_output(servicer, client):
    app = App()

    msg = "Interactive mode requires output to be enabled. (Use the the `modal.enable_output()` context manager.)"
    with pytest.raises(InvalidError, match=re.escape(msg)):
        with app.run(client=client, interactive=True):
            # Verify that interactive mode was disabled
            assert not app.is_interactive


@pytest.mark.asyncio
async def test_deploy_from_container(servicer, container_client):
    app = App(image=Image.debian_slim().pip_install("pandas"))
    app.function()(square)

    # Deploy app
    await app.deploy.aio(name="my-app", client=container_client)
    assert servicer.app_objects["ap-1"]["square"] == "fu-1"
    assert servicer.app_state_history[app.app_id] == [api_pb2.APP_STATE_INITIALIZING, api_pb2.APP_STATE_DEPLOYED]


def test_app_create_bad_environment_name_error(client):
    environment_name = "this=is@not.allowed"
    app = App()
    with pytest.raises(InvalidError, match="Invalid Environment name"):
        with run_app(
            app, environment_name=environment_name, client=client
        ):  # TODO: why isn't environment_name an argument to app.run?
            pass

    if sys.platform != "win32":  # proactor event loop has long lived helper tasks
        assert len(asyncio.all_tasks(synchronizer._loop)) == 1  # no trailing tasks, except the `loop_inner` ever-task


def test_overriding_function_warning(caplog):
    app = App()

    @app.function(serialized=True)
    def func():  # type: ignore
        return 1

    assert len(caplog.messages) == 0

    app_2 = App()
    app_2.include(app)

    assert len(caplog.messages) == 0

    app_3 = App()

    app_3.include(app)
    app_3.include(app_2)

    assert len(caplog.messages) == 0

    app_4 = App()

    @app_4.function(serialized=True)  # type: ignore
    def func():  # noqa: F811
        return 2

    assert len(caplog.messages) == 0

    app_3.include(app_4)
    assert "Overriding existing function" in caplog.messages[0]


@pytest.mark.parametrize("name", ["", " ", "no way", "my-app!", "a" * 65])
def test_lookup_invalid_name(name):
    with pytest.raises(InvalidError, match="Invalid App name"):
        App.lookup(name)


@pytest.mark.parametrize("mode", ["run", "deploy"])
def test_tags(servicer, client, mode):
    tags = {"foo": "bar", "baz": "qux"}
    app = App(name="tag-tester", tags=tags)
    app.function()(square)
    with servicer.intercept() as ctx:
        if mode == "deploy":
            app.deploy(client=client, tag="some-other-tag")
        elif mode == "run":
            with app.run(client=client):
                pass

    request = ctx.pop_request("AppPublish")
    assert request.tags == tags


@pytest.mark.parametrize("s", ["a:b", "x/y", "a b", "a" * 65])
def test_invalid_tags(s):
    with pytest.raises(InvalidError, match=f"Invalid tag key: {s!r}"):
        App(tags={s: "bar"})
    with pytest.raises(InvalidError, match=f"Invalid tag value: {s!r}"):
        App(tags={"foo": s})


@pytest.mark.parametrize("inherit_tags", [True, False])
def test_app_composition_tags(servicer, client, inherit_tags):
    base_app = App(tags={"foo": "bar", "baz": "bum"})
    base_app.function()(square)

    main_app = App(tags={"baz": "qux"})
    main_app.include(base_app, inherit_tags=inherit_tags)

    with servicer.intercept() as ctx:
        main_app.deploy(name="inherit-test", client=client)

    request = ctx.pop_request("AppPublish")
    if inherit_tags:
        assert request.tags == {"foo": "bar", "baz": "qux"}
    else:
        assert request.tags == {"baz": "qux"}



================================================
FILE: test/asgi_wrapper_test.py
================================================
# Copyright Modal Labs 2024
import asyncio
import pytest

import fastapi
from starlette.requests import ClientDisconnect

from modal._runtime.asgi import asgi_app_wrapper
from modal._runtime.execution_context import _set_current_context_ids


class DummyException(Exception):
    pass


app = fastapi.FastAPI()


@app.get("/")
def sync_index():
    return {"some_result": "foo"}


@app.get("/error")
def sync_error():
    raise DummyException()


@app.post("/async_reading_body")
async def async_index_reading_body(req: fastapi.Request):
    body = await req.body()
    return {"some_result": body}


@app.get("/async_error")
async def async_error():
    raise DummyException()


@app.get("/streaming_response")
async def streaming_response():
    from fastapi.responses import StreamingResponse

    async def stream_bytes():
        yield b"foo"
        yield b"bar"

    return StreamingResponse(stream_bytes())


def _asgi_get_scope(path, method="GET"):
    return {
        "type": "http",
        "method": method,
        "path": path,
        "query_string": "",
        "headers": [],
    }


class MockIOManager:
    class get_data_in:
        @staticmethod
        async def aio(_function_call_id, _attempt_token):
            yield {"type": "http.request", "body": b"some_body"}
            await asyncio.sleep(10)


@pytest.mark.asyncio
@pytest.mark.timeout(1)
async def test_success():
    mock_manager = MockIOManager()
    _set_current_context_ids(["in-123"], ["fc-123"], ["fake-attempt-token"])
    wrapped_app, lifespan_manager = asgi_app_wrapper(app, mock_manager)
    asgi_scope = _asgi_get_scope("/")
    outputs = [output async for output in wrapped_app(asgi_scope)]
    assert len(outputs) == 2
    before_body = outputs[0]
    assert before_body["status"] == 200
    assert before_body["type"] == "http.response.start"
    body = outputs[1]
    assert body["body"] == b'{"some_result":"foo"}'
    assert body["type"] == "http.response.body"


@pytest.mark.asyncio
@pytest.mark.parametrize("endpoint_url", ["/error", "/async_error"])
@pytest.mark.timeout(1)
async def test_endpoint_exception(endpoint_url):
    mock_manager = MockIOManager()
    _set_current_context_ids(["in-123"], ["fc-123"], ["fake-attempt-token"])
    wrapped_app, lifespan_manager = asgi_app_wrapper(app, mock_manager)
    asgi_scope = _asgi_get_scope(endpoint_url)
    outputs = []

    with pytest.raises(DummyException):
        async for output in wrapped_app(asgi_scope):
            outputs.append(output)

    assert len(outputs) == 2
    before_body = outputs[0]
    assert before_body["status"] == 500
    assert before_body["type"] == "http.response.start"
    body = outputs[1]
    assert body["body"] == b"Internal Server Error"
    assert body["type"] == "http.response.body"


class BrokenIOManager:
    class get_data_in:
        @staticmethod
        async def aio(_function_call_id, _attempt_token):
            raise DummyException("error while fetching data")
            yield  # noqa (makes this a generator)


@pytest.mark.asyncio
@pytest.mark.timeout(1)
async def test_broken_io_unused(caplog):
    # if IO channel breaks, but the endpoint doesn't actually use
    # any of the body data, it should be allowed to output its data
    # and not raise an exception - but print a warning since it's unexpected
    mock_manager = BrokenIOManager()
    _set_current_context_ids(["in-123"], ["fc-123"], ["fake-attempt-token"])
    wrapped_app, lifespan_manager = asgi_app_wrapper(app, mock_manager)
    asgi_scope = _asgi_get_scope("/")
    outputs = []

    async for output in wrapped_app(asgi_scope):
        outputs.append(output)

    assert len(outputs) == 2
    assert outputs[0]["status"] == 200
    assert outputs[1]["body"] == b'{"some_result":"foo"}'
    assert "Internal error" in caplog.text
    assert "DummyException: error while fetching data" in caplog.text


@pytest.mark.asyncio
@pytest.mark.timeout(10)
async def test_broken_io_used():
    mock_manager = BrokenIOManager()
    _set_current_context_ids(["in-123"], ["fc-123"], ["fake-attempt-token"])
    wrapped_app, lifespan_manager = asgi_app_wrapper(app, mock_manager)
    asgi_scope = _asgi_get_scope("/async_reading_body", "POST")
    outputs = []
    with pytest.raises(ClientDisconnect):
        async for output in wrapped_app(asgi_scope):
            outputs.append(output)

    assert len(outputs) == 2
    assert outputs[0]["status"] == 500


class SlowIOManager:
    class get_data_in:
        @staticmethod
        async def aio(_function_call_id, _attempt_token):
            await asyncio.sleep(5)
            yield  # makes this an async generator


@pytest.mark.asyncio
@pytest.mark.timeout(2)
async def test_first_message_timeout(monkeypatch):
    monkeypatch.setattr("modal._runtime.asgi.FIRST_MESSAGE_TIMEOUT_SECONDS", 0.1)  # simulate timeout
    _set_current_context_ids(["in-123"], ["fc-123"], ["fake-attempt-token"])
    wrapped_app, lifespan_manager = asgi_app_wrapper(app, SlowIOManager())
    asgi_scope = _asgi_get_scope("/async_reading_body", "POST")
    outputs = []
    with pytest.raises(ClientDisconnect):
        async for output in wrapped_app(asgi_scope):
            outputs.append(output)

    assert outputs[0]["status"] == 408
    assert b"Missing request" in outputs[1]["body"]


@pytest.mark.asyncio
async def test_cancellation_cleanup(caplog):
    # this test mostly exists to get some coverage on the cancellation/error paths and
    # ensure nothing unexpected happens there
    _set_current_context_ids(["in-123"], ["fc-123"], ["fake-attempt-token"])
    wrapped_app, lifespan_manager = asgi_app_wrapper(app, SlowIOManager())
    asgi_scope = _asgi_get_scope("/async_reading_body", "POST")
    outputs = []

    async def app_runner():
        async for output in wrapped_app(asgi_scope):
            outputs.append(output)

    app_runner_task = asyncio.create_task(app_runner())
    await asyncio.sleep(0.1)  # let it get started
    app_runner_task.cancel()
    await asyncio.sleep(0.1)  # let it shut down
    assert len(outputs) == 0
    assert caplog.text == ""  # make sure there are no junk traces about dangling tasks etc.


@pytest.mark.asyncio
async def test_streaming_response():
    _set_current_context_ids(["in-123"], ["fc-123"], ["fake-attempt-token"])
    wrapped_app, lifespan_manager = asgi_app_wrapper(app, SlowIOManager())
    asgi_scope = _asgi_get_scope("/streaming_response", "GET")
    outputs = []
    async for output in wrapped_app(asgi_scope):
        outputs.append(output)
    assert outputs == [
        {"headers": [], "status": 200, "type": "http.response.start"},
        {"body": b"foo", "more_body": True, "type": "http.response.body"},
        {"body": b"bar", "more_body": True, "type": "http.response.body"},
        {"body": b"", "more_body": False, "type": "http.response.body"},
    ]


class StreamingIOManager:
    class get_data_in:
        @staticmethod
        async def aio(_function_call_id, _attempt_token):
            yield {"type": "http.request", "body": b"foo", "more_body": True}
            yield {"type": "http.request", "body": b"bar", "more_body": True}
            yield {"type": "http.request", "body": b"baz", "more_body": False}
            yield {"type": "http.request", "body": b"this should not be read", "more_body": False}


@pytest.mark.asyncio
async def test_streaming_body():
    _set_current_context_ids(["in-123"], ["fc-123"], ["fake-attempt-token"])

    wrapped_app, lifespan_manager = asgi_app_wrapper(app, StreamingIOManager())
    asgi_scope = _asgi_get_scope("/async_reading_body", "POST")
    outputs = []
    async for output in wrapped_app(asgi_scope):
        outputs.append(output)
    assert outputs[1] == {"type": "http.response.body", "body": b'{"some_result":"foobarbaz"}'}


@pytest.mark.asyncio
async def test_cancellation_while_waiting_for_first_input():
    # due to an asyncio edge case of cancellation + wait_for(future) resolution there
    # are scenarios in which an asgi task cancellation doesn't actually stop the underlying
    # fetch_data_in task, causing either warnings on shutdown or even infinite stalling on
    # shutdown.
    _set_current_context_ids(["in-123"], ["fc-123"], ["fake-attempt-token"])
    fut: asyncio.Future[None] = asyncio.Future()

    class StreamingIOManager:
        class get_data_in:
            @staticmethod
            async def aio(_function_call_id, _attempt_token):
                await fut  # we never resolve this, unlike in test_cancellation_first_message_race_cleanup
                yield

    wrapped_app, _ = asgi_app_wrapper(app, StreamingIOManager())
    asgi_scope = _asgi_get_scope("/async_reading_body", "POST")

    first_app_output = asyncio.create_task(wrapped_app(asgi_scope).__anext__())  # type: ignore
    await asyncio.sleep(0.1)  # ensure we are in wait_for(first_message_task)
    first_app_output.cancel()
    await asyncio.sleep(0.1)  # resume event loop to resolve tasks if possible
    remaining_tasks = [t for t in asyncio.all_tasks() if t is not asyncio.current_task()]
    assert len(remaining_tasks) == 0


@pytest.mark.asyncio
async def test_cancellation_when_first_input_arrives():
    # due to an asyncio edge case of cancellation + wait_for(future) resolution there
    # are scenarios in which an asgi task cancellation doesn't actually stop the underlying
    # fetch_data_in task, causing either warnings on shutdown or even infinite stalling on
    # shutdown.
    _set_current_context_ids(["in-123"], ["fc-123"], ["fake-attempt-token"])
    fut: asyncio.Future[None] = asyncio.Future()

    class StreamingIOManager:
        class get_data_in:
            @staticmethod
            async def aio(_function_call_id, _attempt_token):
                await fut
                yield {"type": "http.request", "body": b"foo", "more_body": True}
                while 1:
                    yield  # simulate infinite stream

    wrapped_app, _ = asgi_app_wrapper(app, StreamingIOManager())
    asgi_scope = _asgi_get_scope("/async_reading_body", "POST")

    first_app_output = asyncio.create_task(wrapped_app(asgi_scope).__anext__())  # type: ignore
    await asyncio.sleep(0.1)  # ensure we are in wait_for(first_message_task)
    # now lets unblock get_data_in, supplying a request to the waiting asgi app
    # fut.set_result(None)
    # but at the same time, before we resume the event loop, we cancel the full input task
    fut.set_result(None)
    first_app_output.cancel()
    await asyncio.sleep(0.1)  # resume event loop to resolve tasks if possible
    remaining_tasks = [t for t in asyncio.all_tasks() if t is not asyncio.current_task()]
    assert len(remaining_tasks) == 0


@pytest.mark.asyncio
async def test_lifespan_supported():
    lifespan_startup_complete = False
    lifespan_shutdown_complete = False

    async def asgi_app(scope, receive, send):
        if scope["type"] == "lifespan":
            while True:
                message = await receive()
                if message["type"] == "lifespan.startup":
                    nonlocal lifespan_startup_complete
                    lifespan_startup_complete = True
                    await send({"type": "lifespan.startup.complete"})
                elif message["type"] == "lifespan.shutdown":
                    nonlocal lifespan_shutdown_complete
                    lifespan_shutdown_complete = True
                    await send({"type": "lifespan.shutdown.complete"})
                    return
        else:
            await send({"type": "http.response.start", "status": 200})
            await send({"type": "http.response.body", "body": b'{"some_result":"foo"}'})

    mock_manager = MockIOManager()
    _set_current_context_ids(["in-123"], ["fc-123"], ["fake-attempt-token"])
    wrapped_app, lifespan_manager = asgi_app_wrapper(asgi_app, mock_manager)

    bt = asyncio.create_task(lifespan_manager.background_task())
    await lifespan_manager.lifespan_startup()

    assert lifespan_startup_complete

    asgi_scope = _asgi_get_scope("/")
    outputs = [output async for output in wrapped_app(asgi_scope)]
    assert len(outputs) == 2
    before_body = outputs[0]
    assert before_body["status"] == 200
    assert before_body["type"] == "http.response.start"
    body = outputs[1]
    assert body["body"] == b'{"some_result":"foo"}'
    assert body["type"] == "http.response.body"

    await lifespan_manager.lifespan_shutdown()

    assert lifespan_shutdown_complete

    assert lifespan_manager._lifespan_supported
    await bt


@pytest.mark.asyncio
async def test_lifespan_unsupported():
    async def asgi_app(scope, receive, send):
        if scope["type"] == "lifespan":
            raise Exception("broken lifespan scope handler")
        else:
            await send({"type": "http.response.start", "status": 200})
            await send({"type": "http.response.body", "body": b'{"some_result":"foo"}'})

    mock_manager = MockIOManager()
    _set_current_context_ids(["in-123"], ["fc-123"], ["fake-attempt-token"])
    wrapped_app, lifespan_manager = asgi_app_wrapper(asgi_app, mock_manager)

    # Failing lifespan should not affect the app
    bt = asyncio.create_task(lifespan_manager.background_task())
    await lifespan_manager.lifespan_startup()

    # works with stuff after
    asgi_scope = _asgi_get_scope("/")
    outputs = [output async for output in wrapped_app(asgi_scope)]
    assert len(outputs) == 2
    before_body = outputs[0]
    assert before_body["status"] == 200
    assert before_body["type"] == "http.response.start"
    body = outputs[1]
    assert body["body"] == b'{"some_result":"foo"}'
    assert body["type"] == "http.response.body"

    await lifespan_manager.lifespan_shutdown()

    assert not lifespan_manager._lifespan_supported
    await bt



================================================
FILE: test/async_utils_test.py
================================================
# Copyright Modal Labs 2022
import asyncio
import functools
import logging
import os
import platform
import pytest
import subprocess
import sys
import textwrap
import time

import pytest_asyncio
from synchronicity import Synchronizer

from modal._utils import async_utils
from modal._utils.async_utils import (
    TaskContext,
    TimestampPriorityQueue,
    aclosing,
    async_chain,
    async_map,
    async_map_ordered,
    async_merge,
    async_zip,
    callable_to_agen,
    prevent_cancellation_abortion,
    queue_batch_iterator,
    retry,
    sync_or_async_iter,
    synchronize_api,
    warn_if_generator_is_not_consumed,
)
from test import helpers


@pytest_asyncio.fixture(autouse=True)
async def no_dangling_tasks():
    yield
    assert not asyncio.all_tasks() - {asyncio.tasks.current_task()}


skip_github_non_linux = pytest.mark.skipif(
    (os.environ.get("GITHUB_ACTIONS") == "true" and platform.system() != "Linux"),
    reason="sleep is inaccurate on GitHub Actions runners.",
)


class SampleException(Exception):
    pass


class FailNTimes:
    def __init__(self, n_failures, exc=SampleException("Something bad happened")):
        self.n_failures = n_failures
        self.n_calls = 0
        self.exc = exc

    async def __call__(self, x):
        self.n_calls += 1
        if self.n_calls <= self.n_failures:
            raise self.exc
        else:
            return x + 1


@pytest.mark.asyncio
async def test_retry():
    f_retry = retry(FailNTimes(2))
    assert await f_retry(42) == 43

    with pytest.raises(SampleException):
        f_retry = retry(FailNTimes(3))
        assert await f_retry(42) == 43

    f_retry = retry(n_attempts=5)(FailNTimes(4))
    assert await f_retry(42) == 43

    with pytest.raises(SampleException):
        f_retry = retry(n_attempts=5)(FailNTimes(5))
        assert await f_retry(42) == 43


@pytest.mark.asyncio
async def test_task_context():
    async with TaskContext() as task_context:
        t = task_context.create_task(asyncio.sleep(0.1))
        assert not t.done()
        # await asyncio.sleep(0.0)
    await asyncio.sleep(0.0)  # just waste a loop step for the cancellation to go through
    assert t.cancelled()


@pytest.mark.asyncio
async def test_task_context_grace():
    async with TaskContext(grace=0.2) as task_context:
        u = task_context.create_task(asyncio.sleep(0.1))
        v = task_context.create_task(asyncio.sleep(0.3))
        assert not u.done()
        assert not v.done()
    await asyncio.sleep(0.0)
    assert u.done()
    assert v.cancelled()


@skip_github_non_linux
@pytest.mark.asyncio
async def test_task_context_infinite_loop():
    async with TaskContext(grace=0.01) as task_context:
        counter = 0

        async def f():
            nonlocal counter
            counter += 1

        t = task_context.infinite_loop(f, sleep=0.1)
        assert not t.done()
        await asyncio.sleep(0.35)
        assert counter == 4  # at 0.00, 0.10, 0.20, 0.30
    await asyncio.sleep(0.0)  # just waste a loop step for the cancellation to go through
    assert not t.cancelled()
    assert t.done()
    assert counter == 4  # should be exited immediately


@skip_github_non_linux
@pytest.mark.asyncio
async def test_task_context_infinite_loop_non_functions():
    async with TaskContext(grace=0.01) as task_context:

        async def f(x):
            pass

        task_context.infinite_loop(lambda: f(123))
        task_context.infinite_loop(functools.partial(f, 123))


@skip_github_non_linux
@pytest.mark.asyncio
async def test_task_context_infinite_loop_timeout(caplog):
    async with TaskContext(grace=0.01) as task_context:

        async def f():
            await asyncio.sleep(5.0)

        task_context.infinite_loop(f, timeout=0.1)
        await asyncio.sleep(0.15)

    # TODO(elias): Find the tests that leak `Task was destroyed but it is pending` warnings into this test
    # so we can assert a single record here:
    # assert len(caplog.records) == 1
    for record in caplog.records:
        if "timed out" in caplog.text:
            break
    else:
        assert False, "no timeout"


@pytest.mark.asyncio
async def test_task_context_gather():
    state = "none"

    async def t1(error=False):
        nonlocal state
        await asyncio.sleep(0.1)
        state = "t1"
        if error:
            raise ValueError()

    async def t2():
        nonlocal state
        await asyncio.sleep(0.2)
        state = "t2"

    await asyncio.gather(t1(), t2())
    assert state == "t2"

    # On t1 error: asyncio.gather() does not cancel t2, which is bad behavior.
    state = "none"
    with pytest.raises(ValueError):
        await asyncio.gather(t1(error=True), t2())
    assert state == "t1"
    await asyncio.sleep(0.2)
    assert state == "t2"  # t2 still runs because asyncio.gather() does not cancel tasks

    # On t1 error: TaskContext.gather() should cancel the remaining tasks.
    state = "none"
    with pytest.raises(ValueError):
        await TaskContext.gather(t1(error=True), t2())
    assert state == "t1"
    await asyncio.sleep(0.2)
    assert state == "t1"


DEBOUNCE_TIME = 0.1


@pytest.mark.asyncio
async def test_queue_batch_iterator():
    queue: asyncio.Queue = asyncio.Queue()
    await queue.put(1)
    drained_items = []

    async def drain_queue(logs_queue):
        async for batch in queue_batch_iterator(logs_queue, debounce_time=DEBOUNCE_TIME):
            drained_items.extend(batch)

    async with TaskContext(grace=0.0) as tc:
        tc.create_task(drain_queue(queue))

        # Make sure the queue gets drained.
        await asyncio.sleep(0.001)

        assert len(drained_items) == 1

        # Add items to the queue and a sentinel while it's still waiting for DEBOUNCE_TIME.
        await queue.put(2)
        await queue.put(3)
        await queue.put(None)

        await asyncio.sleep(DEBOUNCE_TIME + 0.001)

        assert len(drained_items) == 3


@pytest.mark.asyncio
async def test_warn_if_generator_is_not_consumed(caplog):
    @warn_if_generator_is_not_consumed()
    async def my_generator():
        yield 42

    with caplog.at_level(logging.WARNING):
        g = my_generator()
        assert "my_generator" in repr(g)
        del g  # Force destructor

    assert len(caplog.records) == 1
    assert "my_generator" in caplog.text
    assert "for" in caplog.text
    assert "list" in caplog.text


def test_warn_if_generator_is_not_consumed_sync(caplog):
    @warn_if_generator_is_not_consumed()
    def my_generator():
        yield 42

    with caplog.at_level(logging.WARNING):
        g = my_generator()
        assert "my_generator" in repr(g)
        del g  # Force destructor

    assert len(caplog.records) == 1
    assert "my_generator" in caplog.text
    assert "for" in caplog.text
    assert "list" in caplog.text


@pytest.mark.asyncio
async def test_no_warn_if_generator_is_consumed(caplog):
    @warn_if_generator_is_not_consumed()
    async def my_generator():
        yield 42

    with caplog.at_level(logging.WARNING):
        g = my_generator()
        async for _ in g:
            pass
        del g  # Force destructor

    assert len(caplog.records) == 0


def test_exit_handler():
    result = None
    sync = Synchronizer()

    async def cleanup():
        nonlocal result
        result = "bye"

    async def _setup_code():
        async_utils.on_shutdown(cleanup())

    setup_code = sync.create_blocking(_setup_code)
    setup_code()

    sync._close_loop()  # this is called on exit by synchronicity, which shuts down the event loop
    assert result == "bye"


def test_synchronize_api_blocking_name():
    class _MyClass:
        async def foo(self):
            await asyncio.sleep(0.1)
            return "bar"

    async def _myfunc():
        await asyncio.sleep(0.1)
        return "bar"

    MyClass = synchronize_api(_MyClass)
    assert MyClass.__name__ == "MyClass"
    assert MyClass().foo() == "bar"

    myfunc = synchronize_api(_myfunc)
    assert myfunc.__name__ == "myfunc"
    assert myfunc() == "bar"


@pytest.mark.asyncio
async def test_aclosing():
    result = []
    states = []

    async def foo():
        states.append("enter")
        try:
            yield 1
            yield 2
        finally:
            states.append("exit")

    # test that things are cleaned up when we fully exhaust the generator
    async with aclosing(foo()) as stream:
        async for it in stream:
            result.append(it)

    assert sorted(result) == [1, 2]
    assert states == ["enter", "exit"]

    # test that things are cleaned up when we exit the context manager without fully exhausting the generator
    states.clear()
    result.clear()
    async with aclosing(foo()) as stream:
        async for it in stream:
            break

    assert result == []
    assert states == ["enter", "exit"]


@pytest.mark.asyncio
async def test_sync_or_async_iter_sync_gen():
    result = []

    def sync_gen():
        yield 4
        yield 5
        yield 6

    async for i in sync_or_async_iter(sync_gen()):
        result.append(i)
    assert result == [4, 5, 6]


@pytest.mark.asyncio
async def test_sync_or_async_iter_async_gen():
    result = []
    states = []

    async def async_gen():
        states.append("enter")
        try:
            yield 1
            await asyncio.sleep(0.1)
            yield 2
            await asyncio.sleep(0.1)
            yield 3
        finally:
            states.append("exit")

    # test that things are cleaned up when we fully exhaust the generator
    async for i in sync_or_async_iter(async_gen()):
        result.append(i)
    assert result == [1, 2, 3]
    assert states == ["enter", "exit"]

    # test that things are cleaned up when we exit the context manager without fully exhausting the generator
    result.clear()
    states.clear()
    async with aclosing(sync_or_async_iter(async_gen())) as stream:
        async for _ in stream:
            break
    assert states == ["enter", "exit"]
    assert result == []


@pytest.mark.asyncio
async def test_async_zip():
    states = []
    result = []

    async def gen(x):
        states.append(f"enter {x}")
        try:
            await asyncio.sleep(0.1)
            yield x
            yield x + 1
        finally:
            await asyncio.sleep(0)
            states.append(f"exit {x}")

    async with aclosing(async_zip(gen(1), gen(5), gen(10))) as stream:
        async for item in stream:
            result.append(item)

    assert result == [(1, 5, 10), (2, 6, 11)]
    assert states == ["enter 1", "enter 5", "enter 10", "exit 1", "exit 5", "exit 10"]


@pytest.mark.asyncio
async def test_async_zip_different_lengths():
    states = []
    result = []

    async def gen_short():
        states.append("enter short")
        try:
            await asyncio.sleep(0.1)
            yield 1
            yield 2
        finally:
            await asyncio.sleep(0)
            states.append("exit short")

    async def gen_long():
        states.append("enter long")
        try:
            await asyncio.sleep(0.1)
            yield 3
            yield 4
            yield 5
            yield 6

        finally:
            await asyncio.sleep(0)
            states.append("exit long")

    async with aclosing(async_zip(gen_short(), gen_long())) as stream:
        async for item in stream:
            result.append(item)

    assert result == [(1, 3), (2, 4)]
    assert states == ["enter short", "enter long", "exit short", "exit long"]


@pytest.mark.asyncio
async def test_async_zip_exception():
    states = []
    result = []

    async def gen(x):
        states.append(f"enter {x}")
        try:
            await asyncio.sleep(0.1)
            yield x
            if x == 1:
                raise SampleException("test")
            yield x + 1
        finally:
            await asyncio.sleep(0)
            states.append(f"exit {x}")

    with pytest.raises(SampleException):
        async with aclosing(async_zip(gen(1), gen(5))) as stream:
            async for item in stream:
                result.append(item)

    assert result == [(1, 5)]
    assert states == ["enter 1", "enter 5", "exit 1", "exit 5"]


@pytest.mark.asyncio
async def test_async_zip_parallel():
    ev1 = asyncio.Event()
    ev2 = asyncio.Event()

    async def gen1():
        await asyncio.sleep(0.1)
        ev1.set()
        yield 1
        await ev2.wait()
        yield 2

    async def gen2():
        await ev1.wait()
        yield 3
        await asyncio.sleep(0.1)
        ev2.set()
        yield 4

    result = []
    async for item in async_zip(gen1(), gen2()):
        result.append(item)

    assert result == [(1, 3), (2, 4)]


@pytest.mark.asyncio
async def test_async_zip_cancellation():
    ev = asyncio.Event()

    async def gen1():
        await asyncio.sleep(0.1)
        yield 1
        await ev.wait()
        raise asyncio.CancelledError()
        yield 2

    async def gen2():
        yield 3
        await asyncio.sleep(0.1)
        yield 4

    async def zip_coro():
        async with aclosing(async_zip(gen1(), gen2())) as stream:
            async for _ in stream:
                pass

    zip_task = asyncio.create_task(zip_coro())
    await asyncio.sleep(0.1)
    zip_task.cancel()
    with pytest.raises(asyncio.CancelledError):
        await zip_task


@pytest.mark.asyncio
async def test_async_zip_producer_cancellation():
    async def gen1():
        await asyncio.sleep(0.1)
        yield 1
        raise asyncio.CancelledError()
        yield 2

    async def gen2():
        yield 3
        await asyncio.sleep(0.1)
        yield 4

    await asyncio.sleep(0.1)
    with pytest.raises(asyncio.CancelledError):
        async with aclosing(async_zip(gen1(), gen2())) as stream:
            async for _ in stream:
                pass


@pytest.mark.asyncio
async def test_async_merge():
    result = []
    states = []

    ev1 = asyncio.Event()
    ev2 = asyncio.Event()

    async def gen1():
        states.append("gen1 enter")
        try:
            await asyncio.sleep(0.1)
            yield 1
            ev1.set()
            await ev2.wait()
            yield 2
        finally:
            await asyncio.sleep(0)
            states.append("gen1 exit")

    async def gen2():
        states.append("gen2 enter")
        try:
            await ev1.wait()
            yield 3
            await asyncio.sleep(0.1)
            ev2.set()
            yield 4
        finally:
            await asyncio.sleep(0)
            states.append("gen2 exit")

    async for item in async_merge(gen1(), gen2()):
        result.append(item)

    assert result == [1, 3, 4, 2]
    assert states == [
        "gen1 enter",
        "gen2 enter",
        "gen2 exit",
        "gen1 exit",
    ]


@pytest.mark.asyncio
async def test_async_merge_cleanup():
    states = []

    ev1 = asyncio.Event()
    ev2 = asyncio.Event()

    async def gen1():
        states.append("gen1 enter")
        try:
            await asyncio.sleep(0.1)
            yield 1
            ev1.set()
            await ev2.wait()
            yield 2
        finally:
            await asyncio.sleep(0)
            states.append("gen1 exit")

    async def gen2():
        states.append("gen2 enter")
        try:
            await ev1.wait()
            yield 3
            await asyncio.sleep(0.1)
            ev2.set()
            yield 4
        finally:
            await asyncio.sleep(0)
            states.append("gen2 exit")

    async with aclosing(async_merge(gen1(), gen2())) as stream:
        async for _ in stream:
            break

    assert sorted(states) == [
        "gen1 enter",
        "gen1 exit",
        "gen2 enter",
        "gen2 exit",
    ]


@pytest.mark.asyncio
async def test_async_merge_exception():
    result = []
    states = []

    async def gen1():
        states.append("gen1 enter")
        try:
            await asyncio.sleep(0.1)
            yield 1
            await asyncio.sleep(0.1)  # ensure that 4 gets added by gen2 before the exception cancels it
            raise SampleException("test")
        finally:
            await asyncio.sleep(0)
            states.append("gen1 exit")

    async def gen2():
        states.append("gen2 enter")
        try:
            yield 3
            await asyncio.sleep(0.1)
            yield 4
        finally:
            await asyncio.sleep(0)
            states.append("gen2 exit")

    with pytest.raises(SampleException):
        async for item in async_merge(gen1(), gen2()):
            result.append(item)

    assert sorted(result) == [1, 3, 4]
    assert sorted(states) == [
        "gen1 enter",
        "gen1 exit",
        "gen2 enter",
        "gen2 exit",
    ]


@pytest.mark.asyncio
async def test_async_merge_cancellation():
    ev = asyncio.Event()

    async def gen1():
        await asyncio.sleep(0.1)
        yield 1
        await ev.wait()
        yield 2

    async def gen2():
        yield 3
        await asyncio.sleep(0.1)
        yield 4

    async def merge_coro():
        async with aclosing(async_merge(gen1(), gen2())) as stream:
            async for _ in stream:
                pass

    merge_task = asyncio.create_task(merge_coro())
    await asyncio.sleep(0.1)
    merge_task.cancel()
    with pytest.raises(asyncio.CancelledError):
        await merge_task


@pytest.mark.asyncio
async def test_async_merge_producer_cancellation():
    async def gen1():
        await asyncio.sleep(0.1)
        yield 1
        raise asyncio.CancelledError()
        yield 2

    async def gen2():
        yield 3
        await asyncio.sleep(0.1)
        yield 4

    await asyncio.sleep(0.1)
    with pytest.raises(asyncio.CancelledError):
        async with aclosing(async_merge(gen1(), gen2())) as stream:
            async for _ in stream:
                pass


@pytest.mark.asyncio
async def test_callable_to_agen():
    async def foo():
        await asyncio.sleep(0.1)
        return 42

    result = []
    async for item in callable_to_agen(foo):
        result.append(item)
    assert result == [await foo()]


@pytest.mark.parametrize("in_order", [True, False])
@pytest.mark.asyncio
async def test_async_map(in_order):
    result = []
    states = []

    async def foo():
        states.append("enter")
        try:
            yield 1
            yield 2
            yield 3
        finally:
            states.append("exit")

    async def mapper(x):
        await asyncio.sleep(0.1)  # Simulate some async work
        return x * 2

    if in_order:
        async for item in async_map_ordered(foo(), mapper, concurrency=3):
            result.append(item)
    else:
        async for item in async_map(foo(), mapper, concurrency=3):
            result.append(item)

    assert sorted(result) == [2, 4, 6]
    assert states == ["enter", "exit"]


@pytest.mark.asyncio
@pytest.mark.parametrize("in_order", [True, False])
async def test_async_map_input_exception_async_producer(in_order):
    # test exception async producer
    states = []

    async def mapper_func(x):
        await asyncio.sleep(0.1)
        return x * 2

    async def gen():
        states.append("enter")
        try:
            for i in range(5):
                if i == 3:
                    raise SampleException("test")
                yield i
        finally:
            states.append("exit")

    with pytest.raises(SampleException):
        if in_order:
            async for _ in async_map_ordered(gen(), mapper_func, concurrency=3):
                pass
        else:
            async for _ in async_map(gen(), mapper_func, concurrency=3):
                pass

    assert sorted(states) == ["enter", "exit"]


@pytest.mark.asyncio
@pytest.mark.parametrize("in_order", [True, False])
async def test_async_map_input_cancellation_async_producer(in_order):
    # test cancelling async_map while waiting for input
    states = []

    async def mapper_func(x):
        await asyncio.sleep(0.1)
        return x * 2

    async def gen():
        states.append("enter")
        try:
            for i in range(5):
                if i == 3:
                    raise asyncio.CancelledError()
                yield i
        finally:
            states.append("exit")

    with pytest.raises(asyncio.CancelledError):
        if in_order:
            async for _ in async_map_ordered(gen(), mapper_func, concurrency=3):
                pass
        else:
            async for _ in async_map(gen(), mapper_func, concurrency=3):
                pass

    assert sorted(states) == ["enter", "exit"]


@pytest.mark.asyncio
@pytest.mark.parametrize("in_order", [True, False])
async def test_async_map_cancellation_waiting_for_input(in_order):
    # test cancelling async_map while waiting for input
    result = []
    states = []

    async def mapper_func(x):
        return x * 2

    blocking_event = asyncio.Event()

    async def gen():
        states.append("enter")
        try:
            await blocking_event.wait()
            yield 1
        finally:
            states.append("exit")

    async def mapper_coro():
        if in_order:
            async for item in async_map_ordered(gen(), mapper_func, concurrency=3):
                result.append(item)
        else:
            async for item in async_map(gen(), mapper_func, concurrency=3):
                result.append(item)

    mapper_task = asyncio.create_task(mapper_coro())
    await asyncio.sleep(0.1)
    mapper_task.cancel()
    with pytest.raises(asyncio.CancelledError):
        await mapper_task

    assert sorted(result) == []
    assert sorted(states) == ["enter", "exit"]


@pytest.mark.asyncio
@pytest.mark.parametrize("in_order", [True, False])
async def test_async_map_input_exception_sync_producer(in_order):
    # test exception sync producer
    states = []

    async def mapper_func(x):
        await asyncio.sleep(0.1)
        return x * 2

    def gen():
        states.append("enter")
        try:
            for i in range(5):
                if i == 3:
                    raise SampleException("test")
                yield i
        finally:
            states.append("exit")

    with pytest.raises(SampleException):
        if in_order:
            async for _ in async_map_ordered(sync_or_async_iter(gen()), mapper_func, concurrency=3):
                pass
        else:
            async for _ in async_map(sync_or_async_iter(gen()), mapper_func, concurrency=3):
                pass

    assert sorted(states) == ["enter", "exit"]


@pytest.mark.asyncio
@pytest.mark.parametrize("in_order", [True, False])
async def test_async_map_output_exception_async_func(in_order):
    # test cancelling async mapper function
    result = []
    states = []

    def gen():
        states.append("enter")
        try:
            yield from range(5)
        finally:
            states.append("exit")

    async def mapper_func(x):
        await asyncio.sleep(0.1)
        if x == 3:
            raise SampleException("test")
        return x * 2

    with pytest.raises(SampleException):
        if in_order:
            async for item in async_map_ordered(sync_or_async_iter(gen()), mapper_func, concurrency=3):
                result.append(item)
        else:
            async for item in async_map(sync_or_async_iter(gen()), mapper_func, concurrency=3):
                result.append(item)

    assert sorted(result) == [0, 2, 4]
    assert states == ["enter", "exit"]


@pytest.mark.asyncio
@pytest.mark.parametrize("in_order", [True, False])
async def test_async_map_streaming_input(in_order):
    # ensure we can stream input
    # and dont buffer all the items and return them after
    result = []
    states = []

    async def gen():
        states.append("enter")
        try:
            yield 1
            await asyncio.sleep(1)
            yield 2
            yield 3
        finally:
            states.append("exit")

    async def mapper(x):
        await asyncio.sleep(0.1)
        return x * 2

    import time

    start = time.time()
    if in_order:
        async for item in async_map_ordered(gen(), mapper, concurrency=3):
            if item == 2:
                assert time.time() - start < 0.5
            else:
                assert time.time() - start > 0.5
            result.append(item)
    else:
        async for item in async_map(gen(), mapper, concurrency=3):
            if item == 2:
                assert time.time() - start < 0.5
            else:
                assert time.time() - start > 0.5
            result.append(item)

    assert result == [2, 4, 6]
    assert states == ["enter", "exit"]


@pytest.mark.asyncio
async def test_async_map_concurrency():
    active_mappers = 0
    active_mappers_history = []

    async def mapper(x):
        nonlocal active_mappers
        active_mappers += 1
        await asyncio.sleep(0.1)  # Simulate some async work
        active_mappers_history.append(active_mappers)
        await asyncio.sleep(0.1)  # Simulate some async work
        active_mappers -= 1
        return x * 2

    result = [item async for item in async_map(sync_or_async_iter(range(10)), mapper, concurrency=3)]
    assert sorted(result) == [x * 2 for x in range(10)]
    assert max(active_mappers_history) == 3
    assert active_mappers_history == [3] * 9 + [1]  # first 9 items would have 3 concurrency, last item 1


@pytest.mark.asyncio
@pytest.mark.parametrize("in_order", [True, False])
async def test_async_map_ordering(in_order):
    result = []
    ev = asyncio.Event()

    async def foo():
        yield 1
        yield 2
        yield 3

    async def mapper(x):
        if x == 1:
            await ev.wait()

        if x == 2:
            ev.set()

        return x * 2

    if in_order:
        async for item in async_map_ordered(foo(), mapper, concurrency=3):
            result.append(item)
        assert result == [2, 4, 6]
    else:
        async for item in async_map(foo(), mapper, concurrency=3):
            result.append(item)
        assert result == [4, 6, 2]


@pytest.mark.asyncio
async def test_async_map_ordered_buffer_size():
    processing = []

    async def mapper(x: int) -> int:
        processing.append(x)
        # Item 0 will block, causing buffer to fill up
        if x == 0:
            await asyncio.sleep(0.2)
        await asyncio.sleep(0.01)
        return x

    async def inputs():
        for i in range(100):
            yield i

    # Use small buffer_size to ensure we don't process too far ahead
    results = []
    async for result in async_map_ordered(inputs(), mapper, concurrency=5, buffer_size=3):
        # Check that we never processed more than buffer_size + 1 items ahead
        # (+1 because one item is being yielded while buffer_size items are buffered)
        assert max(processing) - result <= 3
        results.append(result)

    assert results == list(range(100))


@pytest.mark.asyncio
async def test_async_map_ordered_buffer_size2():
    result = []
    ev = asyncio.Event()
    cancel_ev = asyncio.Event()

    async def mapper_func(x: int) -> int:
        if x == 3:
            # Item 3 will block, causing buffer to fill up
            await ev.wait()

        if x == 10:
            # Item 10 will unblock the buffer
            # but with concurrency=3, it will never be reached
            ev.set()
        return x

    async def gen():
        for i in range(100):
            yield i

    async def mapper_coro():
        async with aclosing(async_map_ordered(gen(), mapper_func, concurrency=3)) as stream:
            async for item in stream:
                result.append(item)
                if item == 2:
                    cancel_ev.set()

    mapper_task = asyncio.create_task(mapper_coro())
    await cancel_ev.wait()
    mapper_task.cancel()
    with pytest.raises(asyncio.CancelledError):
        await mapper_task

    assert sorted(result) == [0, 1, 2]


@pytest.mark.asyncio
async def test_async_chain():
    async def gen1():
        await asyncio.sleep(0.1)
        yield 1
        yield 2

    async def gen2():
        yield 3
        await asyncio.sleep(0.1)
        yield 4

    async def gen3():
        yield 5
        yield 6

    result = []
    async for item in async_chain(gen1(), gen2(), gen3()):
        result.append(item)

    assert result == [1, 2, 3, 4, 5, 6]


@pytest.mark.asyncio
async def test_async_chain_sequential():
    ev = asyncio.Event()

    async def gen1():
        await asyncio.sleep(0.1)
        yield 1
        await ev.wait()
        yield 2

    async def gen2():
        yield 3
        ev.set()
        await asyncio.sleep(0.1)
        yield 4

    results = []

    async def concat_coro():
        async with aclosing(async_chain(gen1(), gen2())) as stream:
            async for item in stream:
                results.append(item)

    concat_task = asyncio.create_task(concat_coro())
    await asyncio.sleep(0.5)
    concat_task.cancel()
    with pytest.raises(asyncio.CancelledError):
        await concat_task

    assert results == [1]


@pytest.mark.asyncio
async def test_async_chain_exception():
    # test exception bubbling up
    result = []
    states = []

    async def gen1():
        states.append("enter 1")
        try:
            yield 1
            yield 2
        finally:
            states.append("exit 1")

    async def gen2():
        states.append("enter 2")
        try:
            await asyncio.sleep(0.1)
            yield 3
            raise SampleException("test")
            yield 4
        finally:
            await asyncio.sleep(0)
            states.append("exit 2")

    with pytest.raises(SampleException):
        async for item in async_chain(gen1(), gen2()):
            result.append(item)

    assert result == [1, 2, 3]
    assert states == ["enter 1", "exit 1", "enter 2", "exit 2"]


@pytest.mark.asyncio
async def test_async_chain_cancellation():
    ev = asyncio.Event()

    async def gen1():
        await asyncio.sleep(0.1)
        yield 1
        await ev.wait()
        raise asyncio.CancelledError()
        yield 2

    async def gen2():
        yield 3
        await asyncio.sleep(0.1)
        yield 4

    async def concat_coro():
        async with aclosing(async_chain(gen1(), gen2())) as stream:
            async for _ in stream:
                pass

    concat_task = asyncio.create_task(concat_coro())
    await asyncio.sleep(0.1)
    concat_task.cancel()
    with pytest.raises(asyncio.CancelledError):
        await concat_task


@pytest.mark.asyncio
async def test_async_chain_producer_cancellation():
    async def gen1():
        await asyncio.sleep(0.1)
        yield 1
        raise asyncio.CancelledError()
        yield 2

    async def gen2():
        yield 3
        await asyncio.sleep(0.1)
        yield 4

    await asyncio.sleep(0.1)
    with pytest.raises(asyncio.CancelledError):
        async with aclosing(async_chain(gen1(), gen2())) as stream:
            async for _ in stream:
                pass


@pytest.mark.asyncio
async def test_async_chain_cleanup():
    # test cleanup of generators
    result = []
    states = []

    async def gen1():
        states.append("enter 1")
        try:
            await asyncio.sleep(0.1)
            yield 1
            yield 2
        finally:
            await asyncio.sleep(0)
            states.append("exit 1")

    async def gen2():
        states.append("enter 2")
        try:
            yield 3
            await asyncio.sleep(0.1)
            yield 4
        finally:
            await asyncio.sleep(0)
            states.append("exit 2")

    async with aclosing(async_chain(gen1(), gen2())) as stream:
        async for item in stream:
            result.append(item)
            if item == 3:
                break

    assert result == [1, 2, 3]
    assert states == ["enter 1", "exit 1", "enter 2", "exit 2"]


def test_sigint_run_async_gen_shuts_down_gracefully():
    code = textwrap.dedent(
        """
    import asyncio
    import time
    from itertools import count
    from synchronicity.async_utils import Runner
    from modal._utils.async_utils import run_async_gen
    async def async_gen():
        print("enter")
        try:
            for i in count():
                yield i
                await asyncio.sleep(0.1)
        finally:
            # this could be either CancelledError or GeneratorExit depending on timing
            # CancelledError happens if sigint is during this generator's await
            # GeneratorExit is during the yielded block in the sync caller
            print("cancel")
            await asyncio.sleep(0.1)
            print("bye")
    try:
        with Runner() as runner:
            for res in run_async_gen(runner, async_gen()):
                print("res", res)
    except KeyboardInterrupt:
        print("KeyboardInterrupt")
    """
    )

    p = helpers.PopenWithCtrlC(
        [sys.executable, "-u", "-c", code],
        encoding="utf8",
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
    )

    def line():
        s = p.stdout.readline().rstrip("\n")
        if s == "":
            print(p.stderr.read())
            raise Exception("no stdout")
        print(s)
        return s

    assert line() == "enter"
    assert line() == "res 0"
    assert line() == "res 1"

    p.send_ctrl_c()
    print("sent ctrl-C")
    while (nextline := line()).startswith("res"):
        pass
    assert nextline == "cancel"
    assert line() == "bye"
    assert line() == "KeyboardInterrupt"
    assert p.wait() == 0
    assert p.stdout.read() == ""
    assert p.stderr.read() == ""


@pytest.mark.asyncio
async def test_timed_priority_queue():
    queue: TimestampPriorityQueue = TimestampPriorityQueue()
    now = time.time()

    async def producer():
        await queue.put(now + 0.2, 2)
        await queue.put(now + 0.1, 1)
        await queue.put(now + 0.3, 3)

    async def consumer():
        items = []
        for _ in range(3):
            item = await queue.get()
            items.append(item)
        return items

    await producer()
    assert queue.qsize() == 3
    items = await consumer()
    assert items == [1, 2, 3]


@pytest.mark.asyncio
async def test_timed_priority_queue_duplicates():
    queue: TimestampPriorityQueue = async_utils.TimestampPriorityQueue()
    now = time.time()
    x = now + 0.1

    async def producer():
        await queue.put(1, x)
        await queue.put(1, x)

    async def consumer():
        items = []
        for _ in range(2):
            item = await queue.get()
            items.append(item)
        return items

    await producer()
    assert queue.qsize() == 2
    items = await consumer()
    assert len([it for it in items]) == 2


@pytest.mark.asyncio
async def test_mapper_that_aborts_cancellation_still_exits():
    async def mapper(x):
        # This aborts cancellation but still resolves quickly
        try:
            await asyncio.sleep(0.1)
        except asyncio.CancelledError:
            return x * 100
        return x

    async def it():
        yield 1
        raise SampleException()

    # The following should not inf-loop
    with pytest.raises(SampleException):
        # This should not deadlock!
        async for res in async_map(it(), mapper, 1):
            pass


@pytest.mark.asyncio
async def test_merge_cancellation_timeout():
    cleanup_event = asyncio.Event()

    async def non_cooperative_gen():
        yield 0
        try:
            await asyncio.sleep(0.5)
        except asyncio.CancelledError:
            pass
        await cleanup_event.wait()
        yield 1  # should not happen

    side_effect = []

    async def f():
        async with aclosing(async_merge(non_cooperative_gen(), cancellation_timeout=1.0)) as stream:
            async for res in stream:
                side_effect.append(res)

    t = asyncio.create_task(f())
    await asyncio.sleep(0.2)  # let task get started
    t.cancel()
    t0 = time.monotonic()
    with pytest.raises(asyncio.CancelledError):
        await t
    assert 0.95 < time.monotonic() - t0 < 1.5
    # first output arrives before cancellation, undefined if the ones yielded during cancellation
    # should be in result set - probably not!
    assert side_effect == [0]

    # clean up
    cleanup_event.set()
    await asyncio.sleep(0.1)  # allow the non_cooperative_gen to exit to clean up tasks


@pytest.mark.asyncio
async def test_prevent_cancellation_abortion():
    async def rogue_coro():
        try:
            await asyncio.sleep(1)
            return 0
        except asyncio.CancelledError:
            return 1

    t = asyncio.create_task(prevent_cancellation_abortion(rogue_coro()))
    t2 = asyncio.create_task(rogue_coro())  # without prevent_cancellation_abortion for reference
    await asyncio.sleep(0.1)
    t.cancel()
    t2.cancel()
    assert await t2 == 1
    with pytest.raises(asyncio.CancelledError):
        await t



================================================
FILE: test/auth_token_manager_test.py
================================================
# Copyright Modal Labs 2024
import asyncio
import pytest
import time

import jwt

from modal._utils.async_utils import synchronize_api
from modal._utils.auth_token_manager import _AuthTokenManager
from modal.exception import ExecutionError


@pytest.fixture
def auth_token_manager(client):
    """Create an AuthTokenManager instance for testing."""
    return _AuthTokenManager(client.stub)


@pytest.fixture
def valid_jwt_token():
    """Create a valid JWT token with expiry."""
    # Create a JWT with exp claim set to 1 hour from now
    exp = int(time.time()) + 3600
    payload = {"exp": exp, "type": "valid"}
    return jwt.encode(payload, "my-secret-key", algorithm="HS256")


@pytest.fixture
def another_valid_jwt_token():
    """Create a valid JWT token with expiry."""
    # Create a JWT with exp claim set to 1 hour from now
    exp = int(time.time()) + 3600
    payload = {"exp": exp, "type": "another_valid"}
    return jwt.encode(payload, "my-secret-key", algorithm="HS256")


@pytest.fixture
def expired_jwt_token():
    """Create an expired JWT token."""
    # Create a JWT with exp claim set to 1 hour ago
    exp = int(time.time()) - 3600
    payload = {"exp": exp, "type": "expired"}
    return jwt.encode(payload, "my-secret-key", algorithm="HS256")


@pytest.fixture
def token_without_exp():
    """Create a JWT token without exp claim."""
    payload = {"type": "without_exp"}
    return jwt.encode(payload, "my-secret-key", algorithm="HS256")


@pytest.fixture
def token_near_expiry():
    """Create a JWT token that expires in 4 minutes (within refresh window)."""
    exp = int(time.time()) + 240  # 4 minutes from now
    payload = {"exp": exp, "type": "near_expiry"}
    return jwt.encode(payload, "my-secret-key", algorithm="HS256")


@pytest.mark.asyncio
async def test_get_token_initial_fetch(auth_token_manager, valid_jwt_token, client, servicer):
    """Test getting token when no token exists."""

    # All these tests wrap get_token with @synchronize_api because they hang forever without it.
    @synchronize_api
    async def wrapped_get_token():
        return await auth_token_manager.get_token()

    servicer.auth_token = valid_jwt_token
    assert await wrapped_get_token.aio() == valid_jwt_token


@pytest.mark.asyncio
async def test_get_token_cached(auth_token_manager, valid_jwt_token, servicer):
    """Test that cached token is returned without making new request."""

    @synchronize_api
    async def wrapped_get_token():
        return await auth_token_manager.get_token()

    # Set up initial token
    servicer.auth_token = valid_jwt_token
    assert await wrapped_get_token.aio() == valid_jwt_token

    # Set a bogus token in the servicer, and verify we get the cached valid token
    servicer.auth_token = "bogus"
    token = await wrapped_get_token.aio()
    assert token == valid_jwt_token


@pytest.mark.asyncio
async def test_get_token_expired(auth_token_manager, expired_jwt_token, valid_jwt_token, servicer):
    """Test that expired token triggers refresh."""
    # Set up expired token
    auth_token_manager._token = expired_jwt_token
    auth_token_manager._expiry = exp_time(expired_jwt_token)

    # Set up new token in servicer
    servicer.auth_token = valid_jwt_token

    @synchronize_api
    async def wrapped_get_token():
        return await auth_token_manager.get_token()

    token = await wrapped_get_token.aio()
    assert token == valid_jwt_token
    assert auth_token_manager._token == valid_jwt_token


@pytest.mark.asyncio
async def test_get_token_needs_refresh(auth_token_manager, token_near_expiry, valid_jwt_token, servicer):
    """Test that token is refreshed when it's close to expiry."""
    # Set up token that expires within refresh window
    auth_token_manager._token = token_near_expiry
    auth_token_manager._expiry = exp_time(token_near_expiry)

    # Set up new token in servicer
    servicer.auth_token = valid_jwt_token

    @synchronize_api
    async def wrapped_get_token():
        return await auth_token_manager.get_token()

    token = await wrapped_get_token.aio()
    assert token == valid_jwt_token
    assert auth_token_manager._token == valid_jwt_token


@pytest.mark.asyncio
async def test_get_token_no_exp_claim(auth_token_manager, token_without_exp, servicer):
    """Test handling of token without exp claim."""
    servicer.auth_token = token_without_exp

    @synchronize_api
    async def wrapped_get_token():
        return await auth_token_manager.get_token()

    token = await wrapped_get_token.aio()
    assert token == token_without_exp
    assert auth_token_manager._token == token_without_exp
    # Should use default expiry
    assert auth_token_manager._expiry > time.time()
    assert auth_token_manager._expiry <= time.time() + auth_token_manager.DEFAULT_EXPIRY_OFFSET


@pytest.mark.asyncio
async def test_get_token_empty_response(auth_token_manager, servicer):
    """Test handling of empty token response."""
    servicer.auth_token = ""

    @synchronize_api
    async def wrapped_get_token():
        return await auth_token_manager.get_token()

    with pytest.raises(ExecutionError):
        await wrapped_get_token.aio()


@pytest.mark.asyncio
async def test_get_token_none_response(auth_token_manager, servicer):
    """Test handling of None token response."""
    servicer.auth_token = None

    @synchronize_api
    async def wrapped_get_token():
        return await auth_token_manager.get_token()

    with pytest.raises(ExecutionError):
        await wrapped_get_token.aio()


@pytest.mark.asyncio
async def test_concurrent_token_fetch(auth_token_manager, valid_jwt_token, servicer):
    """Test that concurrent calls don't make multiple requests."""
    servicer.auth_token = valid_jwt_token

    @synchronize_api
    async def wrapped_get_token():
        return await auth_token_manager.get_token()

    # Make concurrent calls
    tasks = [wrapped_get_token.aio() for _ in range(5)]
    results = await asyncio.gather(*tasks)

    # All should return the same token
    assert all(token == valid_jwt_token for token in results)
    # The server should have been called only once.
    assert servicer.auth_tokens_generated == 1


@pytest.mark.asyncio
async def test_concurrent_refresh(auth_token_manager, token_near_expiry, valid_jwt_token, servicer):
    """Test that when get_token is called concurrently, test that old but valid token is returned."""
    # Set up token that needs refresh
    auth_token_manager._token = "old.but.valid.token"
    auth_token_manager._expiry = exp_time(token_near_expiry)

    # Set up new token in servicer
    servicer.auth_token = valid_jwt_token

    @synchronize_api
    async def wrapped_get_token():
        return await auth_token_manager.get_token()

    # Make concurrent calls
    tasks = [wrapped_get_token.aio() for _ in range(10)]
    results = await asyncio.gather(*tasks)

    # At least one call should have returned the new token
    assert valid_jwt_token in results
    # When called concurrently, only one coroutine should fetch a new token, and the others should use the older but
    # still valid token to improve throughput. Note, this isn't guaranteed, just very likely. May need to fix if flakey.
    assert "old.but.valid.token" in results
    # The new token should be cached
    assert auth_token_manager._token == valid_jwt_token


def test_decode_jwt_valid(valid_jwt_token):
    """Test JWT decoding with valid token."""
    decoded = _AuthTokenManager._decode_jwt(valid_jwt_token)
    assert "exp" in decoded
    assert "type" in decoded
    assert decoded["type"] == "valid"


def test_decode_jwt_without_exp(token_without_exp):
    """Test JWT decoding with token that has no exp claim."""
    decoded = _AuthTokenManager._decode_jwt(token_without_exp)
    assert "exp" not in decoded
    assert "type" in decoded
    assert decoded["type"] == "without_exp"


def test_decode_jwt_invalid_format():
    """Test JWT decoding with invalid token format."""
    with pytest.raises(ValueError):
        _AuthTokenManager._decode_jwt("invalid.token")


def test_needs_refresh_true(auth_token_manager):
    """Test _needs_refresh returns True when token expires soon."""
    # Set expiry to 4 minutes from now (within refresh window)
    auth_token_manager._expiry = time.time() + 240
    assert auth_token_manager._needs_refresh() is True


def test_needs_refresh_false(auth_token_manager):
    """Test _needs_refresh returns False when token is not close to expiry."""
    # Set expiry to 10 minutes from now (outside refresh window)
    auth_token_manager._expiry = time.time() + 600
    assert auth_token_manager._needs_refresh() is False


def test_is_expired_true(auth_token_manager):
    """Test _is_expired returns True for expired token."""
    # Set expiry to 1 minute ago
    auth_token_manager._expiry = time.time() - 60
    assert auth_token_manager._is_expired() is True


def test_is_expired_false(auth_token_manager):
    """Test _is_expired returns False for valid token."""
    # Set expiry to 1 minute from now
    auth_token_manager._expiry = time.time() + 60
    assert auth_token_manager._is_expired() is False


@pytest.mark.asyncio
async def test_multiple_refresh_cycles(auth_token_manager, servicer):
    """Test multiple refresh cycles work correctly."""
    exp = int(time.time()) + 3600
    tokens = [
        jwt.encode({"exp": exp, "name": "t0"}, "my-secret-key", algorithm="HS256"),
        jwt.encode({"exp": exp, "name": "t1"}, "my-secret-key", algorithm="HS256"),
        jwt.encode({"exp": exp, "name": "t2"}, "my-secret-key", algorithm="HS256"),
    ]

    @synchronize_api
    async def wrapped_get_token():
        return await auth_token_manager.get_token()

    # First call
    servicer.auth_token = tokens[0]
    token0 = await wrapped_get_token.aio()
    assert token0 == tokens[0]

    # Expire the token
    auth_token_manager._expiry = time.time() - 100

    # Second call
    servicer.auth_token = tokens[1]
    token1 = await wrapped_get_token.aio()
    assert token1 == tokens[1]

    # Expire again
    auth_token_manager._expiry = time.time() - 100

    # Third call
    servicer.auth_token = tokens[2]
    token2 = await wrapped_get_token.aio()
    assert token2 == tokens[2]


def exp_time(token: str):
    return jwt.decode(token, options={"verify_signature": False})["exp"]



================================================
FILE: test/blob_test.py
================================================
# Copyright Modal Labs 2022

import pytest
import random

from modal._utils.async_utils import synchronize_api
from modal._utils.blob_utils import (
    blob_download as _blob_download,
    blob_upload as _blob_upload,
    blob_upload_file as _blob_upload_file,
)
from modal.exception import ExecutionError

from .supports.skip import skip_old_py

blob_upload = synchronize_api(_blob_upload)
blob_download = synchronize_api(_blob_download)
blob_upload_file = synchronize_api(_blob_upload_file)


@pytest.mark.asyncio
async def test_blob_put_get(servicer, blob_server, client):
    # Upload
    blob_id = await blob_upload.aio(b"Hello, world", client.stub)

    # Download
    data = await blob_download.aio(blob_id, client.stub)
    assert data == b"Hello, world"


@pytest.mark.asyncio
async def test_blob_put_failure(servicer, blob_server, client):
    with pytest.raises(ExecutionError):
        await blob_upload.aio(b"FAILURE", client.stub)


@pytest.mark.asyncio
async def test_blob_get_failure(servicer, blob_server, client):
    with pytest.raises(ExecutionError):
        await blob_download.aio("bl-failure", client.stub)


@pytest.mark.asyncio
async def test_blob_large(servicer, blob_server, client):
    data = b"*" * 10_000_000
    blob_id = await blob_upload.aio(data, client.stub)
    assert await blob_download.aio(blob_id, client.stub) == data


@skip_old_py("random.randbytes() was introduced in python 3.9", (3, 9))
@pytest.mark.asyncio
async def test_blob_multipart(servicer, blob_server, client, monkeypatch, tmp_path):
    monkeypatch.setattr("modal._utils.blob_utils.DEFAULT_SEGMENT_CHUNK_SIZE", 128)
    multipart_threshold = 1024
    servicer.blob_multipart_threshold = multipart_threshold
    # - set high # of parts, to test concurrency correctness
    # - make last part significantly shorter than rest, creating uneven upload time.
    data_len = (256 * multipart_threshold) + (multipart_threshold // 2)
    data = random.randbytes(data_len)  # random data will not hide byte re-ordering corruption
    blob_id = await blob_upload.aio(data, client.stub)
    assert await blob_download.aio(blob_id, client.stub) == data

    data_len = (256 * multipart_threshold) + (multipart_threshold // 2)
    data = random.randbytes(data_len)  # random data will not hide byte re-ordering corruption
    data_filepath = tmp_path / "temp.bin"
    data_filepath.write_bytes(data)
    with data_filepath.open("rb") as f:
        blob_id = await blob_upload_file.aio(f, client.stub)
    assert await blob_download.aio(blob_id, client.stub) == data


def test_sync(blob_server, client):
    # just tests that tests running blocking calls that upload to blob storage don't deadlock
    blob_upload(b"adsfadsf", client.stub)



================================================
FILE: test/cli_imports_test.py
================================================
# Copyright Modal Labs 2023
import pytest

from modal.app import App, LocalEntrypoint
from modal.cli.import_refs import (
    AutoRunPriority,
    CLICommand,
    ImportRef,
    MethodReference,
    import_and_filter,
    import_file_or_module,
    list_cli_commands,
    parse_import_ref,
)
from modal.exception import DeprecationError, InvalidError
from modal.functions import Function
from modal.partial_function import method, web_server

# Some helper vars for import_app tests:
local_entrypoint_src = """
import modal

app = modal.App()
@app.local_entrypoint()
def main():
    pass
"""
python_module_src = """
import modal
app = modal.App("FOO")
other_app = modal.App("BAR")
@other_app.function()
def func():
    pass
@app.cls()
class Parent:
    @modal.method()
    def meth(self):
        pass

assert not __package__
"""

python_package_src = """
import modal
app = modal.App("FOO")
other_app = modal.App("BAR")
@other_app.function()
def func():
    pass
assert __package__ == "pack005"
"""

python_subpackage_src = """
import modal
app = modal.App("FOO")
other_app = modal.App("BAR")
@other_app.function()
def func():
    pass
assert __package__ == "pack007.sub009"
"""

python_file_src = """
import modal
app = modal.App("FOO")
other_app = modal.App("BAR")
@other_app.function()
def func():
    pass

assert __package__ == ""
"""

empty_dir_with_python_file = {"mod000.py": python_module_src}


dir_containing_python_package = {
    "dir001": {"sub002": {"mod003.py": python_module_src, "subfile004.py": python_file_src}},
    "pack005": {
        "file006.py": python_file_src,
        "mod007.py": python_package_src,
        "local008.py": local_entrypoint_src,
        "__init__.py": "",
        "sub009": {"mod010.py": python_subpackage_src, "__init__.py": "", "subfile011.py": python_file_src},
    },
}


@pytest.mark.parametrize(
    ["dir_structure", "ref", "returned_runnable_type", "num_error_choices", "use_module_mode"],
    [
        # # file syntax
        (empty_dir_with_python_file, "mod000.py", type(None), 2, False),
        (empty_dir_with_python_file, "mod000.py::app", MethodReference, 2, False),
        (empty_dir_with_python_file, "mod000.py::other_app", Function, 2, False),
        (dir_containing_python_package, "pack005/file006.py", Function, 1, False),
        (dir_containing_python_package, "pack005/sub009/subfile011.py", Function, 1, False),
        (dir_containing_python_package, "dir001/sub002/subfile004.py", Function, 1, False),
        (dir_containing_python_package, "pack005/local008.py::app.main", LocalEntrypoint, 1, False),
        # # python module syntax
        (empty_dir_with_python_file, "mod000::func", Function, 2, True),
        (empty_dir_with_python_file, "mod000::other_app.func", Function, 2, True),
        (empty_dir_with_python_file, "mod000::app.func", type(None), 2, True),
        (empty_dir_with_python_file, "mod000::Parent.meth", MethodReference, 2, True),
        (empty_dir_with_python_file, "mod000::other_app", Function, 2, True),
        (dir_containing_python_package, "pack005.mod007", Function, 1, True),
        (dir_containing_python_package, "pack005.mod007::other_app", Function, 1, True),
    ],
)
def test_import_and_filter(dir_structure, ref, mock_dir, returned_runnable_type, num_error_choices, use_module_mode):
    with mock_dir(dir_structure):
        import_ref = parse_import_ref(ref, use_module_mode=use_module_mode)
        runnable, all_usable_commands = import_and_filter(
            import_ref, base_cmd="dummy", accept_local_entrypoint=True, accept_webhook=False
        )
        print(all_usable_commands)
        assert isinstance(runnable, returned_runnable_type)
        assert len(all_usable_commands) == num_error_choices


def test_import_and_filter_2(monkeypatch, supports_on_path):
    def import_runnable(object_path, accept_local_entrypoint=False, accept_webhook=False):
        return import_and_filter(
            ImportRef("import_and_filter_source", use_module_mode=True, object_path=object_path),
            base_cmd="",
            accept_local_entrypoint=accept_local_entrypoint,
            accept_webhook=accept_webhook,
        )

    runnable, all_usable_commands = import_runnable(
        "app_with_one_web_function", accept_webhook=False, accept_local_entrypoint=True
    )
    assert runnable is None
    assert len(all_usable_commands) == 4

    assert import_runnable("app_with_one_web_function", accept_webhook=True)[0]
    assert import_runnable("app_with_one_function_one_web_endpoint", accept_webhook=False)[0]

    runnable, all_usable_commands = import_runnable("app_with_one_function_one_web_endpoint", accept_webhook=True)
    assert runnable is None
    assert len(all_usable_commands) == 7

    runnable, all_usable_commands = import_runnable("app_with_one_web_method", accept_webhook=False)
    assert runnable is None
    assert len(all_usable_commands) == 3

    assert import_runnable("app_with_one_web_method", accept_webhook=True)[0]

    assert isinstance(
        import_runnable("app_with_local_entrypoint_and_function", accept_local_entrypoint=True)[0], LocalEntrypoint
    )
    assert isinstance(
        import_runnable("app_with_local_entrypoint_and_function", accept_local_entrypoint=False)[0], Function
    )


def test_import_package_and_module_names(monkeypatch, supports_dir):
    # We try to reproduce the package/module naming standard that the `python` command line tool uses,
    # i.e. when loading using a module path (-m flag w/ python) you get a fully qualified package/module name
    # but when loading using a filename, some/mod.py it will not have a __package__

    # The biggest difference is that __name__ of the imported "entrypoint" script
    # is __main__ when using `python` but in the Modal runtime it's the name of the
    # file minus the ".py", since Modal has its own __main__
    monkeypatch.chdir(supports_dir)
    mod1 = import_file_or_module(ImportRef("assert_package", use_module_mode=True))
    assert mod1.__package__ == ""
    assert mod1.__name__ == "assert_package"

    monkeypatch.chdir(supports_dir.parent)
    with pytest.warns(DeprecationError, match=r"\s-m\s"):
        # TODO: this should use use_module_mode=True once we remove the deprecation warning
        mod2 = import_file_or_module(ImportRef("test.supports.assert_package", use_module_mode=False))

    assert mod2.__package__ == "test.supports"
    assert mod2.__name__ == "test.supports.assert_package"

    mod3 = import_file_or_module(ImportRef("supports/assert_package.py", use_module_mode=False))
    assert mod3.__package__ == ""
    assert mod3.__name__ == "assert_package"


def test_invalid_source_file_exception():
    with pytest.raises(InvalidError, match="Invalid Modal source filename: 'foo.bar.py'"):
        import_file_or_module(ImportRef("path/to/foo.bar.py", use_module_mode=False))


def test_list_cli_commands():
    app = App()
    other_app = App()

    @app.function(serialized=True, name="foo")
    def foo():
        pass

    @app.cls(serialized=True)
    class Cls:
        @method()
        def method_1(self):
            pass

        @web_server(8000)
        def web_method(self):
            pass

    def non_modal_func():
        pass

    fake_module = {"app": app, "other_app": other_app, "non_modal_func": non_modal_func, "foo": foo, "Cls": Cls}

    res = list_cli_commands(fake_module)

    assert res == [
        CLICommand(["foo", "app.foo"], foo, False, priority=AutoRunPriority.MODULE_FUNCTION),  # type: ignore
        CLICommand(
            ["Cls.method_1", "app.Cls.method_1"],
            MethodReference(Cls, "method_1"),  # type: ignore
            False,
            priority=AutoRunPriority.MODULE_FUNCTION,
        ),
        CLICommand(
            ["Cls.web_method", "app.Cls.web_method"],
            MethodReference(Cls, "web_method"),  # type: ignore
            True,
            priority=AutoRunPriority.MODULE_FUNCTION,
        ),
    ]



================================================
FILE: test/client_test.py
================================================
# Copyright Modal Labs 2022
import platform
import pytest
import subprocess
import sys
import urllib.parse

from google.protobuf.empty_pb2 import Empty
from grpclib import GRPCError, Status

import modal._runtime
import modal._utils.grpc_utils
from modal import Client
from modal.exception import AuthError, ConnectionError, InvalidError, ServerWarning
from modal_proto import api_pb2

from .supports.skip import skip_windows, skip_windows_unix_socket

TEST_TIMEOUT = 4.0  # align this with the container client timeout in client.py


def test_client_type(servicer, client):
    assert len(servicer.requests) == 0
    client.hello()
    assert len(servicer.requests) == 1
    assert isinstance(servicer.requests[0], Empty)
    assert servicer.last_metadata["x-modal-client-type"] == str(api_pb2.CLIENT_TYPE_CLIENT)


CHALLENGING_PLATFORM_NODE = "\0Ã¤bc \n"


@pytest.fixture
def _patch_platform_node(monkeypatch):
    # Platform info is read when the client is created, so have to patch it before the fixture is requested
    monkeypatch.setattr("modal.client.platform.node", lambda: CHALLENGING_PLATFORM_NODE)


def test_client_node_string(servicer, _patch_platform_node, client):
    client.hello()
    platform_str = urllib.parse.unquote(servicer.last_metadata["x-modal-node"])
    assert platform_str == CHALLENGING_PLATFORM_NODE


def test_client_platform_string(servicer, client):
    client.hello()
    platform_str = urllib.parse.unquote(servicer.last_metadata["x-modal-platform"])
    system, release, machine = platform_str.split("-")
    if platform.system() == "Darwin":
        assert system == "macOS"
        assert release == platform.mac_ver()[0].replace("-", "_")
    else:
        assert system == platform.system().replace("-", "_")
        assert release == platform.release().replace("-", "_")
    assert machine == platform.machine().replace("-", "_")


@pytest.mark.asyncio
async def test_container_client_type(servicer, container_client):
    await container_client.hello.aio()
    assert len(servicer.requests) == 1
    assert isinstance(servicer.requests[0], Empty)
    assert servicer.last_metadata["x-modal-client-type"] == str(api_pb2.CLIENT_TYPE_CONTAINER)


@pytest.fixture
def no_retry_connect_channel(monkeypatch):
    # Make the error appear faster during test
    org_connect_channel = modal._utils.grpc_utils.connect_channel
    monkeypatch.setattr(modal._utils.grpc_utils, "connect_channel", org_connect_channel.__wrapped__)


@pytest.mark.asyncio
@pytest.mark.timeout(TEST_TIMEOUT)
async def test_client_dns_failure(no_retry_connect_channel):
    with pytest.raises(ConnectionError) as excinfo:
        async with Client("https://xyz.invalid", api_pb2.CLIENT_TYPE_CONTAINER, None):
            pass
    assert excinfo.value


@pytest.mark.asyncio
@pytest.mark.timeout(TEST_TIMEOUT)
@skip_windows("Windows test crashes on connection failure")
async def test_client_connection_failure(no_retry_connect_channel):
    with pytest.raises(ConnectionError) as excinfo:
        async with Client("https://localhost:443", api_pb2.CLIENT_TYPE_CONTAINER, None):
            pass
    assert excinfo.value


@pytest.mark.asyncio
@pytest.mark.timeout(TEST_TIMEOUT)
@skip_windows_unix_socket
async def test_client_connection_failure_unix_socket(no_retry_connect_channel):
    with pytest.raises(ConnectionError) as excinfo:
        async with Client("unix:/tmp/xyz.txt", api_pb2.CLIENT_TYPE_CONTAINER, None):
            pass
    assert excinfo.value


@pytest.mark.asyncio
async def test_client_old_version(servicer, credentials):
    async with Client(servicer.client_addr, api_pb2.CLIENT_TYPE_CLIENT, credentials, version="0.0.0") as client:
        with pytest.raises(GRPCError) as excinfo:
            await client.hello.aio()
        assert excinfo.value.status == Status.FAILED_PRECONDITION
        assert excinfo.value.message == "Old client"


@pytest.mark.asyncio
async def test_client_deprecated(servicer, credentials):
    async with Client(servicer.client_addr, api_pb2.CLIENT_TYPE_CLIENT, credentials, version="deprecated") as client:
        with pytest.warns(ServerWarning):
            await client.hello.aio()


@pytest.mark.asyncio
async def test_client_unauthenticated(servicer):
    with pytest.raises(AuthError):
        async with Client(servicer.client_addr, api_pb2.CLIENT_TYPE_CLIENT, None, version="unauthenticated") as client:
            await client.hello.aio()


def client_from_env(client_addr, credentials):
    token_id, token_secret = credentials
    _override_config = {
        "server_url": client_addr,
        "token_id": token_id,
        "token_secret": token_secret,
        "task_id": None,
        "task_secret": None,
    }
    client = Client.from_env(_override_config=_override_config)
    client.hello()
    return client


def test_client_from_env_client(servicer, credentials):
    client_1 = client_from_env(servicer.client_addr, credentials)
    client_2 = client_from_env(servicer.client_addr, credentials)
    assert isinstance(client_1, Client)
    assert isinstance(client_2, Client)
    assert client_1 == client_2


def test_client_from_env_failing(servicer, credentials):
    with pytest.raises(ConnectionError):
        client_from_env("https://foo.invalid", credentials)


def test_client_from_env_reset(servicer, credentials):
    client_1 = client_from_env(servicer.client_addr, credentials)
    Client.set_env_client(None)
    client_2 = client_from_env(servicer.client_addr, credentials)
    assert client_1 != client_2


def test_client_token_auth_in_sandbox(servicer, credentials, monkeypatch) -> None:
    """Ensure that clients can connect with token credentials inside a sandbox.

    This test is needed so that modal.com/playground works, since it relies on
    running a sandbox with token credentials. Also, `modal shell` uses this to
    preserve its auth context inside the shell.
    """
    monkeypatch.setenv("MODAL_TASK_ID", "ta-123")
    _client = client_from_env(servicer.client_addr, credentials)
    assert servicer.last_metadata["x-modal-client-type"] == str(api_pb2.CLIENT_TYPE_CLIENT)


def test_multiple_profile_error(servicer, modal_config):
    config = """
    [prof-1]
    token_id = 'ak-abc'
    token_secret = 'as_xyz'
    active = true

    [prof-2]
    token_id = 'ak-abc'
    token_secret = 'as_xyz'
    active = true
    """
    with modal_config(config):
        with pytest.raises(InvalidError, match="More than one Modal profile is active"):
            Client.from_env()


def test_implicit_default_profile_warning(servicer, modal_config, server_url_env):
    config = """
    [default]
    token_id = 'ak-abc'
    token_secret = 'as_xyz'

    [other]
    token_id = 'ak-abc'
    token_secret = 'as_xyz'
    """
    with modal_config(config):
        with pytest.raises(InvalidError, match="No Modal profile is active"):
            Client.from_env()

    config = """
    [default]
    token_id = 'ak-abc'
    token_secret = 'as_xyz'
    """
    with modal_config(config):
        servicer.required_creds = {"ak-abc": "as_xyz"}
        # A single profile should be fine, even if not explicitly active and named 'default'
        Client.from_env()


def test_import_modal_from_thread(supports_dir):
    # this mainly ensures that we don't make any assumptions about which thread *imports* modal
    # For example, in Python <3.10, creating loop-bound asyncio primitives in global scope would
    # trigger an exception if there is no event loop in the thread (and it's not the main thread)
    subprocess.check_call([sys.executable, supports_dir / "import_modal_from_thread.py"])


def test_from_env_container(servicer, container_env):
    servicer.required_creds = {}  # Disallow default client creds
    client = Client.from_env()
    client.hello()
    assert servicer.last_metadata["x-modal-client-type"] == str(api_pb2.CLIENT_TYPE_CONTAINER)


def test_from_env_container_with_tokens(servicer, container_env, token_env):
    # Even if MODAL_TOKEN_ID and MODAL_TOKEN_SECRET are set, if we're in a containers, ignore those
    servicer.required_creds = {}  # Disallow default client creds
    with pytest.warns(match="token"):
        client = Client.from_env()
    client.hello()
    assert servicer.last_metadata["x-modal-client-type"] == str(api_pb2.CLIENT_TYPE_CONTAINER)


def test_from_credentials_client(servicer, set_env_client, server_url_env, token_env):
    # Note: this explicitly uses a lot of fixtures to make sure those are ignored
    token_id = "ak-foo-1"
    token_secret = "as-bar"
    servicer.required_creds = {token_id: token_secret}
    client = Client.from_credentials(token_id, token_secret)
    client.hello()
    assert servicer.last_metadata["x-modal-client-type"] == str(api_pb2.CLIENT_TYPE_CLIENT)


def test_from_credentials_container(servicer, container_env):
    token_id = "ak-foo-2"
    token_secret = "as-bar"
    servicer.required_creds = {token_id: token_secret}
    client = Client.from_credentials(token_id, token_secret)
    client.hello()
    assert servicer.last_metadata["x-modal-client-type"] == str(api_pb2.CLIENT_TYPE_CLIENT)


def test_client_verify(servicer, client):
    token_id = "ak-foo-2"
    token_secret = "as-bar"
    servicer.required_creds = {token_id: token_secret}

    assert len(servicer.requests) == 0
    client.verify(servicer.client_addr, (token_id, token_secret))
    assert len(servicer.requests) == 1

    with pytest.raises(AuthError):
        client.verify(servicer.client_addr, ("foo", "bar"))

    with pytest.raises(ConnectionError):
        client.verify("https://localhost:443", ("foo", "bar"))



================================================
FILE: test/cloud_bucket_mount_test.py
================================================
# Copyright Modal Labs 2024
import modal


def dummy():
    pass


def test_volume_mount(client, servicer):
    app = modal.App()
    secret = modal.Secret.from_dict({"AWS_ACCESS_KEY_ID": "1", "AWS_SECRET_ACCESS_KEY": "2"})
    cld_bckt_mnt = modal.CloudBucketMount(
        bucket_name="foo",
        key_prefix="dir/",
        bucket_endpoint_url="https://1234.r2.cloudflarestorage.com",
        secret=secret,
        read_only=False,
    )

    _ = app.function(volumes={"/root/foo": cld_bckt_mnt})(dummy)

    with app.run(client=client):
        pass



================================================
FILE: test/cls_test.py
================================================
# Copyright Modal Labs 2022
import dataclasses
import inspect
import pytest
import subprocess
import sys
import threading
import typing
from typing import TYPE_CHECKING

from typing_extensions import assert_type

import modal.experimental
import modal.partial_function
from modal import App, Cls, Function, Image, Volume, enter, exit, method
from modal._partial_function import (
    _find_partial_methods_for_user_cls,
    _PartialFunction,
    _PartialFunctionFlags,
)
from modal._serialization import deserialize, deserialize_params, serialize
from modal._utils.async_utils import synchronizer
from modal._utils.function_utils import FunctionInfo
from modal.cls import _ServiceOptions
from modal.exception import DeprecationError, ExecutionError, InvalidError, NotFoundError
from modal.partial_function import (
    PartialFunction,
    asgi_app,
    fastapi_endpoint,
    web_server,
)
from modal.runner import deploy_app
from modal.running_app import RunningApp
from modal_proto import api_pb2

from .supports.base_class import BaseCls2

app = App("app")


@pytest.fixture(autouse=True)
def auto_use_set_env_client(set_env_client):
    # TODO(elias): remove set_env_client fixture here if/when possible - this is required only since
    #  Client.from_env happens to inject an unused client when loading the
    #  parametrized function
    return


@app.cls()
class NoParamsCls:
    @method()
    def bar(self, x):
        return x**3

    @method()
    def baz(self, x):
        return x**2


@app.cls(cpu=42, _experimental_restrict_output=True)
class Foo:
    @method()
    def bar(self, x: int) -> float:
        return x**3

    @method()
    def baz(self, y: int) -> float:
        return y**4

    @web_server(8080)
    def web(self):
        pass


def test_run_class(client, servicer):
    assert len(servicer.precreated_functions) == 0
    assert servicer.n_functions == 0
    with app.run(client=client):
        method_handle_object_id = Foo._get_class_service_function().object_id  # type: ignore
        assert isinstance(Foo, Cls)
        assert isinstance(NoParamsCls, Cls)
        class_id = Foo.object_id
        class_id2 = NoParamsCls.object_id
        app_id = app.app_id

    assert len(servicer.classes) == 2 and set(servicer.classes) == {class_id, class_id2}
    assert servicer.n_functions == 2
    objects = servicer.app_objects[app_id]
    class_function_id = objects["Foo.*"]
    class_function_id2 = objects["NoParamsCls.*"]
    assert servicer.precreated_functions == {class_function_id, class_function_id2}
    assert method_handle_object_id == class_function_id  # method handle object id will probably go away
    assert len(objects) == 4  # two classes + two class service function
    assert objects["Foo"] == class_id
    assert class_function_id.startswith("fu-")
    assert servicer.app_functions[class_function_id].is_class

    assert servicer.app_functions[class_function_id].method_definitions == {
        "bar": api_pb2.MethodDefinition(
            function_name="Foo.bar",
            function_type=api_pb2.Function.FunctionType.FUNCTION_TYPE_FUNCTION,
            function_schema=api_pb2.FunctionSchema(
                schema_type=api_pb2.FunctionSchema.FunctionSchemaType.FUNCTION_SCHEMA_V1,
                arguments=[
                    api_pb2.ClassParameterSpec(
                        name="x",
                        full_type=api_pb2.GenericPayloadType(
                            base_type=api_pb2.ParameterType.PARAM_TYPE_INT,
                        ),
                    )
                ],
                return_type=api_pb2.GenericPayloadType(
                    base_type=api_pb2.ParameterType.PARAM_TYPE_UNKNOWN,
                ),
            ),
            supported_input_formats=[api_pb2.DATA_FORMAT_PICKLE, api_pb2.DATA_FORMAT_CBOR],
            supported_output_formats=[api_pb2.DATA_FORMAT_CBOR],
        ),
        "baz": api_pb2.MethodDefinition(
            function_name="Foo.baz",
            function_type=api_pb2.Function.FunctionType.FUNCTION_TYPE_FUNCTION,
            function_schema=api_pb2.FunctionSchema(
                schema_type=api_pb2.FunctionSchema.FunctionSchemaType.FUNCTION_SCHEMA_V1,
                arguments=[
                    api_pb2.ClassParameterSpec(
                        name="y",
                        full_type=api_pb2.GenericPayloadType(
                            base_type=api_pb2.ParameterType.PARAM_TYPE_INT,
                        ),
                    )
                ],
                return_type=api_pb2.GenericPayloadType(
                    base_type=api_pb2.ParameterType.PARAM_TYPE_UNKNOWN,
                ),
            ),
            supported_input_formats=[api_pb2.DATA_FORMAT_PICKLE, api_pb2.DATA_FORMAT_CBOR],
            supported_output_formats=[api_pb2.DATA_FORMAT_CBOR],
        ),
        "web": api_pb2.MethodDefinition(
            function_name="Foo.web",
            function_type=api_pb2.Function.FunctionType.FUNCTION_TYPE_FUNCTION,
            webhook_config=api_pb2.WebhookConfig(
                type=api_pb2.WEBHOOK_TYPE_WEB_SERVER,
                async_mode=api_pb2.WEBHOOK_ASYNC_MODE_AUTO,
                web_server_port=8080,
                web_server_startup_timeout=5,
            ),
            supported_input_formats=[api_pb2.DATA_FORMAT_ASGI],
            supported_output_formats=[api_pb2.DATA_FORMAT_ASGI, api_pb2.DATA_FORMAT_GENERATOR_DONE],
            web_url="http://web.internal",
        ),
    }


def test_call_class_sync(client, servicer, set_env_client):
    with servicer.intercept() as ctx:
        with app.run(client=client):
            assert len(ctx.get_requests("FunctionCreate")) == 2  # one for Foo, one for NoParamsCls
            foo: NoParamsCls = NoParamsCls()
            assert len(ctx.get_requests("FunctionCreate")) == 2
            assert len(ctx.get_requests("FunctionBindParams")) == 0  # no binding, yet
            ret: float = foo.bar.remote(42)
            assert ret == 1764
            assert (
                len(ctx.get_requests("FunctionBindParams")) == 0
            )  # reuse class base function when class has no params

    function_creates_requests: list[api_pb2.FunctionCreateRequest] = ctx.get_requests("FunctionCreate")
    assert len(function_creates_requests) == 2
    assert len(ctx.get_requests("ClassCreate")) == 2
    function_creates = {fc.function.function_name: fc for fc in function_creates_requests}
    assert function_creates.keys() == {"Foo.*", "NoParamsCls.*"}
    service_function_id = servicer.app_objects["ap-1"]["NoParamsCls.*"]
    (function_map_request,) = ctx.get_requests("FunctionMap")
    assert function_map_request.function_id == service_function_id


def test_class_with_options(client, servicer):
    unhydrated_volume = modal.Volume.from_name("some_volume", create_if_missing=True)
    unhydrated_secret = modal.Secret.from_dict({"foo": "bar"})
    unhydrated_aws_secret = modal.Secret.from_dict(
        {"AWS_ACCESS_KEY_ID": "my-key", "AWS_SECRET_ACCESS_KEY": "my-secret"}
    )
    cloud_bucket_mount = modal.CloudBucketMount(bucket_name="s3-bucket-name", secret=unhydrated_aws_secret)
    with servicer.intercept() as ctx:
        foo = Foo.with_options(  # type: ignore
            cpu=48,
            retries=5,
            volumes={"/vol": unhydrated_volume, "/cloud_mnt": cloud_bucket_mount},
            secrets=[unhydrated_secret],
            region="us-east-1",
            cloud="aws",
        )()
        assert len(ctx.calls) == 0  # no rpcs in with_options

    with app.run(client=client):
        with servicer.intercept() as ctx:
            res = foo.bar.remote(2)
            function_bind_params: api_pb2.FunctionBindParamsRequest
            (function_bind_params,) = ctx.get_requests("FunctionBindParams")
            assert function_bind_params.function_options.retry_policy.retries == 5
            assert function_bind_params.function_options.resources.milli_cpu == 48000
            assert function_bind_params.function_options.scheduler_placement.regions == ["us-east-1"]
            assert function_bind_params.function_options.cloud_provider_str == "aws"
            assert function_bind_params.function_options.replace_cloud_bucket_mounts
            assert function_bind_params.function_options.replace_secret_ids
            cloud_bucket_mounts = function_bind_params.function_options.cloud_bucket_mounts
            assert len(cloud_bucket_mounts) == 1
            assert cloud_bucket_mounts[0].mount_path == "/cloud_mnt"
            assert cloud_bucket_mounts[0].bucket_name == "s3-bucket-name"

            assert len(ctx.get_requests("VolumeGetOrCreate")) == 1
            assert len(ctx.get_requests("SecretGetOrCreate")) == 2

        with servicer.intercept() as ctx:
            res = foo.bar.remote(2)
            assert len(ctx.get_requests("FunctionBindParams")) == 0  # no need to rebind
            assert len(ctx.get_requests("VolumeGetOrCreate")) == 0  # no need to rehydrate
            assert len(ctx.get_requests("SecretGetOrCreate")) == 0  # no need to rehydrate

        assert res == 4
        assert len(servicer.function_options) == 1
        options: api_pb2.FunctionOptions = list(servicer.function_options.values())[0]
        assert options.resources.milli_cpu == 48_000
        assert options.retry_policy.retries == 5

        with pytest.warns(DeprecationError, match="max_containers"):
            Foo.with_options(concurrency_limit=10)()  # type: ignore


def test_class_multiple_dynamic_parameterization_methods(client, servicer):
    foo = (
        Foo.with_options(max_containers=1)  # type: ignore
        .with_batching(max_batch_size=10, wait_ms=10)  # type: ignore
        .with_concurrency(max_inputs=100)()  # type: ignore
    )

    with app.run(client=client):
        with servicer.intercept() as ctx:
            res = foo.bar.remote(2)
            function_bind_params: api_pb2.FunctionBindParamsRequest
            (function_bind_params,) = ctx.get_requests("FunctionBindParams")
            assert function_bind_params.function_options.concurrency_limit == 1
            assert function_bind_params.function_options.batch_max_size == 10
            assert function_bind_params.function_options.batch_linger_ms == 10
            assert function_bind_params.function_options.max_concurrent_inputs == 100
    assert res == 4


@pytest.mark.parametrize("read_only", [True, False])
def test_class_multiple_with_options_calls(client, servicer, read_only):
    weights_volume = Volume.from_name("weights", create_if_missing=True)

    if read_only:
        weights_volume = weights_volume.read_only()
    foo = (
        Foo.with_options(  # type: ignore
            gpu="A10:4",
            memory=1024,
            cpu=8,
            buffer_containers=2,
            max_containers=5,
            volumes={"/data": Volume.from_name("data", create_if_missing=True)},
        ).with_options(  # type: ignore
            gpu="A100",
            memory=2048,
            max_containers=10,
            volumes={"/weights": weights_volume},
        )()  # type: ignore
    )

    with app.run(client=client):
        with servicer.intercept() as ctx:
            _ = foo.bar.remote(2)
            function_bind_params: api_pb2.FunctionBindParamsRequest
            (function_bind_params,) = ctx.get_requests("FunctionBindParams")
            assert function_bind_params.function_options.resources.milli_cpu == 8000
            assert function_bind_params.function_options.resources.memory_mb == 2048
            assert function_bind_params.function_options.resources.gpu_config.gpu_type == "A100"
            assert function_bind_params.function_options.resources.gpu_config.count == 1
            assert function_bind_params.function_options.buffer_containers == 2
            assert function_bind_params.function_options.concurrency_limit == 10
            assert len(function_bind_params.function_options.volume_mounts) == 1
            assert function_bind_params.function_options.volume_mounts[0].mount_path == "/weights"
            assert function_bind_params.function_options.volume_mounts[0].read_only == read_only


def test_with_options_from_name(servicer):
    unhydrated_volume = modal.Volume.from_name("some_volume", create_if_missing=True)
    unhydrated_secret = modal.Secret.from_dict({"foo": "bar"})

    with servicer.intercept() as ctx:
        SomeClass = modal.Cls.from_name("some_app", "SomeClass")
        OptionedClass = SomeClass.with_options(cpu=10, secrets=[unhydrated_secret], volumes={"/vol": unhydrated_volume})
        inst = OptionedClass(x=10)
        assert len(ctx.calls) == 0

    with servicer.intercept() as ctx:
        ctx.add_response("VolumeGetOrCreate", api_pb2.VolumeGetOrCreateResponse(volume_id="vo-123"))
        ctx.add_response("SecretGetOrCreate", api_pb2.SecretGetOrCreateResponse(secret_id="st-123"))
        ctx.add_response("ClassGet", api_pb2.ClassGetResponse(class_id="cs-123"))
        ctx.add_response(
            "FunctionGet",
            api_pb2.FunctionGetResponse(
                function_id="fu-123",
                handle_metadata=api_pb2.FunctionHandleMetadata(
                    method_handle_metadata={
                        "some_method": api_pb2.FunctionHandleMetadata(
                            use_function_id="fu-123",
                            use_method_name="some_method",
                            function_name="SomeClass.some_method",
                        )
                    }
                ),
            ),
        )
        ctx.add_response("FunctionBindParams", api_pb2.FunctionBindParamsResponse(bound_function_id="fu-124"))
        inst.some_method.remote()

    function_bind_params: api_pb2.FunctionBindParamsRequest
    (function_bind_params,) = ctx.get_requests("FunctionBindParams")
    assert len(function_bind_params.function_options.volume_mounts) == 1
    function_map: api_pb2.FunctionMapRequest
    (function_map,) = ctx.get_requests("FunctionMap")
    assert function_map.function_id == "fu-124"  # the bound function


def test_with_options_prehydrated_payload(client, servicer):
    with servicer.intercept() as ctx, app.run(client=client):
        secret = modal.Secret.from_dict({"foo": "bar"}).hydrate(client=client)
        secret_id = secret.object_id
        foo = Foo.with_options(secrets=[secret])()  # type: ignore  # cls type shadowing :(
        foo.bar.remote(2)

    function_bind_params: api_pb2.FunctionBindParamsRequest
    (function_bind_params,) = ctx.get_requests("FunctionBindParams")
    assert function_bind_params.function_options.secret_ids == [secret_id]


def test_service_options_defaults_untruthiness():
    # For `.with_options()` stacking (method-chaining) to work, the default values of the
    # internal _ServiceOptions dataclass should be be untruthy. This test just asserts that.
    # In the future we may change the implementation to use an "Unset" sentinel default, in
    # which case we wouldn't need this assertion.
    default_options = _ServiceOptions()
    for value in dataclasses.asdict(default_options).values():  # type: ignore  # synchronicity type stubs
        assert not value


# Reusing the app runs into an issue with stale function handles.
# TODO (akshat): have all the client tests use separate apps, and throw
# an exception if the user tries to reuse an app.
app_remote = App()


@app_remote.cls(cpu=42)
class FooRemote:
    x: int = modal.parameter()
    y: str = modal.parameter()

    @method()
    def bar(self, z: int):
        return z**3


def test_call_cls_remote_sync(client):
    with app_remote.run(client=client):
        foo_remote: FooRemote = FooRemote(x=3, y="hello")
        ret: float = foo_remote.bar.remote(8)
        assert ret == 64  # Mock servicer just squares the argument


def test_call_cls_remote_invalid_type(client):
    with app_remote.run(client=client):

        def my_function():
            print("Hello, world!")

        with pytest.raises(ValueError) as excinfo:
            FooRemote(x=42, y=my_function)  # type: ignore

        exc = excinfo.value
        assert "function" in str(exc)


app_2 = App()


@app_2.cls(cpu=42)
class Bar:
    @method()
    def baz(self, x):
        return x**3


@pytest.mark.asyncio
async def test_call_class_async(client, servicer):
    async with app_2.run(client=client):
        bar = Bar()
        assert await bar.baz.remote.aio(42) == 1764


def test_run_class_serialized(client, servicer):
    app_ser = App()

    @app_ser.cls(cpu=42, serialized=True)
    class FooSer:
        @method()
        def bar(self, x):
            return x**3

    assert servicer.n_functions == 0
    with app_ser.run(client=client):
        pass

    assert servicer.n_functions == 1
    class_function = servicer.function_by_name("FooSer.*")
    assert class_function.definition_type == api_pb2.Function.DEFINITION_TYPE_SERIALIZED
    user_cls = deserialize(class_function.class_serialized, client)

    # Create bound method
    obj = user_cls()
    bound_bar = user_cls.bar.__get__(obj)
    # Make sure it's callable
    assert bound_bar(100) == 1000000


app_remote_2 = App()


@app_remote_2.cls(cpu=42)
class BarRemote:
    x: int = modal.parameter()
    y: str = modal.parameter()

    @method()
    def baz(self, z: int):
        return z**3


@pytest.mark.asyncio
async def test_call_cls_remote_async(client):
    async with app_remote_2.run(client=client):
        bar_remote = BarRemote(x=3, y="hello")
        assert await bar_remote.baz.remote.aio(8) == 64  # Mock servicer just squares the argument


app_local = App()


@app_local.cls(cpu=42, enable_memory_snapshot=True)
class FooLocal:
    @property
    def side_effects(self):
        return self.__dict__.setdefault("_side_effects", ["__init__"])

    @enter(snap=True)
    def presnap(self):
        self.side_effects.append("presnap")

    @enter()
    def postsnap(self):
        self.side_effects.append("postsnap")

    @method()
    def bar(self, x):
        return x**3

    @method()
    def baz(self, y):
        return self.bar.local(y + 1)


def test_can_call_locally(client):
    foo = FooLocal()
    assert foo.bar.local(4) == 64
    assert foo.baz.local(4) == 125
    with app_local.run(client=client):
        assert foo.baz.local(2) == 27
        assert foo.side_effects == ["__init__", "presnap", "postsnap"]


def test_can_call_remotely_from_local(client):
    with app_local.run(client=client):
        foo = FooLocal()
        # remote calls use the mockservicer func impl
        # which just squares the arguments
        assert foo.bar.remote(8) == 64
        assert foo.baz.remote(9) == 81


app_remote_3 = App()


@app_remote_3.cls(cpu=42)
class NoArgRemote:
    @method()
    def baz(self, z: int) -> float:
        return z**3.0


def test_call_cls_remote_no_args(client):
    with app_remote_3.run(client=client):
        foo_remote = NoArgRemote()
        assert foo_remote.baz.remote(8) == 64  # Mock servicer just squares the argument


if TYPE_CHECKING:
    # Check that type annotations carry through to the decorated classes
    assert_type(NoParamsCls(), NoParamsCls)
    # can't use assert_type with named arguments, as it will diff in the name
    # vs the anonymous argument in the assertion type
    # assert_type(Foo().bar, Function[[int], float])


def test_from_name_lazy_method_hydration(client, servicer):
    deploy_app(app, "my-cls-app", client=client)
    cls: Cls = Cls.from_name("my-cls-app", "Foo")

    # Make sure we can instantiate the class
    obj = cls("foo", 234)

    # Check that function properties are preserved
    with servicer.intercept() as ctx:
        assert obj.bar.is_generator is False
        assert len(ctx.get_requests("FunctionBindParams")) == 1  # to determine this attribute, hydration is needed

    # Make sure we can methods
    # (mock servicer just returns the sum of the squares of the args)
    with servicer.intercept() as ctx:
        assert obj.bar.remote(42) == 1764
        assert len(ctx.get_requests("FunctionBindParams")) == 0

    with servicer.intercept() as ctx:
        assert obj.baz.remote(42) == 1764
        assert len(ctx.get_requests("FunctionBindParams")) == 0  # other method shouldn't rebind

    with pytest.raises(NotFoundError, match="can't be accessed for remote classes"):
        assert obj.a == 234

    # Make sure local calls fail
    with pytest.raises(ExecutionError, match="locally"):
        assert obj.bar.local(1, 2)

    # Make sure that non-existing methods fail
    with pytest.raises(NotFoundError):
        obj.non_exist.remote("hello")


def test_lookup_lazy_remote(client, servicer):
    # See #972 (PR) and #985 (revert PR): adding unit test to catch regression
    deploy_app(app, "my-cls-app", client=client)
    cls: Cls = Cls.from_name("my-cls-app", "Foo").hydrate(client=client)
    obj = cls("foo", 234)
    assert obj.bar.remote(42, 77) == 7693


def test_lookup_lazy_spawn(client, servicer):
    # See #1071
    deploy_app(app, "my-cls-app", client=client)
    cls: Cls = Cls.from_name("my-cls-app", "Foo").hydrate(client=client)
    obj = cls("foo", 234)
    function_call = obj.bar.spawn(42, 77)
    assert function_call.get() == 7693


def test_failed_lookup_error(client, servicer):
    with pytest.raises(NotFoundError, match="Lookup failed for Cls 'Foo' from the 'my-cls-app' app"):
        Cls.from_name("my-cls-app", "Foo").hydrate(client=client)

    with pytest.raises(NotFoundError, match="in the 'some-env' environment"):
        Cls.from_name("my-cls-app", "Foo", environment_name="some-env").hydrate(client=client)


baz_app = App()


@baz_app.cls()
class Baz:
    x: int = modal.parameter()

    def not_modal_method(self, y: int) -> int:
        return self.x * y


def test_call_not_modal_method():
    baz: Baz = Baz(x=5)
    assert baz.x == 5
    assert baz.not_modal_method(7) == 35


cls_with_enter_app = App()


def get_thread_id():
    return threading.current_thread().name


@cls_with_enter_app.cls()
class ClsWithEnter:
    local_thread_id: str = modal.parameter()
    entered: bool = modal.parameter(default=False)

    @enter()
    def enter(self):
        self.entered = True
        assert get_thread_id() == self.local_thread_id

    def not_modal_method(self, y: int) -> int:
        return y**2

    @method()
    def modal_method(self, y: int) -> int:
        return y**2


def test_dont_enter_on_local_access():
    obj = ClsWithEnter(local_thread_id=get_thread_id())
    with pytest.raises(AttributeError):
        obj.doesnt_exist  # type: ignore
    assert obj.local_thread_id == get_thread_id()
    assert not obj.entered


def test_dont_enter_on_local_non_modal_call():
    obj = ClsWithEnter(local_thread_id=get_thread_id())
    assert obj.not_modal_method(7) == 49
    assert obj.local_thread_id == get_thread_id()
    assert not obj.entered


def test_enter_on_local_modal_call():
    obj = ClsWithEnter(local_thread_id=get_thread_id())
    assert obj.modal_method.local(7) == 49
    assert obj.local_thread_id == get_thread_id()
    assert obj.entered


@cls_with_enter_app.cls()
class ClsWithAsyncEnter:
    inited: bool = modal.parameter(default=False)
    entered = False  # non parameter

    @enter()
    async def enter(self):
        self.entered = True

    @method()
    async def modal_method(self, y: int) -> int:
        return y**2


@pytest.mark.skip("this doesn't actually work - but issue was hidden by `entered` being an obj property")
@pytest.mark.asyncio
async def test_async_enter_on_local_modal_call():
    obj = ClsWithAsyncEnter(inited=True)
    assert await obj.modal_method.local(7) == 49
    assert obj.inited
    assert obj.entered


inheritance_app = App()


class BaseCls:
    @enter()
    def enter(self):
        self.x = 2

    @method()
    def run(self, y):
        return self.x * y


@inheritance_app.cls()
class DerivedCls(BaseCls):
    pass


def test_derived_cls(client, servicer):
    with inheritance_app.run(client=client):
        # default servicer fn just squares the number
        assert DerivedCls().run.remote(3) == 9


inheritance_app_2 = App()


@inheritance_app_2.cls()
class DerivedCls2(BaseCls2):
    pass


def test_derived_cls_external_file(client, servicer):
    with inheritance_app_2.run(client=client):
        # default servicer fn just squares the number
        assert DerivedCls2().run.remote(3) == 9


def test_rehydrate(client, servicer, reset_container_app):
    # Issue introduced in #922 - brief description in #931

    # Sanity check that local calls work
    obj = NoParamsCls()
    assert obj.bar.local(7) == 343

    # Deploy app to get an app id
    app_id = deploy_app(app, "my-cls-app", client=client).app_id

    # Initialize a container
    container_app = RunningApp(app_id)

    # Associate app with app
    app._init_container(client, container_app)

    # Hydration shouldn't overwrite local function definition
    obj = NoParamsCls()
    assert obj.bar.local(7) == 343


app_unhydrated = App()


@app_unhydrated.cls()
class FooUnhydrated:
    @method()
    def bar(self, x): ...


def test_unhydrated():
    foo = FooUnhydrated()
    with pytest.raises(ExecutionError, match="hydrated"):
        foo.bar.remote(42)


app_method_args = App()


@app_method_args.cls(min_containers=5)
class XYZ:
    @method()
    def foo(self): ...

    @method()
    def bar(self): ...


def test_method_args(servicer, client):
    with app_method_args.run(client=client):
        funcs = servicer.app_functions.values()
        assert {f.function_name for f in funcs} == {"XYZ.*"}
        warm_pools = {f.function_name: f.autoscaler_settings.min_containers for f in funcs}
        assert warm_pools == {"XYZ.*": 5}


def test_cls_update_autoscaler(client, servicer):
    app = App()

    @app.cls(serialized=True)
    class ClsWithMethod:
        arg: str = modal.parameter(default="")

        @method()
        def bar(self): ...

    with app.run(client=client):
        assert len(servicer.app_functions) == 1  # only class service function
        cls_service_fun = servicer.function_by_name("ClsWithMethod.*")
        assert cls_service_fun.is_class
        assert cls_service_fun.warm_pool_size == 0

        empty_args_obj = typing.cast(modal.cls.Obj, ClsWithMethod())
        empty_args_obj.update_autoscaler(min_containers=2, buffer_containers=1)
        service_function_id = empty_args_obj._cached_service_function().object_id
        service_function_defn = servicer.app_functions[service_function_id]
        autoscaler_settings = service_function_defn.autoscaler_settings
        assert service_function_defn.warm_pool_size == autoscaler_settings.min_containers == 2
        assert service_function_defn._experimental_buffer_containers == autoscaler_settings.buffer_containers == 1

        param_obj = ClsWithMethod(arg="other-instance")
        param_obj.update_autoscaler(min_containers=5, max_containers=10)  # type: ignore
        assert len(servicer.app_functions) == 3  # base + 2 x instance service function
        assert cls_service_fun.warm_pool_size == 0  # base still has no warm

        instance_service_function_id = param_obj._cached_service_function().object_id  # type: ignore
        instance_service_defn = servicer.app_functions[instance_service_function_id]
        instance_autoscaler_settings = instance_service_defn.autoscaler_settings
        assert instance_service_defn.warm_pool_size == instance_autoscaler_settings.min_containers == 5
        assert instance_service_defn.concurrency_limit == instance_autoscaler_settings.max_containers == 10


def test_cls_lookup_update_autoscaler(client, servicer):
    app = App(name := "my-cls-app")

    @app.cls(serialized=True)
    class ClsWithMethod:
        arg: str = modal.parameter(default="")

        @method()
        def bar(self): ...

    C_pre_deploy = ClsWithMethod()
    with pytest.raises(ExecutionError, match="has not been hydrated"):
        C_pre_deploy.update_autoscaler(min_containers=1)  # type: ignore

    deploy_app(app, name, client=client)

    C = Cls.from_name(name, "ClsWithMethod")
    obj = C()
    obj.update_autoscaler(min_containers=3)

    service_function_id = obj._cached_service_function().object_id
    assert servicer.app_functions[service_function_id].warm_pool_size == 3

    with servicer.intercept() as ctx:
        obj.update_autoscaler(min_containers=4)
        assert len(ctx.get_requests("FunctionBindParams")) == 0  # We did not re-bind


class ClsWithHandlers:
    @enter(snap=True)
    def my_memory_snapshot(self):
        pass

    @enter()
    def my_enter(self):
        pass

    @exit()
    def my_exit(self):
        pass


def test_handlers():
    pfs: dict[str, _PartialFunction]

    pfs = _find_partial_methods_for_user_cls(ClsWithHandlers, _PartialFunctionFlags.ENTER_PRE_SNAPSHOT)
    assert list(pfs.keys()) == ["my_memory_snapshot"]

    pfs = _find_partial_methods_for_user_cls(ClsWithHandlers, _PartialFunctionFlags.ENTER_POST_SNAPSHOT)
    assert list(pfs.keys()) == ["my_enter"]

    pfs = _find_partial_methods_for_user_cls(ClsWithHandlers, _PartialFunctionFlags.EXIT)
    assert list(pfs.keys()) == ["my_exit"]


web_app_app = App()


@web_app_app.cls()
class WebCls:
    @fastapi_endpoint()
    def endpoint(self):
        pass

    @asgi_app()
    def asgi(self):
        pass


def test_web_cls(client):
    with web_app_app.run(client=client):
        c = WebCls()
        assert c.endpoint.get_web_url() == "http://endpoint.internal"
        assert c.asgi.get_web_url() == "http://asgi.internal"


handler_app = App("handler-app")


image = Image.debian_slim().pip_install("xyz")


other_handler_app = App("other-handler-app")


@pytest.mark.parametrize("decorator", [enter, exit])
def test_disallow_lifecycle_decorators_with_method(decorator):
    with pytest.raises(InvalidError, match="cannot be combined with lifecycle decorators"):

        class HasLifecycleOnMethod:
            @decorator()
            @method()
            def f(self):
                pass

    with pytest.raises(InvalidError, match="cannot be combined with lifecycle decorators"):

        class HasMethodOnLifecycle:
            @method()
            @decorator()
            def f(self):
                pass


class HasSnapMethod:
    @enter(snap=True)
    def enter(self):
        pass

    @method()
    def f(self):
        pass


def test_snap_method_without_snapshot_enabled():
    with pytest.raises(InvalidError, match="A class must have `enable_memory_snapshot=True`"):
        app.cls(enable_memory_snapshot=False)(HasSnapMethod)


def test_partial_function_descriptors(client):
    class Foo:
        @modal.enter()
        def enter_method(self):
            pass

        @modal.method()
        def bar(self):
            return "a"

        @modal.fastapi_endpoint()
        def web(self):
            pass

    assert isinstance(Foo.bar, PartialFunction)

    assert Foo().bar() == "a"  # type: ignore   # edge case - using a non-decorated class should just return the bound original method
    assert inspect.ismethod(Foo().bar)
    app = modal.App()

    modal_foo_class = app.cls(serialized=True)(Foo)

    wrapped_method = modal_foo_class().bar
    assert isinstance(wrapped_method, Function)

    serialized_class = serialize(Foo)
    revived_class = deserialize(serialized_class, client)

    assert (
        revived_class().bar() == "a"
    )  # this instantiates the underlying "user_cls", so it should work basically like a normal Python class
    assert isinstance(
        revived_class.bar, PartialFunction
    )  # but it should be a PartialFunction, so it keeps associated metadata!

    # ensure that webhook metadata is kept
    web_partial_function: _PartialFunction = synchronizer._translate_in(revived_class.web)  # type: ignore
    assert web_partial_function.params.webhook_config
    assert web_partial_function.params.webhook_config.type == api_pb2.WEBHOOK_TYPE_FUNCTION


def test_cross_process_userclass_serde(supports_dir):
    res = subprocess.check_output([sys.executable, supports_dir / "serialize_class.py"])
    assert len(res) < 2000  # should be ~1300 bytes as of 2024-06-05
    revived_cls = deserialize(res, None)
    method_without_descriptor_protocol = revived_cls.__dict__["method"]
    assert isinstance(method_without_descriptor_protocol, modal.partial_function.PartialFunction)
    assert revived_cls().method() == "a"  # this should be bound to the object


app2 = App("app2")


@app2.cls()
class UsingAnnotationParameters:
    a: int = modal.parameter()
    b: str = modal.parameter(default="hello")
    c: float = modal.parameter(init=False)
    d: bytes = modal.parameter(default=b"world")

    @method()
    def get_value(self):
        return self.a


init_side_effects = []


def test_implicit_constructor(client, set_env_client):
    c = UsingAnnotationParameters(a=10)

    assert c.a == 10
    assert c.get_value.local() == 10
    assert c.b == "hello"
    assert c.d == b"world"

    d = UsingAnnotationParameters(a=11, b="goodbye", d=b"bye")
    assert d.b == "goodbye"
    assert d.d == b"bye"

    with pytest.raises(InvalidError, match="Missing required parameter: a"):
        with app2.run(client=client):
            UsingAnnotationParameters().get_value.remote()  # type: ignore

    # check that implicit constructors trigger strict parametrization
    function_info: FunctionInfo = synchronizer._translate_in(UsingAnnotationParameters)._class_service_function._info  # type: ignore
    assert function_info.class_parameter_info().format == api_pb2.ClassParameterInfo.PARAM_SERIALIZATION_FORMAT_PROTO


def test_custom_constructor_has_deprecation_warning():
    with pytest.warns(DeprecationError, match="non-default constructor"):

        @app2.cls(serialized=True)
        class UsingCustomConstructor:
            # might want to deprecate this soon
            a: int

            def __init__(self, a: int):
                self._a = a
                init_side_effects.append("did_run")

            @method()
            def get_value(self):
                return self._a

    d = UsingCustomConstructor(10)
    assert not init_side_effects

    assert d._a == 10  # lazily run constructor when accessing non-method attributes (!)
    assert init_side_effects == ["did_run"]

    d2 = UsingCustomConstructor(11)
    assert d2.get_value.local() == 11  # run constructor before running locally
    # check that explicit constructors trigger pickle parametrization
    function_info: FunctionInfo = synchronizer._translate_in(UsingCustomConstructor)._class_service_function._info  # type: ignore
    assert function_info.class_parameter_info().format == api_pb2.ClassParameterInfo.PARAM_SERIALIZATION_FORMAT_PICKLE


class ParametrizedClass1:
    def __init__(self, a):
        pass


class ParametrizedClass1Implicit:
    a: int = modal.parameter()


class ParametrizedClass2:
    def __init__(self, a: int = 1):
        pass


class ParametrizedClass2Implicit:
    a: int = modal.parameter(default=1)


class ParametrizedClass3:
    def __init__(self):
        pass


app_batched = App()


def test_batched_method_duplicate_error(client):
    with pytest.raises(
        InvalidError, match="Modal class BatchedClass_1 with a modal batched function cannot have other modal methods."
    ):

        @app_batched.cls(serialized=True)
        class BatchedClass_1:
            @modal.method()
            def method(self):
                pass

            @modal.batched(max_batch_size=2, wait_ms=0)
            def batched_method(self):
                pass

    with pytest.raises(InvalidError, match="Modal class BatchedClass_2 can only have one batched function."):

        @app_batched.cls(serialized=True)
        class BatchedClass_2:
            @modal.batched(max_batch_size=2, wait_ms=0)
            def batched_method_1(self):
                pass

            @modal.batched(max_batch_size=2, wait_ms=0)
            def batched_method_2(self):
                pass


def test_cls_with_both_constructor_and_parameters_is_invalid():
    with pytest.raises(InvalidError, match="constructor"):

        @app.cls(serialized=True)
        class A:
            a: int = modal.parameter()

            def __init__(self, a):
                self.a = a


def test_unannotated_parameters_are_invalid():
    with pytest.raises(InvalidError, match="annotated"):

        @app.cls(serialized=True)
        class B:
            b = modal.parameter()  # type: ignore


def test_unsupported_type_parameters_raise_errors():
    with pytest.raises(InvalidError, match=r"float is not a supported modal.parameter\(\) type"):

        @app.cls(serialized=True)
        class C:
            c: float = modal.parameter()


def test_unsupported_function_decorators_on_methods():
    with pytest.raises(InvalidError, match="cannot be used on class methods"):

        @app.cls(serialized=True)
        class M:
            @app.function(serialized=True)
            @modal.fastapi_endpoint()
            def f(self):
                pass

    with pytest.raises(InvalidError, match="cannot be used on class methods"):

        @app.cls(serialized=True)
        class D:
            @app.function(serialized=True)
            def f(self):
                pass


def test_using_method_on_uninstantiated_cls(recwarn):
    app = App()

    @app.cls(serialized=True)
    class C:
        @method()
        def method(self):
            pass

    assert len(recwarn) == 0
    with pytest.raises(AttributeError):
        C.blah  # type: ignore   # noqa
    assert len(recwarn) == 0

    assert isinstance(C().method, Function)  # should be fine to access on an instance of the class
    assert len(recwarn) == 0

    # The following should warn since it's accessed on the class directly
    C.method  # noqa  # triggers a deprecation warning
    # TODO: this will be an AttributeError or return a non-modal unbound function in the future:
    assert len(recwarn) == 1
    warning_string = str(recwarn[0].message)
    assert "instantiate the class first" in warning_string
    assert "C().method instead of C.method" in warning_string


def test_bytes_serialization_validation(servicer, client, set_env_client):
    app = modal.App()

    @app.cls(serialized=True)
    class C:
        foo: bytes = modal.parameter(default=b"foo")

        @method()
        def get_foo(self):
            return self.foo

    with servicer.intercept() as ctx:
        with app.run():
            with pytest.raises(TypeError, match="Expected bytes"):
                C(foo="this is a string").get_foo.spawn()  # type: ignore   # string should not be allowed, unspecified encoding

            C(foo=b"this is bytes").get_foo.spawn()  # bytes are allowed
            create_function_req: api_pb2.FunctionCreateRequest = ctx.pop_request("FunctionCreate")
            bind_req: api_pb2.FunctionBindParamsRequest = ctx.pop_request("FunctionBindParams")
            args, kwargs = deserialize_params(bind_req.serialized_params, create_function_req.function, client)
            assert kwargs["foo"] == b"this is bytes"

            C().get_foo.spawn()  # omission when using default is allowed
            bind_req: api_pb2.FunctionBindParamsRequest = ctx.pop_request("FunctionBindParams")
            args, kwargs = deserialize_params(bind_req.serialized_params, create_function_req.function, client)
            assert kwargs["foo"] == b"foo"


def test_class_can_not_use_list_parameter(client):
    # we might want to allow lists in the future though...
    app = modal.App()

    with pytest.raises(InvalidError, match="list is not a supported modal.parameter"):

        @app.cls(serialized=True)
        class A:
            p: list[int] = modal.parameter()


def test_class_can_use_073_schema_definition(servicer, set_env_client):
    # in ~0.74, we introduced the new full_type type generic that supersedes
    # the .type "flat" type. This tests that lookups on classes deployed with
    # the old proto can still be validated when instantiated.

    with servicer.intercept() as ctx:
        ctx.add_response("ClassGet", api_pb2.ClassGetResponse(class_id="cs-123"))
        ctx.add_response(
            "FunctionGet",
            api_pb2.FunctionGetResponse(
                function_id="fu-123",
                handle_metadata=api_pb2.FunctionHandleMetadata(
                    class_parameter_info=api_pb2.ClassParameterInfo(
                        format=api_pb2.ClassParameterInfo.PARAM_SERIALIZATION_FORMAT_PROTO,
                        schema=[api_pb2.ClassParameterSpec(name="p", type=api_pb2.PARAM_TYPE_STRING)],
                    ),
                    method_handle_metadata={"some_method": api_pb2.FunctionHandleMetadata()},
                ),
            ),
        )
        with pytest.raises(TypeError, match="Expected str, got int"):
            # wrong type for p triggers when .remote goes off
            obj = Cls.from_name("some_app", "SomeCls")(p=10)
            obj.some_method.remote(1)


def test_class_can_use_future_full_type_only_schema(servicer, set_env_client):
    # in ~0.74, we introduced the new full_type type generic that supersedes
    # the .type "flat" type. This tests that the client can use a *future
    # version* that drops support for the .type attribute and only fills the
    # full_type in the schema

    with servicer.intercept() as ctx:
        ctx.add_response("ClassGet", api_pb2.ClassGetResponse(class_id="cs-123"))
        ctx.add_response(
            "FunctionGet",
            api_pb2.FunctionGetResponse(
                function_id="fu-123",
                handle_metadata=api_pb2.FunctionHandleMetadata(
                    class_parameter_info=api_pb2.ClassParameterInfo(
                        format=api_pb2.ClassParameterInfo.PARAM_SERIALIZATION_FORMAT_PROTO,
                        schema=[
                            api_pb2.ClassParameterSpec(
                                name="p", full_type=api_pb2.GenericPayloadType(base_type=api_pb2.PARAM_TYPE_STRING)
                            )
                        ],
                    ),
                    method_handle_metadata={"some_method": api_pb2.FunctionHandleMetadata()},
                ),
            ),
        )
        with pytest.raises(TypeError, match="Expected str, got int"):
            # wrong type for p triggers when .remote goes off
            obj = Cls.from_name("some_app", "SomeCls")(p=10)
            obj.some_method.remote(1)


def test_concurrent_decorator_on_method_error():
    app = modal.App()

    with pytest.raises(modal.exception.InvalidError, match="decorate the class"):

        @app.cls(serialized=True)
        class UsesConcurrentDecoratoronMethod:
            @modal.concurrent(max_inputs=10)
            def method(self):
                pass


def test_concurrent_decorator_stacked_with_method_decorator():
    app = modal.App()

    with pytest.raises(modal.exception.InvalidError, match="decorate the class"):

        @app.cls(serialized=True)
        class UsesMethodAndConcurrentDecorators:
            @modal.method()
            @modal.concurrent(max_inputs=10)
            def method(self):
                pass


def test_parameter_inheritance(client):
    app = modal.App("inherit-params")

    class Base:
        a: int = modal.parameter()  # parameter in base class

    @app.cls(serialized=True)
    class ConcatenatingParams(Base):
        b: str = modal.parameter()  # add additional parameter

    @app.cls(serialized=True)
    class RepeatingParams(Base):
        # In versions prior to ~1.0.5, base class parameters were not
        # included, so subclasses would always have to repeat parameters
        # from the base class.
        # We allow this as long as the definitions are the same
        a: int = modal.parameter()  # redefine base class parameter
        b: str = modal.parameter()

    @app.cls(serialized=True)
    class ChangingParameterDefinitions(Base):
        # change type of base class parameter, allowed but frowned upon
        a: str = modal.parameter()  # type: ignore  # this isn't allowed by type checkers

    with app.run(client=client):
        # use .update_autoscaler()
        ConcatenatingParams(a=10, b="hello").update_autoscaler()  # type: ignore
        RepeatingParams(a=10, b="hello").update_autoscaler()  # type: ignore
        with pytest.raises(TypeError):
            ChangingParameterDefinitions(a=10).update_autoscaler()  # type: ignore
        ChangingParameterDefinitions(a="10").update_autoscaler()  # type: ignore


def test_cls_namespace_deprecated(servicer, client):
    # Test from_name with namespace parameter warns
    with pytest.warns(DeprecationError, match="The `namespace` parameter for `modal.Cls.from_name` is deprecated"):
        Cls.from_name("test-app", "test-cls", namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE)

    # Test that from_name without namespace parameter doesn't warn about namespace
    import warnings

    with warnings.catch_warnings(record=True) as record:
        warnings.simplefilter("always")
        Cls.from_name("test-app", "test-cls")
    # Filter out any unrelated warnings
    namespace_warnings = [w for w in record if "namespace" in str(w.message).lower()]
    assert len(namespace_warnings) == 0


clustered_app = App()


def test_clustered_cls(client, servicer):
    @clustered_app.cls(serialized=True)
    @modal.experimental.clustered(size=3, rdma=True)  # type: ignore
    class ClusteredClass:
        @method()
        def run_task(self, x):
            return x * 2

    @clustered_app.cls(serialized=True)
    class RegularClass:
        @method()
        def regular_method(self, x):
            return x * 4

    with clustered_app.run(client=client):
        assert len(servicer.app_functions) == 2

        class_function = servicer.function_by_name("ClusteredClass.*")
        assert class_function._experimental_group_size == 3
        assert class_function.i6pn_enabled is True  # clustered implies i6pn
        assert class_function.resources.rdma == 1

        obj = ClusteredClass()
        assert hasattr(obj, "run_task")

        regular_function = servicer.function_by_name("RegularClass.*")
        assert regular_function._experimental_group_size == 0  # or not set
        assert regular_function.i6pn_enabled is False
        assert regular_function.resources.rdma == 0


invalid_clustered_app = App()


def test_clustered_cls_with_multiple_methods(client, servicer):
    with pytest.raises(
        InvalidError, match="Modal class ClusteredClassMixed cannot have multiple methods when clustered."
    ):

        @invalid_clustered_app.cls(serialized=True)
        @modal.experimental.clustered(size=2)  # type: ignore
        class ClusteredClassMixed:
            @method()
            def clustered_method(self, x):
                return x * 3

            @method()
            def second_clustered_method(self, x):
                return x * 3


def test_cls_get_flash_url(servicer):
    """Test get_flash_url method on Cls.from_name instances"""
    cls = Cls.from_name("dummy-app", "MyClass")

    with servicer.intercept() as ctx:
        ctx.add_response(
            "ClassGet",
            api_pb2.ClassGetResponse(class_id="cs-1"),
        )
        ctx.add_response(
            "FunctionGet",
            api_pb2.FunctionGetResponse(
                function_id="fu-1",
                handle_metadata=api_pb2.FunctionHandleMetadata(
                    function_name="MyClass.*",
                    is_method=False,
                    _experimental_flash_urls=[
                        "https://flash.example.com/service1",
                        "https://flash.example.com/service2",
                    ],
                ),
            ),
        )
        flash_urls = cls._experimental_get_flash_urls()
        assert flash_urls == ["https://flash.example.com/service1", "https://flash.example.com/service2"]


timeout_app = App("timeout-app")


@timeout_app.cls(startup_timeout=30)
class Timeout:
    @enter()
    def start(self):
        pass

    @method()
    def hello(self):
        pass


def test_startup_timeout(client, servicer):
    with servicer.intercept() as ctx:
        with timeout_app.run(client=client):
            pass

    function_creates_requests: list[api_pb2.FunctionCreateRequest] = ctx.get_requests("FunctionCreate")
    assert len(function_creates_requests) == 1
    function_request = function_creates_requests[0]
    assert function_request.function.startup_timeout_secs == 30


timeout_app_default = App("timeout-app-default")


@timeout_app_default.cls(timeout=20)
class TimeoutDefault:
    @enter()
    def start(self):
        pass

    @method()
    def hello(self):
        pass


def test_startup_timeout_default_copies_timeout(client, servicer):
    with servicer.intercept() as ctx:
        with timeout_app_default.run(client=client):
            pass

    function_creates_requests: list[api_pb2.FunctionCreateRequest] = ctx.get_requests("FunctionCreate")
    assert len(function_creates_requests) == 1
    function_request = function_creates_requests[0]
    assert function_request.function.startup_timeout_secs == 20



================================================
FILE: test/config_test.py
================================================
# Copyright Modal Labs 2022
import os
import pathlib
import pytest
import subprocess
import sys

import toml

import modal
from modal._utils.async_utils import synchronize_api
from modal.config import Config, _lookup_workspace, config
from modal.exception import InvalidError


class CLIException(Exception):
    pass


def _cli(args, env={}):
    lib_dir = pathlib.Path(modal.__file__).parent.parent
    args = [sys.executable, "-m", "modal.cli.entry_point"] + args
    env = {
        **os.environ,
        **env,
        # For windows
        "PYTHONUTF8": "1",
    }
    ret = subprocess.run(args, cwd=lib_dir, env=env, capture_output=True)
    stdout = ret.stdout.decode()
    stderr = ret.stderr.decode()
    if ret.returncode != 0:
        raise CLIException(f"Failed with {ret.returncode} stdout: {stdout} stderr: {stderr}")
    return stdout


def _get_config(env={}):
    stdout = _cli(["config", "show", "--no-redact"], env=env)
    return eval(stdout)


def test_config():
    config = _get_config()
    assert config["server_url"]


def test_config_env_override():
    config = _get_config(env={"MODAL_SERVER_URL": "xyz.corp"})
    assert config["server_url"] == "xyz.corp"


def test_config_store_user(servicer, modal_config):
    with modal_config(show_on_error=True) as config_file_path:
        env = {"MODAL_SERVER_URL": servicer.client_addr}

        servicer.required_creds = {"abc": "xyz", "foo": "bar1", "foo2": "bar2", "ABC": "XYZ", "foo3": "bar3"}

        # No token by default
        config = _get_config(env=env)
        assert config["token_id"] is None

        # Set creds to abc / xyz
        _cli(["token", "set", "--token-id", "abc", "--token-secret", "xyz"], env=env)

        # Make sure an incorrect token fails
        with pytest.raises(CLIException):
            _cli(["token", "set", "--token-id", "abc", "--token-secret", "incorrect"], env=env)

        # Set creds to foo / bar1 for the prof_1 profile
        _cli(
            ["token", "set", "--token-id", "foo", "--token-secret", "bar1", "--profile", "prof_1", "--no-activate"],
            env=env,
        )

        # Set creds to foo2 / bar2 for the prof_2 profile (given as an env var)
        _cli(
            ["token", "set", "--token-id", "foo2", "--token-secret", "bar2", "--no-activate"],
            env={"MODAL_PROFILE": "prof_2", **env},
        )

        # Now these should be stored in the user's home directory
        config = _get_config(env=env)
        assert config["token_id"] == "abc"
        assert config["token_secret"] == "xyz"

        # Make sure it can be overridden too
        config = _get_config(env={"MODAL_TOKEN_ID": "foo", **env})
        assert config["token_id"] == "foo"
        assert config["token_secret"] == "xyz"

        # Check that the profile is named after the workspace username by default
        config = _get_config(env={"MODAL_PROFILE": "test-username", **env})
        assert config["token_id"] == "abc"
        assert config["token_secret"] == "xyz"

        # Check that we can get the prof_1 env creds too
        config = _get_config(env={"MODAL_PROFILE": "prof_1", **env})
        assert config["token_id"] == "foo"
        assert config["token_secret"] == "bar1"

        # Check that we can get the prof_2 env creds too
        config = _get_config(env={"MODAL_PROFILE": "prof_2", **env})
        assert config["token_id"] == "foo2"
        assert config["token_secret"] == "bar2"

        # Check that an empty string falls back to the active profile
        config = _get_config(env={"MODAL_PROFILE": "", **env})
        assert config["token_secret"] == "xyz"

        # Test that only the first profile was explicitly activated
        for profile, profile_config in toml.load(config_file_path).items():
            if profile == "test-username":
                assert profile_config["active"] is True
            else:
                assert "active" not in profile_config

        # Check that we can overwrite the default profile
        _cli(["token", "set", "--token-id", "ABC", "--token-secret", "XYZ"], env=env)
        assert toml.load(config_file_path)["test-username"]["token_id"] == "ABC"

        # Check that we activate a profile by default while setting a token
        _cli(
            ["token", "set", "--token-id", "foo3", "--token-secret", "bar3", "--profile", "prof_3"],
            env=env,
        )
        for profile, profile_config in toml.load(config_file_path).items():
            if profile == "prof_3":
                assert profile_config["active"] is True
            else:
                assert "active" not in profile_config


def test_config_env_override_arbitrary_env():
    """config.override_locally() replaces existing env var if not part of config."""
    key = "NVIDIA_VISIBLE_DEVICES"
    value = "0,1"

    # Place old value in memory.
    os.environ[key] = "none"

    # Expect value to be overwritten.
    config.override_locally(key, value)
    assert os.getenv(key) == value


@pytest.mark.asyncio
async def test_workspace_lookup(servicer, server_url_env):
    servicer.required_creds = {"ak-abc": "as-xyz"}
    resp = await synchronize_api(_lookup_workspace).aio(servicer.client_addr, "ak-abc", "as-xyz")
    assert resp.username == "test-username"


@pytest.mark.parametrize("arg", ["false", "'false'", "'False'", "'0'", 0, "''"])
def test_config_boolean(modal_config, arg):
    modal_toml = f"""
    [prof-1]
    token_id = 'ak-abc'
    token_secret = 'as_xyz'
    force_build = {arg}
    """
    with modal_config(modal_toml):
        assert not Config().get("force_build", "prof-1")


def test_malformed_config_better(modal_config):
    modal_toml = """
    token_id = 'ak-abc'
    token_secret = 'as_xyz'
    """
    with pytest.raises(InvalidError, match="must contain table sections"):
        with modal_config(modal_toml):
            pass

    modal_toml = """
    [default]
    token_id
    """
    with pytest.raises(InvalidError, match="Key name found without value"):
        with modal_config(modal_toml):
            pass



================================================
FILE: test/container_app_test.py
================================================
# Copyright Modal Labs 2022
import json
import os
import pytest
import time
from contextlib import contextmanager
from unittest import mock

from google.protobuf.empty_pb2 import Empty
from google.protobuf.message import Message

from modal import App, interact
from modal._runtime.container_io_manager import ContainerIOManager
from modal._utils.async_utils import synchronize_api
from modal._utils.grpc_utils import retry_transient_errors
from modal.exception import InvalidError
from modal.running_app import RunningApp
from modal_proto import api_pb2


def my_f_1(x):
    pass


def temp_restore_path(tmpdir):
    # Write out a restore file so that snapshot+restore will complete
    restore_path = tmpdir.join("fake-restore-state.json")
    restore_path.write_text(
        json.dumps(
            {
                "task_id": "ta-i-am-restored",
                "task_secret": "ts-i-am-restored",
                "function_id": "fu-i-am-restored",
            }
        ),
        encoding="utf-8",
    )
    return restore_path


@pytest.mark.asyncio
async def test_container_function_lazily_imported(container_client):
    function_ids: dict[str, str] = {
        "my_f_1": "fu-123",
    }
    object_handle_metadata: dict[str, Message] = {
        "fu-123": api_pb2.FunctionHandleMetadata(),
    }
    container_app = RunningApp("ap-123", function_ids=function_ids, object_handle_metadata=object_handle_metadata)
    app = App()

    # This is normally done in _container_entrypoint
    app._init_container(container_client, container_app)

    # Now, let's create my_f after the app started running and make sure it works
    my_f_container = app.function()(my_f_1)
    assert await my_f_container.remote.aio(42) == 1764  # type: ignore


def square(x):
    pass


@synchronize_api
async def stop_app(client, app_id):
    # helper to ensur we run the rpc from the synchronicity loop - otherwise we can run into weird deadlocks
    return await retry_transient_errors(client.stub.AppStop, api_pb2.AppStopRequest(app_id=app_id))


@contextmanager
def set_env_vars(restore_path, container_addr):
    with mock.patch.dict(
        os.environ,
        {
            "MODAL_RESTORE_STATE_PATH": str(restore_path),
            "MODAL_SERVER_URL": container_addr,
            "MODAL_TASK_ID": "ta-123",
            "MODAL_IS_REMOTE": "1",
        },
    ):
        yield


@pytest.mark.asyncio
async def test_container_snapshot_reference_capture(container_client, tmpdir, servicer, client):
    app = App()
    from modal import Function
    from modal.runner import deploy_app

    app.function()(square)
    app_name = "my-app"
    app_id = deploy_app(app, app_name, client=container_client).app_id
    f = Function.from_name(app_name, "square").hydrate(container_client)
    assert f.object_id == "fu-1"
    await f.remote.aio()
    assert f.object_id == "fu-1"
    io_manager = ContainerIOManager(api_pb2.ContainerArguments(checkpoint_id="ch-123"), container_client)
    restore_path = temp_restore_path(tmpdir)
    with set_env_vars(restore_path, servicer.container_addr):
        io_manager.memory_snapshot()

    # Stop the App, invalidating the fu- ID stored in `f`.
    stop_app(client, app_id)
    # After snapshot-restore the previously looked-up Function should get refreshed and have the
    # new fu- ID. ie. the ID should not be stale and invalid.
    new_app_id = deploy_app(app, app_name, client=client).app_id
    assert new_app_id != app_id
    await f.remote.aio()
    assert f.object_id == "fu-2"
    # Purposefully break FunctionGet to check the hydration is cached.
    del servicer.app_objects[new_app_id]
    await f.remote.aio()  # remote call succeeds because it didn't re-hydrate Function
    assert f.object_id == "fu-2"


def test_container_snapshot_restore_heartbeats(tmpdir, servicer, container_client):
    io_manager = ContainerIOManager(api_pb2.ContainerArguments(checkpoint_id="ch-123"), container_client)
    restore_path = temp_restore_path(tmpdir)

    # Ensure that heartbeats only run after the snapshot
    heartbeat_interval_secs = 0.01
    with io_manager.heartbeats(True):
        with set_env_vars(restore_path, servicer.container_addr):
            with mock.patch("modal.runner.HEARTBEAT_INTERVAL", heartbeat_interval_secs):
                time.sleep(heartbeat_interval_secs * 2)
                assert not list(
                    filter(lambda req: isinstance(req, api_pb2.ContainerHeartbeatRequest), servicer.requests)
                )
                io_manager.memory_snapshot()
                time.sleep(heartbeat_interval_secs * 2)
                assert list(filter(lambda req: isinstance(req, api_pb2.ContainerHeartbeatRequest), servicer.requests))


@pytest.mark.asyncio
async def test_container_debug_snapshot(container_client, tmpdir, servicer):
    # Get an IO manager, where restore takes place
    io_manager = ContainerIOManager(api_pb2.ContainerArguments(checkpoint_id="ch-123"), container_client)
    restore_path = tmpdir.join("fake-restore-state.json")
    # Write the restore file to start a debugger
    restore_path.write_text(
        json.dumps({"snapshot_debug": "1"}),
        encoding="utf-8",
    )

    # Test that the breakpoint was called
    test_breakpoint = mock.Mock()
    with mock.patch("sys.breakpointhook", test_breakpoint):
        with set_env_vars(restore_path, servicer.container_addr):
            io_manager.memory_snapshot()
            test_breakpoint.assert_called_once()


@pytest.mark.asyncio
async def test_rpc_wrapping_restores(container_client, servicer, tmpdir):
    import modal

    io_manager = ContainerIOManager(api_pb2.ContainerArguments(checkpoint_id="ch-123"), container_client)
    restore_path = temp_restore_path(tmpdir)

    d = modal.Dict.from_name("my-amazing-dict", create_if_missing=True).hydrate(container_client)
    d["xyz"] = 123
    d["abc"] = 42

    with set_env_vars(restore_path, servicer.container_addr):
        io_manager.memory_snapshot()

    # TODO(Jonathon): These RPC wrappers are tested directly because I could not
    # find a way to construct in this test a UnaryStreamWrapper with a stale snapshotted client.
    @synchronize_api
    async def exercise_rpcs():
        n = 0
        # Test UnaryStreamWrapper
        async for _ in container_client.stub.DictContents.unary_stream(
            api_pb2.DictContentsRequest(dict_id=d.object_id, keys=True)
        ):
            n += 1
        assert n == 2
        # Test UnaryUnaryWrapper
        await container_client.stub.DictClear(api_pb2.DictClearRequest(dict_id=d.object_id))

    await exercise_rpcs.aio()


def test_interact(container_client, servicer):
    # Initialize container singleton
    function_def = api_pb2.Function(pty_info=api_pb2.PTYInfo(pty_type=api_pb2.PTYInfo.PTY_TYPE_SHELL))
    ContainerIOManager(api_pb2.ContainerArguments(function_def=function_def), container_client)
    with servicer.intercept() as ctx:
        ctx.add_response("FunctionStartPtyShell", Empty())
        interact()


def test_interact_no_pty_error(container_client, servicer):
    # Initialize container singleton
    ContainerIOManager(api_pb2.ContainerArguments(), container_client)
    with pytest.raises(InvalidError, match=r"modal.interact\(\) without running Modal in interactive mode"):
        interact()



================================================
FILE: test/container_buffer_test.py
================================================
# Copyright Modal Labs 2024
from modal import App

app = App()


@app.function(
    buffer_containers=10,
)
def f1():
    pass


def test_fn_container_buffer(servicer, client):
    with app.run(client=client):
        assert len(servicer.app_functions) == 1
        fn1 = servicer.app_functions["fu-1"]  # f1
        # Test forward / backward compatibility
        assert fn1._experimental_buffer_containers == fn1.autoscaler_settings.buffer_containers == 10



================================================
FILE: test/decorator_test.py
================================================
# Copyright Modal Labs 2023
import pytest

from modal import App, asgi_app, fastapi_endpoint, method, wsgi_app
from modal.exception import InvalidError


def test_local_entrypoint_forgot_parentheses():
    app = App()

    with pytest.raises(InvalidError, match="local_entrypoint()"):

        @app.local_entrypoint  # type: ignore
        def f():
            pass


def test_function_forgot_parentheses():
    app = App()

    with pytest.raises(InvalidError, match="function()"):

        @app.function  # type: ignore
        def f():
            pass


def test_cls_forgot_parentheses():
    app = App()

    with pytest.raises(InvalidError, match="cls()"):

        @app.cls  # type: ignore
        class XYZ:
            pass


def test_method_forgot_parentheses():
    app = App()

    with pytest.raises(InvalidError, match="method()"):

        @app.cls()
        class XYZ:
            @method  # type: ignore
            def f(self):
                pass


def test_invalid_web_decorator_usage():
    app = App()

    with pytest.raises(InvalidError, match="fastapi_endpoint()"):

        @app.function()  # type: ignore
        @fastapi_endpoint  # type: ignore
        def my_handle():
            pass

    with pytest.raises(InvalidError, match="asgi_app()"):

        @app.function()  # type: ignore
        @asgi_app  # type: ignore
        def my_handle_asgi():
            pass

    with pytest.raises(InvalidError, match="wsgi_app()"):

        @app.function()  # type: ignore
        @wsgi_app  # type: ignore
        def my_handle_wsgi():
            pass


def test_fastapi_endpoint_method():
    app = App()

    with pytest.raises(InvalidError, match="cannot be combined"):

        @app.cls(serialized=True)
        class Container:
            @method()  # type: ignore
            @fastapi_endpoint()
            def generate(self):
                pass



================================================
FILE: test/deprecation_test.py
================================================
# Copyright Modal Labs 2022
import inspect
import pytest

from modal._utils.deprecation import renamed_parameter
from modal.exception import DeprecationError

from .supports.functions import deprecated_function

# Not a pytest unit test, but an extra assertion that we catch issues in global scope too
# See #2228
exc = None
try:
    deprecated_function(42)
except Exception as e:
    exc = e
finally:
    assert isinstance(exc, DeprecationError)  # If you see this, try running `pytest client/client_test`


def test_deprecation():
    # See conftest.py in the root of the repo
    # All deprecation warnings in modal during tests will trigger exceptions
    with pytest.raises(DeprecationError):
        deprecated_function(42)

    # With this context manager, it doesn't raise an exception, but we record
    # the warning. This is the normal behavior outside of pytest.
    with pytest.warns(DeprecationError) as record:
        res = deprecated_function(42)
        assert res == 1764

    # Make sure it raises in the right file
    from .supports import functions

    assert record[0].filename == functions.__file__


@renamed_parameter((2024, 12, 1), "foo", "bar")
def my_func(bar: int) -> int:
    return bar**2


def test_renamed_parameter():
    message = "The 'foo' parameter .+ has been renamed to 'bar'"
    with pytest.warns(DeprecationError, match=message):
        res = my_func(foo=2)  # type: ignore
        assert res == 4
    assert my_func(bar=3) == 9
    assert my_func(4) == 16

    sig = inspect.signature(my_func)
    assert "bar" in sig.parameters
    assert "foo" not in sig.parameters



================================================
FILE: test/dict_test.py
================================================
# Copyright Modal Labs 2022
import pytest
import sys
import time

from modal import Dict
from modal.exception import AlreadyExistsError, DeprecationError, InvalidError, NotFoundError
from modal_proto import api_pb2


def test_dict_named(servicer, client):
    name = "my-amazing-dict"
    d = Dict.from_name(name, create_if_missing=True)
    assert d.name == name

    d.hydrate(client)
    info = d.info()
    assert info.name == name
    assert info.created_by == servicer.default_username

    d["xyz"] = 123
    d["foo"] = 42
    d["foo"] += 5
    assert d["foo"] == 47
    assert d.len() == 2

    assert sorted(d.keys()) == ["foo", "xyz"]
    assert sorted(d.values()) == [47, 123]
    assert sorted(d.items()) == [("foo", 47), ("xyz", 123)]

    d.clear()
    assert d.len() == 0
    with pytest.raises(KeyError):
        _ = d["foo"]

    assert d.get("foo", default=True)
    d["foo"] = None
    assert d["foo"] is None

    Dict.objects.delete("my-amazing-dict", client=client)
    with pytest.raises(NotFoundError):
        Dict.from_name("my-amazing-dict").hydrate(client)
    Dict.objects.delete("my-amazing-dict", client=client, allow_missing=True)


def test_dict_ephemeral(servicer, client):
    assert servicer.n_dict_heartbeats == 0
    with Dict.ephemeral(client=client, _heartbeat_sleep=1) as d:
        d["foo"] = 42
        assert d.len() == 1
        assert d["foo"] == 42
        time.sleep(1.5)  # Make time for 2 heartbeats
    assert servicer.n_dict_heartbeats == 2


def test_dict_lazy_hydrate_named(set_env_client, servicer):
    with servicer.intercept() as ctx:
        d = Dict.from_name("foo", create_if_missing=True)
        assert len(ctx.get_requests("DictGetOrCreate")) == 0  # sanity check that the get request is lazy
        d["foo"] = 42
        assert d["foo"] == 42
        assert len(ctx.get_requests("DictGetOrCreate")) == 1  # just sanity check that object is only hydrated once...


@pytest.mark.parametrize("name", ["has space", "has/slash", "a" * 65])
def test_invalid_name(servicer, client, name):
    with pytest.raises(InvalidError, match="Invalid Dict name"):
        Dict.from_name(name).hydrate(client)


def test_dict_update(servicer, client):
    with Dict.ephemeral(client=client, _heartbeat_sleep=1) as d:
        d.update({"foo": 1, "bar": 2}, foo=3, baz=4)
        items = list(d.items())
        assert sorted(items) == [("bar", 2), ("baz", 4), ("foo", 3)]


def test_dict_put_skip_if_exists(client):
    with Dict.ephemeral(client=client, _heartbeat_sleep=1) as d:
        assert d.put("foo", 1, skip_if_exists=True)
        assert not d.put("foo", 2, skip_if_exists=True)
        items = list(d.items())
        assert items == [("foo", 1)]


def test_dict_namespace_deprecated(servicer, client):
    # Test from_name with namespace parameter warns
    with pytest.warns(
        DeprecationError,
        match="The `namespace` parameter for `modal.Dict.from_name` is deprecated",
    ):
        Dict.from_name("test-dict", namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE)

    # Test that from_name without namespace parameter doesn't warn about namespace
    import warnings

    with warnings.catch_warnings(record=True) as record:
        warnings.simplefilter("always")
        Dict.from_name("test-dict")
    # Filter out any unrelated warnings
    namespace_warnings = [w for w in record if "namespace" in str(w.message).lower()]
    assert len(namespace_warnings) == 0


def test_dict_list(servicer, client):
    for i in range(5):
        Dict.from_name(f"test-dict-{i}", create_if_missing=True).hydrate(client)
    if sys.platform == "win32":
        time.sleep(1 / 32)

    print(servicer.deployed_dicts)

    dict_list = Dict.objects.list(client=client)
    assert len(dict_list) == 5
    assert all(d.name.startswith("test-dict-") for d in dict_list)
    assert all(d.info().created_by == servicer.default_username for d in dict_list)

    dict_list = Dict.objects.list(max_objects=2, client=client)
    assert len(dict_list) == 2


def test_dict_create(servicer, client):
    Dict.objects.create(name="test-dict-create", client=client)
    Dict.from_name("test-dict-create").hydrate(client)
    with pytest.raises(AlreadyExistsError):
        Dict.objects.create(name="test-dict-create", client=client)
    Dict.objects.create(name="test-dict-create", allow_existing=True, client=client)
    with pytest.raises(InvalidError, match="Invalid Dict name"):
        Dict.objects.create(name="has space", client=client)


def test_dict_pop(servicer, client):
    with Dict.ephemeral(client=client, _heartbeat_sleep=1) as d:
        d["existing_key"] = "existing_value"

        assert d.pop("existing_key") == "existing_value"
        assert "existing_key" not in d

        assert d.pop("non_existent_key", "default_value") == "default_value"
        assert d.pop("non_existent_key", None) is None

        with pytest.raises(KeyError):
            d.pop("non_existent_key")



================================================
FILE: test/docker_utils_test.py
================================================
# Copyright Modal Labs 2024
import pytest
from pathlib import Path
from tempfile import NamedTemporaryFile, TemporaryDirectory

from modal._utils.docker_utils import extract_copy_command_patterns, find_dockerignore_file


@pytest.mark.parametrize(
    ("copy_commands", "expected_patterns"),
    [
        (
            ["CoPY files/dir1 ./smth_copy"],
            {"files/dir1"},
        ),
        (
            ["COPY files/*.txt /dest/", "COPY files/**/*.py /dest/"],
            {"files/*.txt", "files/**/*.py"},
        ),
        (
            ["COPY files/special/file[[]1].txt /dest/"],
            {"files/special/file[[]1].txt"},
        ),
        (
            ["COPY files/*.txt files/**/*.py /dest/"],
            {"files/*.txt", "files/**/*.py"},
        ),
        (
            [
                "copy --from=a b",
                "copy ./smth \\",
                "./foo.py \\",
                "# this is a comment",
                "./bar.py \\",
                "/x",
            ],
            {"./smth", "./foo.py", "./bar.py"},
        ),
        (
            [
                "COPY --from=a b",
            ],
            set(),
        ),
    ],
)
def test_extract_copy_command_patterns(copy_commands, expected_patterns):
    copy_command_sources = set(extract_copy_command_patterns(copy_commands))
    assert copy_command_sources == expected_patterns


@pytest.mark.usefixtures("tmp_cwd")
def test_image_dockerfile_copy_messy():
    with TemporaryDirectory(dir="./") as tmp_dir:
        dockerfile = NamedTemporaryFile("w", delete=False)
        dockerfile.write(
            f"""
FROM python:3.12-slim

WORKDIR /my-app

RUN ls

# COPY simple directory
    CoPY {tmp_dir}/dir1 ./smth_copy

RUN ls -la

# COPY multiple sources
        COPY {tmp_dir}/test.py {tmp_dir}/file10.txt /

RUN ls \\
    -l

# COPY multiple lines
copy {tmp_dir}/dir2 \\
    {tmp_dir}/file1.txt \\
# this is a comment
    {tmp_dir}/file2.txt \\
    /x

        RUN ls
        """
        )
        dockerfile.close()

        with open(dockerfile.name) as f:
            lines = f.readlines()

        assert sorted(extract_copy_command_patterns(lines)) == sorted(
            [
                f"{tmp_dir}/dir1",
                f"{tmp_dir}/test.py",
                f"{tmp_dir}/file10.txt",
                f"{tmp_dir}/dir2",
                f"{tmp_dir}/file1.txt",
                f"{tmp_dir}/file2.txt",
            ]
        )


@pytest.mark.usefixtures("tmp_cwd")
def test_find_generic_cwd_dockerignore_file():
    test_cwd = Path.cwd()
    with TemporaryDirectory(dir=test_cwd) as tmp_dir:
        tmp_path = Path(tmp_dir)
        dir1 = tmp_path / "dir1"
        dir1.mkdir()

        dockerfile_path = dir1 / "Dockerfile"
        dockerignore_path = tmp_path / ".dockerignore"
        dockerignore_path.write_text("**/*")
        assert find_dockerignore_file(test_cwd / tmp_dir, dockerfile_path) == dockerignore_path


@pytest.mark.usefixtures("tmp_cwd")
def test_dont_find_specific_dockerignore_file():
    test_cwd = Path.cwd()
    with TemporaryDirectory(dir=test_cwd) as tmp_dir:
        tmp_path = Path(tmp_dir)
        dir1 = tmp_path / "dir1"
        dir1.mkdir()

        dockerfile_path = dir1 / "foo"
        dockerignore_path = tmp_path / "foo.dockerignore"
        dockerignore_path.write_text("**/*")
        assert find_dockerignore_file(test_cwd / tmp_dir, dockerfile_path) is None


@pytest.mark.usefixtures("tmp_cwd")
def test_prefer_specific_cwd_dockerignore_file():
    test_cwd = Path.cwd()
    with TemporaryDirectory(dir=test_cwd) as tmp_dir:
        tmp_path = Path(tmp_dir)
        dir1 = tmp_path / "dir1"
        dir1.mkdir()

        dockerfile_path = tmp_path / "Dockerfile"
        generic_dockerignore_path = tmp_path / ".dockerignore"
        generic_dockerignore_path.write_text("**/*.py")
        specific_dockerignore_path = tmp_path / "Dockerfile.dockerignore"
        specific_dockerignore_path.write_text("**/*")
        assert find_dockerignore_file(test_cwd / tmp_dir, dockerfile_path) == specific_dockerignore_path
        assert find_dockerignore_file(test_cwd / tmp_dir, dockerfile_path) != generic_dockerignore_path


@pytest.mark.usefixtures("tmp_cwd")
def test_dont_find_nested_dockerignore_file():
    test_cwd = Path.cwd()
    with TemporaryDirectory(dir=test_cwd) as tmp_dir:
        tmp_path = Path(tmp_dir)
        dir1 = tmp_path / "dir1"
        dir1.mkdir()
        dir2 = dir1 / "dir2"
        dir2.mkdir()

        dockerfile_path = dir1 / "Dockerfile"
        dockerfile_path.write_text("COPY . /dummy")

        # should ignore parent ones
        generic_dockerignore_path = tmp_path / ".dockerignore"
        generic_dockerignore_path.write_text("**/*")
        specific_dockerignore_path = tmp_path / "Dockerfile.dockerignore"
        specific_dockerignore_path.write_text("**/*")

        # should ignore nested ones
        nested_generic_dockerignore_path = dir2 / ".dockerignore"
        nested_generic_dockerignore_path.write_text("**/*")
        nested_specific_dockerignore_path = dir2 / "Dockerfile.dockerignore"
        nested_specific_dockerignore_path.write_text("**/*")

        assert find_dockerignore_file(dir1, dockerfile_path) is None


@pytest.mark.usefixtures("tmp_cwd")
def test_find_next_to_dockerfile_dockerignore_file():
    test_cwd = Path.cwd()
    with TemporaryDirectory(dir=test_cwd) as tmp_dir:
        tmp_path = Path(tmp_dir)
        dir1 = tmp_path / "dir1"
        dir1.mkdir()

        dockerfile_path = dir1 / "Dockerfile"
        dockerignore_path = tmp_path / ".dockerignore"
        dockerignore_path.write_text("**/*")

        assert find_dockerignore_file(test_cwd / tmp_dir, dockerfile_path) == dockerignore_path



================================================
FILE: test/e2e_test.py
================================================
# Copyright Modal Labs 2022
import os
import pathlib
import subprocess
import sys


def _cli(args, server_url, credentials, extra_env={}, check=True) -> tuple[int, str, str]:
    lib_dir = pathlib.Path(__file__).parent.parent
    args = [sys.executable] + args
    token_id, token_secret = credentials
    env = {
        "MODAL_SERVER_URL": server_url,
        "MODAL_TOKEN_ID": token_id,
        "MODAL_TOKEN_SECRET": token_secret,
        **os.environ,
        "PYTHONUTF8": "1",  # For windows
        **extra_env,
    }
    ret = subprocess.run(args, cwd=lib_dir, env=env, capture_output=True)

    stdout = ret.stdout.decode()
    stderr = ret.stderr.decode()
    if check and ret.returncode != 0:
        raise Exception(f"Failed with {ret.returncode} stdout: {stdout} stderr: {stderr}")
    return ret.returncode, stdout, stderr


def test_run_e2e(servicer, credentials):
    _, _, err = _cli(["-m", "test.supports.script"], servicer.client_addr, credentials)
    assert err == ""


def test_run_progress_info(servicer, credentials):
    returncode, stdout, stderr = _cli(["-m", "test.supports.progress_info"], servicer.client_addr, credentials)
    assert returncode == 0
    assert stderr == ""
    lines = stdout.splitlines()
    assert "Initialized. View run at https://modaltest.com/apps/ap-123" in lines[0]
    assert "App completed" in lines[-1]


def test_run_profiler(servicer, credentials):
    _cli(["-m", "cProfile", "-m", "test.supports.script"], servicer.client_addr, credentials)


def test_run_unconsumed_map(servicer, credentials):
    _, _, err = _cli(["-m", "test.supports.unconsumed_map"], servicer.client_addr, credentials)
    assert "map" in err
    assert "for-loop" in err

    _, _, err = _cli(["-m", "test.supports.consumed_map"], servicer.client_addr, credentials)
    assert "map" not in err
    assert "for-loop" not in err


def test_auth_failure_last_line(servicer, credentials):
    returncode, out, err = _cli(
        ["-m", "test.supports.script"],
        servicer.client_addr,
        credentials,
        extra_env={"MODAL_TOKEN_ID": "bad", "MODAL_TOKEN_SECRET": "bad"},
        check=False,
    )
    try:
        assert returncode != 0
        assert "token" in err.strip().split("\n")[-1]  # err msg should be on the last line
    except Exception:
        print("out:", repr(out))
        print("err:", repr(err))
        raise



================================================
FILE: test/error_test.py
================================================
# Copyright Modal Labs 2022
from modal import Error
from modal.exception import NotFoundError


def test_modal_errors():
    assert issubclass(NotFoundError, Error)



================================================
FILE: test/experimental_test.py
================================================
# Copyright Modal Labs 2025
import pytest

import modal
import modal.experimental
from modal.exception import NotFoundError

app = modal.App(include_source=False)


@app.function()
def f():
    pass


@app.cls()
class C:
    @modal.method()
    def method(self):
        pass


def test_app_get_objects(client, servicer):
    app.deploy(name="test", environment_name="dev", client=client)
    res = modal.experimental.get_app_objects("test", environment_name="dev", client=client)
    assert res.keys() == {"C", "f"}
    assert isinstance(res["C"], modal.Cls)
    assert isinstance(res["f"], modal.Function)


def test_image_delete(client, servicer):
    with app.run(client=client):
        image = modal.Image.debian_slim().build(app)

    assert image.object_id in servicer.images
    modal.experimental.image_delete(image.object_id, client=client)
    assert image.object_id not in servicer.images

    with pytest.raises(NotFoundError):
        modal.experimental.image_delete("im-nonexistent", client=client)



================================================
FILE: test/file_io_test.py
================================================
# Copyright Modal Labs 2024
import json
import pytest

from grpclib import Status
from grpclib.exceptions import GRPCError

from modal.file_io import (  # type: ignore
    WRITE_CHUNK_SIZE,
    WRITE_FILE_SIZE_LIMIT,
    FileIO,
    FileWatchEvent,
    FileWatchEventType,
    delete_bytes,
    replace_bytes,
)
from modal_proto import api_pb2

OPEN_EXEC_ID = "exec-open-123"
READ_EXEC_ID = "exec-read-123"
READLINE_EXEC_ID = "exec-readline-123"
READLINES_EXEC_ID = "exec-readlines-123"
WRITE_EXEC_ID = "exec-write-123"
FLUSH_EXEC_ID = "exec-flush-123"
SEEK_EXEC_ID = "exec-seek-123"
WRITE_REPLACE_EXEC_ID = "exec-write-replace-123"
DELETE_EXEC_ID = "exec-delete-123"
CLOSE_EXEC_ID = "exec-close-123"
WATCH_EXEC_ID = "exec-watch-123"
LS_EXEC_ID = "exec-ls-123"
MKDIR_EXEC_ID = "exec-mkdir-123"
RM_EXEC_ID = "exec-rm-123"


async def container_filesystem_exec(servicer, stream):
    req = await stream.recv_message()

    if req.HasField("file_open_request"):
        await stream.send_message(
            api_pb2.ContainerFilesystemExecResponse(exec_id=OPEN_EXEC_ID, file_descriptor="fd-123")
        )
    elif req.HasField("file_read_request"):
        await stream.send_message(
            api_pb2.ContainerFilesystemExecResponse(
                exec_id=READ_EXEC_ID,
            )
        )
    elif req.HasField("file_read_line_request"):
        await stream.send_message(
            api_pb2.ContainerFilesystemExecResponse(
                exec_id=READLINE_EXEC_ID,
            )
        )
    elif req.HasField("file_write_request"):
        await stream.send_message(
            api_pb2.ContainerFilesystemExecResponse(
                exec_id=WRITE_EXEC_ID,
            )
        )
    elif req.HasField("file_write_replace_bytes_request"):
        await stream.send_message(
            api_pb2.ContainerFilesystemExecResponse(
                exec_id=WRITE_REPLACE_EXEC_ID,
            )
        )
    elif req.HasField("file_delete_bytes_request"):
        await stream.send_message(
            api_pb2.ContainerFilesystemExecResponse(
                exec_id=DELETE_EXEC_ID,
            )
        )
    elif req.HasField("file_seek_request"):
        await stream.send_message(
            api_pb2.ContainerFilesystemExecResponse(
                exec_id=SEEK_EXEC_ID,
            )
        )
    elif req.HasField("file_flush_request"):
        await stream.send_message(
            api_pb2.ContainerFilesystemExecResponse(
                exec_id=FLUSH_EXEC_ID,
            )
        )
    elif req.HasField("file_close_request"):
        await stream.send_message(api_pb2.ContainerFilesystemExecResponse(exec_id=CLOSE_EXEC_ID))
    elif req.HasField("file_watch_request"):
        await stream.send_message(api_pb2.ContainerFilesystemExecResponse(exec_id=WATCH_EXEC_ID))
    elif req.HasField("file_ls_request"):
        await stream.send_message(api_pb2.ContainerFilesystemExecResponse(exec_id=LS_EXEC_ID))
    elif req.HasField("file_mkdir_request"):
        await stream.send_message(api_pb2.ContainerFilesystemExecResponse(exec_id=MKDIR_EXEC_ID))
    elif req.HasField("file_rm_request"):
        await stream.send_message(api_pb2.ContainerFilesystemExecResponse(exec_id=RM_EXEC_ID))


def test_file_read(servicer, client):
    """Test file reading."""
    content = "foo\nbar\nbaz\n"

    async def container_filesystem_exec_get_output(servicer, stream):
        req = await stream.recv_message()
        if req.exec_id == READ_EXEC_ID:
            await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(output=[content.encode()]))
        await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(eof=True))

    with servicer.intercept() as ctx:
        ctx.set_responder("ContainerFilesystemExec", container_filesystem_exec)
        ctx.set_responder("ContainerFilesystemExecGetOutput", container_filesystem_exec_get_output)

        f = FileIO.create("/test.txt", "r", client, "task-123")
        assert f.read() == content
        f.close()


def test_file_write(servicer, client):
    """Test file writing."""
    content = "foo\nbar\nbaz\n"

    async def container_filesystem_exec_get_output(servicer, stream):
        req = await stream.recv_message()
        if req.exec_id == READ_EXEC_ID:
            await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(output=[content.encode()]))
        await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(eof=True))

    with servicer.intercept() as ctx:
        ctx.set_responder("ContainerFilesystemExec", container_filesystem_exec)
        ctx.set_responder("ContainerFilesystemExecGetOutput", container_filesystem_exec_get_output)

        f = FileIO.create("/test.txt", "a+", client, "task-123")
        f.write(content)
        assert f.read() == content
        f.close()


def test_file_write_large(servicer, client):
    """Test file write chunking logic."""
    content = "A" * WRITE_FILE_SIZE_LIMIT
    write_counter = 0

    async def container_filesystem_exec_get_output(servicer, stream):
        nonlocal write_counter
        req = await stream.recv_message()
        if req.exec_id == WRITE_EXEC_ID:
            write_counter += 1
        await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(eof=True))

    with servicer.intercept() as ctx:
        ctx.set_responder("ContainerFilesystemExec", container_filesystem_exec)
        ctx.set_responder("ContainerFilesystemExecGetOutput", container_filesystem_exec_get_output)

        f = FileIO.create("/test.txt", "a+", client, "task-123")
        f.write(content)
        assert write_counter == WRITE_FILE_SIZE_LIMIT // WRITE_CHUNK_SIZE
        f.close()


def test_file_write_too_large(servicer, client):
    """Test that writing a file larger than WRITE_FILE_SIZE_LIMIT raises an error."""
    content = "A" * (WRITE_FILE_SIZE_LIMIT + 1)

    async def container_filesystem_exec_get_output(servicer, stream):
        await stream.recv_message()
        await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(eof=True))

    with servicer.intercept() as ctx:
        ctx.set_responder("ContainerFilesystemExec", container_filesystem_exec)
        ctx.set_responder("ContainerFilesystemExecGetOutput", container_filesystem_exec_get_output)

        with pytest.raises(ValueError):
            FileIO.create("/test.txt", "a+", client, "task-123").write(content)


def test_file_readline(servicer, client):
    """Test file reading line by line."""
    lines = ["foo\n", "bar\n", "baz\n", "end"]
    content = "".join(lines)
    index = 0

    async def container_filesystem_exec_get_output(servicer, stream):
        nonlocal index
        req = await stream.recv_message()
        if req.exec_id == READLINE_EXEC_ID:
            await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(output=[lines[index].encode()]))
            index += 1
        await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(eof=True))

    with servicer.intercept() as ctx:
        ctx.set_responder("ContainerFilesystemExec", container_filesystem_exec)
        ctx.set_responder("ContainerFilesystemExecGetOutput", container_filesystem_exec_get_output)

        f = FileIO.create("/test.txt", "w+", client, "task-123")
        f.write(content)
        assert f.readline() == "foo\n"
        assert f.readline() == "bar\n"
        assert f.readline() == "baz\n"
        assert f.readline() == "end"
        f.close()


def test_file_readlines_no_newline_end(servicer, client):
    """Test file reading lines."""
    lines = ["foo\n", "bar\n", "baz\n", "end"]
    content = "".join(lines)

    async def container_filesystem_exec_get_output(servicer, stream):
        req = await stream.recv_message()
        if req.exec_id == READ_EXEC_ID:
            await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(output=[content.encode()]))
        await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(eof=True))

    with servicer.intercept() as ctx:
        ctx.set_responder("ContainerFilesystemExec", container_filesystem_exec)
        ctx.set_responder("ContainerFilesystemExecGetOutput", container_filesystem_exec_get_output)

        f = FileIO.create("/test.txt", "w+", client, "task-123")
        f.write(content)
        assert f.readlines() == lines
        f.close()


def test_file_readlines_newline_end(servicer, client):
    """Test file reading lines."""
    lines = ["foo\n", "bar\n", "baz\n", "end\n"]
    content = "".join(lines)

    async def container_filesystem_exec_get_output(servicer, stream):
        req = await stream.recv_message()
        if req.exec_id == READ_EXEC_ID:
            await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(output=[content.encode()]))
        await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(eof=True))

    with servicer.intercept() as ctx:
        ctx.set_responder("ContainerFilesystemExec", container_filesystem_exec)
        ctx.set_responder("ContainerFilesystemExecGetOutput", container_filesystem_exec_get_output)

        f = FileIO.create("/test.txt", "w+", client, "task-123")
        f.write(content)
        assert f.readlines() == lines
        f.close()


def test_file_readlines_multiple_newline_end(servicer, client):
    """Test file reading lines."""
    lines = ["foo\n", "bar\n", "baz\n", "end\n", "\n", "\n"]
    content = "".join(lines)

    async def container_filesystem_exec_get_output(servicer, stream):
        req = await stream.recv_message()
        if req.exec_id == READ_EXEC_ID:
            await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(output=[content.encode()]))
        await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(eof=True))

    with servicer.intercept() as ctx:
        ctx.set_responder("ContainerFilesystemExec", container_filesystem_exec)
        ctx.set_responder("ContainerFilesystemExecGetOutput", container_filesystem_exec_get_output)

        f = FileIO.create("/test.txt", "w+", client, "task-123")
        f.write(content)
        assert f.readlines() == lines
        f.close()


def test_file_flush(servicer, client):
    """Test file flushing."""

    async def container_filesystem_exec_get_output(servicer, stream):
        await stream.recv_message()
        await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(eof=True))

    with servicer.intercept() as ctx:
        ctx.set_responder("ContainerFilesystemExec", container_filesystem_exec)
        ctx.set_responder("ContainerFilesystemExecGetOutput", container_filesystem_exec_get_output)

        f = FileIO.create("/test.txt", "w+", client, "task-123")
        f.write("foo")
        f.flush()
        f.close()


def test_file_seek(servicer, client):
    """Test file seeking."""
    index = 0
    expected_outputs = ["foo\nbar\nbaz\n", "bar\nbaz\n", "baz\n", ""]

    async def container_filesystem_exec_get_output(servicer, stream):
        nonlocal index
        req = await stream.recv_message()
        if req.exec_id == READ_EXEC_ID:
            await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(output=[expected_outputs[index].encode()]))
            index += 1
        await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(eof=True))

    with servicer.intercept() as ctx:
        ctx.set_responder("ContainerFilesystemExec", container_filesystem_exec)
        ctx.set_responder("ContainerFilesystemExecGetOutput", container_filesystem_exec_get_output)

        f = FileIO.create("/test.txt", "a+", client, "task-123")
        f.write("foo\nbar\nbaz\n")
        f.close()
        f = FileIO.create("/test.txt", "r", client, "task-123")
        for i in range(4):
            f.seek(3)
            assert f.read() == expected_outputs[i]
        f.close()


def test_file_write_replace_bytes(servicer, client):
    """Test file write replace bytes."""
    index = 0
    expected_outputs = ["foo\nbar\nbaz\n", "foo\nbarbar\nbaz\n"]

    async def container_filesystem_exec_get_output(servicer, stream):
        nonlocal index
        req = await stream.recv_message()
        if req.exec_id == READ_EXEC_ID:
            await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(output=[expected_outputs[index].encode()]))
            index += 1
        await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(eof=True))

    with servicer.intercept() as ctx:
        ctx.set_responder("ContainerFilesystemExec", container_filesystem_exec)
        ctx.set_responder("ContainerFilesystemExecGetOutput", container_filesystem_exec_get_output)

        f = FileIO.create("/test.txt", "a+", client, "task-123")
        f.write("foo\nbar\nbaz\n")
        assert f.read() == expected_outputs[0]
        replace_bytes(f, data=b"barbar", start=4, end=7)
        assert f.read() == expected_outputs[1]
        f.close()


def test_file_delete_bytes(servicer, client):
    """Test file delete bytes."""
    index = 0
    expected_outputs = ["foo\nbar\nbaz\n", "foo\nbaz\n"]

    async def container_filesystem_exec_get_output(servicer, stream):
        nonlocal index
        req = await stream.recv_message()
        if req.exec_id == READ_EXEC_ID:
            await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(output=[expected_outputs[index].encode()]))
            index += 1
        await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(eof=True))

    with servicer.intercept() as ctx:
        ctx.set_responder("ContainerFilesystemExec", container_filesystem_exec)
        ctx.set_responder("ContainerFilesystemExecGetOutput", container_filesystem_exec_get_output)

        f = FileIO.create("/test.txt", "a+", client, "task-123")
        f.write("foo\nbar\nbaz\n")
        assert f.read() == expected_outputs[0]
        delete_bytes(f, start=4, end=7)
        assert f.read() == expected_outputs[1]
        f.close()


def test_invalid_mode(servicer, client):
    """Test a variety of invalid modes."""
    invalid_modes = [
        "",  # empty mode
        "invalid",  # invalid mode
        "rr",  # duplicate letters
        "rab",  # too many modes
        "r++",  # too many modes
        "+",  # plus without read/write mode
        "x+r",  # too many modes
        "wx",  # too many modes
        "rbb",  # too many binary flags
        " r",  # whitespace
        "r ",  # whitespace
        "R",  # uppercase
        "W",  # uppercase
        "\n",  # newline
    ]
    for mode in invalid_modes:
        with pytest.raises(ValueError):
            FileIO.create("/test.txt", mode, client, "task-123")  # type: ignore


def test_client_retry(servicer, client):
    """Test client retry."""
    retries = 5
    content = "foo\nbar\nbaz\n"

    async def container_filesystem_exec_get_output(servicer, stream):
        nonlocal retries
        req = await stream.recv_message()
        if req.exec_id == READ_EXEC_ID:
            if retries > 0:
                retries -= 1
                raise GRPCError(Status.UNAVAILABLE, "test")
            await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(output=[content.encode()]))
        await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(eof=True))

    with servicer.intercept() as ctx:
        ctx.set_responder("ContainerFilesystemExec", container_filesystem_exec)
        ctx.set_responder("ContainerFilesystemExecGetOutput", container_filesystem_exec_get_output)

        f = FileIO.create("/test.txt", "w+", client, "task-123")
        f.write(content)
        assert f.read() == content
        f.close()


def test_file_watch(servicer, client):
    """Test file watching."""
    expected_events = [
        FileWatchEvent(paths=["/foo.txt"], type=FileWatchEventType.Access),
        FileWatchEvent(paths=["/bar.txt"], type=FileWatchEventType.Create),
        FileWatchEvent(paths=["/baz.txt", "/baz/foo.txt"], type=FileWatchEventType.Modify),
    ]

    async def container_filesystem_exec_get_output(servicer, stream):
        req = await stream.recv_message()
        if req.exec_id == WATCH_EXEC_ID:
            for event in expected_events:
                await stream.send_message(
                    api_pb2.FilesystemRuntimeOutputBatch(
                        output=[
                            f'{{"paths": {json.dumps(event.paths)}, "event_type": "{event.type.value}"}}\n\n'.encode()
                        ]
                    )
                )
        await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(eof=True))

    with servicer.intercept() as ctx:
        ctx.set_responder("ContainerFilesystemExec", container_filesystem_exec)
        ctx.set_responder("ContainerFilesystemExecGetOutput", container_filesystem_exec_get_output)

        events = FileIO.watch("/test.txt", client, "task-123")
        seen_events: list[FileWatchEvent] = []
        for event in events:
            seen_events.append(event)
        assert len(seen_events) == len(expected_events)
        for e, se in zip(expected_events, seen_events):
            assert e.paths == se.paths
            assert e.type == se.type


def test_file_watch_with_filter(servicer, client):
    """Test file watching with filter."""
    expected_events = [
        FileWatchEvent(paths=["/foo.txt"], type=FileWatchEventType.Access),
        FileWatchEvent(paths=["/bar.txt"], type=FileWatchEventType.Create),
        FileWatchEvent(paths=["/baz.txt", "/baz/foo.txt"], type=FileWatchEventType.Modify),
    ]

    async def container_filesystem_exec_get_output(servicer, stream):
        req = await stream.recv_message()
        if req.exec_id == WATCH_EXEC_ID:
            for event in expected_events:
                await stream.send_message(
                    api_pb2.FilesystemRuntimeOutputBatch(
                        output=[
                            f'{{"paths": {json.dumps(event.paths)}, "event_type": "{event.type.value}"}}\n\n'.encode()
                        ]
                    )
                )
        await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(eof=True))

    with servicer.intercept() as ctx:
        ctx.set_responder("ContainerFilesystemExec", container_filesystem_exec)
        ctx.set_responder("ContainerFilesystemExecGetOutput", container_filesystem_exec_get_output)

        events = FileIO.watch("/test.txt", client, "task-123", filter=[FileWatchEventType.Access])
        seen_events: list[FileWatchEvent] = []
        for event in events:
            seen_events.append(event)
        assert len(seen_events) == 1
        assert seen_events[0].paths == expected_events[0].paths
        assert seen_events[0].type == expected_events[0].type


def test_file_watch_ignore_invalid_events(servicer, client):
    """Test file watching ignores invalid events."""
    index = 0
    expected_events = [
        FileWatchEvent(paths=["/foo.txt"], type=FileWatchEventType.Access),
        FileWatchEvent(paths=["/bar.txt"], type=FileWatchEventType.Create),
        FileWatchEvent(paths=["/baz.txt", "/baz/foo.txt"], type=FileWatchEventType.Modify),
    ]
    raw_events = []
    for i, event in enumerate(expected_events):
        raw_events.append(f'{{"paths": {json.dumps(event.paths)}, "event_type": "{event.type.value}"}}\n\n'.encode())
        if i % 2 == 0:
            # interweave invalid events to test that they are ignored
            raw_events.append(b"invalid\n\n")

    async def container_filesystem_exec_get_output(servicer, stream):
        nonlocal index
        req = await stream.recv_message()
        if req.exec_id == WATCH_EXEC_ID:
            for event in raw_events:
                await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(output=[event]))
        await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(eof=True))

    with servicer.intercept() as ctx:
        ctx.set_responder("ContainerFilesystemExec", container_filesystem_exec)
        ctx.set_responder("ContainerFilesystemExecGetOutput", container_filesystem_exec_get_output)

        events = FileIO.watch("/test.txt", client, "task-123")
        seen_events: list[FileWatchEvent] = []
        for event in events:
            seen_events.append(event)
        assert len(seen_events) == len(expected_events)
        for e, se in zip(expected_events, seen_events):
            assert e.paths == se.paths
            assert e.type == se.type


@pytest.mark.asyncio
async def test_file_io_async_context_manager(servicer, client):
    """Test file io context manager."""
    content = "foo\nbar\nbaz\n"

    async def container_filesystem_exec_get_output(servicer, stream):
        req = await stream.recv_message()
        if req.exec_id == READ_EXEC_ID:
            await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(output=[content.encode()]))
        await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(eof=True))

    with servicer.intercept() as ctx:
        ctx.set_responder("ContainerFilesystemExec", container_filesystem_exec)
        ctx.set_responder("ContainerFilesystemExecGetOutput", container_filesystem_exec_get_output)

        async with FileIO.create("/test.txt", "w+", client, "task-123") as f:
            await f.write.aio(content)
            assert await f.read.aio() == content


def test_file_io_sync_context_manager(servicer, client):
    """Test file io context manager."""
    content = "foo\nbar\nbaz\n"

    async def container_filesystem_exec_get_output(servicer, stream):
        req = await stream.recv_message()
        if req.exec_id == READ_EXEC_ID:
            await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(output=[content.encode()]))
        await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(eof=True))

    with servicer.intercept() as ctx:
        ctx.set_responder("ContainerFilesystemExec", container_filesystem_exec)
        ctx.set_responder("ContainerFilesystemExecGetOutput", container_filesystem_exec_get_output)

        with FileIO.create("/test.txt", "w+", client, "task-123") as f:
            f.write(content)
            assert f.read() == content


def test_ls(servicer, client):
    """Test ls."""

    async def container_filesystem_exec_get_output(servicer, stream):
        req = await stream.recv_message()
        if req.exec_id == LS_EXEC_ID:
            await stream.send_message(
                api_pb2.FilesystemRuntimeOutputBatch(output=[b'{"paths": ["foo", "bar", "baz"]}'])
            )
            await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(eof=True))
        else:
            raise Exception("Unexpected exec_id: " + req.exec_id)

    with servicer.intercept() as ctx:
        ctx.set_responder("ContainerFilesystemExec", container_filesystem_exec)
        ctx.set_responder("ContainerFilesystemExecGetOutput", container_filesystem_exec_get_output)

        files = FileIO.ls("/test.txt", client, "task-123")
        assert files == ["foo", "bar", "baz"]


def test_mkdir(servicer, client):
    """Test mkdir."""

    async def container_filesystem_exec_get_output(servicer, stream):
        req = await stream.recv_message()
        if req.exec_id == MKDIR_EXEC_ID:
            await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(eof=True))
        else:
            raise Exception("Unexpected exec_id: " + req.exec_id)

    with servicer.intercept() as ctx:
        ctx.set_responder("ContainerFilesystemExec", container_filesystem_exec)
        ctx.set_responder("ContainerFilesystemExecGetOutput", container_filesystem_exec_get_output)

        FileIO.mkdir("/test.txt", client, "task-123")


def test_rm(servicer, client):
    """Test rm."""

    async def container_filesystem_exec_get_output(servicer, stream):
        req = await stream.recv_message()
        if req.exec_id == RM_EXEC_ID:
            await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(eof=True))
        else:
            raise Exception("Unexpected exec_id: " + req.exec_id)

    with servicer.intercept() as ctx:
        ctx.set_responder("ContainerFilesystemExec", container_filesystem_exec)
        ctx.set_responder("ContainerFilesystemExecGetOutput", container_filesystem_exec_get_output)

        FileIO.rm("/test.txt", client, "task-123")



================================================
FILE: test/file_pattern_matcher_test.py
================================================
# Copyright Modal Labs 2024

"""Tests for file_pattern_matcher.py.

These are ported from the original patternmatcher Go library.
"""

import contextlib
import os
import os.path
import platform
import pytest
from pathlib import Path

from modal import FilePatternMatcher


def test_wildcard_matches():
    assert FilePatternMatcher("*")._matches("fileutils.go")


def test_pattern_matches():
    assert FilePatternMatcher("*.go")._matches("fileutils.go")


def test_exclusion_pattern_matches_pattern_before():
    assert FilePatternMatcher("!fileutils.go", "*.go")._matches("fileutils.go")


def test_pattern_matches_folder_exclusions():
    assert not FilePatternMatcher("docs", "!docs/README.md")._matches("docs/README.md")


def test_pattern_matches_folder_with_slash_exclusions():
    assert not FilePatternMatcher("docs/", "!docs/README.md")._matches("docs/README.md")


def test_pattern_matches_folder_wildcard_exclusions():
    assert not FilePatternMatcher("docs/*", "!docs/README.md")._matches("docs/README.md")


def test_exclusion_pattern_matches_pattern_after():
    assert not FilePatternMatcher("*.go", "!fileutils.go")._matches("fileutils.go")


def test_exclusion_pattern_matches_whole_directory():
    assert not FilePatternMatcher("*.go")._matches(".")


def test_single_exclamation_error():
    try:
        FilePatternMatcher("!")
    except ValueError as e:
        assert str(e) == 'Illegal exclusion pattern: "!"'


def test_matches_with_no_patterns():
    assert not FilePatternMatcher()._matches("/any/path/there")


def test_matches_with_malformed_patterns():
    try:
        FilePatternMatcher("[")
    except ValueError as e:
        assert str(e) == "Bad pattern: ["


def test_matches():
    tests = [
        ("**", "file", True),
        ("**", "file/", True),
        ("**/", "file", True),  # weird one
        ("**/", "file/", True),
        ("**", "/", True),
        ("**/", "/", True),
        ("**", "dir/file", True),
        ("**/", "dir/file", True),
        ("**", "dir/file/", True),
        ("**/", "dir/file/", True),
        ("**/**", "dir/file", True),
        ("**/**", "dir/file/", True),
        ("dir/**", "dir/file", True),
        ("dir/**", "dir/file/", True),
        ("dir/**", "dir/dir2/file", True),
        ("dir/**", "dir/dir2/file/", True),
        ("**/dir", "dir", True),
        ("**/dir", "dir/file", True),
        ("**/dir2/*", "dir/dir2/file", True),
        ("**/dir2/*", "dir/dir2/file/", True),
        ("**/dir2/**", "dir/dir2/dir3/file", True),
        ("**/dir2/**", "dir/dir2/dir3/file/", True),
        ("**file", "file", True),
        ("**file", "dir/file", True),
        ("**/file", "dir/file", True),
        ("**file", "dir/dir/file", True),
        ("**/file", "dir/dir/file", True),
        ("**/file*", "dir/dir/file", True),
        ("**/file*", "dir/dir/file.txt", True),
        ("**/file*txt", "dir/dir/file.txt", True),
        ("**/file*.txt", "dir/dir/file.txt", True),
        ("**/file*.txt*", "dir/dir/file.txt", True),
        ("**/**/*.txt", "dir/dir/file.txt", True),
        ("**/**/*.txt2", "dir/dir/file.txt", False),
        ("**/*.txt", "file.txt", True),
        ("**/**/*.txt", "file.txt", True),
        ("a**/*.txt", "a/file.txt", True),
        ("a**/*.txt", "a/dir/file.txt", True),
        ("a**/*.txt", "a/dir/dir/file.txt", True),
        ("a/*.txt", "a/dir/file.txt", False),
        ("a/*.txt", "a/file.txt", True),
        ("a/*.txt**", "a/file.txt", True),
        ("a[b-d]e", "ae", False),
        ("a[b-d]e", "ace", True),
        ("a[b-d]e", "aae", False),
        ("a[^b-d]e", "aze", True),
        (".*", ".foo", True),
        (".*", "foo", False),
        ("abc.def", "abcdef", False),
        ("abc.def", "abc.def", True),
        ("abc.def", "abcZdef", False),
        ("abc?def", "abcZdef", True),
        ("abc?def", "abcdef", False),
        ("a\\\\", "a\\", True),
        ("**/foo/bar", "foo/bar", True),
        ("**/foo/bar", "dir/foo/bar", True),
        ("**/foo/bar", "dir/dir2/foo/bar", True),
        ("abc/**", "abc", False),
        ("abc/**", "abc/def", True),
        ("abc/**", "abc/def/ghi", True),
        ("**/.foo", ".foo", True),
        ("**/.foo", "bar.foo", False),
        ("a(b)c/def", "a(b)c/def", True),
        ("a(b)c/def", "a(b)c/xyz", False),
        ("a.|)$(}+{bc", "a.|)$(}+{bc", True),
        (
            "dist/proxy.py-2.4.0rc3.dev36+g08acad9-py3-none-any.whl",
            "dist/proxy.py-2.4.0rc3.dev36+g08acad9-py3-none-any.whl",
            True,
        ),
        ("dist/*.whl", "dist/proxy.py-2.4.0rc3.dev36+g08acad9-py3-none-any.whl", True),
    ]

    multi_pattern_tests = [
        (["**", "!util/docker/web"], "util/docker/web/foo", False),
        (["**", "!util/docker/web", "util/docker/web/foo"], "util/docker/web/foo", True),
        (
            ["**", "!dist/proxy.py-2.4.0rc3.dev36+g08acad9-py3-none-any.whl"],
            "dist/proxy.py-2.4.0rc3.dev36+g08acad9-py3-none-any.whl",
            False,
        ),
        (["**", "!dist/*.whl"], "dist/proxy.py-2.4.0rc3.dev36+g08acad9-py3-none-any.whl", False),
    ]

    for pattern, text, expected in tests:
        assert FilePatternMatcher(pattern)._matches(text) is expected

    for patterns, text, expected in multi_pattern_tests:
        assert FilePatternMatcher(*patterns)._matches(text) is expected


def test_clean_patterns():
    patterns = ["docs", "config"]
    pm = FilePatternMatcher(*patterns)
    cleaned = pm.patterns
    assert len(cleaned) == 2


def test_clean_patterns_strip_empty_patterns():
    patterns = ["docs", "config", ""]
    pm = FilePatternMatcher(*patterns)
    cleaned = pm.patterns
    assert len(cleaned) == 2


def test_clean_patterns_exception_flag():
    patterns = ["docs", "!docs/README.md"]
    pm = FilePatternMatcher(*patterns)
    assert any(p.exclusion for p in pm.patterns)


def test_clean_patterns_leading_space_trimmed():
    patterns = ["docs", "  !docs/README.md"]
    pm = FilePatternMatcher(*patterns)
    assert any(p.exclusion for p in pm.patterns)


def test_clean_patterns_trailing_space_trimmed():
    patterns = ["docs", "!docs/README.md  "]
    pm = FilePatternMatcher(*patterns)
    assert any(p.exclusion for p in pm.patterns)


def test_clean_patterns_error_single_exception():
    patterns = ["!"]
    try:
        FilePatternMatcher(*patterns)
    except ValueError as e:
        assert str(e) == 'Illegal exclusion pattern: "!"'


def test_match():
    match_tests = [
        # (pattern , text, should_match, expected_error)
        ("abc", "abc", True, None),
        ("*", "abc", True, None),
        ("*c", "abc", True, None),
        ("a*", "a", True, None),
        ("a*", "abc", True, None),
        ("a*", "ab/c", True, None),
        ("a*/b", "abc/b", True, None),
        ("a*/b", "a/c/b", False, None),
        ("a*b*c*d*e*/f", "axbxcxdxe/f", True, None),
        ("a*b*c*d*e*/f", "axbxcxdxexxx/f", True, None),
        ("a*b*c*d*e*/f", "axbxcxdxe/xxx/f", False, None),
        ("a*b*c*d*e*/f", "axbxcxdxexxx/fff", False, None),
        ("a*b?c*x", "abxbbxdbxebxczzx", True, None),
        ("a*b?c*x", "abxbbxdbxebxczzy", False, None),
        ("ab[c]", "abc", True, None),
        ("ab[b-d]", "abc", True, None),
        ("ab[e-g]", "abc", False, None),
        ("ab[^c]", "abc", False, None),
        ("ab[^b-d]", "abc", False, None),
        ("ab[^e-g]", "abc", True, None),
        ("a\\*b", "a*b", True, None),
        ("a\\*b", "ab", False, None),
        ("a?b", "aâ˜ºb", True, None),
        ("a[^a]b", "aâ˜ºb", True, None),
        ("a???b", "aâ˜ºb", False, None),
        ("a[^a][^a][^a]b", "aâ˜ºb", False, None),
        ("[a-Î¶]*", "Î±", True, None),
        ("*[a-Î¶]", "A", False, None),
        ("a?b", "a/b", False, None),
        ("a*b", "a/b", False, None),
        ("[\\]a]", "]", True, None),
        ("[\\-]", "-", True, None),
        ("[x\\-]", "x", True, None),
        ("[x\\-]", "-", True, None),
        ("[x\\-]", "z", False, None),
        ("[\\-x]", "x", True, None),
        ("[\\-x]", "-", True, None),
        ("[\\-x]", "a", False, None),
        ("/abc", "abc", True, None),
        ("abc/", "abc", True, None),
        ("/abc/xyz", "abc/xyz", True, None),
        ("/abc/xyz/", "abc/xyz", True, None),
        # These do not return errors because the Python re.compile() method does
        # not raise an error on invalid syntax like Go does. We can omit the
        # tests though since it doesn't affect behavior on _correct_ syntax.
        #
        # ("[]a]", "]", False, ValueError),
        # ("[-]", "-", False, ValueError),
        # ("[x-]", "x", False, ValueError),
        # ("[x-]", "-", False, ValueError),
        # ("[x-]", "z", False, ValueError),
        # ("[-x]", "x", False, ValueError),
        # ("[-x]", "-", False, ValueError),
        # ("[-x]", "a", False, ValueError),
        # ("\\", "a", False, ValueError),
        # ("[a-b-c]", "a", False, ValueError),
        # ("[", "a", False, ValueError),
        # ("[^", "a", False, ValueError),
        # ("[^bc", "a", False, ValueError),
        # ("a[", "a", False, ValueError),
        # ("a[", "ab", False, ValueError),
        ("*x", "xxx", True, None),
    ]

    for pattern, text, expected, error in match_tests:
        if platform.system() == "Windows":
            if "\\" in pattern:
                # No escape allowed on Windows.
                continue
            pattern = os.path.normpath(pattern)
            text = os.path.normpath(text)

        with pytest.raises(error) if error else contextlib.nullcontext():
            assert FilePatternMatcher(pattern)._matches(text) is expected


def __helper_get_file_paths(tmp_path: Path) -> list[Path]:
    file_paths = []
    for root, _, files in os.walk(tmp_path):
        for file in files:
            file_paths.append(Path(os.path.join(root, file)))
    return file_paths


def test_against_paths(tmp_path_with_content):
    tmp_path = tmp_path_with_content
    file_paths = __helper_get_file_paths(tmp_path)

    # match everything that's not ignored
    lff = FilePatternMatcher("**/*", "!**/module")

    for file_path in file_paths:
        if "module" in str(file_path):
            assert not lff(file_path)
        else:
            assert lff(file_path)

    lff = FilePatternMatcher("**/*.py")

    for file_path in file_paths:
        if str(file_path).endswith(".py"):
            assert lff(file_path)
        else:
            assert not lff(file_path)


def test_empty_patterns(tmp_path_with_content):
    tmp_path = tmp_path_with_content
    file_paths = __helper_get_file_paths(tmp_path)
    lff = FilePatternMatcher()

    for file_path in file_paths:
        assert not lff(file_path)


def test_invert_patterns(tmp_path_with_content):
    tmp_path = tmp_path_with_content
    file_paths = __helper_get_file_paths(tmp_path)

    # match everything that's not ignored
    lff = ~FilePatternMatcher("**/*", "!**/module")

    for file_path in file_paths:
        if "module" in str(file_path):
            assert lff(file_path)
        else:
            assert not lff(file_path)

    # empty patterns should match nothing
    # inverted empty patterns should match everything
    lff = ~FilePatternMatcher()
    for file_path in file_paths:
        assert lff(file_path)

    # single negative pattern should match nothing
    lff = FilePatternMatcher("!**/*.txt")
    for file_path in file_paths:
        assert not lff(file_path)


@pytest.mark.usefixtures("tmp_cwd")
@pytest.mark.parametrize("as_type", [Path, str])
def test_from_file(as_type):
    rel_top_dir = Path("top")
    rel_top_dir.mkdir()
    ignore_file = rel_top_dir / "pattern_file"
    ignore_file.write_text("**/*.txt")

    lff = FilePatternMatcher.from_file(as_type(ignore_file))
    assert lff.can_prune_directories()
    assert lff(Path("top/data.txt"))
    assert not lff(Path("top/data.py"))


@pytest.mark.parametrize(
    "patterns,expected",
    [
        (("*.tmp", "node_modules", "dist/**"), True),
        (("**", "!*.py"), False),
        (("node_modules", "!node_modules/keep.txt"), False),
        ((), True),
    ],
)
def test_can_prune_directories(patterns, expected):
    matcher = FilePatternMatcher(*patterns)
    assert matcher.can_prune_directories() is expected


def test_can_prune_directories_negated():
    matcher_negated = ~FilePatternMatcher("*.py")
    assert matcher_negated.can_prune_directories() is False



================================================
FILE: test/flash_test.py
================================================
# Copyright Modal Labs 2025
# pyright: reportMissingImports=false
import asyncio
import os
import pytest
from types import MethodType
from unittest.mock import AsyncMock, MagicMock, patch
from urllib.parse import urlparse

from modal.experimental.flash import _FlashManager, _FlashPrometheusAutoscaler


class _DummyContainer:
    def __init__(self, host: str):
        self.host = host
        self.port = 443
        self.task_id = f"task-{host}"  # Add missing task_id attribute


class _DummySample:
    def __init__(self, value: float):
        self.value = value


class TestFlashAutoscalerLogic:
    @pytest.fixture
    def autoscaler(self, client):
        with patch("aiohttp.ClientSession"):
            return _FlashPrometheusAutoscaler(
                client=client,
                app_name="test_app",
                cls_name="test_cls",
                metrics_endpoint="metrics",
                target_metric="test_metric",
                target_metric_value=10,
                min_containers=None,
                max_containers=None,
                scale_up_tolerance=0.1,
                scale_down_tolerance=0.1,
                scale_up_stabilization_window_seconds=0,
                scale_down_stabilization_window_seconds=300,
                autoscaling_interval_seconds=15,
                buffer_containers=None,
            )

    def _make_autoscaler(self, autoscaler: _FlashPrometheusAutoscaler, metrics_by_host: dict[str, float]):
        async def _get_all_containers(_self):
            return [_DummyContainer(h) for h in metrics_by_host.keys()]

        async def _get_metrics(_self, url: str):
            host = urlparse(url).hostname or ""
            value = metrics_by_host.get(host, None)
            if value is None:
                return None
            return {"test_metric": [_DummySample(value)]}

        autoscaler._get_all_containers = MethodType(_get_all_containers, autoscaler)  # type: ignore
        autoscaler._get_metrics = MethodType(_get_metrics, autoscaler)  # type: ignore
        return autoscaler

    @pytest.mark.asyncio
    @pytest.mark.parametrize(
        "metrics_by_host,current_replicas,overprovision_containers,expected_replicas",
        [
            # No containers discovered; current_replicas == 0 should return 1
            ({}, 0, None, 1),
            # 3 containers, all emit value 15 with target 10 -> avg=15 -> ratio=1.5 -> ceil(3*1.5)=5
            ({"h1": 15.0, "h2": 15.0, "h3": 15.0}, 3, None, 5),
            # 4 containers, all emit value 5 with target 10 -> avg=5 -> ratio=0.5 -> ceil(4*0.5)=2
            ({"h1": 5.0, "h2": 5.0, "h3": 5.0, "h4": 5.0}, 4, None, 2),
            # 3 containers, only one emits 10 (target), two unhealthy -> both assumed at target for scale-up calc
            ({"h1": 10.0, "h2": None, "h3": None}, 3, None, 5),
            # current_replicas (1) < discoverable containers (3) -> adjusted up to 3; metrics equal target -> stay at 3
            ({"h1": 10.0, "h2": 10.0, "h3": 10.0}, 1, None, 3),
            # Overprovision reduces denominator in scale-up avg: 3 containers at 15,
            # overprov=1 -> (45)/(3-1)=22.5 -> ratio=2.25 -> ceil((3-1)*2.25)=5
            ({"h1": 15.0, "h2": 15.0, "h3": 15.0}, 3, 1, 5),
            # All below target triggers scale-down; overprovision should not change scale-down computation
            ({"h1": 5.0, "h2": 5.0, "h3": 5.0, "h4": 5.0}, 4, 2, 2),
            # Overprovision greater than discoverable containers -> denominator would be <= 0 -> floored to 1
            ({"h1": 15.0}, 1, 5, 2),
            # One healthy at target, one unhealthy; with overprovision=1 denominator becomes 1, leading to scale-up.
            ({"h1": 10.0, "h2": None}, 2, 1, 3),
            # 5 containers, h2 and h4 unhealthy, others at 15.0
            ({"h1": 15.0, "h2": None, "h3": 15.0, "h4": None, "h5": 15.0}, 5, None, 8),
            # 5 containers, mixed metrics and some unhealthy
            ({"h1": 12.0, "h2": 8.0, "h3": 15.0, "h4": None, "h5": None}, 5, None, 7),
            # 4 containers all with the same metric value
            ({"h1": 5.0, "h2": 5.0, "h3": 5.0, "h4": 5.0}, 4, None, 2),
            # Custom target values, tolerances, and overprovision settings
            ({"h1": 25.0, "h2": 30.0, "h3": None}, 3, 1, 8),
            # All unhealthy, 20 current replicas, 17 provisioned, target_metric_value=20.0
            (
                {f"h{i}": None for i in range(1, 21)},
                20,
                1,
                33,
            ),
            # 13 containers, 0 healthy, 9 unhealthy, 1 missing, 3 buffer
            # expected number should just be > 10
            (
                {f"h{i}": None for i in range(1, 10)},
                13,
                3,
                15,
            ),
        ],
    )
    async def test_metric_scaling(
        self,
        metrics_by_host,
        current_replicas,
        overprovision_containers,
        expected_replicas,
        autoscaler,
    ):
        autoscaler = self._make_autoscaler(autoscaler, metrics_by_host)

        if overprovision_containers is not None:
            autoscaler.buffer_containers = overprovision_containers

        result = await autoscaler._compute_target_containers(current_replicas=current_replicas)
        assert result == expected_replicas


_MAX_FAILURES = 10


class TestFlashInternalMetricAutoscalerLogic:
    @pytest.fixture
    def autoscaler(self, client):
        """Single autoscaler instance shared across all tests."""
        with patch("aiohttp.ClientSession"):
            return _FlashPrometheusAutoscaler(
                client=client,
                app_name="test_app",
                cls_name="test_cls",
                metrics_endpoint="internal",
                target_metric="cpu_usage_percent",
                target_metric_value=0.5,
                min_containers=None,
                max_containers=None,
                scale_up_tolerance=0.1,
                scale_down_tolerance=0.1,
                scale_up_stabilization_window_seconds=0,
                scale_down_stabilization_window_seconds=300,
                autoscaling_interval_seconds=15,
                buffer_containers=None,
            )

    @pytest.mark.asyncio
    @pytest.mark.parametrize(
        "flash_metric,target_value,metrics,current_replicas,expected_replicas",
        [
            # Case 1: High CPU single container
            ("cpu_usage_percent", 0.5, [0.8], 1, 2),
            # Case 2: Low CPU multiple containers
            ("cpu_usage_percent", 0.5, [0.1, 0.05, 0.05], 2, 1),
            # Case 3: Low CPU single container with missing containers
            ("cpu_usage_percent", 0.5, [0.2], 3, 3),
            # Case 4: High CPU multiple containers
            ("cpu_usage_percent", 0.5, [0.6, 0.7, 0.8, 0.9], 5, 6),
            # Case 5: High memory usage
            ("memory_usage_percent", 0.6, [0.9], 2, 2),
            # Case 6: Low memory usage within tolerance
            ("memory_usage_percent", 0.6, [0.52], 2, 2),
        ],
    )
    async def test_metric_scaling(
        self,
        autoscaler,
        flash_metric,
        target_value,
        metrics,
        current_replicas,
        expected_replicas,
    ):
        autoscaler.target_metric = flash_metric
        autoscaler.target_metric_value = target_value

        mock_containers = [MagicMock() for _ in range(len(metrics))]
        autoscaler._get_all_containers = AsyncMock(return_value=mock_containers)

        autoscaler._get_container_metrics = AsyncMock(
            side_effect=[MagicMock(metrics=MagicMock(**{flash_metric: value})) for value in metrics]
        )

        result = await autoscaler._compute_target_containers(current_replicas=current_replicas)
        assert result == expected_replicas

    @pytest.mark.asyncio
    async def test_no_metrics_returns_current(self, autoscaler):
        mock_container = MagicMock()
        mock_container.id = "container_1"

        autoscaler._get_all_containers = AsyncMock(return_value=[mock_container])
        autoscaler._get_container_metrics = AsyncMock(return_value=None)

        result = await autoscaler._compute_target_containers(current_replicas=3)
        assert result == 3


class TestFlashManagerStopping:
    @pytest.fixture
    def mock_tunnel_manager(self):
        """Mock the tunnel manager async context manager."""
        mock_tunnel_manager = MagicMock()
        mock_tunnel = MagicMock()
        mock_tunnel.url = "https://test.modal.test"
        mock_tunnel_manager.__aenter__ = AsyncMock(return_value=mock_tunnel)
        mock_tunnel_manager.__aexit__ = AsyncMock()
        return mock_tunnel_manager

    @pytest.fixture
    def flash_manager(self, client, mock_tunnel_manager):
        """Create a FlashManager with mocked dependencies."""
        with (
            patch.dict(os.environ, {"MODAL_TASK_ID": "test-task-123"}),
            patch(
                "modal.experimental.flash._forward_tunnel",
                return_value=mock_tunnel_manager,
            ),
        ):
            manager = _FlashManager(client=client, port=8000)
            return manager

    @pytest.mark.asyncio
    async def test_heartbeat_failure_increments_counter(self, flash_manager):
        """Test that heartbeat failures properly increment the failure counter."""

        flash_manager.tunnel = MagicMock()
        flash_manager.tunnel.url = "https://test.modal.test"
        flash_manager.client.stub.FlashContainerRegister = AsyncMock()
        flash_manager.client.stub.FlashContainerDeregister = AsyncMock()
        flash_manager.is_port_connection_healthy = AsyncMock(
            return_value=(False, Exception("Persistent network error"))
        )

        heartbeat_task = asyncio.create_task(flash_manager._run_heartbeat("test.modal.test", 443))
        await asyncio.sleep(1)
        try:
            heartbeat_task.cancel()
            await heartbeat_task
        except asyncio.CancelledError:
            pass  # Expected when task is cancelled

        # Check that failures were recorded
        assert flash_manager.num_failures > 0

    @pytest.mark.asyncio
    async def test_heartbeat_success_resets_counter(self, flash_manager):
        """Test that heartbeat failures properly increment the failure counter."""

        flash_manager.tunnel = MagicMock()
        flash_manager.tunnel.url = "https://test.modal.test"
        flash_manager.client.stub.FlashContainerRegister = AsyncMock()
        flash_manager.client.stub.FlashContainerDeregister = AsyncMock()
        flash_manager.is_port_connection_healthy = AsyncMock(return_value=(True, None))

        heartbeat_task = asyncio.create_task(flash_manager._run_heartbeat("test.modal.test", 443))
        await asyncio.sleep(1)
        try:
            heartbeat_task.cancel()
            await heartbeat_task
        except asyncio.CancelledError:
            pass  # Expected when task is cancelled

        # Check that failures were recorded
        assert flash_manager.num_failures == 0

    @pytest.mark.asyncio
    async def test_heartbeat_triggers_failure(self, flash_manager):
        """Test that heartbeat failures properly increment the failure counter."""

        flash_manager.tunnel = MagicMock()
        flash_manager.client.stub.FlashContainerRegister = AsyncMock()
        flash_manager.client.stub.FlashContainerDeregister = AsyncMock()
        heartbeat_task = asyncio.create_task(flash_manager._run_heartbeat("test.modal.test", 443))
        drain_task = asyncio.create_task(flash_manager._drain_container())

        with patch.object(flash_manager, "stop", new_callable=AsyncMock) as mock_stop:
            for i in range(_MAX_FAILURES, _MAX_FAILURES + 2):
                flash_manager.num_failures = i
                if i <= _MAX_FAILURES:
                    assert flash_manager.num_failures == i
                    assert mock_stop.call_count == 0
                else:
                    assert flash_manager.num_failures > _MAX_FAILURES
                await asyncio.sleep(1)
        assert mock_stop.call_count == 1

        try:
            heartbeat_task.cancel()
            drain_task.cancel()
            await heartbeat_task
            await drain_task
        except asyncio.CancelledError:
            pass  # Expected when task is cancelled

        # Check that failures were recorded
        assert flash_manager.num_failures > 0

    @pytest.mark.asyncio
    async def test_full_failure_and_stop_integration(self, flash_manager):
        """Test the full integration: failures -> drain -> stop."""

        with (
            patch.object(flash_manager, "stop", new_callable=AsyncMock) as mock_stop,
        ):
            # Set up mocks
            flash_manager.tunnel = MagicMock()
            flash_manager.tunnel.url = "https://test.modal.test"

            # Mock HTTP client to always fail
            flash_manager.is_port_connection_healthy = AsyncMock(
                return_value=(False, Exception("Persistent network error"))
            )

            # Mock client stub methods
            flash_manager.client.stub.FlashContainerRegister = AsyncMock()
            flash_manager.client.stub.FlashContainerDeregister = AsyncMock()
            flash_manager.client.stub.ContainerStop = AsyncMock()

            flash_manager.num_failures = _MAX_FAILURES

            # Start both background tasks
            heartbeat_task = asyncio.create_task(flash_manager._run_heartbeat("test.modal.test", 443))
            drain_task = asyncio.create_task(flash_manager._drain_container())

            await asyncio.sleep(1)

            heartbeat_task.cancel()
            drain_task.cancel()

            try:
                await heartbeat_task
            except asyncio.CancelledError:
                pass

            try:
                await drain_task
            except asyncio.CancelledError:
                pass

            assert flash_manager.num_failures > _MAX_FAILURES, (
                f"Expected > {_MAX_FAILURES} failures, got {flash_manager.num_failures}"
            )

            mock_stop.assert_called_once()



================================================
FILE: test/fork_test.py
================================================
# Copyright Modal Labs 2024
import pytest
import subprocess
import sys

from test.supports.skip import skip_windows


@skip_windows("fork not supported on windows")
@pytest.mark.parametrize(
    "test_case", ["test_stub_method", "test_stub_reference", "test_default_stub", "test_default_stub_reference"]
)
def test_process_fork(supports_dir, server_url_env, token_env, test_case):
    output = subprocess.check_output(
        [sys.executable, supports_dir / "forking.py", test_case], timeout=4, encoding="utf8"
    )

    success_pids = {int(x) for x in output.split()}
    assert len(success_pids) == 2



================================================
FILE: test/function_retry_test.py
================================================
# Copyright Modal Labs 2024
import pytest

import modal
from modal import App
from modal.exception import RemoteError
from modal.retries import RetryManager
from modal_proto import api_pb2

function_call_count = 0


@pytest.fixture(autouse=True)
def reset_function_call_count(monkeypatch):
    # Set default retry delay to something small so we don't slow down tests
    monkeypatch.setattr("modal.retries.MIN_INPUT_RETRY_DELAY_MS", 0.00001)
    global function_call_count
    function_call_count = 0


class FunctionCallCountException(Exception):
    """
    An exception which lets us report to the test how many times a function was called.
    """

    def __init__(self, function_call_count):
        self.function_call_count = function_call_count


def counting_function(return_success_on_retry_count: int):
    """
    A function that updates the global function_call_count counter each time it is called.

    """
    global function_call_count
    function_call_count += 1
    if function_call_count < return_success_on_retry_count:
        raise FunctionCallCountException(function_call_count)
    return function_call_count


@pytest.fixture
def setup_app_and_function(servicer):
    app = App()
    servicer.function_body(counting_function)
    retries = modal.Retries(
        max_retries=3,
        backoff_coefficient=1.0,
        initial_delay=0,
    )
    f = app.function(retries=retries)(counting_function)
    return app, f


def fetch_input_plane_request_counts(ctx):
    retried_requests = 0
    first_time_requests = 0
    for request in ctx.get_requests("MapStartOrContinue"):
        for item in request.items:
            if item.attempt_token == "":
                first_time_requests += 1
            else:
                retried_requests += 1
    return first_time_requests, retried_requests


@pytest.fixture
def setup_app_and_function_inputplane(servicer):
    app = App()
    servicer.function_body(counting_function)
    f = app.function(experimental_options={"input_plane_region": "us-east"}, retries=modal.Retries(max_retries=3))(
        counting_function
    )
    return app, f


def test_all_retries_fail_raises_error(client, setup_app_and_function, servicer):
    servicer.sync_client_retries_enabled = True
    app, f = setup_app_and_function
    with app.run(client=client):
        with pytest.raises(FunctionCallCountException) as exc_info:
            # The client should give up after the 4th call.
            f.remote(5)
        # Assert the function was called 4 times - the original call plus 3 retries
        assert exc_info.value.function_call_count == 4


def test_failures_followed_by_success(client, setup_app_and_function, servicer):
    servicer.sync_client_retries_enabled = True
    app, f = setup_app_and_function
    with servicer.intercept() as ctx:
        with app.run(client=client):
            function_call_count = f.remote(3)
            assert function_call_count == 3

    assert len(ctx.get_requests("FunctionRetryInputs")) == 2


def test_no_retries_when_first_call_succeeds(client, setup_app_and_function, servicer):
    servicer.sync_client_retries_enabled = True
    app, f = setup_app_and_function
    with app.run(client=client):
        function_call_count = f.remote(1)
        assert function_call_count == 1


def test_no_retries_when_call_cancelled(client, setup_app_and_function, servicer):
    servicer.sync_client_retries_enabled = True
    app, f = setup_app_and_function
    with servicer.intercept() as ctx:
        ctx.add_response(
            "FunctionGetOutputs",
            api_pb2.FunctionGetOutputsResponse(
                outputs=[
                    api_pb2.FunctionGetOutputsItem(
                        result=api_pb2.GenericResult(
                            status=api_pb2.GenericResult.GENERIC_STATUS_TERMINATED, exception="cancelled"
                        ),
                    )
                ]
            ),
        )

        with app.run(client=client):
            with pytest.raises(RemoteError, match="cancelled"):
                f.remote(1)

        assert not ctx.get_requests("FunctionRetryInputs")  # no retries


def test_no_retries_when_client_retries_disabled(client, setup_app_and_function, servicer):
    servicer.sync_client_retries_enabled = False
    app, f = setup_app_and_function
    with app.run(client=client):
        with pytest.raises(FunctionCallCountException) as exc_info:
            f.remote(2)
        assert exc_info.value.function_call_count == 1


def test_retry_delay_ms():
    with pytest.raises(ValueError):
        RetryManager._retry_delay_ms(0, api_pb2.FunctionRetryPolicy())

    retry_policy = api_pb2.FunctionRetryPolicy(retries=2, backoff_coefficient=3, initial_delay_ms=2000)
    assert RetryManager._retry_delay_ms(1, retry_policy) == 2000

    retry_policy = api_pb2.FunctionRetryPolicy(retries=2, backoff_coefficient=3, initial_delay_ms=2000)
    assert RetryManager._retry_delay_ms(2, retry_policy) == 6000


def test_lost_inputs_retried(client, setup_app_and_function, servicer):
    servicer.sync_client_retries_enabled = True
    app, f = setup_app_and_function
    # The client should retry if it receives a internal failure status.
    servicer.failure_status = api_pb2.GenericResult.GENERIC_STATUS_INTERNAL_FAILURE

    with app.run(client=client):
        f.remote(10)
        # Assert the function was called 10 times
        assert function_call_count == 10


def test_map_fails_immediately_without_retries(client, setup_app_and_function, servicer):
    servicer.sync_client_retries_enabled = False
    app, f = setup_app_and_function
    with app.run(client=client):
        with pytest.raises(FunctionCallCountException) as exc_info:
            list(f.map([999, 999, 999]))
        assert exc_info.value.function_call_count == 1


def test_map_all_retries_fail_raises_error(client, setup_app_and_function, servicer):
    servicer.sync_client_retries_enabled = True
    app, f = setup_app_and_function
    with servicer.intercept() as ctx:
        with app.run(client=client):
            with pytest.raises(FunctionCallCountException) as exc_info:
                list(f.map([999]))
            assert exc_info.value.function_call_count == 4
    assert len(ctx.get_requests("FunctionRetryInputs")) == 3


def test_map_all_retries_fail_raises_error_inputplane(client, setup_app_and_function_inputplane, servicer):
    servicer.sync_client_retries_enabled = True
    app, f = setup_app_and_function_inputplane
    with servicer.intercept() as ctx:
        with app.run(client=client):
            with pytest.raises(FunctionCallCountException) as exc_info:
                list(f.map([999]))
            assert exc_info.value.function_call_count == 4

    first_time_requests, retried_requests = fetch_input_plane_request_counts(ctx)
    assert first_time_requests == 1
    assert retried_requests == 3


def test_map_failures_followed_by_success(client, setup_app_and_function, servicer):
    servicer.sync_client_retries_enabled = True
    app, f = setup_app_and_function
    with servicer.intercept() as ctx:
        with app.run(client=client):
            results = list(f.map([3, 3, 3]))
            assert set(results) == {3, 4, 5}

    assert len(ctx.get_requests("FunctionRetryInputs")) == 2


def test_map_failures_followed_by_success_inputplane(client, setup_app_and_function_inputplane, servicer):
    servicer.sync_client_retries_enabled = True
    app, f = setup_app_and_function_inputplane
    with servicer.intercept() as ctx:
        with app.run(client=client):
            results = list(f.map([3, 3, 3]))
            assert set(results) == {3, 4, 5}

    first_time_requests, retried_requests = fetch_input_plane_request_counts(ctx)
    assert first_time_requests == 3
    assert retried_requests == 2


def test_map_no_retries_when_first_call_succeeds(client, setup_app_and_function, servicer):
    servicer.sync_client_retries_enabled = True
    app, f = setup_app_and_function
    with app.run(client=client):
        results = list(f.map([1, 1, 1]))
        assert set(results) == {1, 2, 3}


def test_map_no_retries_when_first_call_succeeds_inputplane(client, setup_app_and_function_inputplane, servicer):
    servicer.sync_client_retries_enabled = True
    app, f = setup_app_and_function_inputplane
    with servicer.intercept() as ctx:
        with app.run(client=client):
            results = list(f.map([1, 1, 1]))
            assert set(results) == {1, 2, 3}

    first_time_requests, retried_requests = fetch_input_plane_request_counts(ctx)
    assert first_time_requests == 3
    assert retried_requests == 0


def test_map_lost_inputs_retried(client, setup_app_and_function, servicer):
    servicer.sync_client_retries_enabled = True
    app, f = setup_app_and_function
    # The client should retry if it receives a internal failure status.
    servicer.failure_status = api_pb2.GenericResult.GENERIC_STATUS_INTERNAL_FAILURE

    with app.run(client=client):
        results = list(f.map([3, 3, 3]))
        assert set(results) == {3, 4, 5}


def test_map_lost_inputs_retried_inputplane(client, setup_app_and_function_inputplane, servicer):
    servicer.sync_client_retries_enabled = True
    app, f = setup_app_and_function_inputplane
    # The client should retry if it receives a internal failure status.
    servicer.failure_status = api_pb2.GenericResult.GENERIC_STATUS_INTERNAL_FAILURE

    with app.run(client=client):
        results = list(f.map([3, 3, 3]))
        assert set(results) == {3, 4, 5}


def test_map_cancelled_inputs_not_retried(client, setup_app_and_function, servicer):
    servicer.sync_client_retries_enabled = True
    app, f = setup_app_and_function
    # The client should retry if it receives a internal failure status.
    servicer.failure_status = api_pb2.GenericResult.GENERIC_STATUS_INTERNAL_FAILURE

    with servicer.intercept() as ctx:

        async def FunctionGetOutputs(servicer, stream):
            # don't send response until an input arrives - otherwise it could cause a race
            await servicer.function_call_inputs_update_event.wait()
            await stream.send_message(
                api_pb2.FunctionGetOutputsResponse(
                    outputs=[
                        api_pb2.FunctionGetOutputsItem(
                            result=api_pb2.GenericResult(
                                status=api_pb2.GenericResult.GENERIC_STATUS_TERMINATED, exception="cancelled"
                            ),
                        )
                    ]
                )
            )

        ctx.set_responder("FunctionGetOutputs", FunctionGetOutputs)

        with app.run(client=client):
            with pytest.raises(RemoteError, match="cancelled"):
                list(f.map([3, 3, 3]))

        assert ctx.get_requests("FunctionRetryInputs") == []


def test_map_cancelled_inputs_not_retried_inputplane(client, setup_app_and_function_inputplane, servicer):
    servicer.sync_client_retries_enabled = True
    app, f = setup_app_and_function_inputplane
    # The client should retry if it receives a internal failure status.
    servicer.failure_status = api_pb2.GenericResult.GENERIC_STATUS_INTERNAL_FAILURE

    with servicer.intercept() as ctx:

        async def MapAwait(servicer, stream):
            # don't send response until an input arrives - otherwise it could cause a race
            await servicer.function_call_inputs_update_event.wait()
            await stream.send_message(
                api_pb2.MapAwaitResponse(
                    outputs=[
                        api_pb2.FunctionGetOutputsItem(
                            idx=1,
                            result=api_pb2.GenericResult(
                                status=api_pb2.GenericResult.GENERIC_STATUS_TERMINATED, exception="cancelled"
                            ),
                        )
                    ],
                    last_entry_id="1",
                )
            )

        ctx.set_responder("MapAwait", MapAwait)

        with app.run(client=client):
            with pytest.raises(RemoteError, match="cancelled"):
                list(f.map([3, 3, 3]))

        _, retried_requests = fetch_input_plane_request_counts(ctx)
        assert retried_requests == 0



================================================
FILE: test/function_serialization_test.py
================================================
# Copyright Modal Labs 2023
import pytest

from modal import App
from modal._serialization import deserialize


@pytest.mark.asyncio
async def test_serialize_deserialize_function(servicer, client):
    app = App()

    @app.function(serialized=True, name="foo")
    def foo():
        2 * foo.remote()

    assert not foo.is_hydrated
    with pytest.raises(Exception):
        foo.object_id  # noqa

    with app.run(client=client):
        object_id = foo.object_id

    assert object_id is not None
    assert {object_id} == servicer.precreated_functions

    foo_def = servicer.app_functions[object_id]

    assert len(servicer.function_call_inputs) == 0

    deserialized_function_body = deserialize(foo_def.function_serialized, client)
    deserialized_function_body()  # call locally as if in container, this should trigger a "remote" foo() call
    assert len(servicer.function_call_inputs) == 1
    function_call_id = list(servicer.function_call_inputs.keys())[0]
    assert servicer.function_id_for_function_call[function_call_id] == object_id



================================================
FILE: test/function_utils_test.py
================================================
# Copyright Modal Labs 2023
import functools
import pytest
import time

from grpclib import Status

from modal import fastapi_endpoint, method
from modal._serialization import serialize_data_format
from modal._utils import async_utils
from modal._utils.function_utils import (
    FunctionInfo,
    _stream_function_call_data,
    callable_has_non_self_non_default_params,
    callable_has_non_self_params,
)
from modal_proto import api_pb2

GLOBAL_VARIABLE = "whatever"


def hasarg(a): ...


def noarg(): ...


def defaultarg(a="hello"): ...


def wildcard_args(*wildcard_list, **wildcard_dict): ...


def test_is_nullary():
    assert not FunctionInfo(hasarg).is_nullary()
    assert FunctionInfo(noarg).is_nullary()
    assert FunctionInfo(defaultarg).is_nullary()
    assert FunctionInfo(wildcard_args).is_nullary()


class Cls:
    def f1(self):
        pass

    def f2(self, x):
        pass

    def f3(self, *args):
        pass

    def f4(self, x=1):
        pass


def f5():
    pass


def f6(x):
    pass


def f7(x=1):
    pass


def test_callable_has_non_self_params():
    assert not callable_has_non_self_params(Cls.f1)
    assert not callable_has_non_self_params(Cls().f1)
    assert callable_has_non_self_params(Cls.f2)
    assert callable_has_non_self_params(Cls().f2)
    assert callable_has_non_self_params(Cls.f3)
    assert callable_has_non_self_params(Cls().f3)
    assert callable_has_non_self_params(Cls.f4)
    assert callable_has_non_self_params(Cls().f4)
    assert not callable_has_non_self_params(f5)
    assert callable_has_non_self_params(f6)
    assert callable_has_non_self_params(f7)


def test_callable_has_non_self_non_default_params():
    assert not callable_has_non_self_non_default_params(Cls.f1)
    assert not callable_has_non_self_non_default_params(Cls().f1)
    assert callable_has_non_self_non_default_params(Cls.f2)
    assert callable_has_non_self_non_default_params(Cls().f2)
    assert callable_has_non_self_non_default_params(Cls.f3)
    assert callable_has_non_self_non_default_params(Cls().f3)
    assert not callable_has_non_self_non_default_params(Cls.f4)
    assert not callable_has_non_self_non_default_params(Cls().f4)
    assert not callable_has_non_self_non_default_params(f5)
    assert callable_has_non_self_non_default_params(f6)
    assert not callable_has_non_self_non_default_params(f7)


class Foo:
    def __init__(self):
        pass

    @method()
    def bar(self):
        return "hello"

    @fastapi_endpoint()
    def web(self):
        pass


# run test on same event loop as servicer
@async_utils.synchronize_api
async def test_stream_function_call_data(servicer, client):
    req = api_pb2.FunctionCallPutDataRequest(
        function_call_id="fc-bar",
        data_chunks=[
            api_pb2.DataChunk(
                data_format=api_pb2.DATA_FORMAT_PICKLE,
                data=serialize_data_format("hello", api_pb2.DATA_FORMAT_PICKLE),
                index=1,
            ),
            api_pb2.DataChunk(
                data_format=api_pb2.DATA_FORMAT_PICKLE,
                data=serialize_data_format("world", api_pb2.DATA_FORMAT_PICKLE),
                index=2,
            ),
        ],
    )
    await client.stub.FunctionCallPutDataOut(req)

    t0 = time.time()
    gen = _stream_function_call_data(client, None, "fc-bar", variant="data_out")
    servicer.fail_get_data_out = [Status.INTERNAL] * 3
    assert await gen.__anext__() == "hello"
    elapsed = time.time() - t0
    assert 0.111 <= elapsed < 1.0

    assert await gen.__anext__() == "world"


def decorator(f):
    @functools.wraps(f)
    def wrapper(*args, **kwargs):
        return f

    return wrapper


def has_global_ref():
    assert GLOBAL_VARIABLE


@pytest.mark.parametrize("func", [has_global_ref, decorator(has_global_ref)])
def test_global_variable_extraction(func):
    info = FunctionInfo(func)
    assert info.get_globals().get("GLOBAL_VARIABLE") == GLOBAL_VARIABLE



================================================
FILE: test/git_utils_test.py
================================================
# Copyright Modal Labs 2025
import asyncio
import pytest
import sys
from unittest import mock

from modal._utils.git_utils import get_git_commit_info, is_valid_commit_info, run_command_fallible
from modal_proto import api_pb2


@mock.patch("asyncio.create_subprocess_exec")
@pytest.mark.asyncio
async def test_run_command_fallible_success_mocked(mock_subprocess):
    mock_process = mock.AsyncMock()
    mock_process.communicate.return_value = (b"test output", b"")
    mock_process.returncode = 0
    mock_subprocess.return_value = mock_process

    result = await run_command_fallible(["git", "status"])

    assert result == "test output"
    mock_subprocess.assert_called_once_with(
        "git", "status", stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE
    )


@mock.patch("asyncio.create_subprocess_exec")
@pytest.mark.asyncio
async def test_run_command_fallible_failure(mock_subprocess):
    mock_process = mock.AsyncMock()
    mock_process.communicate.return_value = (b"", b"error message")
    mock_process.returncode = 1
    mock_subprocess.return_value = mock_process

    result = await run_command_fallible(["git", "status"])

    assert result is None


@mock.patch("asyncio.create_subprocess_exec")
@pytest.mark.asyncio
async def test_run_command_fallible_exception(mock_subprocess):
    mock_subprocess.side_effect = Exception("Command failed")

    result = await run_command_fallible(["git", "status"])

    assert result is None


@pytest.mark.skipif(sys.platform == "win32", reason="Skipping on Windows: feel free to fix if you can")
@pytest.mark.asyncio
async def test_run_command_fallible_success_real():
    result = await run_command_fallible(["echo", "hello world"])

    assert result == "hello world"


@pytest.mark.asyncio
async def test_run_command_fallible_unknown_command_real():
    result = await run_command_fallible(["command_that_does_not_exist"])

    assert result is None


def test_is_valid_commit_info():
    valid_commit = api_pb2.CommitInfo(
        vcs="git",
        commit_hash="0123456789abcdef0123456789abcdef01234567",
        branch="main",
        repo_url="https://github.com/modal-labs/modal-client.git",
        author_name="Test User",
        author_email="test@example.com",
    )
    is_valid, error_message = is_valid_commit_info(valid_commit)
    assert is_valid is True
    assert error_message == ""

    invalid_hash = api_pb2.CommitInfo(
        vcs="git",
        commit_hash="0123456789",  # Too short, should be 40 chars
        branch="main",
        repo_url="https://github.com/modal-labs/modal-client.git",
        author_name="Test User",
        author_email="test@example.com",
    )
    is_valid, error_message = is_valid_commit_info(invalid_hash)
    assert is_valid is False
    assert error_message == "Invalid commit hash"

    long_branch = api_pb2.CommitInfo(
        vcs="git",
        commit_hash="0123456789abcdef0123456789abcdef01234567",
        branch="x" * 256,  # Too long
        repo_url="https://github.com/modal-labs/modal-client.git",
        author_name="Test User",
        author_email="test@example.com",
    )
    is_valid, error_message = is_valid_commit_info(long_branch)
    assert is_valid is False
    assert error_message == "Branch name too long"


def assert_commit_info(result, expected_values):
    """Helper function to assert CommitInfo fields match expected values."""
    assert result is not None
    for field, expected in expected_values.items():
        assert getattr(result, field) == expected, (
            f"Field {field} mismatch: expected {expected}, got {getattr(result, field)}"
        )


valid_hash = "0123456789abcdef0123456789abcdef01234567"


@mock.patch("modal._utils.git_utils.run_command_fallible")
@pytest.mark.asyncio
async def test_get_git_commit_info_success(mock_run_command):
    mock_run_command.side_effect = [
        f"{valid_hash}\n1609459200\nTest User\ntest@example.com",  # git log
        "main",  # git branch
        "",  # git status (clean)
        "git@github.com:modal-labs/modal-client.git",  # git remote
    ]

    result = await get_git_commit_info()

    expected = {
        "vcs": "git",
        "commit_hash": valid_hash,
        "commit_timestamp": 1609459200,
        "author_name": "Test User",
        "author_email": "test@example.com",
        "branch": "main",
        "dirty": False,
        "repo_url": "git@github.com:modal-labs/modal-client.git",
    }
    assert_commit_info(result, expected)


@mock.patch("modal._utils.git_utils.run_command_fallible")
@pytest.mark.asyncio
async def test_get_git_commit_info_dirty_repo(mock_run_command):
    mock_run_command.side_effect = [
        f"{valid_hash}\n1609459200\nTest User\ntest@example.com",
        "main",
        "?? main.py",
        "https://github.com/modal-labs/modal-client.git",
    ]

    result = await get_git_commit_info()

    expected = {
        "vcs": "git",
        "commit_hash": valid_hash,
        "commit_timestamp": 1609459200,
        "author_name": "Test User",
        "author_email": "test@example.com",
        "branch": "main",
        "dirty": True,
        "repo_url": "https://github.com/modal-labs/modal-client.git",
    }
    assert_commit_info(result, expected)


@mock.patch("modal._utils.git_utils.run_command_fallible")
@pytest.mark.asyncio
async def test_get_git_commit_info_missing_remote(mock_run_command):
    mock_run_command.side_effect = [
        f"{valid_hash}\n1609459200\nTest User\ntest@example.com",
        "main",
        "",
        None,  # git remote fails with "error: No such remote 'origin'"
    ]

    result = await get_git_commit_info()

    expected = {
        "vcs": "git",
        "commit_hash": valid_hash,
        "commit_timestamp": 1609459200,
        "author_name": "Test User",
        "author_email": "test@example.com",
        "branch": "main",
        "dirty": False,
        "repo_url": "",
    }
    assert_commit_info(result, expected)


@mock.patch("modal._utils.git_utils.run_command_fallible")
@pytest.mark.asyncio
async def test_get_git_commit_info_missing_author_email(mock_run_command):
    mock_run_command.side_effect = [
        f"{valid_hash}\n1609459200\nTest User\n",
        "main",
        "",
        "https://github.com/modal-labs/modal-client.git",
    ]

    result = await get_git_commit_info()

    expected = {
        "vcs": "git",
        "commit_hash": valid_hash,
        "commit_timestamp": 1609459200,
        "author_name": "Test User",
        "author_email": "",
        "branch": "main",
        "dirty": False,
        "repo_url": "https://github.com/modal-labs/modal-client.git",
    }
    assert_commit_info(result, expected)


@mock.patch("modal._utils.git_utils.run_command_fallible")
@pytest.mark.asyncio
async def test_get_git_commit_info_new_repo(mock_run_command):
    # Responses for a new repository with no commits
    mock_run_command.side_effect = [
        None,  # git log fails with "fatal: ambiguous argument 'HEAD'..."
        None,  # git rev-parse fails with "fatal: ambiguous argument 'HEAD'..."
        "?? main.py",  # Modified file
        None,  # git remote fails with "error: No such remote 'origin'"
    ]

    result = await get_git_commit_info()

    assert result is None


@mock.patch("modal._utils.git_utils.run_command_fallible")
@pytest.mark.asyncio
async def test_get_git_commit_info_long_branch_name(mock_run_command):
    long_branch = "feature/" + "x" * 250

    mock_run_command.side_effect = [
        f"{valid_hash}\n1609459200\nTest User\ntest@example.com",
        long_branch,
        "",
        "https://github.com/modal-labs/modal-client.git",
    ]

    result = await get_git_commit_info()

    assert result is None



================================================
FILE: test/gpu_fallbacks_test.py
================================================
# Copyright Modal Labs 2024
from modal import App
from modal_proto import api_pb2

app = App()


@app.function(gpu=["a10g"])
def f1():
    pass


@app.function(gpu=["a10g", "t4:2"])
def f2():
    pass


@app.function(gpu=["h100:2", "a100-80gb:2"])
def f3():
    pass


def test_gpu_fallback(servicer, client):
    with app.run(client=client):
        assert len(servicer.app_functions) == 3

        a10_1 = api_pb2.Resources(
            gpu_config=api_pb2.GPUConfig(
                gpu_type="A10G",
                count=1,
            )
        )
        t4_2 = api_pb2.Resources(
            gpu_config=api_pb2.GPUConfig(
                gpu_type="T4",
                count=2,
            )
        )
        h100_2 = api_pb2.Resources(
            gpu_config=api_pb2.GPUConfig(
                gpu_type="H100",
                count=2,
            )
        )
        a100_80gb_2 = api_pb2.Resources(
            gpu_config=api_pb2.GPUConfig(type=api_pb2.GPU_TYPE_A100_80GB, count=2, gpu_type="A100-80GB")
        )

        fn1 = servicer.app_functions["fu-1"]  # f1
        assert len(fn1.ranked_functions) == 1
        assert fn1.ranked_functions[0].function.resources.gpu_config.gpu_type == a10_1.gpu_config.gpu_type
        assert fn1.ranked_functions[0].function.resources.gpu_config.count == a10_1.gpu_config.count

        fn2 = servicer.app_functions["fu-2"]  # f2
        assert len(fn2.ranked_functions) == 2
        assert fn2.ranked_functions[0].function.resources.gpu_config.gpu_type == a10_1.gpu_config.gpu_type
        assert fn2.ranked_functions[0].function.resources.gpu_config.count == a10_1.gpu_config.count
        assert fn2.ranked_functions[1].function.resources.gpu_config.gpu_type == t4_2.gpu_config.gpu_type
        assert fn2.ranked_functions[1].function.resources.gpu_config.count == t4_2.gpu_config.count

        fn3 = servicer.app_functions["fu-3"]  # f3
        assert len(fn3.ranked_functions) == 2
        assert fn3.ranked_functions[0].function.resources.gpu_config.gpu_type == h100_2.gpu_config.gpu_type
        assert fn3.ranked_functions[0].function.resources.gpu_config.count == h100_2.gpu_config.count
        assert fn3.ranked_functions[1].function.resources.gpu_config.gpu_type == a100_80gb_2.gpu_config.gpu_type
        assert fn3.ranked_functions[1].function.resources.gpu_config.count == a100_80gb_2.gpu_config.count
        assert fn3.ranked_functions[1].function.resources.gpu_config.gpu_type == a100_80gb_2.gpu_config.gpu_type



================================================
FILE: test/gpu_test.py
================================================
# Copyright Modal Labs 2022
import pytest

import modal.gpu
from modal import App
from modal.exception import InvalidError
from modal_proto import api_pb2


def dummy():
    pass  # not actually used in test (servicer returns sum of square of all args)


def test_gpu_any_function(client, servicer):
    app = App()

    app.function(gpu="any")(dummy)
    with app.run(client=client):
        pass

    assert len(servicer.app_functions) == 1
    func_def = next(iter(servicer.app_functions.values()))
    assert func_def.resources.gpu_config.count == 1


@pytest.mark.parametrize(
    "gpu_arg,gpu_type,count",
    [
        ("A100-40GB", "A100-40GB", 1),
        ("a100-40gb", "A100-40GB", 1),
        ("a10g", "A10G", 1),
        ("t4:7", "T4", 7),
        ("a100-80GB:5", "A100-80GB", 5),
        ("l40s:2", "L40S", 2),
    ],
)
def test_gpu_string_config(client, servicer, gpu_arg, gpu_type, count):
    app = App()

    app.function(gpu=gpu_arg)(dummy)
    with app.run(client=client):
        pass

    assert len(servicer.app_functions) == 1
    func_def = next(iter(servicer.app_functions.values()))
    assert func_def.resources.gpu_config.gpu_type == gpu_type
    assert func_def.resources.gpu_config.count == count


@pytest.mark.parametrize("gpu_arg", ["foo", "a10g:hello", "nonexistent:2"])
def test_invalid_gpu_string_config(client, servicer, gpu_arg):
    app = App()

    # Invalid enum value.
    with pytest.raises(InvalidError):
        app.function(gpu=gpu_arg)(dummy)
        with app.run(client=client):
            pass


def test_gpu_config_function(client, servicer):
    app = App()

    with pytest.warns(match='gpu="A100-40GB"'):
        app.function(gpu=modal.gpu.A100())(dummy)
    with app.run(client=client):
        pass

    assert len(servicer.app_functions) == 1
    func_def = next(iter(servicer.app_functions.values()))
    assert func_def.resources.gpu_config.count == 1


def test_gpu_config_function_more(client, servicer):
    # Make sure some other GPU types also throw warnings
    with pytest.warns(match='gpu="A100-80GB"'):
        modal.gpu.A100(size="80GB")
    with pytest.warns(match='gpu="T4:7"'):
        modal.gpu.T4(count=7)


def test_cloud_provider_selection(client, servicer):
    app = App()

    app.function(gpu="A100", cloud="gcp")(dummy)
    with app.run(client=client):
        pass

    assert len(servicer.app_functions) == 1
    func_def = next(iter(servicer.app_functions.values()))
    assert func_def.cloud_provider == api_pb2.CLOUD_PROVIDER_UNSPECIFIED  # No longer set
    assert func_def.cloud_provider_str == "gcp"

    assert func_def.resources.gpu_config.gpu_type == "A100"
    assert func_def.resources.gpu_config.count == 1


def test_invalid_cloud_provider_selection(client, servicer):
    app = App()

    # Invalid enum value.
    with pytest.raises(InvalidError):
        app.function(cloud="foo")(dummy)
        with app.run(client=client):
            pass


@pytest.mark.parametrize(
    "memory_arg,gpu_type",
    [
        ("40GB", "A100-40GB"),
        ("80GB", "A100-80GB"),
    ],
)
def test_memory_selection_gpu_variant(client, servicer, memory_arg, gpu_type):
    app = App()
    with pytest.warns(match='gpu="A100'):
        app.function(gpu=modal.gpu.A100(size=memory_arg))(dummy)

    with app.run(client=client):
        pass

    func_def = next(iter(servicer.app_functions.values()))

    assert func_def.resources.gpu_config.count == 1
    assert func_def.resources.gpu_config.gpu_type == gpu_type


def test_gpu_unsupported_config():
    app = App()

    with pytest.raises(ValueError, match="size='20GB' is invalid"):
        app.function(gpu=modal.gpu.A100(size="20GB"))(dummy)


@pytest.mark.parametrize("count", [1, 2, 3, 4])
def test_gpu_type_selection_from_count(client, servicer, count):
    app = App()

    # Task type does not change when user asks more than 1 GPU on an A100.
    app.function(gpu=f"A100:{count}")(dummy)
    with app.run(client=client):
        pass

    func_def = next(iter(servicer.app_functions.values()))

    assert func_def.resources.gpu_config.count == count



================================================
FILE: test/grpc_utils_test.py
================================================
# Copyright Modal Labs 2022
import pytest
import time

from grpclib import GRPCError, Status

from modal import __version__
from modal._utils.async_utils import synchronize_api
from modal._utils.grpc_utils import (
    connect_channel,
    create_channel,
    retry_transient_errors,
)
from modal_proto import api_grpc, api_pb2

from .supports.skip import skip_windows_unix_socket


@pytest.mark.asyncio
async def test_http_channel(servicer, credentials):
    token_id, token_secret = credentials
    metadata = {
        "x-modal-client-type": str(api_pb2.CLIENT_TYPE_CLIENT),
        "x-modal-python-version": "3.12.1",
        "x-modal-client-version": __version__,
        "x-modal-token-id": token_id,
        "x-modal-token-secret": token_secret,
    }
    assert servicer.client_addr.startswith("http://")
    channel = create_channel(servicer.client_addr)
    client_stub = api_grpc.ModalClientStub(channel)

    req = api_pb2.BlobCreateRequest()
    resp = await client_stub.BlobCreate(req, metadata=metadata)
    assert len(resp.blob_ids) > 0

    channel.close()


@skip_windows_unix_socket
@pytest.mark.asyncio
async def test_unix_channel(servicer):
    metadata = {
        "x-modal-client-type": str(api_pb2.CLIENT_TYPE_CONTAINER),
        "x-modal-python-version": "3.12.1",
        "x-modal-client-version": __version__,
    }
    assert servicer.container_addr.startswith("unix://")
    channel = create_channel(servicer.container_addr)
    client_stub = api_grpc.ModalClientStub(channel)

    req = api_pb2.BlobCreateRequest()
    resp = await client_stub.BlobCreate(req, metadata=metadata)
    assert len(resp.blob_ids) > 0

    channel.close()


@pytest.mark.asyncio
async def test_http_broken_channel():
    ch = create_channel("https://xyz.invalid")
    with pytest.raises(OSError):
        await connect_channel(ch)


@pytest.mark.asyncio
async def test_retry_transient_errors(servicer, client):
    client_stub = client.stub

    @synchronize_api
    async def wrapped_blob_create(req, **kwargs):
        return await retry_transient_errors(client_stub.BlobCreate, req, **kwargs)

    # Use the BlobCreate request for retries
    req = api_pb2.BlobCreateRequest()

    # Fail 3 times -> should still succeed
    servicer.fail_blob_create = [Status.UNAVAILABLE] * 3
    await wrapped_blob_create.aio(req)
    assert servicer.blob_create_metadata.get("x-idempotency-key")
    assert servicer.blob_create_metadata.get("x-retry-attempt") == "3"

    # Fail 4 times -> should fail
    servicer.fail_blob_create = [Status.UNAVAILABLE] * 4
    with pytest.raises(GRPCError):
        await wrapped_blob_create.aio(req)
    assert servicer.blob_create_metadata.get("x-idempotency-key")
    assert servicer.blob_create_metadata.get("x-retry-attempt") == "3"

    # Fail 5 times, but set max_retries to infinity
    servicer.fail_blob_create = [Status.UNAVAILABLE] * 5
    assert await wrapped_blob_create.aio(req, max_retries=None, base_delay=0)
    assert servicer.blob_create_metadata.get("x-idempotency-key")
    assert servicer.blob_create_metadata.get("x-retry-attempt") == "5"

    # Not a transient error.
    servicer.fail_blob_create = [Status.PERMISSION_DENIED]
    with pytest.raises(GRPCError):
        assert await wrapped_blob_create.aio(req, max_retries=None, base_delay=0)
    assert servicer.blob_create_metadata.get("x-idempotency-key")
    assert servicer.blob_create_metadata.get("x-retry-attempt") == "0"

    # Make sure to respect total_timeout
    t0 = time.time()
    servicer.fail_blob_create = [Status.UNAVAILABLE] * 99
    with pytest.raises(GRPCError):
        assert await wrapped_blob_create.aio(req, max_retries=None, total_timeout=3)
    total_time = time.time() - t0
    assert total_time <= 3.1

    # Check input_plane_region included
    servicer.fail_blob_create = []  # Reset to no failures
    await wrapped_blob_create.aio(req, metadata=[("x-modal-input-plane-region", "us-east")])
    assert servicer.blob_create_metadata.get("x-modal-input-plane-region") == "us-east"

    # Check input_plane_region not included
    servicer.fail_blob_create = [Status.UNAVAILABLE] * 3
    await wrapped_blob_create.aio(req)
    assert servicer.blob_create_metadata.get("x-idempotency-key")
    assert servicer.blob_create_metadata.get("x-retry-attempt") == "3"
    assert servicer.blob_create_metadata.get("x-modal-input-plane-region") is None

    # Check all metadata is included
    servicer.fail_blob_create = [Status.UNAVAILABLE] * 3
    await wrapped_blob_create.aio(req, metadata=[("x-modal-input-plane-region", "us-east")])
    assert servicer.blob_create_metadata.get("x-idempotency-key")
    assert servicer.blob_create_metadata.get("x-retry-attempt") == "3"
    assert servicer.blob_create_metadata.get("x-modal-input-plane-region") == "us-east"



================================================
FILE: test/helpers.py
================================================
# Copyright Modal Labs 2023
import os
import pathlib
import signal
import subprocess
import sys
from typing import Optional


def deploy_app_externally(
    servicer,
    credentials: tuple[str, str],
    file_or_module: str,
    app_variable: Optional[str] = None,
    deployment_name="Deployment",
    cwd=None,
    env={},
    capture_output=True,
) -> Optional[str]:
    # deploys an app from another interpreter to prevent leaking state from client into a container process
    # (apart from what goes through the servicer) also has the advantage that no modules imported by the
    # test files themselves will be added to sys.modules and included in mounts etc.
    windows_support: dict[str, str] = {}

    if sys.platform == "win32":
        windows_support = {
            **os.environ.copy(),
            **{"PYTHONUTF8": "1"},
        }  # windows apparently needs a bunch of env vars to start python...

    token_id, token_secret = credentials
    env = {
        **windows_support,
        "MODAL_SERVER_URL": servicer.client_addr,
        "MODAL_TOKEN_ID": token_id,
        "MODAL_TOKEN_SECRET": token_secret,
        "MODAL_ENVIRONMENT": "main",
        **env,
    }
    if cwd is None:
        cwd = pathlib.Path(__file__).parent.parent

    app_ref = file_or_module if app_variable is None else f"{file_or_module}::{app_variable}"

    p = subprocess.Popen(
        [sys.executable, "-m", "modal.cli.entry_point", "deploy", app_ref, "--name", deployment_name],
        cwd=cwd,
        env=env,
        stderr=subprocess.STDOUT,
        stdout=subprocess.PIPE if capture_output else None,
    )
    stdout_b, stderr_b = p.communicate()
    stdout_s, stderr_s = (b.decode() if b is not None else None for b in (stdout_b, stderr_b))
    if p.returncode != 0:
        print(f"Deploying app failed!\n### stdout ###\n{stdout_s}\n### stderr ###\n{stderr_s}")
        raise Exception("Test helper failed to deploy app")
    return stdout_s


class PopenWithCtrlC(subprocess.Popen):
    def __init__(self, *args, creationflags=0, **kwargs):
        if sys.platform == "win32":
            # needed on windows to separate ctrl-c lifecycle of subprocess from parent:
            creationflags = creationflags | subprocess.CREATE_NEW_CONSOLE  # type: ignore

        super().__init__(*args, **kwargs, creationflags=creationflags)

    def send_ctrl_c(self):
        # platform independent way to replicate the behavior of Ctrl-C:ing a cli app
        if sys.platform == "win32":
            # windows doesn't support sigint, and subprocess.CTRL_C_EVENT has a bunch
            # of gotchas since it's bound to a console which is the same for the parent
            # process by default, and can't be sent using the python standard library
            # to a separate process's console
            import console_ctrl

            console_ctrl.send_ctrl_c(self.pid)  # noqa [E731]
        else:
            self.send_signal(signal.SIGINT)



================================================
FILE: test/i6pn_clustered_test.py
================================================
# Copyright Modal Labs 2024
import modal
import modal.experimental
from modal import App

app = App()


@app.function()
@modal.experimental.clustered(size=2)
def f1():
    pass


@app.function()
def f2():
    pass


@app.function(i6pn=True)
def f3():
    pass


def test_experimental_cluster(servicer, client):
    with app.run(client=client):
        assert len(servicer.app_functions) == 3

        fn1 = servicer.app_functions["fu-1"]  # f1
        assert fn1._experimental_group_size == 2
        assert fn1.i6pn_enabled is True

        fn2 = servicer.app_functions["fu-2"]  # f2
        assert not fn2._experimental_group_size
        assert fn2.i6pn_enabled is False

        fn3 = servicer.app_functions["fu-3"]  # f3
        assert not fn3._experimental_group_size
        assert fn3.i6pn_enabled is True


def test_run_experimental_cluster(client, servicer, monkeypatch):
    with app.run(client=client):
        # The servicer returns the sum of the squares of all arguments
        assert f1.remote(2, 4) == 2**2 + 4**2



================================================
FILE: test/input_plane_test.py
================================================
# Copyright Modal Labs 2025
import modal
from modal import App
from modal.client import Client
from modal.functions import Function
from modal.runner import deploy_app
from test.conftest import MockClientServicer

app = App()


@app.function(experimental_options={"input_plane_region": "us-east"})
def foo():
    return "foo"


def test_foo(client: Client, servicer: MockClientServicer):
    # This verifies that FunctionCreate returns the input_plane_region in the response, and call the input plane.
    servicer.function_body(foo.get_raw_f())
    with app.run(client=client):
        assert foo.remote() == "foo"
        assert foo._get_metadata().input_plane_url is not None
        assert foo._get_metadata().input_plane_region == "us-east"


def test_lookup_foo(client: Client, servicer: MockClientServicer):
    # This verifies that FunctionGet returns the input_plane_region in the response, and we call the input plane.
    servicer.function_body(foo.get_raw_f())
    modal.App()
    deploy_app(app, "app", client=client)
    f = Function.from_name("app", "foo").hydrate(client)
    assert f.remote() == "foo"
    assert f._get_metadata().input_plane_url is not None
    assert f._get_metadata().input_plane_region == "us-east"



================================================
FILE: test/io_streams_test.py
================================================
# Copyright Modal Labs 2024
import asyncio
import pytest
import time

from grpclib import Status
from grpclib.exceptions import GRPCError

from modal import enable_output
from modal._utils.async_utils import aclosing, sync_or_async_iter
from modal.io_streams import StreamReader
from modal_proto import api_pb2


def test_stream_reader(servicer, client):
    """Tests that the stream reader works with clean inputs."""
    lines = ["foo\n", "bar\n", "baz\n"]

    async def sandbox_get_logs(servicer, stream):
        await stream.recv_message()

        for line in lines:
            log = api_pb2.TaskLogs(data=line, file_descriptor=api_pb2.FILE_DESCRIPTOR_STDOUT)
            await stream.send_message(api_pb2.TaskLogsBatch(entry_id=line, items=[log]))

        # send EOF
        await stream.send_message(api_pb2.TaskLogsBatch(eof=True))

    with servicer.intercept() as ctx:
        ctx.set_responder("SandboxGetLogs", sandbox_get_logs)

        with enable_output():
            stdout: StreamReader[str] = StreamReader(
                file_descriptor=api_pb2.FILE_DESCRIPTOR_STDOUT,
                object_id="sb-123",
                object_type="sandbox",
                client=client,
            )

            out = []
            for line in stdout:
                out.append(line)

            assert out == lines


def test_stream_reader_processed(servicer, client):
    """Tests that the stream reader with logs by line works with clean inputs."""
    lines = ["foo\n", "bar\n", "baz\n"]

    async def sandbox_get_logs(servicer, stream):
        await stream.recv_message()

        for line in lines:
            log = api_pb2.TaskLogs(data=line, file_descriptor=api_pb2.FILE_DESCRIPTOR_STDOUT)
            await stream.send_message(api_pb2.TaskLogsBatch(entry_id=line, items=[log]))

        # send EOF
        await stream.send_message(api_pb2.TaskLogsBatch(eof=True))

    with servicer.intercept() as ctx:
        ctx.set_responder("SandboxGetLogs", sandbox_get_logs)

        with enable_output():
            stdout: StreamReader[str] = StreamReader(
                file_descriptor=api_pb2.FILE_DESCRIPTOR_STDOUT,
                object_id="sb-123",
                object_type="sandbox",
                client=client,
                by_line=True,
            )

            out = []
            for line in stdout:
                out.append(line)

            assert out == lines


def test_stream_reader_processed_multiple(servicer, client):
    """Tests that the stream reader with logs by line splits multiple lines."""

    async def sandbox_get_logs(servicer, stream):
        await stream.recv_message()

        log = api_pb2.TaskLogs(
            data="foo\nbar\nbaz",
            file_descriptor=api_pb2.FILE_DESCRIPTOR_STDOUT,
        )
        await stream.send_message(api_pb2.TaskLogsBatch(entry_id="0", items=[log]))

        # send EOF
        await stream.send_message(api_pb2.TaskLogsBatch(eof=True))

    with servicer.intercept() as ctx:
        ctx.set_responder("SandboxGetLogs", sandbox_get_logs)

        with enable_output():
            stdout: StreamReader[str] = StreamReader(
                file_descriptor=api_pb2.FILE_DESCRIPTOR_STDOUT,
                object_id="sb-123",
                object_type="sandbox",
                client=client,
                by_line=True,
            )

            out = []
            for line in stdout:
                out.append(line)

            assert out == ["foo\n", "bar\n", "baz"]


def test_stream_reader_processed_partial_lines(servicer, client):
    """Test that the stream reader with logs by line joins partial lines together."""

    async def sandbox_get_logs(servicer, stream):
        await stream.recv_message()

        log1 = api_pb2.TaskLogs(
            data="foo",
            file_descriptor=api_pb2.FILE_DESCRIPTOR_STDOUT,
        )
        await stream.send_message(api_pb2.TaskLogsBatch(entry_id="0", items=[log1]))

        log2 = api_pb2.TaskLogs(
            data="bar\n",
            file_descriptor=api_pb2.FILE_DESCRIPTOR_STDOUT,
        )
        await stream.send_message(api_pb2.TaskLogsBatch(entry_id="1", items=[log2]))

        log3 = api_pb2.TaskLogs(
            data="baz",
            file_descriptor=api_pb2.FILE_DESCRIPTOR_STDOUT,
        )
        await stream.send_message(api_pb2.TaskLogsBatch(entry_id="2", items=[log3]))

        # send EOF
        await stream.send_message(api_pb2.TaskLogsBatch(eof=True))

    with servicer.intercept() as ctx:
        ctx.set_responder("SandboxGetLogs", sandbox_get_logs)

        with enable_output():
            stdout: StreamReader[str] = StreamReader(
                file_descriptor=api_pb2.FILE_DESCRIPTOR_STDOUT,
                object_id="sb-123",
                object_type="sandbox",
                client=client,
                by_line=True,
            )

            out = []
            for line in stdout:
                out.append(line)

            assert out == ["foobar\n", "baz"]


@pytest.mark.asyncio
async def test_stream_reader_bytes_mode(servicer, client):
    """Test that the stream reader works in bytes mode."""

    async def container_exec_get_output(servicer, stream):
        await stream.recv_message()

        await stream.send_message(
            api_pb2.RuntimeOutputBatch(batch_index=0, items=[api_pb2.RuntimeOutputMessage(message_bytes=b"foo\n")])
        )

        await stream.send_message(api_pb2.RuntimeOutputBatch(exit_code=0))

    with servicer.intercept() as ctx:
        ctx.set_responder("ContainerExecGetOutput", container_exec_get_output)

        with enable_output():
            stdout: StreamReader[bytes] = StreamReader(
                file_descriptor=api_pb2.FILE_DESCRIPTOR_STDOUT,
                object_id="tp-123",
                object_type="container_process",
                client=client,
                text=False,
            )

            assert await stdout.read.aio() == b"foo\n"


def test_stream_reader_line_buffered_bytes(servicer, client):
    """Test that using line-buffering with bytes mode fails."""

    with pytest.raises(ValueError):
        StreamReader(
            file_descriptor=api_pb2.FILE_DESCRIPTOR_STDOUT,
            object_id="tp-123",
            object_type="container_process",
            client=client,
            by_line=True,
            text=False,
        )


@pytest.mark.asyncio
async def test_stream_reader_async_iter(servicer, client):
    """Test that StreamReader behaves as a proper async iterator."""

    async def sandbox_get_logs(servicer, stream):
        await stream.recv_message()

        log1 = api_pb2.TaskLogs(
            data="foo",
            file_descriptor=api_pb2.FILE_DESCRIPTOR_STDOUT,
        )
        await stream.send_message(api_pb2.TaskLogsBatch(entry_id="0", items=[log1]))

        log2 = api_pb2.TaskLogs(
            data="bar",
            file_descriptor=api_pb2.FILE_DESCRIPTOR_STDOUT,
        )
        await stream.send_message(api_pb2.TaskLogsBatch(entry_id="1", items=[log2]))

        # send EOF
        await stream.send_message(api_pb2.TaskLogsBatch(eof=True))

    with servicer.intercept() as ctx:
        ctx.set_responder("SandboxGetLogs", sandbox_get_logs)

        expected = "foobar"

        stdout: StreamReader[str] = StreamReader(
            file_descriptor=api_pb2.FILE_DESCRIPTOR_STDOUT,
            object_id="sb-123",
            object_type="sandbox",
            client=client,
            by_line=True,
        )

        out = ""
        async with aclosing(sync_or_async_iter(stdout)) as stream:
            async for line in stream:
                out += line

        assert out == expected


@pytest.mark.asyncio
async def test_stream_reader_container_process_retry(servicer, client):
    """Test that StreamReader handles container process stream failures and retries."""

    batch_idx = 0

    async def container_exec_get_output(servicer, stream):
        nonlocal batch_idx
        await stream.recv_message()

        for _ in range(3):
            await stream.send_message(
                api_pb2.RuntimeOutputBatch(
                    batch_index=batch_idx,
                    items=[api_pb2.RuntimeOutputMessage(message_bytes=f"msg{batch_idx}\n".encode())],
                )
            )
            batch_idx += 1

        # Simulate failure on the first connection
        if batch_idx == 3:
            raise GRPCError(Status.INTERNAL, "internal error")

        await stream.send_message(api_pb2.RuntimeOutputBatch(exit_code=0))

    with servicer.intercept() as ctx:
        ctx.set_responder("ContainerExecGetOutput", container_exec_get_output)

        with enable_output():
            stdout: StreamReader[str] = StreamReader(
                file_descriptor=api_pb2.FILE_DESCRIPTOR_STDOUT,
                object_id="tp-123",
                object_type="container_process",
                client=client,
                by_line=True,
            )

            output = []
            async for line in stdout:
                output.append(line)

            assert output == [f"msg{i}\n" for i in range(6)]


@pytest.mark.asyncio
async def test_stream_reader_timeout(servicer, client):
    """Test that StreamReader stops reading messages after the given deadline, and that
    messages are received within the deadline"""

    time_first_send = 0.0

    async def container_exec_get_output(servicer, stream):
        nonlocal time_first_send
        await stream.recv_message()
        # Send three messages, third one heavily delayed
        for i in range(3):
            if i == 2:
                await asyncio.sleep(3)
            await stream.send_message(
                api_pb2.RuntimeOutputBatch(
                    batch_index=i,
                    items=[api_pb2.RuntimeOutputMessage(message_bytes=f"msg{i}\n".encode())],
                )
            )
            if i == 0:
                time_first_send = time.monotonic()
        await stream.send_message(api_pb2.RuntimeOutputBatch(exit_code=0))

    with servicer.intercept() as ctx:
        ctx.set_responder("ContainerExecGetOutput", container_exec_get_output)
        with enable_output():
            stdout: StreamReader[str] = StreamReader(
                file_descriptor=api_pb2.FILE_DESCRIPTOR_STDOUT,
                object_id="tp-123",
                object_type="container_process",
                client=client,
                by_line=True,
                deadline=time.monotonic() + 2,  # use a 2-second timeout
            )
            output: list[str] = []
            async for line in stdout:
                output.append(line)
            # message 3 should not be received, due to the timeout
            assert output == [f"msg{i}\n" for i in range(2)]
            assert time.monotonic() - time_first_send <= 4



================================================
FILE: test/live_reload_test.py
================================================
# Copyright Modal Labs 2023
import asyncio
import pytest
import threading
import time
from unittest import mock

from modal import Function, enable_output
from modal.cli.import_refs import ImportRef
from modal.serving import serve_app

from .supports.app_run_tests.webhook import app
from .supports.skip import skip_windows


@pytest.fixture
def import_ref(test_dir):
    return ImportRef(str(test_dir / "supports" / "app_run_tests" / "webhook.py"), use_module_mode=False)


@pytest.mark.asyncio
async def test_live_reload(import_ref, server_url_env, token_env, servicer):
    async with serve_app.aio(app, import_ref):
        await asyncio.sleep(3.0)
    assert servicer.app_publish_count == 1
    assert servicer.app_client_disconnect_count == 1
    assert servicer.app_get_logs_initial_count == 0


@pytest.mark.asyncio
async def test_live_reload_with_logs(import_ref, server_url_env, token_env, servicer):
    with enable_output():
        async with serve_app.aio(app, import_ref):
            await asyncio.sleep(3.0)
    assert servicer.app_publish_count == 1
    assert servicer.app_client_disconnect_count == 1
    assert servicer.app_get_logs_initial_count == 1


@skip_windows("live-reload not supported on windows")
def test_file_changes_trigger_reloads(import_ref, server_url_env, token_env, servicer, capfd):
    watcher_done = threading.Event()

    async def fake_watch():
        for i in range(3):
            yield {"/some/file"}
        watcher_done.set()

    with serve_app(app, import_ref, _watcher=fake_watch()):
        watcher_done.wait()  # wait until watcher loop is done
        foo: Function = app.registered_functions["foo"]
        assert foo.get_web_url().startswith("http://")

    stderr = capfd.readouterr().err
    print(stderr)
    assert "Traceback" not in stderr
    # TODO ideally we would assert the specific expected number here, but this test
    # is consistently flaking in CI and I cannot reproduce locally to debug.
    # I'm relaxing the assertion for now to stop the test from blocking deployments.
    # assert servicer.app_publish_count == 4  # 1 + number of file changes
    assert servicer.app_publish_count > 1
    assert servicer.app_client_disconnect_count == 1


@pytest.mark.asyncio
async def test_no_change(import_ref, server_url_env, token_env, servicer):
    async def fake_watch():
        # Iterator that returns immediately, yielding nothing
        if False:
            yield

    async with serve_app.aio(app, import_ref, _watcher=fake_watch()):
        pass

    assert servicer.app_publish_count == 1  # Should create the initial app once
    assert servicer.app_client_disconnect_count == 1


@pytest.mark.asyncio
async def test_heartbeats(import_ref, server_url_env, token_env, servicer):
    with mock.patch("modal.runner.HEARTBEAT_INTERVAL", 1):
        t0 = time.time()
        async with serve_app.aio(app, import_ref):
            await asyncio.sleep(3.1)
        total_secs = int(time.time() - t0)

    apps = list(servicer.app_heartbeats.keys())
    assert len(apps) == 1
    # Typically [0s, 1s, 2s, 3s], but asyncio.sleep may lag.
    actual_heartbeats = servicer.app_heartbeats[apps[0]]
    assert abs(actual_heartbeats - (total_secs + 1)) <= 1



================================================
FILE: test/logging_test.py
================================================
# Copyright Modal Labs 2025
import importlib
import json
import pytest
import re

import modal.config


@pytest.fixture
def clean_logger(modal_config):
    # this makes sure a new logger is set up for every test using this,
    # using a specific the modal.toml config
    # Since the logging module is based on a bunch of side effects, this
    # has to do some hacky module reloading and resetting of log handlers

    def f(conf):
        with modal_config(conf):
            modal.config.logger.handlers.clear()
            importlib.reload(modal.config)  # necessary since loggers are configured in global scope
        return modal.config.logger

    yield f
    modal.config.logger.handlers.clear()
    importlib.reload(modal.config)  # reset to normal log config


def test_log_level_configuration(clean_logger, capsys):
    conf = """[main]
active = true
loglevel = "INFO"
"""
    logger = clean_logger(conf)
    logger.info("dummy")
    log_output = capsys.readouterr().err
    assert re.match(r"^\[modal-client] [^ ]+ dummy$", log_output)


def test_log_format_json(clean_logger, capsys):
    conf = """[main]
active = true
log_format = "JSON"
"""
    logger = clean_logger(conf)
    logger.warning("dummy")
    json_line = capsys.readouterr().err
    log_struct = json.loads(json_line)
    assert log_struct["message"] == "dummy"
    assert log_struct["levelname"] == "WARNING"


def test_custom_log_pattern(clean_logger, capsys):
    conf = """[main]
active = true
log_pattern = "custom %(message)s"
"""
    logger = clean_logger(conf)
    logger.warning("dummy")
    assert capsys.readouterr().err == "custom dummy\n"



================================================
FILE: test/lookup_test.py
================================================
# Copyright Modal Labs 2023
import pytest

from modal import App, Function, Volume, fastapi_endpoint, web_endpoint
from modal.exception import DeprecationError, ExecutionError, NotFoundError, ServerWarning
from modal.runner import deploy_app
from modal_proto import api_pb2


def test_persistent_object(servicer, client):
    Volume.objects.create("my-volume", client=client)

    v = Volume.from_name("my-volume").hydrate(client)
    assert v.object_id

    with pytest.raises(NotFoundError):
        Volume.from_name("bazbazbaz").hydrate(client)


def square(x):
    # This function isn't deployed anyway
    pass


def test_lookup_function(servicer, client):
    app = App()

    app.function()(square)
    deploy_app(app, "my-function", client=client)

    f = Function.from_name("my-function", "square").hydrate(client)
    assert f.object_id == "fu-1"

    # Call it using two arguments
    f = Function.from_name("my-function", "square").hydrate(client)
    assert f.object_id == "fu-1"
    with pytest.raises(NotFoundError):
        f = Function.from_name("my-function", "cube").hydrate(client)

    # Make sure we can call this function
    assert f.remote(2, 4) == 20
    assert [r for r in f.map([5, 2], [4, 3])] == [41, 13]

    # Make sure the new-style local calls raise an error
    with pytest.raises(ExecutionError):
        assert f.local(2, 4) == 20


def test_fastapi_endpoint_lookup(servicer, client):
    app = App()
    app.function()(fastapi_endpoint(method="POST")(square))
    deploy_app(app, "my-webhook", client=client)

    f = Function.from_name("my-webhook", "square").hydrate(client)
    assert f.get_web_url()


def test_web_endpoint_legacy_lookup(servicer, client):
    app = App()
    with pytest.warns(DeprecationError, match="web_endpoint"):
        app.function()(web_endpoint(method="POST")(square))
    deploy_app(app, "my-webhook", client=client)

    f = Function.from_name("my-webhook", "square").hydrate(client)
    assert f.get_web_url()


def test_deploy_exists(servicer, client):
    with pytest.raises(NotFoundError):
        Volume.from_name("my-volume").hydrate(client)
    Volume.objects.create("my-volume", client=client)
    v1 = Volume.from_name("my-volume").hydrate(client)
    v2 = Volume.from_name("my-volume").hydrate(client)
    assert v1.object_id == v2.object_id


def test_create_if_missing(servicer, client):
    v1 = Volume.from_name("my-volume", create_if_missing=True).hydrate(client)
    v2 = Volume.from_name("my-volume").hydrate(client)
    assert v1.object_id == v2.object_id


def test_lookup_server_warnings(servicer, client):
    app = App()

    app.function()(square)
    deploy_app(app, "my-function", client=client)

    servicer.function_get_server_warnings = [
        api_pb2.Warning(
            type=api_pb2.Warning.WARNING_TYPE_CLIENT_DEPRECATION,
            message="xyz",
        )
    ]
    with pytest.warns(ServerWarning, match="xyz"):
        Function.from_name("my-function", "square").hydrate(client)

    servicer.function_get_server_warnings = [api_pb2.Warning(message="abc")]
    with pytest.warns(ServerWarning, match="abc"):
        Function.from_name("my-function", "square").hydrate(client)



================================================
FILE: test/map_item_context_test.py
================================================
# Copyright Modal Labs 2025
import asyncio
import pytest

from modal._utils.async_utils import TimestampPriorityQueue
from modal.parallel_map import _MapItemContext, _MapItemState, _OutputType
from modal.retries import RetryManager
from modal_proto import api_pb2
from test.supports.map_item_test_utils import (
    InputJwtData,
    assert_context_is,
    assert_retry_item_is,
    result_failure,
    result_internal_failure,
    result_success,
)

retry_policy = api_pb2.FunctionRetryPolicy(
    backoff_coefficient=1.0,
    initial_delay_ms=500,
    max_delay_ms=500,
    retries=2,
)

now_seconds = 1738439812
input_data = api_pb2.FunctionInput(args=b"test")


@pytest.fixture
def retry_queue():
    return TimestampPriorityQueue()


def test_ctx_initial_state():
    ctx = _MapItemContext(input=input_data, retry_manager=RetryManager(retry_policy), sync_client_retries_enabled=True)
    assert_context_is(ctx, _MapItemState.SENDING, 0, None, None, input_data.args)


@pytest.mark.asyncio
async def test_successful_output(retry_queue):
    ctx = _MapItemContext(input=input_data, retry_manager=RetryManager(retry_policy), sync_client_retries_enabled=True)
    input_jwt_data = InputJwtData.of(0, 0)
    # Put inputs
    response_item = api_pb2.FunctionPutInputsResponseItem(idx=0, input_id="in-0", input_jwt=input_jwt_data.to_jwt())
    ctx.handle_put_inputs_response(response_item)
    assert_context_is(ctx, _MapItemState.WAITING_FOR_OUTPUT, 0, "in-0", input_jwt_data, input_data.args)

    # Get outputs
    output_type = await ctx.handle_get_outputs_response(
        api_pb2.FunctionGetOutputsItem(idx=0, result=result_success),
        now_seconds,
        api_pb2.FunctionCallInvocationType.FUNCTION_CALL_INVOCATION_TYPE_SYNC,
        retry_queue,
    )

    assert output_type == _OutputType.SUCCESSFUL_COMPLETION
    assert_context_is(ctx, _MapItemState.COMPLETE, 0, "in-0", input_jwt_data, input_data.args)


@pytest.mark.asyncio
async def test_failed_output_zero_retries(retry_queue):
    retry_policy = api_pb2.FunctionRetryPolicy(retries=0)
    ctx = _MapItemContext(input=input_data, retry_manager=RetryManager(retry_policy), sync_client_retries_enabled=True)
    input_jwt_data = InputJwtData.of(0, 0)
    # Put inputs
    response_item = api_pb2.FunctionPutInputsResponseItem(idx=0, input_id="in-0", input_jwt=input_jwt_data.to_jwt())
    ctx.handle_put_inputs_response(response_item)
    assert_context_is(ctx, _MapItemState.WAITING_FOR_OUTPUT, 0, "in-0", input_jwt_data, input_data.args)

    # Get outputs
    output_type = await ctx.handle_get_outputs_response(
        api_pb2.FunctionGetOutputsItem(idx=0, result=result_failure),
        now_seconds,
        api_pb2.FunctionCallInvocationType.FUNCTION_CALL_INVOCATION_TYPE_SYNC,
        retry_queue,
    )

    assert output_type == _OutputType.FAILED_COMPLETION
    assert_context_is(ctx, _MapItemState.COMPLETE, 1, "in-0", input_jwt_data, input_data.args)
    assert retry_queue.empty()


@pytest.mark.asyncio
async def test_failed_output_retries_disabled(retry_queue):
    retry_policy = api_pb2.FunctionRetryPolicy(retries=3)
    ctx = _MapItemContext(input=input_data, retry_manager=RetryManager(retry_policy), sync_client_retries_enabled=False)
    input_jwt_data = InputJwtData.of(0, 0)
    # Put inputs
    response_item = api_pb2.FunctionPutInputsResponseItem(idx=0, input_id="in-0", input_jwt=input_jwt_data.to_jwt())
    ctx.handle_put_inputs_response(response_item)
    assert_context_is(ctx, _MapItemState.WAITING_FOR_OUTPUT, 0, "in-0", input_jwt_data, input_data.args)

    # Get outputs
    output_type = await ctx.handle_get_outputs_response(
        api_pb2.FunctionGetOutputsItem(idx=0, result=result_failure),
        now_seconds,
        api_pb2.FunctionCallInvocationType.FUNCTION_CALL_INVOCATION_TYPE_SYNC,
        retry_queue,
    )

    assert output_type == _OutputType.FAILED_COMPLETION
    assert_context_is(ctx, _MapItemState.COMPLETE, 0, "in-0", input_jwt_data, input_data.args)
    assert retry_queue.empty()


@pytest.mark.asyncio
async def test_failed_output_retries_then_succeeds(retry_queue):
    retry_policy = api_pb2.FunctionRetryPolicy(retries=2)
    ctx = _MapItemContext(input=input_data, retry_manager=RetryManager(retry_policy), sync_client_retries_enabled=True)
    input_jwt_data_0 = InputJwtData.of(0, 0)
    # Put inputs
    response_item = api_pb2.FunctionPutInputsResponseItem(idx=0, input_id="in-0", input_jwt=input_jwt_data_0.to_jwt())
    ctx.handle_put_inputs_response(response_item)
    assert_context_is(ctx, _MapItemState.WAITING_FOR_OUTPUT, 0, "in-0", input_jwt_data_0, input_data.args)

    # Get outputs
    output_type = await ctx.handle_get_outputs_response(
        api_pb2.FunctionGetOutputsItem(idx=0, result=result_failure, retry_count=0),
        now_seconds,
        api_pb2.FunctionCallInvocationType.FUNCTION_CALL_INVOCATION_TYPE_SYNC,
        retry_queue,
    )

    assert output_type == _OutputType.RETRYING
    assert_context_is(ctx, _MapItemState.WAITING_TO_RETRY, 1, "in-0", input_jwt_data_0, input_data.args)
    assert len(retry_queue) == 1

    # Retry input
    retry_item = await ctx.prepare_item_for_retry()
    assert_retry_item_is(retry_item, input_jwt_data_0, 1, input_data.args)
    assert_context_is(ctx, _MapItemState.RETRYING, 1, "in-0", None, input_data.args)
    await retry_queue.clear()

    input_jwt_data_1 = InputJwtData.of(0, 1)
    ctx.handle_retry_response(input_jwt_data_1.to_jwt())
    assert_context_is(ctx, _MapItemState.WAITING_FOR_OUTPUT, 1, "in-0", input_jwt_data_1, input_data.args)

    # Get outputs
    output_type = await ctx.handle_get_outputs_response(
        api_pb2.FunctionGetOutputsItem(idx=0, result=result_failure, retry_count=1),
        now_seconds,
        api_pb2.FunctionCallInvocationType.FUNCTION_CALL_INVOCATION_TYPE_SYNC,
        retry_queue,
    )

    assert output_type == _OutputType.RETRYING
    assert_context_is(ctx, _MapItemState.WAITING_TO_RETRY, 2, "in-0", input_jwt_data_1, input_data.args)
    assert len(retry_queue) == 1

    # Retry input
    retry_item = await ctx.prepare_item_for_retry()
    assert_retry_item_is(retry_item, input_jwt_data_1, 2, input_data.args)
    assert_context_is(ctx, _MapItemState.RETRYING, 2, "in-0", None, input_data.args)
    await retry_queue.clear()

    input_jwt_data_2 = InputJwtData.of(0, 2)
    ctx.handle_retry_response(input_jwt_data_2.to_jwt())
    assert_context_is(ctx, _MapItemState.WAITING_FOR_OUTPUT, 2, "in-0", input_jwt_data_2, input_data.args)

    # Get outputs
    output_type = await ctx.handle_get_outputs_response(
        api_pb2.FunctionGetOutputsItem(idx=0, result=result_success, retry_count=2),
        now_seconds,
        api_pb2.FunctionCallInvocationType.FUNCTION_CALL_INVOCATION_TYPE_SYNC,
        retry_queue,
    )
    assert output_type == _OutputType.SUCCESSFUL_COMPLETION
    assert_context_is(ctx, _MapItemState.COMPLETE, 2, "in-0", input_jwt_data_2, input_data.args)


@pytest.mark.asyncio
async def test_lost_input_retries_then_succeeds(retry_queue):
    retry_policy = api_pb2.FunctionRetryPolicy(retries=1)
    ctx = _MapItemContext(input=input_data, retry_manager=RetryManager(retry_policy), sync_client_retries_enabled=True)
    input_jwt_data_0 = InputJwtData.of(0, 0)
    # Put inputs
    response_item = api_pb2.FunctionPutInputsResponseItem(idx=0, input_id="in-0", input_jwt=input_jwt_data_0.to_jwt())
    ctx.handle_put_inputs_response(response_item)
    assert_context_is(ctx, _MapItemState.WAITING_FOR_OUTPUT, 0, "in-0", input_jwt_data_0, input_data.args)

    # Get outputs
    output_type = await ctx.handle_get_outputs_response(
        api_pb2.FunctionGetOutputsItem(idx=0, result=result_internal_failure),
        now_seconds,
        api_pb2.FunctionCallInvocationType.FUNCTION_CALL_INVOCATION_TYPE_SYNC,
        retry_queue,
    )

    assert output_type == _OutputType.RETRYING
    assert_context_is(ctx, _MapItemState.WAITING_TO_RETRY, 1, "in-0", input_jwt_data_0, input_data.args)
    assert len(retry_queue) == 1

    # Retry input
    retry_item = await ctx.prepare_item_for_retry()
    assert_retry_item_is(retry_item, input_jwt_data_0, 1, input_data.args)
    assert_context_is(ctx, _MapItemState.RETRYING, 1, "in-0", None, input_data.args)

    input_jwt_data_1 = InputJwtData.of(0, 1)
    ctx.handle_retry_response(input_jwt_data_1.to_jwt())
    assert_context_is(ctx, _MapItemState.WAITING_FOR_OUTPUT, 1, "in-0", input_jwt_data_1, input_data.args)

    # Get outputs
    output_type = await ctx.handle_get_outputs_response(
        api_pb2.FunctionGetOutputsItem(idx=0, result=result_success, retry_count=1),
        now_seconds,
        api_pb2.FunctionCallInvocationType.FUNCTION_CALL_INVOCATION_TYPE_SYNC,
        retry_queue,
    )
    assert output_type == _OutputType.SUCCESSFUL_COMPLETION
    assert_context_is(ctx, _MapItemState.COMPLETE, 1, "in-0", input_jwt_data_1, input_data.args)


@pytest.mark.asyncio
async def test_failed_output_exhausts_retries(retry_queue):
    retry_policy = api_pb2.FunctionRetryPolicy(retries=1)
    ctx = _MapItemContext(input=input_data, retry_manager=RetryManager(retry_policy), sync_client_retries_enabled=True)
    input_jwt_data_0 = InputJwtData.of(0, 0)
    # Put inputs
    response_item = api_pb2.FunctionPutInputsResponseItem(idx=0, input_id="in-0", input_jwt=input_jwt_data_0.to_jwt())
    ctx.handle_put_inputs_response(response_item)
    assert_context_is(ctx, _MapItemState.WAITING_FOR_OUTPUT, 0, "in-0", input_jwt_data_0, input_data.args)

    # Get outputs
    output_type = await ctx.handle_get_outputs_response(
        api_pb2.FunctionGetOutputsItem(idx=0, result=result_failure),
        now_seconds,
        api_pb2.FunctionCallInvocationType.FUNCTION_CALL_INVOCATION_TYPE_SYNC,
        retry_queue,
    )

    assert output_type == _OutputType.RETRYING
    assert_context_is(ctx, _MapItemState.WAITING_TO_RETRY, 1, "in-0", input_jwt_data_0, input_data.args)
    assert len(retry_queue) == 1

    # Retry input
    retry_item = await ctx.prepare_item_for_retry()
    assert_retry_item_is(retry_item, input_jwt_data_0, 1, input_data.args)
    assert_context_is(ctx, _MapItemState.RETRYING, 1, "in-0", None, input_data.args)

    input_jwt_data_1 = InputJwtData.of(0, 1)
    ctx.handle_retry_response(input_jwt_data_1.to_jwt())
    assert_context_is(ctx, _MapItemState.WAITING_FOR_OUTPUT, 1, "in-0", input_jwt_data_1, input_data.args)

    # Get outputs
    output_type = await ctx.handle_get_outputs_response(
        api_pb2.FunctionGetOutputsItem(idx=0, result=result_failure, retry_count=1),
        now_seconds,
        api_pb2.FunctionCallInvocationType.FUNCTION_CALL_INVOCATION_TYPE_SYNC,
        retry_queue,
    )
    assert output_type == _OutputType.FAILED_COMPLETION
    assert_context_is(ctx, _MapItemState.COMPLETE, 2, "in-0", input_jwt_data_1, input_data.args)


@pytest.mark.asyncio
async def test_get_successful_output_before_put_inputs_completes(retry_queue):
    ctx = _MapItemContext(input=input_data, retry_manager=RetryManager(retry_policy), sync_client_retries_enabled=True)
    input_jwt_data = InputJwtData.of(0, 0)

    # Get outputs
    output_type = await ctx.handle_get_outputs_response(
        api_pb2.FunctionGetOutputsItem(idx=0, result=result_success),
        now_seconds,
        api_pb2.FunctionCallInvocationType.FUNCTION_CALL_INVOCATION_TYPE_SYNC,
        retry_queue,
    )

    assert output_type == _OutputType.SUCCESSFUL_COMPLETION
    assert_context_is(ctx, _MapItemState.COMPLETE, 0, None, None, input_data.args)
    assert retry_queue.empty()

    # Put inputs
    response_item = api_pb2.FunctionPutInputsResponseItem(idx=0, input_id="in-0", input_jwt=input_jwt_data.to_jwt())
    ctx.handle_put_inputs_response(response_item)
    assert_context_is(ctx, _MapItemState.COMPLETE, 0, "in-0", input_jwt_data, input_data.args)


@pytest.mark.asyncio
async def test_get_failed_output_before_put_inputs_completes(retry_queue):
    ctx = _MapItemContext(input=input_data, retry_manager=RetryManager(retry_policy), sync_client_retries_enabled=True)
    input_jwt_data = InputJwtData.of(0, 0)

    # Get outputs
    output_type = await ctx.handle_get_outputs_response(
        api_pb2.FunctionGetOutputsItem(idx=0, result=result_failure),
        now_seconds,
        api_pb2.FunctionCallInvocationType.FUNCTION_CALL_INVOCATION_TYPE_SYNC,
        retry_queue,
    )

    assert output_type == _OutputType.RETRYING
    assert_context_is(ctx, _MapItemState.WAITING_TO_RETRY, 1, None, None, input_data.args)
    assert len(retry_queue) == 1

    # Put inputs
    response_item = api_pb2.FunctionPutInputsResponseItem(idx=0, input_id="in-0", input_jwt=input_jwt_data.to_jwt())
    ctx.handle_put_inputs_response(response_item)
    assert_context_is(ctx, _MapItemState.WAITING_TO_RETRY, 1, "in-0", input_jwt_data, input_data.args)
    assert len(retry_queue) == 1


@pytest.mark.asyncio
async def test_retry_failed_output_before_put_inputs_completes(retry_queue):
    ctx = _MapItemContext(input=input_data, retry_manager=RetryManager(retry_policy), sync_client_retries_enabled=True)
    input_jwt_data = InputJwtData.of(0, 0)

    # Get outputs
    output_type = await ctx.handle_get_outputs_response(
        api_pb2.FunctionGetOutputsItem(idx=0, result=result_failure, retry_count=0),
        now_seconds,
        api_pb2.FunctionCallInvocationType.FUNCTION_CALL_INVOCATION_TYPE_SYNC,
        retry_queue,
    )

    assert output_type == _OutputType.RETRYING
    assert_context_is(ctx, _MapItemState.WAITING_TO_RETRY, 1, None, None, input_data.args)
    assert len(retry_queue) == 1

    # Retry input
    # prepare_item_for_retry should block waiting for put_inputs to set the input_jwt. Create a task for it,
    # so it can be in a blocked state, while we continue doing other things.
    task = asyncio.create_task(ctx.prepare_item_for_retry())
    # Sleep to allow prepare_item_for_retry to start and block on input_jwt.
    await asyncio.sleep(0.1)
    # Assert that the task is still blocked waiting for input_jwt.
    assert not task.done()
    assert_context_is(ctx, _MapItemState.RETRYING, 1, None, None, input_data.args)

    # Put inputs
    response_item = api_pb2.FunctionPutInputsResponseItem(idx=0, input_id="in-0", input_jwt=input_jwt_data.to_jwt())
    ctx.handle_put_inputs_response(response_item)
    assert_context_is(ctx, _MapItemState.RETRYING, 1, "in-0", input_jwt_data, input_data.args)
    assert len(retry_queue) == 1

    # Retry input
    # The task should now be able to complete, since input_jwt has been set.
    retry_item = await task
    assert_retry_item_is(retry_item, input_jwt_data, 1, input_data.args)
    assert_context_is(ctx, _MapItemState.RETRYING, 1, "in-0", None, input_data.args)
    await retry_queue.clear()

    input_jwt_data_1 = InputJwtData.of(0, 1)
    ctx.handle_retry_response(input_jwt_data_1.to_jwt())
    assert_context_is(ctx, _MapItemState.WAITING_FOR_OUTPUT, 1, "in-0", input_jwt_data_1, input_data.args)


@pytest.mark.asyncio
async def test_ignore_stale_failed_output(retry_queue):
    ctx = _MapItemContext(input=input_data, retry_manager=RetryManager(retry_policy), sync_client_retries_enabled=True)
    input_jwt_data = InputJwtData.of(0, 0)
    # Put inputs
    response_item = api_pb2.FunctionPutInputsResponseItem(idx=0, input_id="in-0", input_jwt=input_jwt_data.to_jwt())
    ctx.handle_put_inputs_response(response_item)
    assert_context_is(ctx, _MapItemState.WAITING_FOR_OUTPUT, 0, "in-0", input_jwt_data, input_data.args)

    # Get outputs
    output_type = await ctx.handle_get_outputs_response(
        api_pb2.FunctionGetOutputsItem(idx=0, result=result_failure, retry_count=0),
        now_seconds,
        api_pb2.FunctionCallInvocationType.FUNCTION_CALL_INVOCATION_TYPE_SYNC,
        retry_queue,
    )
    assert output_type == _OutputType.RETRYING
    # The retry count is now incremented to 1
    assert_context_is(ctx, _MapItemState.WAITING_TO_RETRY, 1, "in-0", input_jwt_data, input_data.args)

    # Get outputs
    # We get a duplicate output
    output_type = await ctx.handle_get_outputs_response(
        api_pb2.FunctionGetOutputsItem(idx=0, result=result_failure, retry_count=0),
        now_seconds,
        api_pb2.FunctionCallInvocationType.FUNCTION_CALL_INVOCATION_TYPE_SYNC,
        retry_queue,
    )
    assert output_type == _OutputType.STALE_RETRY_DUPLICATE
    # The output should have been ignored because it has retry count 0, but the ctx is on retry count 1.
    # Assert that state has not changed since.
    assert_context_is(ctx, _MapItemState.WAITING_TO_RETRY, 1, "in-0", input_jwt_data, input_data.args)


@pytest.mark.asyncio
async def test_ignore_duplicate_successful_output(retry_queue):
    ctx = _MapItemContext(input=input_data, retry_manager=RetryManager(retry_policy), sync_client_retries_enabled=True)
    input_jwt_data = InputJwtData.of(0, 0)
    # Put inputs
    response_item = api_pb2.FunctionPutInputsResponseItem(idx=0, input_id="in-0", input_jwt=input_jwt_data.to_jwt())
    ctx.handle_put_inputs_response(response_item)
    assert_context_is(ctx, _MapItemState.WAITING_FOR_OUTPUT, 0, "in-0", input_jwt_data, input_data.args)

    # Get outputs
    output_type = await ctx.handle_get_outputs_response(
        api_pb2.FunctionGetOutputsItem(idx=0, result=result_success, retry_count=0),
        now_seconds,
        api_pb2.FunctionCallInvocationType.FUNCTION_CALL_INVOCATION_TYPE_SYNC,
        retry_queue,
    )
    assert output_type == _OutputType.SUCCESSFUL_COMPLETION
    assert_context_is(ctx, _MapItemState.COMPLETE, 0, "in-0", input_jwt_data, input_data.args)

    # Get outputs
    # We get a duplicate output
    output_type = await ctx.handle_get_outputs_response(
        api_pb2.FunctionGetOutputsItem(idx=0, result=result_success, retry_count=0),
        now_seconds,
        api_pb2.FunctionCallInvocationType.FUNCTION_CALL_INVOCATION_TYPE_SYNC,
        retry_queue,
    )
    # Output type should be duplicate since it was already complete.
    assert output_type == _OutputType.ALREADY_COMPLETE_DUPLICATE
    # The output should have been ignored because it is already complete.
    # Assert that state has not changed since.
    assert_context_is(ctx, _MapItemState.COMPLETE, 0, "in-0", input_jwt_data, input_data.args)


@pytest.mark.asyncio
async def test_ignore_duplicate_failed_output(retry_queue):
    retry_policy = api_pb2.FunctionRetryPolicy(retries=1)
    ctx = _MapItemContext(input=input_data, retry_manager=RetryManager(retry_policy), sync_client_retries_enabled=True)
    input_jwt_data_0 = InputJwtData.of(0, 0)
    # Put inputs
    response_item = api_pb2.FunctionPutInputsResponseItem(idx=0, input_id="in-0", input_jwt=input_jwt_data_0.to_jwt())
    ctx.handle_put_inputs_response(response_item)
    assert_context_is(ctx, _MapItemState.WAITING_FOR_OUTPUT, 0, "in-0", input_jwt_data_0, input_data.args)

    # Get outputs
    output_type = await ctx.handle_get_outputs_response(
        api_pb2.FunctionGetOutputsItem(idx=0, result=result_failure),
        now_seconds,
        api_pb2.FunctionCallInvocationType.FUNCTION_CALL_INVOCATION_TYPE_SYNC,
        retry_queue,
    )

    assert output_type == _OutputType.RETRYING
    assert_context_is(ctx, _MapItemState.WAITING_TO_RETRY, 1, "in-0", input_jwt_data_0, input_data.args)
    assert len(retry_queue) == 1

    # Retry input
    retry_item = await ctx.prepare_item_for_retry()
    assert_retry_item_is(retry_item, input_jwt_data_0, 1, input_data.args)
    assert_context_is(ctx, _MapItemState.RETRYING, 1, "in-0", None, input_data.args)

    input_jwt_data_1 = InputJwtData.of(0, 1)
    ctx.handle_retry_response(input_jwt_data_1.to_jwt())
    assert_context_is(ctx, _MapItemState.WAITING_FOR_OUTPUT, 1, "in-0", input_jwt_data_1, input_data.args)

    # Get outputs
    output_type = await ctx.handle_get_outputs_response(
        api_pb2.FunctionGetOutputsItem(idx=0, result=result_failure, retry_count=1),
        now_seconds,
        api_pb2.FunctionCallInvocationType.FUNCTION_CALL_INVOCATION_TYPE_SYNC,
        retry_queue,
    )
    assert output_type == _OutputType.FAILED_COMPLETION
    assert_context_is(ctx, _MapItemState.COMPLETE, 2, "in-0", input_jwt_data_1, input_data.args)

    # Get outputs (duplicate)
    output_type = await ctx.handle_get_outputs_response(
        api_pb2.FunctionGetOutputsItem(idx=0, result=result_failure, retry_count=1),
        now_seconds,
        api_pb2.FunctionCallInvocationType.FUNCTION_CALL_INVOCATION_TYPE_SYNC,
        retry_queue,
    )
    # Output type should be duplicate because it was already complete.
    assert output_type == _OutputType.ALREADY_COMPLETE_DUPLICATE
    assert_context_is(ctx, _MapItemState.COMPLETE, 2, "in-0", input_jwt_data_1, input_data.args)



================================================
FILE: test/map_item_mananger_test.py
================================================
# Copyright Modal Labs 2025
import pytest

from modal._utils.async_utils import TimestampPriorityQueue
from modal.parallel_map import _MapItemsManager, _MapItemState, _OutputType
from modal_proto import api_pb2
from test.supports.map_item_test_utils import (
    InputJwtData,
    assert_context_is,
    assert_retry_item_is,
    result_failure,
    result_internal_failure,
    result_success,
)

retry_policy = api_pb2.FunctionRetryPolicy(
    backoff_coefficient=1.0,
    initial_delay_ms=500,
    max_delay_ms=500,
    retries=2,
)
retry_queue: TimestampPriorityQueue
manager: _MapItemsManager
now_seconds = 1738439812
count = 10


@pytest.fixture(autouse=True)
def reset_state():
    global retry_queue, manager
    retry_queue = TimestampPriorityQueue()
    manager = _MapItemsManager(
        retry_policy=retry_policy,
        function_call_invocation_type=api_pb2.FunctionCallInvocationType.FUNCTION_CALL_INVOCATION_TYPE_SYNC,
        retry_queue=retry_queue,
        sync_client_retries_enabled=True,
        max_inputs_outstanding=1000,
    )


async def add_items():
    put_items = [
        api_pb2.FunctionPutInputsItem(idx=i, input=api_pb2.FunctionInput(args=f"{i}".encode())) for i in range(count)
    ]
    await manager.add_items(put_items)
    assert len(manager) == count
    for i in range(count):
        ctx = manager.get_item_context(i)
        assert_context_is(ctx, _MapItemState.SENDING, 0, None, None, f"{i}".encode())


async def handle_put_inputs_response(state: _MapItemState):
    response_items = [
        api_pb2.FunctionPutInputsResponseItem(idx=i, input_id=f"in-{i}", input_jwt=InputJwtData.of(i, 0).to_jwt())
        for i in range(count)
    ]
    manager.handle_put_inputs_response(response_items)
    if state == _MapItemState.COMPLETE:
        assert len(manager) == 0
    else:
        for i in range(count):
            ctx = manager.get_item_context(i)
            assert_context_is(ctx, state, 0, f"in-{i}", InputJwtData.of(i, 0), f"{i}".encode())


def get_input_jwts_waiting_for_output(retry_count: int):
    assert [InputJwtData.from_jwt(input_jwt) for input_jwt in manager.get_input_jwts_waiting_for_output()] == [
        InputJwtData.of(i, retry_count) for i in range(count)
    ]


async def handle_get_outputs_response(
    result: api_pb2.GenericResult,
    state: _MapItemState,
    retry_count: int,
    output_type: _OutputType,
    include_input_jwt: bool = True,
):
    for i in range(count):
        _output_type = await manager.handle_get_outputs_response(
            api_pb2.FunctionGetOutputsItem(idx=i, result=result, retry_count=retry_count), now_seconds
        )
        assert _output_type == output_type
        ctx = manager.get_item_context(i)
        if state == _MapItemState.COMPLETE:
            assert ctx is None
        else:
            input_jwt = InputJwtData.of(i, retry_count) if include_input_jwt else None
            # we add 1 to the retry count because it gets incremented during handling of the response
            assert_context_is(ctx, state, retry_count + 1, f"in-{i}", input_jwt, f"{i}".encode())
    if state == _MapItemState.COMPLETE:
        assert len(manager) == 0
    else:
        assert len(manager) == count
    if state == _MapItemState.WAITING_TO_RETRY:
        assert len(retry_queue) == count
    else:
        assert len(retry_queue) == 0


async def prepare_items_for_retry(retry_count: int):
    retry_items: list[api_pb2.FunctionRetryInputsItem] = await manager.prepare_items_for_retry(
        [i for i in range(count)]
    )
    for i in range(count):
        assert_retry_item_is(retry_items[i], InputJwtData.of(i, retry_count - 1), retry_count, f"{i}".encode())


def handle_retry_response(retry_count: int):
    response_items = [InputJwtData.of(i, retry_count).to_jwt() for i in range(count)]
    manager.handle_retry_response(response_items)
    for i in range(count):
        ctx = manager.get_item_context(i)
        assert_context_is(
            ctx,
            _MapItemState.WAITING_FOR_OUTPUT,
            retry_count,
            f"in-{i}",
            InputJwtData.of(i, retry_count),
            f"{i}".encode(),
        )


@pytest.mark.asyncio
async def test_happy_path():
    # pump_inputs - retry count 0
    await add_items()
    await handle_put_inputs_response(_MapItemState.WAITING_FOR_OUTPUT)
    # get_all_outputs
    get_input_jwts_waiting_for_output(0)
    await handle_get_outputs_response(result_success, _MapItemState.COMPLETE, 0, _OutputType.SUCCESSFUL_COMPLETION)


@pytest.mark.asyncio
async def test_retry():
    # pump_inputs - retry count 0
    await add_items()
    await handle_put_inputs_response(_MapItemState.WAITING_FOR_OUTPUT)

    # get_all_outputs - retry count 0
    get_input_jwts_waiting_for_output(0)
    await handle_get_outputs_response(result_failure, _MapItemState.WAITING_TO_RETRY, 0, _OutputType.RETRYING)

    # retry_inputs - retry count 1
    await prepare_items_for_retry(1)
    await retry_queue.clear()
    handle_retry_response(1)

    # get_all_outputs - retry count 1
    get_input_jwts_waiting_for_output(1)
    await handle_get_outputs_response(result_success, _MapItemState.COMPLETE, 1, _OutputType.SUCCESSFUL_COMPLETION)


@pytest.mark.asyncio
async def test_retry_lost_input():
    # pump_inputs - retry count 0
    await add_items()
    await handle_put_inputs_response(_MapItemState.WAITING_FOR_OUTPUT)

    # get_all_outputs - retry count 0
    get_input_jwts_waiting_for_output(0)
    await handle_get_outputs_response(result_internal_failure, _MapItemState.WAITING_TO_RETRY, 0, _OutputType.RETRYING)

    # retry_inputs - retry count 1
    await prepare_items_for_retry(1)
    await retry_queue.clear()
    handle_retry_response(1)

    # get_all_outputs - retry count 1
    get_input_jwts_waiting_for_output(1)
    await handle_get_outputs_response(result_success, _MapItemState.COMPLETE, 1, _OutputType.SUCCESSFUL_COMPLETION)


@pytest.mark.asyncio
async def test_duplicate_successful_outputs():
    # pump_inputs - retry count 0
    await add_items()
    await handle_put_inputs_response(_MapItemState.WAITING_FOR_OUTPUT)

    # get_all_outputs - retry count 0
    get_input_jwts_waiting_for_output(0)
    await handle_get_outputs_response(result_success, _MapItemState.COMPLETE, 0, _OutputType.SUCCESSFUL_COMPLETION)

    # get_all_outputs - retry count 0 (duplicate)
    # No items should be waiting for output since we already processed all the outputs
    assert manager.get_input_jwts_waiting_for_output() == []
    await handle_get_outputs_response(result_success, _MapItemState.COMPLETE, 0, _OutputType.NO_CONTEXT_DUPLICATE)


@pytest.mark.asyncio
async def test_duplicate_failed_outputs():
    # pump_inputs - retry count 0
    await add_items()
    await handle_put_inputs_response(_MapItemState.WAITING_FOR_OUTPUT)

    # get_all_outputs - retry_count 0
    get_input_jwts_waiting_for_output(0)
    await handle_get_outputs_response(result_failure, _MapItemState.WAITING_TO_RETRY, 0, _OutputType.RETRYING)

    # get_all_outputs - retry_count 0 (duplicate)
    # No items should be waiting for output since we already processed all the outputs
    assert manager.get_input_jwts_waiting_for_output() == []
    await handle_get_outputs_response(
        result_failure, _MapItemState.WAITING_TO_RETRY, 0, _OutputType.STALE_RETRY_DUPLICATE
    )


@pytest.mark.asyncio
async def test_get_outputs_completes_before_put_inputs():
    # There is a race condition where we can send inputs to the server with PutInputs, but before it returns,
    # a call to GetOutputs executing in a coroutine fetches the output and completes. Ensure we handle this
    # properly.
    manager = _MapItemsManager(
        retry_policy=retry_policy,
        function_call_invocation_type=api_pb2.FunctionCallInvocationType.FUNCTION_CALL_INVOCATION_TYPE_SYNC,
        retry_queue=retry_queue,
        sync_client_retries_enabled=True,
        max_inputs_outstanding=1000,
    )
    # pump_inputs - retry_count 0 - send request
    await add_items()

    # get_all_outputs - retry_count 0
    # Verify there are no input_jwts waiting for output yet. The input_jwt is returned in the PutInputsResponse,
    # which we have not received yet.
    assert manager.get_input_jwts_waiting_for_output() == []
    await handle_get_outputs_response(result_success, _MapItemState.COMPLETE, 0, _OutputType.SUCCESSFUL_COMPLETION)

    # pump_inputs - retry_count 0 - receive response
    await handle_put_inputs_response(_MapItemState.COMPLETE)


@pytest.mark.asyncio
async def test_get_outputs_completes_before_function_retry():
    # pump_inputs - retry_count 0
    await add_items()
    await handle_put_inputs_response(_MapItemState.WAITING_FOR_OUTPUT)

    # get_all_outputs - retry_count 0
    get_input_jwts_waiting_for_output(0)
    await handle_get_outputs_response(result_failure, _MapItemState.WAITING_TO_RETRY, 0, _OutputType.RETRYING)

    # First retry fails

    # retry_inputs - retry_count 1
    await prepare_items_for_retry(1)
    await retry_queue.clear()

    # get_all_outputs - retry_count 1
    # The retry call has not returned yet, so there are no input_jwts waiting for output.
    assert manager.get_input_jwts_waiting_for_output() == []
    await handle_get_outputs_response(result_failure, _MapItemState.WAITING_TO_RETRY, 1, _OutputType.RETRYING, False)

    # retry_inputs -  retry_count 1 - handle response
    response_items = [InputJwtData.of(i, 1).to_jwt() for i in range(count)]
    manager.handle_retry_response(response_items)
    for i in range(count):
        # Even though this the response for retry attempt 1, the retry count will be 2 because the above call to
        # handle_get_outputs_response would have bumped the count. The jwt will still be for retry attempt 1.
        assert_context_is(
            manager.get_item_context(i),
            _MapItemState.WAITING_FOR_OUTPUT,
            2,
            f"in-{i}",
            InputJwtData.of(i, 1),
            f"{i}".encode(),
        )

    # Second retry succeeds

    # retry_inputs - retry_count 2
    await prepare_items_for_retry(2)
    await retry_queue.clear()

    # get_all_outputs - retry_count 2
    # The retry call has not returned yet, so there are not input_jwts waiting for output.
    assert manager.get_input_jwts_waiting_for_output() == []
    await handle_get_outputs_response(result_success, _MapItemState.COMPLETE, 2, _OutputType.SUCCESSFUL_COMPLETION)

    # retry_inputs - retry_count 2 - handle response
    response_items = [InputJwtData.of(i, 2).to_jwt() for i in range(count)]
    manager.handle_retry_response(response_items)
    assert len(manager) == 0



================================================
FILE: test/mdmd_test.py
================================================
# Copyright Modal Labs 2023
import importlib
import os
from enum import IntEnum

from modal_docs.mdmd import mdmd


def test_simple_function():
    def foo():
        pass

    assert (
        mdmd.function_str("bar", foo)
        == """```python
def bar():
```\n\n"""
    )


def test_simple_async_function():
    async def foo():
        pass

    assert (
        mdmd.function_str("bar", foo)
        == """```python
async def bar():
```\n\n"""
    )


def test_async_gen_function():
    async def foo():
        yield

    assert (
        mdmd.function_str("bar", foo)
        == """```python
async def bar():
```\n\n"""
    )


def test_complex_function_signature():
    def foo(a: str, *args, **kwargs):
        pass

    assert (
        mdmd.function_str("foo", foo)
        == """```python
def foo(a: str, *args, **kwargs):
```\n\n"""
    )


def test_complex_function_signature_with_line_hidden():
    def foo(
        a: str,
        *args,  # mdmd:line-hidden
        **kwargs,
    ):
        pass

    assert (
        mdmd.function_str("foo", foo)
        == """```python
def foo(
    a: str,
    **kwargs,
):
```\n\n"""
    )


def test_function_has_docstring():
    def foo():
        """short description

        longer description"""

    assert (
        mdmd.function_str("foo", foo)
        == """```python
def foo():
```

short description

longer description
"""
    )


def test_simple_class_with_docstring():
    class Foo:
        """The all important Foo"""

        def bar(self, baz: str):
            """Bars the foo with the baz"""

    assert (
        mdmd.class_str("Foo", Foo)
        == """```python
class Foo(object)
```

The all important Foo

### bar

```python
def bar(self, baz: str):
```

Bars the foo with the baz
"""
    )


def test_simple_class_with_docstring_with_line_hidden():
    class Foo:
        """The all important Foo mdmd:line-hidden"""

        def bar(self, baz: str):
            """Bars the foo with the baz

            This won't be included mdmd:line-hidden
            """

    assert (
        mdmd.class_str("Foo", Foo)
        == """```python
class Foo(object)
```

### bar

```python
def bar(self, baz: str):
```

Bars the foo with the baz
"""
    )


def test_enum():
    class Eee(IntEnum):
        FOO = 1
        BAR = 2
        XYZ = 3

    expected = """```python
class bar(enum.IntEnum)
```

An enumeration.

The possible values are:

* `FOO`
* `BAR`
* `XYZ`
"""

    assert mdmd.class_str("bar", Eee) == expected


def test_class_with_classmethod():
    class Foo:
        @classmethod
        def create_foo(cls, some_arg):
            pass

    assert (
        mdmd.class_str("Foo", Foo)
        == """```python
class Foo(object)
```

### create_foo

```python
@classmethod
def create_foo(cls, some_arg):
```

"""
    )


def test_class_with_baseclass_includes_base_methods():
    class Foo:
        def foo(self):
            pass

    class Bar(Foo):
        def bar(self):
            pass

    out = mdmd.class_str("Bar", Bar)
    assert "def foo(self):" in out


def test_module(monkeypatch):
    test_data_dir = os.path.join(os.path.dirname(__file__), "mdmd_data")
    monkeypatch.chdir(test_data_dir)
    monkeypatch.syspath_prepend(test_data_dir)
    test_module = importlib.import_module("foo")
    expected_output = open("./foo-expected.md").read()
    assert mdmd.module_str("foo", test_module) == expected_output


def test_docstring_format_reindents_code():
    assert (
        mdmd.format_docstring(
            """```python
        foo
            bar
        ```"""
        )
        == """```python
foo
    bar
```
"""
    )


def test_synchronicity_async_and_blocking_interfaces():
    from synchronicity import Synchronizer

    class Foo:
        """docky mcdocface"""

        async def foo(self):
            pass

        def bar(self):
            pass

    s = Synchronizer()
    BlockingFoo = s.create_blocking(Foo, "BlockingFoo")

    assert (
        mdmd.class_str("BlockingFoo", BlockingFoo)
        == """```python
class BlockingFoo(object)
```

docky mcdocface

### foo

```python
def foo(self):
```

### bar

```python
def bar(self):
```

"""
    )


def test_synchronicity_constructors():
    from synchronicity import Synchronizer

    class Foo:
        """docky mcdocface"""

        def __init__(self):
            """constructy mcconstructorface"""

    s = Synchronizer()
    BlockingFoo = s.create_blocking(Foo, "BlockingFoo")

    assert (
        mdmd.class_str("BlockingFoo", BlockingFoo)
        == """```python
class BlockingFoo(object)
```

docky mcdocface

```python
def __init__(self):
```

constructy mcconstructorface
"""
    )


def test_get_all_signature_comments():
    def foo(
        # prefix comment
        one,  # one comment
        two,  # two comment
        # postfix comment
    ) -> str:  # return value comment
        pass

    assert (
        mdmd.function_str("foo", foo)
        == """```python
def foo(
    # prefix comment
    one,  # one comment
    two,  # two comment
    # postfix comment
) -> str:  # return value comment
```

"""
    )


def test_get_decorators():
    BLA = 1

    def my_deco(arg):
        def wrapper(f):
            return f

        return wrapper

    @my_deco(BLA)
    def foo():
        pass

    assert (
        mdmd.function_str("foo", foo)
        == """```python
@my_deco(BLA)
def foo():
```

"""
    )



================================================
FILE: test/mount_test.py
================================================
# Copyright Modal Labs 2022
import hashlib
import os
import platform
import pytest
import re
from pathlib import Path, PurePosixPath

import modal
from modal import App, FilePatternMatcher
from modal._utils.blob_utils import LARGE_FILE_LIMIT
from modal.mount import Mount, client_mount_name, module_mount_condition, module_mount_ignore_condition


@pytest.mark.asyncio
async def test_get_files(servicer, client, tmpdir):
    small_content = b"# not much here"
    large_content = b"a" * (LARGE_FILE_LIMIT + 1)

    tmpdir.join("small.py").write(small_content)
    tmpdir.join("large.py").write(large_content)
    tmpdir.join("fluff").write("hello")

    files = {}
    m = Mount._from_local_dir(Path(tmpdir), remote_path="/", condition=lambda fn: fn.endswith(".py"), recursive=True)
    async for upload_spec in Mount._get_files.aio(m.entries):
        files[upload_spec.mount_filename] = upload_spec

    os.umask(umask := os.umask(0o022))  # Get the current umask
    expected_mode = 0o644 if platform.system() == "Windows" else 0o666 & ~umask

    assert "/small.py" in files
    assert "/large.py" in files
    assert "/fluff" not in files
    assert files["/small.py"].use_blob is False
    assert files["/small.py"].content == small_content
    assert files["/small.py"].sha256_hex == hashlib.sha256(small_content).hexdigest()
    assert files["/small.py"].mode == expected_mode, f"{oct(files['/small.py'].mode)} != {oct(expected_mode)}"

    assert files["/large.py"].use_blob is True
    assert files["/large.py"].content is None
    assert files["/large.py"].sha256_hex == hashlib.sha256(large_content).hexdigest()
    assert files["/large.py"].mode == expected_mode, f"{oct(files['/large.py'].mode)} != {oct(expected_mode)}"

    await m._deploy.aio("my-mount", client=client)
    blob_id = max(servicer.blobs.keys())  # last uploaded one
    assert len(servicer.blobs[blob_id]) == len(large_content)
    assert servicer.blobs[blob_id] == large_content

    assert servicer.files_sha2data[files["/large.py"].sha256_hex] == {"data": b"", "data_blob_id": blob_id}
    assert servicer.files_sha2data[files["/small.py"].sha256_hex] == {
        "data": small_content,
        "data_blob_id": "",
    }


def test_create_mount(servicer, client):
    local_dir, cur_filename = os.path.split(__file__)

    def condition(fn):
        return fn.endswith(".py")

    m = Mount._from_local_dir(local_dir, remote_path="/foo", condition=condition)

    m._deploy("my-mount", client=client)

    assert m.object_id == "mo-1"
    assert f"/foo/{cur_filename}" in servicer.files_name2sha
    sha256_hex = servicer.files_name2sha[f"/foo/{cur_filename}"]
    assert sha256_hex in servicer.files_sha2data
    assert servicer.files_sha2data[sha256_hex]["data"] == open(__file__, "rb").read()
    assert repr(Path(local_dir)) in repr(m)


def test_create_mount_file_errors(servicer, tmp_path, client):
    invalid_dir = tmp_path / "xyz"
    m = Mount._from_local_dir(invalid_dir, remote_path="/xyz")
    msg = re.escape(f"local dir {os.fspath(invalid_dir)} does not exist")
    with pytest.raises(FileNotFoundError, match=msg):
        m._deploy("my-mount", client=client)

    invalid_file = tmp_path / "xyz.txt"
    m = Mount._from_local_file(invalid_file, remote_path="/xyz.txt")
    msg = re.escape(f"local file {os.fspath(invalid_file)} does not exist")
    with pytest.raises(FileNotFoundError, match=msg):
        m._deploy("my-mount", client=client)

    not_a_dir = tmp_path / "abc"
    with open(not_a_dir, "w"):
        pass
    m = Mount._from_local_dir(not_a_dir, remote_path="/abc")

    msg = re.escape(f"local dir {os.fspath(not_a_dir)} is not a directory")
    with pytest.raises(NotADirectoryError, match=msg):
        m._deploy("my-mount", client=client)


def dummy():
    pass


def test_add_local_python_source(servicer, client, test_dir, monkeypatch):
    app = App()

    monkeypatch.syspath_prepend((test_dir / "supports").as_posix())

    app.function(image=modal.Image.debian_slim().add_local_python_source("pkg_a", "pkg_b", "standalone_file"))(dummy)

    with app.run(client=client):
        files = set(servicer.files_name2sha.keys())
        expected_files = {
            "/root/pkg_a/a.py",
            "/root/pkg_a/b/c.py",
            "/root/pkg_b/f.py",
            "/root/pkg_b/g/h.py",
            "/root/standalone_file.py",
        }
        assert expected_files.issubset(files)

        assert "/root/pkg_c/i.py" not in files
        assert "/root/pkg_c/j/k.py" not in files


def test_chained_entries(test_dir):
    # TODO: remove when public Mount is deprecated
    a_txt = str(test_dir / "a.txt")
    b_txt = str(test_dir / "b.txt")
    with open(a_txt, "w") as f:
        f.write("A")
    with open(b_txt, "w") as f:
        f.write("B")
    mount = Mount._from_local_file(a_txt).add_local_file(b_txt)
    entries = mount.entries
    assert len(entries) == 2
    files = [file for file in Mount._get_files(entries)]
    assert len(files) == 2
    files.sort(key=lambda file: file.source_description)
    assert files[0].source_description.name == "a.txt"
    assert files[0].mount_filename.endswith("/a.txt")
    assert files[0].content == b"A"
    m = hashlib.sha256()
    m.update(b"A")
    assert files[0].sha256_hex == m.hexdigest()
    assert files[0].use_blob is False


def test_module_mount_condition():
    condition = module_mount_condition(Path("/a/.venv/site-packages/mymod"))
    ignore_condition = module_mount_ignore_condition(Path("/a/.venv/site-packages/mymod"))

    include_paths = [
        Path("/a/.venv/site-packages/mymod/foo.py"),
        Path("/a/my_mod/config/foo.txt"),
        Path("/a/my_mod/config/foo.py"),
    ]
    exclude_paths = [
        Path("/a/site-packages/mymod/foo.pyc"),
        Path("/a/site-packages/mymod/__pycache__/foo.py"),
        Path("/a/my_mod/.config/foo.py"),
    ]
    for path in include_paths:
        assert condition(path)
        assert not ignore_condition(path)
    for path in exclude_paths:
        assert not condition(path)
        assert ignore_condition(path)


def test_mount_from_local_dir_ignore(test_dir, tmp_path_with_content):
    ignore = FilePatternMatcher("**/*.txt", "**/module", "!**/*.txt", "!**/*.py")
    expected = {
        "/foo/module/sub.py",
        "/foo/module/sub/sub.py",
        "/foo/data/sub",
        "/foo/module/__init__.py",
        "/foo/data.txt",
        "/foo/module/sub/__init__.py",
    }

    mount = Mount._add_local_dir(tmp_path_with_content, PurePosixPath("/foo"), ignore=ignore)

    file_names = [file.mount_filename for file in Mount._get_files(entries=mount.entries)]
    assert set(file_names) == expected


def test_client_mount_name():
    # This is expected to raise if we cannot parse the version correctly
    assert client_mount_name().startswith("modal-client-mount")



================================================
FILE: test/mount_utils_test.py
================================================
# Copyright Modal Labs 2024
import pytest

from modal._utils.mount_utils import validate_mount_points, validate_network_file_systems, validate_volumes
from modal.exception import InvalidError
from modal.network_file_system import _NetworkFileSystem
from modal.volume import _Volume


def test_validate_mount_points():
    # valid mount points
    dict_input = {"/foo/bar": _NetworkFileSystem.from_name("_NetworkFileSystem", create_if_missing=False)}
    validate_mount_points("_NetworkFileSystem", dict_input)  # type: ignore

    # invalid list input, should be dicts
    list_input = [_NetworkFileSystem.from_name("_NetworkFileSystem", create_if_missing=False)]

    with pytest.raises(InvalidError, match="volume_likes"):
        validate_mount_points("_NetworkFileSystem", list_input)  # type: ignore


@pytest.mark.parametrize("path", ["/", "/root", "/tmp", "foo/bar"])
def test_validate_mount_points_invalid_paths(path):
    validated_mount_points = {path: _NetworkFileSystem.from_name("_NetworkFileSystem", create_if_missing=False)}
    with pytest.raises(InvalidError, match="_NetworkFileSystem"):
        validate_mount_points("_NetworkFileSystem", validated_mount_points)


def test_validate_network_file_systems(client, servicer):
    # valid network_file_systems input
    network_file_systems = {"/my/path": _NetworkFileSystem.from_name("foo", create_if_missing=False)}
    validate_network_file_systems(network_file_systems)  # type: ignore

    # invalid non network_file_systems input
    not_network_file_systems = {"/my/path": _Volume.from_name("foo", create_if_missing=False)}
    with pytest.raises(InvalidError, match="_Volume"):
        validate_network_file_systems(not_network_file_systems)  # type: ignore


def test_validate_volumes(client, servicer):
    # valid volume input
    volumes = {"/my/path": _Volume.from_name("foo", create_if_missing=False)}
    validate_volumes(volumes)  # type: ignore

    # invalid non volume input
    not_volumes = {"/my/path": _NetworkFileSystem.from_name("foo", create_if_missing=False)}
    with pytest.raises(InvalidError, match="_NetworkFileSystem"):
        validate_volumes(not_volumes)  # type: ignore

    # invalid attempt mount volume twice
    vol = _Volume.from_name("foo", create_if_missing=False)
    bad_path_volumes = {"/my/path": vol, "/my/other/path": vol}
    with pytest.raises(InvalidError, match="Volume"):
        validate_volumes(bad_path_volumes)  # type: ignore



================================================
FILE: test/mounted_files_test.py
================================================
# Copyright Modal Labs 2022
import os
import pytest
import subprocess
from pathlib import Path, PurePosixPath

import modal
from modal.file_pattern_matcher import FilePatternMatcher, _AbstractPatternMatcher
from modal.mount import Mount, _MountDir

from . import helpers


@pytest.fixture
def path_with_symlinked_files(tmp_path):
    src = tmp_path / "foo.txt"
    src.write_text("Hello")
    trg = tmp_path / "bar.txt"
    trg.symlink_to(src)
    return tmp_path, {src, trg}


script_path = "pkg_a/script.py"


def f():
    pass


serialized_fn_path = "pkg_a/serialized_fn.py"


def serialized_function_no_automount(servicer, credentials, supports_dir, server_url_env):
    helpers.deploy_app_externally(servicer, credentials, serialized_fn_path, cwd=supports_dir)
    files = set(servicer.files_name2sha.keys())
    # We don't automount anymore, nothing should be mounted
    assert files == {}


def test_mounted_files_package(supports_dir, servicer, server_url_env, token_env):
    p = subprocess.run(["modal", "run", "pkg_a.package"], cwd=supports_dir)
    assert p.returncode == 0

    files = set(servicer.files_name2sha.keys())
    # Assert we include everything from `pkg_a` and `pkg_b` but not `pkg_c`:
    assert files == {
        "/root/pkg_a/__init__.py",
        "/root/pkg_a/a.py",
        "/root/pkg_a/b/c.py",
        "/root/pkg_a/d.py",
        "/root/pkg_a/b/e.py",
        "/root/pkg_a/script.py",
        "/root/pkg_a/serialized_fn.py",
        "/root/pkg_a/package.py",
    }


def test_mounted_files_config(servicer, supports_dir, server_url_env, token_env):
    p = subprocess.run(["modal", "run", "pkg_a/script.py"], cwd=supports_dir, env={**os.environ})
    assert p.returncode == 0
    files = set(servicer.files_name2sha.keys())
    assert files == {
        "/root/script.py",
    }


def test_e2e_modal_run_py_file_mounts(servicer, credentials, supports_dir):
    helpers.deploy_app_externally(servicer, credentials, "hello.py", cwd=supports_dir)
    # Reactivate the following mount assertions when we remove auto-mounting of dev-installed packages
    # assert len(servicer.files_name2sha) == 1
    # assert servicer.n_mounts == 1  # there should be a single mount
    # assert servicer.n_mount_files == 1
    assert "/root/hello.py" in servicer.files_name2sha


def test_e2e_modal_run_py_module_mounts(servicer, credentials, supports_dir):
    helpers.deploy_app_externally(servicer, credentials, "hello", cwd=supports_dir)
    # Reactivate the following mount assertions when we remove auto-mounting of dev-installed packages
    # assert len(servicer.files_name2sha) == 1
    # assert servicer.n_mounts == 1  # there should be a single mount
    # assert servicer.n_mount_files == 1
    assert "/root/hello.py" in servicer.files_name2sha


def foo():
    pass


def test_image_mounts_are_not_traversed_on_declaration(supports_dir, monkeypatch, client, server_url_env):
    return_values = []
    original = modal.mount._MountDir.get_files_to_upload

    def mock_get_files_to_upload(self):
        r = list(original(self))
        return_values.append(r)
        return r

    monkeypatch.setattr("modal.mount._MountDir.get_files_to_upload", mock_get_files_to_upload)
    app = modal.App()
    image_mount_with_many_files = modal.Image.debian_slim().add_local_dir(supports_dir / "pkg_a", remote_path="/test")
    app.function(image=image_mount_with_many_files)(foo)
    assert len(return_values) == 0  # ensure we don't look at the files yet

    with app.run(client=client):
        pass

    assert return_values  # at this point we should have gotten all the mount files
    # flatten inspected files
    files = set()
    for r in return_values:
        for fn, _ in r:
            files.add(fn)
    # sanity check - this test file should be included since we mounted the test dir
    assert Path(__file__) in files  # this test file should have been included


def test_get_files_to_upload_ignore(mock_dir):
    with mock_dir(
        {
            "venv": {"file_venv": ""},
            "dir_a": {"dir_a_a": {"file_a_a": ""}, "venv": {"file_a_a_venv": ""}},
            "dir_b": {"dir_b_a": {"file_b_a": ""}, "venv": {"file_b_a_venv": ""}},
            "dir_c": {"venv": ""},  # a file named venv
        }
    ) as mock_dir:
        mock_path = Path(mock_dir).resolve()

        mount_dir = _MountDir(
            local_dir=mock_path,
            remote_path=PurePosixPath("/root"),
            ignore=FilePatternMatcher("**/venv/"),
            recursive=True,
        )

        # _walk_and_prune should prune out all ignored directories, but not yet files
        included_files = set(mount_dir._walk_and_prune(mock_path))
        assert len(included_files) == 3
        expected_files = {
            str(mock_path / "dir_a" / "dir_a_a" / "file_a_a"),
            str(mock_path / "dir_b" / "dir_b_a" / "file_b_a"),
            str(mock_path / "dir_c" / "venv"),  # a file named venv, not ignored
        }
        assert included_files == expected_files

        # after get_files_to_upload, both files and directories should be pruned out
        files = list(mount_dir.get_files_to_upload())
        assert len(files) == 2
        included_files = {str(file[0]) for file in files}
        expected_files = {
            str(mock_path / "dir_a" / "dir_a_a" / "file_a_a"),
            str(mock_path / "dir_b" / "dir_b_a" / "file_b_a"),
        }
        assert included_files == expected_files


@pytest.mark.parametrize(
    "ignore_config, expected_can_prune, expected_included",
    [
        (
            FilePatternMatcher("venv/**"),
            True,
            {"toplevel.py", "toplevel.pyc"},
        ),
        (
            FilePatternMatcher("**", "!**/*.py"),
            False,
            {"toplevel.py", "lib.py"},
        ),
        (
            ~FilePatternMatcher("**/*.py"),
            False,
            {"toplevel.py", "lib.py"},
        ),
    ],
)
def test_directory_pruning_behavior(mock_dir, ignore_config, expected_can_prune, expected_included):
    """Test that directory pruning is conditionally applied based on can_prune_directories()"""
    with mock_dir(
        {
            "toplevel.py": "",
            "toplevel.pyc": "",
            "venv": {
                "lib.py": "",
                "lib.pyc": "",
            },
        }
    ) as mock_dir:
        mock_path = Path(mock_dir).resolve()

        mount_dir = _MountDir(
            local_dir=mock_path,
            remote_path=PurePosixPath("/root"),
            ignore=ignore_config,
            recursive=True,
        )

        assert isinstance(mount_dir.ignore, _AbstractPatternMatcher)
        assert mount_dir.ignore.can_prune_directories() is expected_can_prune
        files = list(mount_dir.get_files_to_upload())
        file_paths = {f[0].name for f in files}

        assert file_paths == expected_included


def test_mount_dedupe_explicit(servicer, credentials, supports_dir, server_url_env):
    normally_not_included_file = supports_dir / "pkg_a" / "normally_not_included.pyc"
    normally_not_included_file.touch(exist_ok=True)
    print(
        helpers.deploy_app_externally(
            # two explicit mounts of the same package
            servicer,
            credentials,
            "mount_dedupe.py",
            cwd=supports_dir,
        )
    )
    assert servicer.n_mounts == 3

    # mounts are loaded in parallel, but there
    mounted_files_sets = {frozenset(m.keys()) for m in servicer.mounts_excluding_published_client().values()}
    assert {"/root/mount_dedupe.py"} in mounted_files_sets
    mounted_files_sets.remove(frozenset({"/root/mount_dedupe.py"}))

    # find one mount that includes normally_not_included.py
    for mount_with_pyc in mounted_files_sets:
        if "/root/pkg_a/normally_not_included.pyc" in mount_with_pyc:
            break
    else:
        assert False, "could not find a mount with normally_not_included.pyc"
    mounted_files_sets.remove(mount_with_pyc)

    # and one without it
    remaining_mount = list(mounted_files_sets)[0]
    assert "/root/pkg_a/normally_not_included.pyc" not in remaining_mount
    for fn in remaining_mount:
        assert fn.startswith("/root/pkg_a")

    assert len(mount_with_pyc) == len(remaining_mount) + 1
    normally_not_included_file.unlink()  # cleanup


def test_mount_dedupe_relative_path_entrypoint(servicer, credentials, supports_dir, server_url_env, monkeypatch):
    workdir = supports_dir / "pkg_a"
    target_app = "../hello.py"  # in parent directory - requiring `..` expansion in path normalization

    helpers.deploy_app_externally(
        # two explicit mounts of the same package
        servicer,
        credentials,
        target_app,
        cwd=workdir,
    )
    # should be only one unique set of files in mounts
    mounted_files_sets = {frozenset(m.keys()) for m in servicer.mounts_excluding_published_client().values()}
    assert len(mounted_files_sets) == 1

    # but there should also be only one actual mount if deduplication works as expected
    assert len(servicer.mounts_excluding_published_client()) == 1


def test_mount_directory_with_symlinked_file(path_with_symlinked_files, servicer, client):
    path, files = path_with_symlinked_files
    mount = Mount._from_local_dir(path)
    mount._deploy("mo-1", client=client)
    pkg_a_mount = servicer.mount_contents["mo-1"]
    for src_f in files:
        assert any(mnt_f.endswith(src_f.name) for mnt_f in pkg_a_mount)


def test_module_with_dot_prefixed_parent_can_be_mounted(tmp_path, monkeypatch, servicer, client):
    # the typical usecase would be to have a `.venv` directory with a virualenv
    # that could possibly contain local site-packages that a user wants to mount

    # set up some dummy packages:
    # .parent
    #    |---- foo.py
    #    |---- bar
    #    |------|--baz.py
    #    |------|--.hidden_dir
    #    |------|------|-----mod.py
    #    |------|--.hidden_mod.py

    parent_dir = Path(tmp_path) / ".parent"
    parent_dir.mkdir()
    foo_py = parent_dir / "foo.py"
    foo_py.touch()
    bar_package = parent_dir / "bar"
    bar_package.mkdir()
    (bar_package / "__init__.py").touch()
    (bar_package / "baz.py").touch()
    (bar_package / ".hidden_dir").mkdir()
    (bar_package / ".hidden_dir" / "mod.py").touch()  # should be excluded
    (bar_package / ".hidden_mod.py").touch()  # should be excluded

    monkeypatch.syspath_prepend(parent_dir)
    foo_mount = Mount._from_local_python_packages("foo")
    foo_mount._deploy("mo-1", client=client)
    foo_mount_content = servicer.mount_contents["mo-1"]
    assert foo_mount_content.keys() == {"/root/foo.py"}

    bar_mount = Mount._from_local_python_packages("bar")
    bar_mount._deploy("mo-2", client=client)

    bar_mount_content = servicer.mount_contents["mo-2"]
    assert bar_mount_content.keys() == {"/root/bar/__init__.py", "/root/bar/baz.py"}



================================================
FILE: test/network_file_system_test.py
================================================
# Copyright Modal Labs 2022
import pytest
import time
from io import BytesIO
from unittest import mock

import modal
from modal.exception import DeprecationError, InvalidError, NotFoundError
from modal_proto import api_pb2


def dummy():
    pass


def test_network_file_system_files(client, test_dir, servicer):
    app = modal.App()
    nfs = modal.NetworkFileSystem.from_name("xyz", create_if_missing=True)

    dummy_modal = app.function(network_file_systems={"/root/foo": nfs})(dummy)

    with app.run(client=client):
        dummy_modal.remote()


def test_network_file_system_bad_paths():
    app = modal.App()
    nfs = modal.NetworkFileSystem.from_name("xyz", create_if_missing=True)

    def _f():
        pass

    with pytest.raises(InvalidError):
        app.function(network_file_systems={"/root/../../foo": nfs})(dummy)

    with pytest.raises(InvalidError):
        app.function(network_file_systems={"/": nfs})(dummy)

    with pytest.raises(InvalidError):
        app.function(network_file_systems={"/tmp/": nfs})(dummy)


def test_network_file_system_handle_single_file(client, tmp_path, servicer):
    local_file_path = tmp_path / "some_file"
    local_file_path.write_text("hello world")

    with modal.NetworkFileSystem.ephemeral(client=client) as nfs:
        nfs.add_local_file(local_file_path)
        nfs.add_local_file(local_file_path.as_posix(), remote_path="/foo/other_destination")
        object_id = nfs.object_id

    assert servicer.nfs_files[object_id].keys() == {
        "/some_file",
        "/foo/other_destination",
    }
    assert servicer.nfs_files[object_id]["/some_file"].data == b"hello world"
    assert servicer.nfs_files[object_id]["/foo/other_destination"].data == b"hello world"


@pytest.mark.asyncio
async def test_network_file_system_handle_dir(client, tmp_path, servicer):
    local_dir = tmp_path / "some_dir"
    local_dir.mkdir()
    (local_dir / "smol").write_text("###")

    subdir = local_dir / "subdir"
    subdir.mkdir()
    (subdir / "other").write_text("####")

    with modal.NetworkFileSystem.ephemeral(client=client) as nfs:
        nfs.add_local_dir(local_dir)
        object_id = nfs.object_id

    assert servicer.nfs_files[object_id].keys() == {
        "/some_dir/smol",
        "/some_dir/subdir/other",
    }
    assert servicer.nfs_files[object_id]["/some_dir/smol"].data == b"###"
    assert servicer.nfs_files[object_id]["/some_dir/subdir/other"].data == b"####"


@pytest.mark.asyncio
async def test_network_file_system_handle_big_file(client, tmp_path, servicer, blob_server, *args):
    with mock.patch("modal.network_file_system.LARGE_FILE_LIMIT", 10):
        local_file_path = tmp_path / "bigfile"
        local_file_path.write_text("hello world, this is a lot of text")

        async with modal.NetworkFileSystem.ephemeral(client=client) as nfs:
            await nfs.add_local_file.aio(local_file_path)
            object_id = nfs.object_id

        assert servicer.nfs_files[object_id].keys() == {"/bigfile"}
        assert servicer.nfs_files[object_id]["/bigfile"].data == b""
        assert servicer.nfs_files[object_id]["/bigfile"].data_blob_id == "bl-1"

        _, blobs, _, _ = blob_server
        assert blobs["bl-1"] == b"hello world, this is a lot of text"


def test_read_file(client, tmp_path, servicer):
    with modal.NetworkFileSystem.ephemeral(client=client) as nfs:
        with pytest.raises(FileNotFoundError):
            for _ in nfs.read_file("idontexist.txt"):
                ...


def test_write_file(client, tmp_path, servicer):
    local_file_path = tmp_path / "some_file"
    local_file_path.write_text("hello world")

    with modal.NetworkFileSystem.ephemeral(client=client) as nfs:
        nfs.write_file("remote_path.txt", open(local_file_path, "rb"))

        # Make sure we can write through the provider too
        nfs.write_file("remote_path.txt", open(local_file_path, "rb"))


def test_persisted(servicer, client):
    # Lookup should fail since it doesn't exist
    with pytest.raises(NotFoundError):
        modal.NetworkFileSystem.from_name("xyz").hydrate(client)

    # Create it
    modal.NetworkFileSystem.from_name("xyz", create_if_missing=True).hydrate(client)

    # Lookup should succeed now
    modal.NetworkFileSystem.from_name("xyz").hydrate(client)


def test_nfs_ephemeral(servicer, client, tmp_path):
    local_file_path = tmp_path / "some_file"
    local_file_path.write_text("hello world")

    assert servicer.n_nfs_heartbeats == 0
    with modal.NetworkFileSystem.ephemeral(client=client, _heartbeat_sleep=1) as nfs:
        assert nfs.listdir("/") == []
        nfs.write_file("xyz.txt", open(local_file_path, "rb"))
        (entry,) = nfs.listdir("/")
        assert entry.path == "xyz.txt"

        time.sleep(1.5)  # Make time for 2 heartbeats
    assert servicer.n_nfs_heartbeats == 2


def test_nfs_lazy_hydration_from_name(set_env_client):
    nfs = modal.NetworkFileSystem.from_name("nfs", create_if_missing=True)
    bio = BytesIO(b"content")
    nfs.write_file("blah", bio)


@pytest.mark.parametrize("name", ["has space", "has/slash", "a" * 65])
def test_invalid_name(name):
    with pytest.raises(InvalidError, match="Invalid NetworkFileSystem name"):
        modal.NetworkFileSystem.from_name(name)


def test_attempt_mount_volume(client, servicer):
    app = modal.App()
    modal.Volume.objects.create("my-other-vol", client=client)
    vol = modal.NetworkFileSystem.from_name("my-other-vol", create_if_missing=False)
    f = app.function(network_file_systems={"/data": vol})(dummy)
    with pytest.raises(InvalidError, match="already exists as a Volume"):
        with app.run(client=client):
            f.remote()


def test_network_file_system_namespace_deprecated(servicer, client):
    # Test from_name with namespace parameter warns
    with pytest.warns(
        DeprecationError, match="The `namespace` parameter for `modal.NetworkFileSystem.from_name` is deprecated"
    ):
        modal.NetworkFileSystem.from_name("test-nfs", namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE)

    # Test that from_name without namespace parameter doesn't warn about namespace
    import warnings

    with warnings.catch_warnings(record=True) as record:
        warnings.simplefilter("always")
        modal.NetworkFileSystem.from_name("test-nfs")
    # Filter out any unrelated warnings
    namespace_warnings = [w for w in record if "namespace" in str(w.message).lower()]
    assert len(namespace_warnings) == 0



================================================
FILE: test/notebook_test.py
================================================
# Copyright Modal Labs 2022
import pytest
import warnings
from pathlib import Path

with warnings.catch_warnings():
    warnings.simplefilter("ignore")
    import jupytext

try:
    from nbclient.exceptions import CellExecutionError
except (ModuleNotFoundError, DeprecationWarning):
    # TODO(erikbern): sometimes my local jupyter packages end up in a bad state,
    # but we don't want that to cause pytest to fail on startup.
    warnings.warn("failed importing nbclient")


@pytest.fixture
def notebook_runner(servicer):
    import nbformat
    from nbclient import NotebookClient

    def runner(notebook_path: Path):
        output_notebook_path = notebook_path.with_suffix(".output.ipynb")

        nb = jupytext.read(
            notebook_path,
        )

        parameter_cell = nb["cells"][0]
        assert "parameters" in parameter_cell["metadata"]["tags"]  # like in papermill
        parameter_cell["source"] = f'server_addr = "{servicer.client_addr}"'

        client = NotebookClient(nb)

        try:
            client.execute()
        except CellExecutionError:
            nbformat.write(nb, output_notebook_path)
            pytest.fail(
                f"""There was an error when executing the notebook.

Inspect the output notebook: {output_notebook_path}
"""
            )
        tagged_cells = {}
        for cell in nb["cells"]:
            for tag in cell["metadata"].get("tags", []):
                tagged_cells[tag] = cell

        return tagged_cells

    return runner


# for some reason this import is failing due to a circular import of IPython.terminal.embed
# but only when running in CI (sometimes?), causing these tests to fail:
# from IPython.terminal import interactiveshell


@pytest.mark.skip("temporarily disabled until IPython import issues in CI are resolved")
def test_notebook_outputs_status(notebook_runner, test_dir):
    input_notebook_path = test_dir / "supports" / "notebooks" / "simple.notebook.py"
    tagged_cells = notebook_runner(input_notebook_path)
    combined_output = "\n".join(c["data"]["text/plain"] for c in tagged_cells["main"]["outputs"])
    assert "Initialized" in combined_output
    assert "Created objects." in combined_output
    assert "App completed." in combined_output



================================================
FILE: test/object_test.py
================================================
# Copyright Modal Labs 2022
import pytest

from modal import Secret
from modal._object import _Object
from modal.dict import Dict, _Dict
from modal.exception import InvalidError
from modal.queue import _Queue


def test_new_hydrated(client):
    assert isinstance(_Dict._new_hydrated("di-123", client, None), _Dict)
    assert isinstance(_Queue._new_hydrated("qu-123", client, None), _Queue)

    with pytest.raises(InvalidError):
        _Queue._new_hydrated("di-123", client, None)  # Wrong prefix for type

    assert isinstance(_Object._new_hydrated("qu-123", client, None), _Queue)
    assert isinstance(_Object._new_hydrated("di-123", client, None), _Dict)

    with pytest.raises(InvalidError):
        _Object._new_hydrated("xy-123", client, None)


def test_on_demand_hydration(client):
    obj = Dict.from_name("test-dict", create_if_missing=True).hydrate(client)
    assert obj.object_id is not None


def test_constructor():
    with pytest.raises(InvalidError) as excinfo:
        Secret({"foo": 123})

    assert "Secret" in str(excinfo.value)
    assert "constructor" in str(excinfo.value)


def test_types():
    assert _Object._get_type_from_id("di-123") == _Dict
    assert _Dict._is_id_type("di-123")
    assert not _Dict._is_id_type("qu-123")
    assert _Queue._is_id_type("qu-123")
    assert not _Queue._is_id_type("di-123")



================================================
FILE: test/package_utils_test.py
================================================
# Copyright Modal Labs 2022
import platform
import pytest

from modal._utils.package_utils import get_module_mount_info
from modal.exception import ModuleNotMountable


def test_get_module_mount_info():
    res = get_module_mount_info("modal")
    assert len(res) == 1
    assert res[0][0] == True

    res = get_module_mount_info("asyncio")
    assert len(res) == 1
    assert res[0][0] == True

    res = get_module_mount_info("six")
    assert len(res) == 1
    assert res[0][0] == False

    if platform.system() != "Windows":
        # TODO This assertion fails on windows; I assume that compiled file formats are different there?
        with pytest.raises(ModuleNotMountable, match="aiohttp can't be mounted because it contains binary file"):
            get_module_mount_info("aiohttp")



================================================
FILE: test/queue_test.py
================================================
# Copyright Modal Labs 2022
import pytest
import queue
import sys
import time

from modal import Queue
from modal.exception import AlreadyExistsError, DeprecationError, InvalidError, NotFoundError
from modal_proto import api_pb2

from .supports.skip import skip_macos, skip_windows


def test_queue_named(servicer, client):
    name = "some-random-queue"
    q = Queue.from_name(name, create_if_missing=True)
    assert isinstance(q, Queue)
    assert q.name == name

    q.hydrate(client)

    info = q.info()
    assert info.name == name
    assert info.created_by == servicer.default_username

    assert q.len() == 0
    q.put(42)
    assert q.len() == 1
    assert q.get() == 42
    with pytest.raises(queue.Empty):
        q.get(timeout=0)
    assert q.len() == 0

    # test iter
    q.put_many([1, 2, 3])
    t0 = time.time()
    assert [v for v in q.iterate(item_poll_timeout=1.0)] == [1, 2, 3]
    assert 1.0 < time.time() - t0 < 2.0
    assert [v for v in q.iterate(item_poll_timeout=0.0)] == [1, 2, 3]

    Queue.objects.delete("some-random-queue", client=client)
    with pytest.raises(NotFoundError):
        Queue.from_name("some-random-queue").hydrate(client)
    Queue.objects.delete("some-random-queue", client=client, allow_missing=True)


def test_queue_ephemeral(servicer, client):
    with Queue.ephemeral(client=client, _heartbeat_sleep=1) as q:
        q.put("hello")
        assert q.len() == 1
        assert q.get() == "hello"
        time.sleep(1.5)  # enough to trigger two heartbeats

    assert servicer.n_queue_heartbeats == 2


@skip_macos("TODO(erikbern): this consistently fails on OSX. Unclear why.")
@skip_windows("TODO(Jonathon): figure out why timeouts don't occur on Windows.")
@pytest.mark.parametrize(
    ["put_timeout_secs", "min_queue_full_exc_count", "max_queue_full_exc_count"],
    [
        (0.02, 1, 100),  # a low timeout causes some exceptions
        (10.0, 0, 0),  # a high timeout causes zero exceptions
        (0.00, 1, 100),  # zero-len timeout causes some exceptions
        (None, 0, 0),  # no timeout causes zero exceptions
    ],
)
def test_queue_blocking_put(put_timeout_secs, min_queue_full_exc_count, max_queue_full_exc_count, servicer, client):
    import queue
    import threading

    producer_delay = 0.001
    consumer_delay = producer_delay * 5

    queue_full_exceptions = 0
    with Queue.ephemeral(client=client) as q:

        def producer():
            nonlocal queue_full_exceptions
            for i in range(servicer.queue_max_len * 2):
                item = f"Item {i}"
                try:
                    q.put(item, block=True, timeout=put_timeout_secs)  # type: ignore
                except queue.Full:
                    queue_full_exceptions += 1
                time.sleep(producer_delay)

        def consumer():
            while True:
                time.sleep(consumer_delay)
                item = q.get(block=True)  # type: ignore
                if item is None:
                    break  # Exit if a None item is received

        producer_thread = threading.Thread(target=producer)
        consumer_thread = threading.Thread(target=consumer)
        producer_thread.start()
        consumer_thread.start()
        producer_thread.join()
        # Stop the consumer by sending a None item
        q.put(None)  # type: ignore
        consumer_thread.join()

        assert queue_full_exceptions >= min_queue_full_exc_count
        assert queue_full_exceptions <= max_queue_full_exc_count


def test_queue_nonblocking_put(servicer, client):
    with Queue.ephemeral(client=client) as q:
        # Non-blocking PUTs don't tolerate a full queue and will raise exception.
        with pytest.raises(queue.Full) as excinfo:
            for i in range(servicer.queue_max_len + 1):
                q.put(i, block=False)  # type: ignore

    assert str(servicer.queue_max_len) in str(excinfo.value)
    assert i == servicer.queue_max_len


def test_queue_deploy(servicer, client):
    d = Queue.from_name("xyz", create_if_missing=True).hydrate(client)
    d.put(123)


def test_queue_lazy_hydrate_from_name(set_env_client):
    q = Queue.from_name("foo", create_if_missing=True)
    q.put(123)
    assert q.get() == 123


@pytest.mark.parametrize("name", ["has space", "has/slash", "a" * 65])
def test_invalid_name(name):
    with pytest.raises(InvalidError, match="Invalid Queue name"):
        Queue.from_name(name)


def test_queue_namespace_deprecated(servicer, client):
    # Test from_name with namespace parameter warns
    with pytest.warns(
        DeprecationError,
        match="The `namespace` parameter for `modal.Queue.from_name` is deprecated",
    ):
        Queue.from_name("test-queue", namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE)

    # Test that from_name without namespace parameter doesn't warn about namespace
    import warnings

    with warnings.catch_warnings(record=True) as record:
        warnings.simplefilter("always")
        Queue.from_name("test-queue")
    # Filter out any unrelated warnings
    namespace_warnings = [w for w in record if "namespace" in str(w.message).lower()]
    assert len(namespace_warnings) == 0


def test_queue_list(servicer, client):
    for i in range(5):
        Queue.from_name(f"test-queue-{i}", create_if_missing=True).hydrate(client)
    if sys.platform == "win32":
        time.sleep(1 / 32)

    queue_list = Queue.objects.list(client=client)
    assert len(queue_list) == 5
    assert all(q.name.startswith("test-queue-") for q in queue_list)
    assert all(q.info().created_by == servicer.default_username for q in queue_list)

    queue_list = Queue.objects.list(max_objects=2, client=client)
    assert len(queue_list) == 2


def test_queue_create(servicer, client):
    Queue.objects.create(name="test-queue-create", client=client)
    Queue.from_name("test-queue-create").hydrate(client)
    with pytest.raises(AlreadyExistsError):
        Queue.objects.create(name="test-queue-create", client=client)
    Queue.objects.create(name="test-queue-create", allow_existing=True, client=client)
    with pytest.raises(InvalidError, match="Invalid Queue name"):
        Queue.objects.create(name="has space", client=client)



================================================
FILE: test/resolver_test.py
================================================
# Copyright Modal Labs 2023
import asyncio
import pytest
import time
from typing import Optional

from modal._object import _Object
from modal._resolver import Resolver


@pytest.mark.flaky(max_runs=2)
@pytest.mark.asyncio
async def test_multi_resolve_sequential_loads_once(client):
    resolver = Resolver(client, environment_name="", app_id=None)

    load_count = 0

    class _DumbObject(_Object, type_prefix="zz"):
        pass

    async def _load(self: _DumbObject, resolver: Resolver, existing_object_id: Optional[str]):
        nonlocal load_count
        load_count += 1
        self._hydrate("zz-123", resolver.client, None)
        await asyncio.sleep(0.1)

    obj = _DumbObject._from_loader(_load, "DumbObject()")

    t0 = time.monotonic()
    await resolver.load(obj)
    await resolver.load(obj)
    assert 0.08 < time.monotonic() - t0 < 0.15

    assert load_count == 1


@pytest.mark.asyncio
async def test_multi_resolve_concurrent_loads_once(client):
    resolver = Resolver(client, environment_name="", app_id=None)

    load_count = 0

    class _DumbObject(_Object, type_prefix="zz"):
        pass

    async def _load(self: _DumbObject, resolver: Resolver, existing_object_id: Optional[str]):
        nonlocal load_count
        load_count += 1
        self._hydrate("zz-123", resolver.client, None)
        await asyncio.sleep(0.1)

    obj = _DumbObject._from_loader(_load, "DumbObject()")
    t0 = time.monotonic()
    await asyncio.gather(resolver.load(obj), resolver.load(obj))
    assert 0.08 < time.monotonic() - t0 < 0.17
    assert load_count == 1


def test_resolver_without_rich(no_rich, client):
    resolver = Resolver(client, environment_name="", app_id=None)
    resolver.add_status_row()
    with resolver.display():
        pass



================================================
FILE: test/retries_test.py
================================================
# Copyright Modal Labs 2022
import pytest

import modal
from modal.exception import InvalidError


def default_retries_from_int():
    pass


def fixed_delay_retries():
    pass


def exponential_backoff():
    return 67


def exponential_with_max_delay():
    return 67


def dummy():
    pass


def zero_retries():
    pass


def test_retries(client):
    app = modal.App()

    default_retries_from_int_modal = app.function(retries=5)(default_retries_from_int)
    fixed_delay_retries_modal = app.function(retries=modal.Retries(max_retries=5, backoff_coefficient=1.0))(
        fixed_delay_retries
    )

    exponential_backoff_modal = app.function(
        retries=modal.Retries(max_retries=2, initial_delay=2.0, backoff_coefficient=2.0)
    )(exponential_backoff)

    exponential_with_max_delay_modal = app.function(
        retries=modal.Retries(max_retries=2, backoff_coefficient=2.0, max_delay=30.0)
    )(exponential_with_max_delay)

    zero_retries_modal = app.function(retries=0)(zero_retries)

    with pytest.raises(TypeError):
        # Reject no-args constructions, which is unreadable and harder to support long-term
        app.function(retries=modal.Retries())(dummy)  # type: ignore

    # Reject weird inputs:
    # Don't need server to detect and reject nonsensical input. Can do client-side.
    with pytest.raises(InvalidError):
        app.function(retries=modal.Retries(max_retries=-2))(dummy)

    with pytest.raises(InvalidError):
        app.function(retries=modal.Retries(max_retries=2, backoff_coefficient=0.0))(dummy)

    with app.run(client=client):
        default_retries_from_int_modal.remote()
        fixed_delay_retries_modal.remote()
        exponential_backoff_modal.remote()
        exponential_with_max_delay_modal.remote()
        zero_retries_modal.remote()



================================================
FILE: test/runner_test.py
================================================
# Copyright Modal Labs 2023
import asyncio
import contextlib
import pytest
import time
import typing
from unittest import mock

import modal
from modal.client import Client
from modal.exception import AuthError, DeprecationError
from modal.runner import deploy_app, run_app
from modal_proto import api_pb2

T = typing.TypeVar("T")


def test_run_app(servicer, client):
    dummy_app = modal.App()
    with servicer.intercept() as ctx:
        with run_app(dummy_app, client=client):
            pass

    ctx.pop_request("AppCreate")
    ctx.pop_request("AppPublish")
    ctx.pop_request("AppClientDisconnect")


def test_run_app_shutdown_cleanliness(servicer, client, caplog):
    dummy_app = modal.App()

    heartbeat_interval_secs = 1.0

    # Introduce jittery response delay to catch race conditions between
    # concurrently executing RPCs.
    servicer.set_resp_jitter(heartbeat_interval_secs)

    with mock.patch("modal.runner.HEARTBEAT_INTERVAL", heartbeat_interval_secs):
        with modal.enable_output(), run_app(dummy_app, client=client):
            time.sleep(heartbeat_interval_secs)

    # Verify no ERROR logs were emitted, during shutdown or otherwise.
    error_logs = [record for record in caplog.records if record.levelname == "ERROR"]
    assert len(error_logs) == 0, f"Found unexpected error logs: {error_logs}"


def test_run_app_unauthenticated(servicer):
    dummy_app = modal.App()
    with Client.anonymous(servicer.client_addr) as client:
        with pytest.raises(AuthError):
            with run_app(dummy_app, client=client):
                pass


def dummy(): ...


def test_run_app_profile_env_with_refs(servicer, client, monkeypatch):
    monkeypatch.setenv("MODAL_ENVIRONMENT", "profile_env")
    with servicer.intercept() as ctx:
        dummy_app = modal.App()
        ref = modal.Secret.from_name("some_secret")
        dummy_app.function(secrets=[ref])(dummy)

    assert ctx.calls == []  # all calls should be deferred

    with servicer.intercept() as ctx:
        ctx.add_response("SecretGetOrCreate", api_pb2.SecretGetOrCreateResponse(secret_id="st-123"))
        with run_app(dummy_app, client=client):
            pass

    with pytest.raises(Exception):
        ctx.pop_request("SecretCreate")  # should not create a new secret...

    app_create = ctx.pop_request("AppCreate")
    assert app_create.environment_name == "profile_env"

    secret_get_or_create = ctx.pop_request("SecretGetOrCreate")
    assert secret_get_or_create.environment_name == "profile_env"


def test_run_app_custom_env_with_refs(servicer, client, monkeypatch):
    monkeypatch.setenv("MODAL_ENVIRONMENT", "profile_env")
    dummy_app = modal.App()
    own_env_secret = modal.Secret.from_name("own_env_secret")
    other_env_secret = modal.Secret.from_name("other_env_secret", environment_name="third")  # explicit lookup

    dummy_app.function(secrets=[own_env_secret, other_env_secret])(dummy)

    with servicer.intercept() as ctx:
        ctx.add_response("SecretGetOrCreate", api_pb2.SecretGetOrCreateResponse(secret_id="st-123"))
        ctx.add_response("SecretGetOrCreate", api_pb2.SecretGetOrCreateResponse(secret_id="st-456"))
        with run_app(dummy_app, client=client, environment_name="custom"):
            pass

    with pytest.raises(Exception):
        ctx.pop_request("SecretCreate")

    app_create = ctx.pop_request("AppCreate")
    assert app_create.environment_name == "custom"

    secret_get_or_create = ctx.pop_request("SecretGetOrCreate")
    assert secret_get_or_create.environment_name == "custom"

    secret_get_or_create_2 = ctx.pop_request("SecretGetOrCreate")
    assert secret_get_or_create_2.environment_name == "third"


def test_deploy_without_rich(servicer, client, no_rich):
    app = modal.App("dummy-app")
    app.function()(dummy)
    deploy_app(app, client=client)


@pytest.mark.asyncio
@pytest.mark.parametrize("build_validation", ["error", "warn", "ignore"])
async def test_mid_build_modifications(servicer, client, tmp_path, monkeypatch, build_validation):
    monkeypatch.setenv("MODAL_BUILD_VALIDATION", build_validation)

    (large_dir := tmp_path / "large_files").mkdir()
    for i in range(512 + 1):  # Equivalent to file upload concurrency
        (large_dir / f"{i:02d}.txt").write_bytes(f"large {i:02d}".encode())

    image = modal.Image.debian_slim().add_local_dir(large_dir, "/root/large_files")

    app = modal.App(image=image, include_source=False)
    app.function()(dummy)

    async def change_file_after_delay():
        await asyncio.sleep(0.2)  # "Uploading" large should take 2 seconds; see mock MountPutFile
        for f in large_dir.iterdir():
            f.touch()

    handler_assertion: contextlib.AbstractContextManager
    if build_validation == "error":
        handler_assertion = pytest.raises(modal.exception.ExecutionError, match="modified during build")
    elif build_validation == "warn":
        handler_assertion = pytest.warns(UserWarning, match="modified during build")
    else:
        handler_assertion = contextlib.nullcontext()

    at = asyncio.create_task(change_file_after_delay())
    try:
        with handler_assertion:
            async with app.run.aio(client=client):
                ...
    finally:
        await at


def test_deploy_app_namespace_deprecated(servicer, client):
    # Test deploy_app with namespace parameter warns
    app = modal.App("test-app")

    with pytest.warns(
        DeprecationError,
        match="The `namespace` parameter for `modal.runner.deploy_app` is deprecated",
    ):
        deploy_app(app, name="test-deploy", namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE, client=client)

    # Test that deploy_app without namespace parameter doesn't warn about namespace
    import warnings

    with warnings.catch_warnings(record=True) as record:
        warnings.simplefilter("always")
        deploy_app(app, name="test-deploy", client=client)
    # Filter out any unrelated warnings
    namespace_warnings = [w for w in record if "namespace" in str(w.message).lower()]
    assert len(namespace_warnings) == 0


def test_run_app_interactive_no_spinner(servicer, client):
    """Don't show status spinner in interactive mode to avoid interfering with breakpoints."""
    app = modal.App()

    with mock.patch("modal._output.OutputManager.show_status_spinner") as mock_spinner:
        with modal.enable_output():
            with run_app(app, client=client, interactive=True):
                pass
        mock_spinner.return_value.__enter__.assert_not_called()

    with mock.patch("modal._output.OutputManager.show_status_spinner") as mock_spinner:
        with modal.enable_output():
            with run_app(app, client=client, interactive=False):
                pass
        mock_spinner.return_value.__enter__.assert_called_once()



================================================
FILE: test/sandbox_test.py
================================================
# Copyright Modal Labs 2022

import hashlib
import pytest
import time
from pathlib import Path
from unittest import mock

from modal import App, Image, NetworkFileSystem, Proxy, Sandbox, SandboxSnapshot, Secret, Volume
from modal.exception import InvalidError
from modal.stream_type import StreamType
from modal_proto import api_pb2

from .supports.skip import skip_windows

skip_non_subprocess = skip_windows("Needs subprocess support")


@pytest.fixture
def app(client):
    app = App()
    with app.run(client=client):
        yield app


@skip_non_subprocess
def test_sandbox(app, servicer):
    sb = Sandbox.create("bash", "-c", "echo bye >&2 && sleep 1 && echo hi && exit 42", timeout=600, app=app)

    assert sb.poll() is None

    t0 = time.time()
    sb.wait()
    # Test that we actually waited for the sandbox to finish.
    assert time.time() - t0 > 0.3

    assert sb.stdout.read() == "hi\n"
    assert sb.stderr.read() == "bye\n"
    # read a second time
    assert sb.stdout.read() == ""
    assert sb.stderr.read() == ""

    assert sb.returncode == 42
    assert sb.poll() == 42


@skip_non_subprocess
def test_sandbox_mount_layer(app, servicer, tmpdir):
    tmpdir.join("a.py").write(b"foo")

    sb = Sandbox.create("echo", "hi", image=Image.debian_slim().add_local_dir(Path(tmpdir), remote_path="/m"), app=app)
    sb.wait()

    sha = hashlib.sha256(b"foo").hexdigest()
    assert servicer.files_sha2data[sha]["data"] == b"foo"


@skip_non_subprocess
def test_sandbox_image(app, servicer, tmpdir):
    tmpdir.join("a.py").write(b"foo")

    sb = Sandbox.create("echo", "hi", image=Image.debian_slim().pip_install("foo", "bar", "potato"), app=app)
    sb.wait()

    idx = max(servicer.images.keys())
    last_image = servicer.images[idx]

    assert all(c in last_image.dockerfile_commands[-1] for c in ["foo", "bar", "potato"])


@skip_non_subprocess
def test_sandbox_secret(app, servicer, tmpdir):
    sb = Sandbox.create("echo", "$FOO", secrets=[Secret.from_dict({"FOO": "BAR"})], app=app)
    sb.wait()

    assert len(servicer.sandbox_defs[0].secret_ids) == 1


@skip_non_subprocess
def test_sandbox_nfs(client, app, servicer, tmpdir):
    with NetworkFileSystem.ephemeral(client=client) as nfs:
        with pytest.raises(InvalidError):
            Sandbox.create("echo", "foo > /cache/a.txt", network_file_systems={"/": nfs}, app=app)

        Sandbox.create("echo", "foo > /cache/a.txt", network_file_systems={"/cache": nfs}, app=app)

    assert len(servicer.sandbox_defs[0].nfs_mounts) == 1


@skip_non_subprocess
def test_sandbox_from_id(app, client, servicer):
    sb = Sandbox.create("bash", "-c", "echo foo && exit 42", timeout=600, app=app)
    sb.wait()

    sb2 = Sandbox.from_id(sb.object_id, client=client)
    assert sb2.stdout.read() == "foo\n"
    assert sb2.returncode == 42


@skip_non_subprocess
def test_sandbox_terminate(app, servicer):
    sb = Sandbox.create("bash", "-c", "sleep 10000", app=app)
    sb.terminate()

    assert sb.returncode != 0


@skip_non_subprocess
@pytest.mark.asyncio
async def test_sandbox_stdin_async(app, servicer):
    sb = await Sandbox.create.aio("bash", "-c", "while read line; do echo $line; done && exit 13", app=app)

    sb.stdin.write(b"foo\n")
    sb.stdin.write(b"bar\n")

    sb.stdin.write_eof()

    await sb.stdin.drain.aio()

    await sb.wait.aio()

    assert await sb.stdout.read.aio() == "foo\nbar\n"
    assert sb.returncode == 13


@skip_non_subprocess
def test_sandbox_stdin(app, servicer):
    sb = Sandbox.create("bash", "-c", "while read line; do echo $line; done && exit 13", app=app)

    sb.stdin.write(b"foo\n")
    sb.stdin.write(b"bar\n")

    sb.stdin.write_eof()

    sb.stdin.drain()

    sb.wait()

    assert sb.stdout.read() == "foo\nbar\n"
    assert sb.returncode == 13


@skip_non_subprocess
def test_sandbox_stdin_write_str(app, servicer):
    sb = Sandbox.create("bash", "-c", "while read line; do echo $line; done && exit 13", app=app)

    sb.stdin.write("foo\n")
    sb.stdin.write("bar\n")

    sb.stdin.write_eof()

    sb.stdin.drain()

    sb.wait()

    assert sb.stdout.read() == "foo\nbar\n"
    assert sb.returncode == 13


@skip_non_subprocess
def test_sandbox_stdin_write_after_terminate(app, servicer):
    sb = Sandbox.create("bash", "-c", "echo foo", app=app)
    sb.wait()
    with pytest.raises(ValueError):
        sb.stdin.write(b"foo")
        sb.stdin.drain()


@skip_non_subprocess
def test_sandbox_stdin_write_after_eof(app, servicer):
    sb = Sandbox.create(app=app)
    sb.stdin.write_eof()
    with pytest.raises(ValueError):
        sb.stdin.write(b"foo")
    sb.terminate()


@skip_non_subprocess
def test_sandbox_stdout(app, servicer):
    """Test that reads from sandboxes are fully line-buffered, i.e.,
    that we don't read partial lines or multiple lines at once."""

    # normal sequence of reads
    sb = Sandbox.create("bash", "-c", "for i in $(seq 1 3); do echo foo $i; done", app=app)
    out = []
    for line in sb.stdout:
        out.append(line)
    assert out == ["foo 1\n", "foo 2\n", "foo 3\n"]

    # multiple newlines
    sb = Sandbox.create("bash", "-c", "echo 'foo 1\nfoo 2\nfoo 3'", app=app)
    out = []
    for line in sb.stdout:
        out.append(line)
    assert out == ["foo 1\n", "foo 2\n", "foo 3\n"]

    # partial lines
    sb = Sandbox.create("sleep", "infinity", app=app)
    cp = sb.exec("bash", "-c", "while read line; do echo $line; done")

    cp.stdin.write(b"foo 1\n")
    cp.stdin.write(b"foo 2")
    cp.stdin.write(b"foo 3\n")
    cp.stdin.write_eof()
    cp.stdin.drain()

    assert cp.stdout.read() == "foo 1\nfoo 2foo 3\n"


@skip_non_subprocess
def test_sandbox_stdout_next(app, servicer):
    """Test that we can iterate on a StreamReader directly, without a call to __aiter__()."""

    # normal sequence of reads
    sb = Sandbox.create("bash", "-c", "for i in $(seq 1 3); do echo foo $i; done", app=app)
    line = next(sb.stdout)
    assert line == "foo 1\n"
    line = next(sb.stdout)
    assert line == "foo 2\n"
    line = next(sb.stdout)
    assert line == "foo 3\n"


@skip_non_subprocess
@pytest.mark.asyncio
async def test_sandbox_async_for(app, servicer):
    sb = await Sandbox.create.aio("bash", "-c", "echo hello && echo world && echo bye >&2", app=app)

    out = ""

    async for message in sb.stdout:
        out += message
    assert out == "hello\nworld\n"

    # test streaming stdout a second time
    out2 = ""
    async for message in sb.stdout:
        out2 += message
    assert out2 == ""

    err = ""
    async for message in sb.stderr:
        err += message

    assert err == "bye\n"

    # test reading after receiving EOF
    assert await sb.stdout.read.aio() == ""
    assert await sb.stderr.read.aio() == ""


@skip_non_subprocess
def test_sandbox_exec_stdout_bytes_mode(app, servicer):
    """Test that the stream reader works in bytes mode."""

    sb = Sandbox.create(app=app)

    p = sb.exec("echo", "foo", text=False)
    assert p.stdout.read() == b"foo\n"

    p = sb.exec("echo", "foo", text=False)
    for line in p.stdout:
        assert line == b"foo\n"


@skip_non_subprocess
def test_app_sandbox(client, servicer):
    image = Image.debian_slim().pip_install("xyz").add_local_file(__file__, remote_path="/xyz")
    secret = Secret.from_dict({"FOO": "bar"})

    with pytest.raises(InvalidError, match="require an App"):
        Sandbox.create("bash", "-c", "echo bye >&2 && echo hi", image=image, secrets=[secret])

    app = App()
    with app.run(client=client):
        # Create sandbox
        sb = Sandbox.create("bash", "-c", "echo bye >&2 && echo hi", image=image, secrets=[secret], app=app)
        sb.wait()
        assert sb.stderr.read() == "bye\n"
        assert sb.stdout.read() == "hi\n"


@skip_non_subprocess
def test_sandbox_exec(app, servicer):
    sb = Sandbox.create("sleep", "infinity", app=app)

    cp = sb.exec("bash", "-c", "while read line; do echo $line; done")
    assert str(cp) == "ContainerProcess(process_id='container_exec_id')"

    cp.stdin.write(b"foo\n")
    cp.stdin.write(b"bar\n")
    cp.stdin.write_eof()
    cp.stdin.drain()

    assert cp.stdout.read() == "foo\nbar\n"


@skip_non_subprocess
def test_sandbox_exec_wait(app, servicer):
    sb = Sandbox.create("sleep", "infinity", app=app)

    cp = sb.exec("bash", "-c", "sleep 0.5 && exit 42")

    assert cp.poll() is None

    t0 = time.time()
    assert cp.wait() == 42
    assert time.time() - t0 > 0.2

    assert cp.poll() == 42


@mock.patch("modal.sandbox.CONTAINER_EXEC_TIMEOUT_BUFFER", 0)
@skip_non_subprocess
def test_sandbox_exec_wait_timeout(app, servicer):
    sb = Sandbox.create("sleep", "infinity", app=app)

    cp = sb.exec("sleep", "999", timeout=1)
    t0 = time.monotonic()
    assert cp.wait() == -1
    assert 0.8 < time.monotonic() - t0 <= 1.2


@mock.patch("modal.sandbox.CONTAINER_EXEC_TIMEOUT_BUFFER", 0)
@skip_non_subprocess
def test_sandbox_exec_poll_timeout(app, servicer):
    sb = Sandbox.create("sleep", "infinity", app=app)

    cp = sb.exec("sleep", "999", timeout=1)
    assert not cp.poll()
    time.sleep(1.2)
    assert cp.poll() == -1


@mock.patch("modal.sandbox.CONTAINER_EXEC_TIMEOUT_BUFFER", 0)
@skip_non_subprocess
def test_sandbox_exec_output_timeout(app, servicer):
    sb = Sandbox.create("sleep", "infinity", app=app)

    cp = sb.exec("sh", "-c", "echo hi; sleep 999", timeout=1)
    t1 = time.monotonic()
    assert cp.stdout.read() == "hi\n"
    assert 1 < time.monotonic() - t1 < 2.0
    assert cp.wait() == -1


@skip_non_subprocess
def test_sandbox_exec_output_double_read(app, servicer):
    sb = Sandbox.create("sleep", "infinity", app=app)

    cp = sb.exec("sh", "-c", "echo hi")
    assert cp.stdout.read() == "hi\n"
    assert cp.stdout.read() == ""
    assert cp.wait() == 0


@skip_non_subprocess
def test_sandbox_create_and_exec_with_bad_args(app, servicer):
    too_big = 130_000
    single_arg_size = too_big // 10
    too_big_args = ["a" * single_arg_size for _ in range(10)]
    with pytest.raises(InvalidError):
        Sandbox.create(*too_big_args, app=app)

    sb = Sandbox.create("sleep", "infinity", app=app)
    with pytest.raises(InvalidError):
        sb.exec("echo", 1)  # type: ignore

    with pytest.raises(InvalidError):
        sb.exec(*too_big_args)


@skip_non_subprocess
def test_sandbox_on_app_lookup(client, servicer):
    app = App.lookup("my-app", create_if_missing=True, client=client)
    sb = Sandbox.create("echo", "hi", app=app)
    sb.wait()
    assert sb.stdout.read() == "hi\n"
    assert servicer.sandbox_app_id == app.app_id


@skip_non_subprocess
def test_sandbox_list_env(app, client, servicer):
    sb = Sandbox.create("bash", "-c", "sleep 10000", app=app)
    assert len(list(Sandbox.list(client=client))) == 1
    sb.terminate()
    sb.wait(raise_on_termination=False)
    assert not list(Sandbox.list(client=client))


@skip_non_subprocess
def test_sandbox_list_app(client, servicer):
    image = Image.debian_slim().pip_install("xyz").add_local_file(__file__, "/xyz")
    secret = Secret.from_dict({"FOO": "bar"})

    app = App()

    with app.run(client=client):
        # Create sandbox
        sb = Sandbox.create("bash", "-c", "sleep 10000", image=image, secrets=[secret], app=app)
        assert len(list(Sandbox.list(app_id=app.app_id, client=client))) == 1
        sb.terminate()
        sb.wait(raise_on_termination=False)
        assert not list(Sandbox.list(app_id=app.app_id, client=client))


@skip_non_subprocess
def test_sandbox_list_tags(app, client, servicer):
    sb = Sandbox.create("bash", "-c", "sleep 10000", app=app)
    assert sb.get_tags() == {}
    sb.set_tags({"foo": "bar", "baz": "qux"})
    assert sb.get_tags() == {"foo": "bar", "baz": "qux"}

    assert len(list(Sandbox.list(tags={"foo": "bar"}, client=client))) == 1
    assert not list(Sandbox.list(tags={"foo": "notbar"}, client=client))
    sb.terminate()
    sb.wait(raise_on_termination=False)
    assert not list(Sandbox.list(tags={"baz": "qux"}, client=client))


@skip_non_subprocess
def test_sandbox_network_access(app, servicer):
    with pytest.raises(InvalidError):
        Sandbox.create("echo", "test", block_network=True, cidr_allowlist=["10.0.0.0/8"], app=app)

    # Test that blocking works
    sb = Sandbox.create("echo", "test", block_network=True, app=app)
    assert (
        servicer.sandbox_defs[0].network_access.network_access_type == api_pb2.NetworkAccess.NetworkAccessType.BLOCKED
    )
    assert len(servicer.sandbox_defs[0].network_access.allowed_cidrs) == 0
    sb.terminate()

    # Test that allowlisting works
    sb = Sandbox.create("echo", "test", block_network=False, cidr_allowlist=["10.0.0.0/8"], app=app)
    assert (
        servicer.sandbox_defs[1].network_access.network_access_type == api_pb2.NetworkAccess.NetworkAccessType.ALLOWLIST
    )
    assert len(servicer.sandbox_defs[1].network_access.allowed_cidrs) == 1
    assert servicer.sandbox_defs[1].network_access.allowed_cidrs[0] == "10.0.0.0/8"
    sb.terminate()

    # Test that no rules means allow all
    sb = Sandbox.create("echo", "test", block_network=False, app=app)
    assert servicer.sandbox_defs[2].network_access.network_access_type == api_pb2.NetworkAccess.NetworkAccessType.OPEN
    assert len(servicer.sandbox_defs[2].network_access.allowed_cidrs) == 0
    sb.terminate()


@skip_non_subprocess
def test_sandbox_block_network_with_ports(app, servicer):
    """Test that specifying open ports when block_network is enabled raises an error."""

    # Test with encrypted_ports
    with pytest.raises(InvalidError, match="Cannot specify open ports when `block_network` is enabled"):
        Sandbox.create("echo", "test", block_network=True, encrypted_ports=[8080], app=app)

    # Test with h2_ports
    with pytest.raises(InvalidError, match="Cannot specify open ports when `block_network` is enabled"):
        Sandbox.create("echo", "test", block_network=True, h2_ports=[8080], app=app)

    # Test with unencrypted_ports
    with pytest.raises(InvalidError, match="Cannot specify open ports when `block_network` is enabled"):
        Sandbox.create("echo", "test", block_network=True, unencrypted_ports=[8080], app=app)

    # Test with multiple port types
    with pytest.raises(InvalidError, match="Cannot specify open ports when `block_network` is enabled"):
        Sandbox.create(
            "echo",
            "test",
            block_network=True,
            encrypted_ports=[8080],
            h2_ports=[9090],
            unencrypted_ports=[3000],
            app=app,
        )

    # Test that it works fine when block_network is False
    sb = Sandbox.create(
        "echo", "test", block_network=False, encrypted_ports=[8080], h2_ports=[9090], unencrypted_ports=[3000], app=app
    )
    sb.terminate()


@skip_non_subprocess
def test_sandbox_no_entrypoint(app, servicer):
    sb = Sandbox.create(app=app)

    p = sb.exec("echo", "hi")
    p.wait()
    assert p.returncode == 0
    assert p.stdout.read() == "hi\n"

    sb.terminate()


@skip_non_subprocess
def test_sandbox_gpu_fallbacks_support(client, servicer):
    with pytest.raises(InvalidError, match="do not support"):
        Sandbox.create(client=client, gpu=["t4", "a100"])  # type: ignore


@skip_non_subprocess
def test_sandbox_exec_stdout(app, servicer, capsys):
    sb = Sandbox.create("sleep", "infinity", app=app)

    cp = sb.exec("bash", "-c", "echo hi", stdout=StreamType.STDOUT)
    cp.wait()

    assert capsys.readouterr().out == "hi\n"

    with pytest.raises(InvalidError):
        cp.stdout.read()


@skip_non_subprocess
def test_sandbox_snapshot(app, client, servicer):
    sb = Sandbox.create(app=app, _experimental_enable_snapshot=True)
    sandbox_snapshot = sb._experimental_snapshot()
    snapshot_id = sandbox_snapshot.object_id
    assert snapshot_id == "sn-123"
    sb.terminate()

    sandbox_snapshot = SandboxSnapshot.from_id(snapshot_id, client=client)
    assert sandbox_snapshot.object_id == snapshot_id

    sb = Sandbox._experimental_from_snapshot(sandbox_snapshot, client=client)
    sb.terminate()


@skip_non_subprocess
def test_sandbox_snapshot_fs(app, servicer):
    sb = Sandbox.create(app=app)
    image = sb.snapshot_filesystem()
    sb.terminate()

    sb2 = Sandbox.create(image=image, app=app)
    sb2.terminate()

    assert image.object_id == "im-123"
    assert servicer.sandbox_defs[1].image_id == "im-123"


@skip_non_subprocess
def test_sandbox_cpu_request(app, servicer):
    _ = Sandbox.create(cpu=2.0, app=app)

    assert servicer.sandbox_defs[0].resources.milli_cpu == 2000
    assert servicer.sandbox_defs[0].resources.milli_cpu_max == 0


@skip_non_subprocess
def test_sandbox_cpu_limit(app, servicer):
    _ = Sandbox.create(cpu=(2, 4), app=app)

    assert servicer.sandbox_defs[0].resources.milli_cpu == 2000
    assert servicer.sandbox_defs[0].resources.milli_cpu_max == 4000


@skip_non_subprocess
def test_sandbox_proxy(app, servicer):
    _ = Sandbox.create(proxy=Proxy.from_name("my-proxy"), app=app)

    assert servicer.sandbox_defs[0].proxy_id == "pr-123"


def test_sandbox_list_sets_correct_returncode_for_running(client, servicer):
    with servicer.intercept() as ctx:
        # test generic status
        ctx.add_response(
            "SandboxList",
            api_pb2.SandboxListResponse(
                sandboxes=[
                    api_pb2.SandboxInfo(
                        id="sb-123",
                        task_info=api_pb2.TaskInfo(
                            result=api_pb2.GenericResult(status=api_pb2.GenericResult.GENERIC_STATUS_UNSPECIFIED)
                        ),
                    )
                ]
            ),
        )
        ctx.add_response(
            "SandboxList", api_pb2.SandboxListResponse(sandboxes=[])
        )  # list will loop for older sandboxes until no more arrive
        (list_result,) = list(Sandbox.list(client=client))
    assert list_result.returncode is None


def test_sandbox_list_sets_correct_returncode_for_stopped(client, servicer):
    with servicer.intercept() as ctx:
        # test generic status
        ctx.add_response(
            "SandboxList",
            api_pb2.SandboxListResponse(
                sandboxes=[
                    api_pb2.SandboxInfo(
                        id="sb-123",
                        task_info=api_pb2.TaskInfo(
                            result=api_pb2.GenericResult(
                                status=api_pb2.GenericResult.GENERIC_STATUS_SUCCESS, exitcode=0
                            )
                        ),
                    )
                ]
            ),
        )
        ctx.add_response(
            "SandboxList", api_pb2.SandboxListResponse(sandboxes=[])
        )  # list will loop for older sandboxes until no more arrive
        (list_result,) = list(Sandbox.list(client=client))
    assert list_result.returncode == 0


@pytest.mark.parametrize("read_only", [True, False])
@skip_non_subprocess
def test_sandbox_volume(app, servicer, read_only):
    volume = Volume.from_name("my-volume", create_if_missing=True)

    if read_only:
        volume = volume.read_only()

    with servicer.intercept() as ctx:
        Sandbox.create(
            "bash",
            "-c",
            "echo bye >&2 && sleep 1 && echo hi && exit 42",
            timeout=600,
            app=app,
            volumes={"/mnt": volume},
        )
        req = ctx.pop_request("SandboxCreate")
        assert req.definition.volume_mounts[0].read_only == read_only


@skip_non_subprocess
def test_sandbox_create_pty(app, servicer):
    with servicer.intercept() as ctx:
        Sandbox.create("echo", "hi", pty=True, app=app)
        req = ctx.pop_request("SandboxCreate")

        assert req.definition.pty_info is not None
        assert req.definition.pty_info.enabled is True
        assert req.definition.pty_info.pty_type == api_pb2.PTYInfo.PTY_TYPE_SHELL
        assert req.definition.pty_info.no_terminate_on_idle_stdin is True


@skip_non_subprocess
def test_sandbox_exec_pty(app, servicer):
    with servicer.intercept() as ctx:
        sb = Sandbox.create("sleep", "infinity", app=app)
        sb.exec("echo", "hello", pty=True)
        req = ctx.pop_request("ContainerExec")

        assert req.pty_info is not None
        assert req.pty_info.enabled is True
        assert req.pty_info.pty_type == api_pb2.PTYInfo.PTY_TYPE_SHELL
        assert req.pty_info.no_terminate_on_idle_stdin is True



================================================
FILE: test/schedule_test.py
================================================
# Copyright Modal Labs 2022
from modal import App, Period
from modal_proto import api_pb2

app = App()


@app.function(schedule=Period(seconds=5))
def f():
    pass


def test_schedule(servicer, client):
    with app.run(client=client):
        assert servicer.function2schedule == {"fu-1": api_pb2.Schedule(period=api_pb2.Schedule.Period(seconds=5.0))}



================================================
FILE: test/scheduler_placement_test.py
================================================
# Copyright Modal Labs 2024
from modal import App, Sandbox, SchedulerPlacement
from modal_proto import api_pb2

from .supports.skip import skip_windows

app = App()


@app.function(
    _experimental_scheduler_placement=SchedulerPlacement(
        region="us-east-1",
        zone="us-east-1a",
        spot=False,
        instance_type="g4dn.xlarge",
    ),
)
def f1():
    pass


@app.function(
    region="us-east-1",
)
def f2():
    pass


@app.function(
    region=["us-east-1", "us-west-2"],
)
def f3():
    pass


def test_fn_scheduler_placement(servicer, client):
    with app.run(client=client):
        assert len(servicer.app_functions) == 3
        fn1 = servicer.app_functions["fu-1"]  # f1
        assert fn1.scheduler_placement == api_pb2.SchedulerPlacement(
            regions=["us-east-1"],
            _zone="us-east-1a",
            _lifecycle="on-demand",
            _instance_types=["g4dn.xlarge"],
        )

        fn2 = servicer.app_functions["fu-2"]  # f2
        assert fn2.scheduler_placement == api_pb2.SchedulerPlacement(
            regions=["us-east-1"],
        )

        fn3 = servicer.app_functions["fu-3"]  # f3
        assert fn3.scheduler_placement == api_pb2.SchedulerPlacement(
            regions=["us-east-1", "us-west-2"],
        )


@skip_windows("needs subprocess")
def test_sandbox_scheduler_placement(client, servicer):
    with app.run(client=client):
        Sandbox.create(
            "bash",
            "-c",
            "echo bye >&2 && sleep 1 && echo hi && exit 42",
            timeout=600,
            region="us-east-1",
            app=app,
        )

        assert len(servicer.sandbox_defs) == 1
        sb_def = servicer.sandbox_defs[0]
        assert sb_def.scheduler_placement == api_pb2.SchedulerPlacement(
            regions=["us-east-1"],
        )



================================================
FILE: test/secret_test.py
================================================
# Copyright Modal Labs 2022
import os
import pytest
import sys
import tempfile
import time
from unittest import mock

from modal import App, Secret
from modal.exception import AlreadyExistsError, DeprecationError, InvalidError, NotFoundError
from modal_proto import api_pb2

from .supports.skip import skip_old_py


def dummy(): ...


def test_secret_from_dict(servicer, client):
    app = App()
    secret = Secret.from_dict({"FOO": "hello, world"})
    app.function(secrets=[secret])(dummy)
    with app.run(client=client):
        assert secret.object_id == "st-0"
        assert servicer.secrets["st-0"] == {"FOO": "hello, world"}


@skip_old_py("python-dotenv requires python3.8 or higher", (3, 8))
def test_secret_from_dotenv(servicer, client):
    with tempfile.TemporaryDirectory() as tmpdirname:
        with open(os.path.join(tmpdirname, ".env"), "w") as f:
            f.write("# My settings\nUSER=user\nPASSWORD=abc123\n")

        with open(os.path.join(tmpdirname, ".env-dev"), "w") as f:
            f.write("# My settings\nUSER=user2\nPASSWORD=abc456\n")

        app = App()
        secret = Secret.from_dotenv(tmpdirname)
        app.function(secrets=[secret])(dummy)
        with app.run(client=client):
            assert secret.object_id == "st-0"
            assert servicer.secrets["st-0"] == {"USER": "user", "PASSWORD": "abc123"}

        app = App()
        secret = Secret.from_dotenv(tmpdirname, filename=".env-dev")
        app.function(secrets=[secret])(dummy)
        with app.run(client=client):
            assert secret.object_id == "st-1"
            assert servicer.secrets["st-1"] == {"USER": "user2", "PASSWORD": "abc456"}


@mock.patch.dict(os.environ, {"FOO": "easy", "BAR": "1234"})
def test_secret_from_local_environ(servicer, client):
    app = App()
    secret = Secret.from_local_environ(["FOO", "BAR"])
    app.function(secrets=[secret])(dummy)
    with app.run(client=client):
        assert secret.object_id == "st-0"
        assert servicer.secrets["st-0"] == {"FOO": "easy", "BAR": "1234"}

    with pytest.raises(InvalidError, match="NOTFOUND"):
        Secret.from_local_environ(["FOO", "NOTFOUND"])


def test_init_types():
    with pytest.raises(InvalidError):
        Secret.from_dict({"foo": 1.0})  # type: ignore


def test_secret_from_dict_none(servicer, client):
    app = App()
    secret = Secret.from_dict({"FOO": os.getenv("xyz"), "BAR": os.environ.get("abc"), "BAZ": "baz"})
    app.function(secrets=[secret])(dummy)
    with app.run(client=client):
        assert servicer.secrets["st-0"] == {"BAZ": "baz"}


def test_secret_from_name(servicer, client):
    # Deploy secret
    name = "my-secret"
    Secret.objects.create(name, {"FOO": "123"}, client=client)

    # Look up secret
    secret = Secret.from_name(name)
    assert secret.name == name
    secret.hydrate(client)
    secret_id = secret.object_id

    info = secret.info()
    assert info.name == name
    assert info.created_by == servicer.default_username

    # Look up secret through app
    app = App()
    secret = Secret.from_name("my-secret")
    app.function(secrets=[secret])(dummy)
    with app.run(client=client):
        assert secret.object_id == secret_id

    Secret.objects.delete("my-secret", client=client)
    with pytest.raises(NotFoundError):
        Secret.from_name("my-secret").hydrate(client)
    Secret.objects.delete("my-secret", client=client, allow_missing=True)


def test_secret_namespace_deprecated(servicer, client):
    with pytest.warns(
        DeprecationError,
        match="The `namespace` parameter for `modal.Secret.from_name` is deprecated",
    ):
        Secret.from_name("my-secret", namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE)

    with pytest.warns(
        DeprecationError,
        match="The `namespace` parameter for `modal.Secret.create_deployed` is deprecated",
    ):
        Secret._create_deployed(
            "my-secret", {"FOO": "123"}, namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE, client=client
        )

    with pytest.warns(DeprecationError, match="The `namespace` parameter"):
        Secret.from_name("my-secret", namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE)


def test_secret_list(servicer, client):
    for i in range(5):
        Secret.objects.create(f"test-secret-{i}", {"FOO": "123"}, client=client)
    if sys.platform == "win32":
        time.sleep(1 / 32)

    secrets = Secret.objects.list(client=client)
    assert len(secrets) == 5
    assert all(s.name.startswith("test-secret-") for s in secrets)
    assert all(s.info().created_by == servicer.default_username for s in secrets)


def test_secret_create(servicer, client):
    env_dict = {"FOO": "123"}
    Secret.objects.create(name="test-secret-create", env_dict=env_dict, client=client)
    Secret.from_name("test-secret-create").hydrate(client)
    with pytest.raises(AlreadyExistsError):
        Secret.objects.create(name="test-secret-create", env_dict=env_dict, client=client)
    Secret.objects.create(name="test-secret-create", env_dict=env_dict, allow_existing=True, client=client)
    with pytest.raises(InvalidError, match="Invalid Secret name"):
        Secret.objects.create(name="has space", env_dict=env_dict, client=client)



================================================
FILE: test/serialization_test.py
================================================
# Copyright Modal Labs 2022
import inspect
import pytest
import random
import typing

from modal import Queue
from modal._serialization import (
    apply_defaults,
    deserialize,
    deserialize_data_format,
    deserialize_proto_params,
    get_callable_schema,
    serialize,
    serialize_data_format,
    serialize_proto_params,
    signature_to_parameter_specs,
    validate_parameter_values,
)
from modal._type_manager import parameter_serde_registry
from modal._utils.rand_pb_testing import rand_pb
from modal.exception import DeserializationError, InvalidError
from modal_proto import api_pb2

from .supports.skip import skip_old_py


@pytest.mark.asyncio
async def test_roundtrip(servicer, client):
    async with Queue.ephemeral(client=client) as q:
        data = serialize(q)
        # TODO: strip synchronizer reference from synchronicity entities!
        assert len(data) < 350  # Used to be 93...
        # Note: if this blows up significantly, it's most likely because
        # cloudpickle can't find a class in the global scope. When this
        # happens, it tries to serialize the entire class along with the
        # object. The reason it doesn't find the class in the global scope
        # is most likely because the name doesn't match. To fix this, make
        # sure that cls.__name__ (which is something synchronicity sets)
        # is the same as the symbol defined in the global scope.
        q_roundtrip = deserialize(data, client)
        assert isinstance(q_roundtrip, Queue)
        assert q.object_id == q_roundtrip.object_id


@skip_old_py("random.randbytes() was introduced in python 3.9", (3, 9))
@pytest.mark.asyncio
async def test_asgi_roundtrip():
    rand = random.Random(42)
    for _ in range(10000):
        msg = rand_pb(api_pb2.Asgi, rand)
        buf = msg.SerializeToString()
        asgi_obj = deserialize_data_format(buf, api_pb2.DATA_FORMAT_ASGI, None)
        assert asgi_obj is None or (isinstance(asgi_obj, dict) and asgi_obj["type"])
        buf = serialize_data_format(asgi_obj, api_pb2.DATA_FORMAT_ASGI)
        asgi_obj_roundtrip = deserialize_data_format(buf, api_pb2.DATA_FORMAT_ASGI, None)
        assert asgi_obj == asgi_obj_roundtrip


def test_deserialization_error(client):
    # Curated object that we should not be able to deserialize
    obj = (
        b"\x80\x04\x95(\x00\x00\x00\x00\x00\x00\x00\x8c\x17"
        b"undeserializable_module\x94\x8c\x05Dummy\x94\x93\x94)\x81\x94."
    )
    with pytest.raises(DeserializationError, match="'undeserializable_module' .+ local environment"):
        deserialize(obj, client)


@pytest.mark.parametrize(
    ["pydict", "expected_bytes"],
    [
        (
            {"foo": "bar", "i": 5},
            # only update this byte sequence if you are aware of the consequences of changing
            # serialization byte output - it could invalidate existing container pools for users
            # on redeployment, and possibly cause startup crashes if new containers can't
            # deserialize old proto parameters.
            b"\n\x0c\n\x03foo\x10\x01\x1a\x03bar\n\x07\n\x01i\x10\x02 \x05",
        ),
        ({"x": b"\x00"}, b"\n\x08\n\x01x\x10\x042\x01\x00"),
    ],
)
def test_proto_serde_params_success(pydict, expected_bytes):
    serialized_params = serialize_proto_params(pydict)
    # it's important that the serialization doesn't change, since the serialized params bytes
    # are used as a key for the container pooling of parameterized services (classes)
    assert serialized_params == expected_bytes
    reconstructed = deserialize_proto_params(serialized_params)
    assert reconstructed == pydict


def test_proto_serde_failure_incomplete_params():
    # construct an incorrect serialization:
    schema = [api_pb2.ClassParameterSpec(name="x", type=api_pb2.PARAM_TYPE_STRING)]
    with pytest.raises(InvalidError, match="Missing required parameter: x"):
        validate_parameter_values({"a": "b"}, schema)

    with pytest.raises(TypeError, match="Expected str, got bytes"):
        validate_parameter_values({"x": b"b"}, schema)

    with pytest.raises(InvalidError, match="provided but are not defined"):
        validate_parameter_values({"x": "y", "a": "b"}, schema)

    # this should pass:
    validate_parameter_values({"x": "y"}, schema)


def test_apply_defaults():
    schema = [
        api_pb2.ClassParameterSpec(name="x", type=api_pb2.PARAM_TYPE_STRING, has_default=True, string_default="hello")
    ]
    assert apply_defaults({}, schema) == {"x": "hello"}
    assert apply_defaults({"x": "goodbye"}, schema) == {"x": "goodbye"}
    assert apply_defaults({"y": "goodbye"}, schema) == {"x": "hello", "y": "goodbye"}


def test_non_implemented_proto_type():
    with pytest.raises(InvalidError, match="No class parameter decoder implemented for type PARAM_TYPE_LIST"):
        # This tests if attempt to get the manager for a type we don't support, like list
        parameter_serde_registry.decode(api_pb2.ClassParameterValue(type=api_pb2.PARAM_TYPE_LIST))

    with pytest.raises(InvalidError, match="No class parameter decoder implemented for type 1000"):
        # Test for an enum value that isn't even defined in this version
        parameter_serde_registry.decode(api_pb2.ClassParameterValue(type=1000))  # type: ignore


def test_schema_extraction_unknown():
    def with_empty(a): ...

    def with_any(a: typing.Any): ...

    class Custom:
        pass

    def with_custom(a: Custom): ...

    for func in [with_empty, with_any, with_custom]:
        print(func.__name__)
        fields = list(get_callable_schema(func, is_web_endpoint=False).arguments)
        assert fields == [
            api_pb2.ClassParameterSpec(
                name="a",
                has_default=False,
                full_type=api_pb2.GenericPayloadType(base_type=api_pb2.PARAM_TYPE_UNKNOWN),
            )
        ]

    def with_default(a=5): ...

    fields = list(get_callable_schema(with_default, is_web_endpoint=False).arguments)
    assert fields == [
        api_pb2.ClassParameterSpec(
            name="a", full_type=api_pb2.GenericPayloadType(base_type=api_pb2.PARAM_TYPE_UNKNOWN), has_default=True
        )
    ]


def test_schema_extraction_int():
    def f(int_value: int = 1337): ...

    sig = inspect.signature(f)
    (int_spec,) = signature_to_parameter_specs(sig)
    assert int_spec == api_pb2.ClassParameterSpec(
        name="int_value",
        type=api_pb2.PARAM_TYPE_INT,
        full_type=api_pb2.GenericPayloadType(base_type=api_pb2.PARAM_TYPE_INT),
        has_default=True,
        int_default=1337,
    )


def test_schema_extraction_str():
    def foo(str_value: str = "foo"):
        pass

    (str_spec,) = signature_to_parameter_specs(inspect.signature(foo))
    assert str_spec == api_pb2.ClassParameterSpec(
        name="str_value",
        type=api_pb2.PARAM_TYPE_STRING,
        full_type=api_pb2.GenericPayloadType(base_type=api_pb2.PARAM_TYPE_STRING),
        has_default=True,
        string_default="foo",
    )


def test_schema_extraction_bytes():
    def foo(a: bytes = b"foo"):
        pass

    (bytes_spec,) = signature_to_parameter_specs(inspect.signature(foo))
    assert bytes_spec == api_pb2.ClassParameterSpec(
        name="a",
        type=api_pb2.PARAM_TYPE_BYTES,  # for backward compatibility
        has_default=True,
        bytes_default=b"foo",  # for backward compatibility
        full_type=api_pb2.GenericPayloadType(base_type=api_pb2.PARAM_TYPE_BYTES),
    )


def test_schema_extraction_list():
    def new_f(simple_list: list[int]): ...
    def old_f(simple_list: typing.List[int]): ...

    for f in [new_f, old_f]:
        (list_spec,) = get_callable_schema(f, is_web_endpoint=False).arguments
        assert list_spec == api_pb2.ClassParameterSpec(
            name="simple_list",
            full_type=api_pb2.GenericPayloadType(
                base_type=api_pb2.PARAM_TYPE_LIST,
                sub_types=[api_pb2.GenericPayloadType(base_type=api_pb2.PARAM_TYPE_INT)],
            ),
            has_default=False,
        )


def test_schema_extraction_nested_list():
    def f(nested_list: list[list[bytes]]): ...

    (list_spec,) = get_callable_schema(f, is_web_endpoint=False).arguments
    assert list_spec == api_pb2.ClassParameterSpec(
        name="nested_list",
        full_type=api_pb2.GenericPayloadType(
            base_type=api_pb2.PARAM_TYPE_LIST,
            sub_types=[
                api_pb2.GenericPayloadType(
                    base_type=api_pb2.PARAM_TYPE_LIST,
                    sub_types=[api_pb2.GenericPayloadType(base_type=api_pb2.PARAM_TYPE_BYTES)],
                )
            ],
        ),
        has_default=False,
    )


def test_schema_extraction_nested_dict():
    def f(nested_dict: dict[str, dict[str, bytes]] = {}): ...

    (dict_spec,) = get_callable_schema(f, is_web_endpoint=False).arguments
    assert dict_spec == api_pb2.ClassParameterSpec(
        name="nested_dict",
        full_type=api_pb2.GenericPayloadType(
            base_type=api_pb2.PARAM_TYPE_DICT,
            sub_types=[
                api_pb2.GenericPayloadType(
                    base_type=api_pb2.PARAM_TYPE_STRING,
                ),
                api_pb2.GenericPayloadType(
                    base_type=api_pb2.PARAM_TYPE_DICT,
                    sub_types=[
                        api_pb2.GenericPayloadType(base_type=api_pb2.PARAM_TYPE_STRING),
                        api_pb2.GenericPayloadType(base_type=api_pb2.PARAM_TYPE_BYTES),
                    ],
                ),
            ],
        ),
        has_default=True,
    )


def test_schema_extraction_dict_with_non_str_key_is_unknown():
    def f(dct: dict): ...

    (dict_spec,) = get_callable_schema(f, is_web_endpoint=False).arguments
    print(dict_spec)
    assert dict_spec == api_pb2.ClassParameterSpec(
        name="dct",
        full_type=api_pb2.GenericPayloadType(
            base_type=api_pb2.PARAM_TYPE_DICT,
        ),
    )


def test_schema_extraction_bool():
    def f(bool_val: bool = True): ...

    (bool_spec,) = signature_to_parameter_specs(inspect.signature(f))
    print(bool_spec)
    assert bool_spec == api_pb2.ClassParameterSpec(
        name="bool_val",
        type=api_pb2.PARAM_TYPE_BOOL,
        full_type=api_pb2.GenericPayloadType(
            base_type=api_pb2.PARAM_TYPE_BOOL,
        ),
        has_default=True,
        bool_default=True,
    )


@pytest.mark.parametrize("v", [True, False])
def test_parameter_value_serde_bool(v):
    encoded = parameter_serde_registry.encode(v)
    assert encoded == api_pb2.ClassParameterValue(type=api_pb2.PARAM_TYPE_BOOL, bool_value=v)
    decoded = parameter_serde_registry.decode(encoded)
    assert decoded is v


def test_parameter_validate_bool():
    with pytest.raises(TypeError):
        parameter_serde_registry.validate_value_for_enum_type(api_pb2.PARAM_TYPE_BOOL, 1)



================================================
FILE: test/should_upload_test.py
================================================
# Copyright Modal Labs 2025
from dataclasses import dataclass

from modal._utils.blob_utils import MAX_ASYNC_OBJECT_SIZE_BYTES, MAX_OBJECT_SIZE_BYTES
from modal._utils.function_utils import should_upload
from modal_proto import api_pb2


@dataclass
class Input:
    size: int
    is_async: bool
    should_use_blob: bool


def test_should_upload():
    SMALL = MAX_ASYNC_OBJECT_SIZE_BYTES // 2
    LARGE = MAX_ASYNC_OBJECT_SIZE_BYTES * 2
    HUGE = MAX_OBJECT_SIZE_BYTES * 2
    test_cases = [
        Input(size=SMALL, is_async=True, should_use_blob=False),
        Input(size=SMALL, is_async=False, should_use_blob=False),
        Input(size=LARGE, is_async=True, should_use_blob=True),
        Input(size=LARGE, is_async=False, should_use_blob=False),
        Input(size=HUGE, is_async=True, should_use_blob=True),
        Input(size=HUGE, is_async=False, should_use_blob=True),
    ]

    for case in test_cases:
        used_blob = should_upload(
            case.size,
            max_object_size_bytes=MAX_OBJECT_SIZE_BYTES,
            function_call_invocation_type=(
                api_pb2.FUNCTION_CALL_INVOCATION_TYPE_ASYNC
                if case.is_async
                else api_pb2.FUNCTION_CALL_INVOCATION_TYPE_SYNC
            ),
        )

        assert used_blob == case.should_use_blob


def test_should_upload_with_max_object_size_bytes():
    # below threshold should not upload
    assert not should_upload(
        999,
        max_object_size_bytes=1000,
        function_call_invocation_type=api_pb2.FUNCTION_CALL_INVOCATION_TYPE_SYNC,
    )
    # equal to threshold should not upload
    assert not should_upload(
        1000,
        max_object_size_bytes=1000,
        function_call_invocation_type=api_pb2.FUNCTION_CALL_INVOCATION_TYPE_SYNC,
    )
    # above threshold should upload
    assert should_upload(
        1001,
        max_object_size_bytes=1000,
        function_call_invocation_type=api_pb2.FUNCTION_CALL_INVOCATION_TYPE_SYNC,
    )



================================================
FILE: test/shutdown_test.py
================================================
# Copyright Modal Labs 2024
import asyncio
import pytest
import threading
import time

import grpclib

import modal
from modal._utils.async_utils import synchronize_api
from modal.client import Client
from modal.exception import ClientClosed
from modal_proto import api_pb2


def close_client_soon(client):
    def cb():
        time.sleep(0.1)
        client._close()

    threading.Thread(target=cb).start()


@pytest.mark.timeout(5)
def test_client_shutdown_raises_client_closed(servicer, credentials):
    # Queue.get() loops rpc calls until it gets a response - make sure it shuts down
    # if the client is closed and doesn't stay in an indefinite retry loop
    with Client(servicer.client_addr, api_pb2.CLIENT_TYPE_CLIENT, credentials) as client:
        with modal.Queue.ephemeral(client=client) as q:
            close_client_soon(client)  # simulate an early shutdown of the client
            with pytest.raises(modal.exception.ClientClosed):
                # ensure that ongoing rcp calls are aborted
                q.get()

            with pytest.raises(modal.exception.ClientClosed):
                # ensure the client isn't doesn't allow for *new* connections
                # after shutdown either
                q.get()


@pytest.mark.timeout(5)
@pytest.mark.asyncio
async def test_client_shutdown_raises_client_closed_streaming(servicer, credentials, caplog):
    # Queue.get() loops rpc calls until it gets a response - make sure it shuts down
    # if the client is closed and doesn't stay in an indefinite retry loop

    async def _mocked_logs_loop(client: Client, app_id: str):
        request = api_pb2.AppGetLogsRequest(
            app_id=app_id,
            task_id="",
            timeout=55,
            last_entry_id="",
        )
        async for _ in client.stub.AppGetLogs.unary_stream(request):
            pass

    sync_log_loop = synchronize_api(_mocked_logs_loop)

    with Client(servicer.client_addr, api_pb2.CLIENT_TYPE_CLIENT, credentials) as client:
        t = asyncio.create_task(sync_log_loop.aio(client, "ap-1"))
        await asyncio.sleep(0.1)  # in loop

    with pytest.raises(ClientClosed):
        await t

    with Client(servicer.client_addr, api_pb2.CLIENT_TYPE_CLIENT, credentials) as client:
        t = asyncio.create_task(_mocked_logs_loop(client, "ap-1"))
        await asyncio.sleep(0.1)  # in loop

    with pytest.raises(grpclib.exceptions.StreamTerminatedError):
        await t
    assert len(caplog.records) == 3  # open, send and recv called outside of task context
    for rec in caplog.records:
        assert "made outside of task context" in rec.message


@pytest.mark.timeout(5)
@pytest.mark.asyncio
async def test_client_close_cancellation_context_only_used_in_correct_event_loop(servicer, credentials, caplog):
    with Client(servicer.client_addr, api_pb2.CLIENT_TYPE_CLIENT, credentials) as client:
        with modal.Queue.ephemeral(client=client) as q:
            request = api_pb2.QueueGetRequest(
                queue_id=q.object_id,
                partition_key=b"",
                timeout=10,
                n_values=1,
            )
            # this request should not use task context since it's not issued from the same loop
            # that the task context is triggered from, otherwise we'll get cross-event loop
            # waits/cancellations etc.
            t = asyncio.create_task(client.stub.QueueGet(request))
            await asyncio.sleep(0.1)
    with pytest.raises(grpclib.exceptions.StreamTerminatedError):
        await t
    expected_warnings = [msg for msg in caplog.messages if "QueueGet made outside of task context" in msg]
    assert len(expected_warnings) == 1



================================================
FILE: test/slow_dependencies_test.py
================================================
# Copyright Modal Labs 2024
import subprocess
import sys


def test_slow_dependencies_local(supports_dir):
    # Make sure that "import modal" doesn't load some big dependencies like aiohttp
    subprocess.check_output([sys.executable, supports_dir / "slow_dependencies_local.py"])


def test_slow_dependencies_container(supports_dir):
    # Make sure that "import modal._container_entrypoint" doesn't load some big dependencies like aiohttp
    subprocess.check_output([sys.executable, supports_dir / "slow_dependencies_container.py"])



================================================
FILE: test/static_types_test.py
================================================
# Copyright Modal Labs 2024
import pytest
import subprocess

from test.supports.skip import skip_old_py, skip_windows


@pytest.fixture(scope="module")
def generate_type_stubs():
    subprocess.check_call(["inv", "type-stubs"])


@skip_windows("Type tests fail on windows since they don't exclude non-windows features")
@skip_old_py("can't generate type stubs w/ Concatenate on <3.10", (3, 10))
@pytest.mark.usefixtures("generate_type_stubs")
def test_remote_call_keeps_original_return_value():
    subprocess.check_call(["mypy", "test/supports/type_assertions.py"])


@skip_windows("Type tests fail on windows since they don't exclude non-windows features")
@skip_old_py("can't generate type stubs w/ Concatenate on <3.10", (3, 10))
@pytest.mark.usefixtures("generate_type_stubs")
def test_negative_assertions():
    p = subprocess.Popen(
        ["mypy", "test/supports/type_assertions_negative.py"],
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        encoding="utf8",
    )
    stdout, _ = p.communicate()
    assert p.returncode == 1
    print(stdout)
    assert "Found 6 errors in 1 file" in stdout
    assert 'Unexpected keyword argument "b" for "__call__"' in stdout
    assert 'Argument "a" to "__call__" of "__remote_spec" has incompatible type "int"' in stdout
    assert 'Unexpected keyword argument "c" for "local" of "Function"' in stdout
    assert 'Argument "a" to "local" of "Function" has incompatible type "int"' in stdout
    assert 'Unexpected keyword argument "e" for "aio" of "__remote_spec"' in stdout
    assert 'Argument "a" to "aio" of "__remote_spec" has incompatible type "float"' in stdout



================================================
FILE: test/telemetry_test.py
================================================
# Copyright Modal Labs 2024

import json
import logging
import os
import pytest
import queue
import socket
import sys
import tempfile
import threading
import time
import typing
import uuid
from pathlib import Path
from struct import unpack

from modal._runtime.telemetry import (
    MESSAGE_HEADER_FORMAT,
    MESSAGE_HEADER_LEN,
    ImportInterceptor,
    instrument_imports,
    supported_platform,
)


class TelemetryConsumer:
    socket_filename: Path
    server: socket.socket
    connections: set[socket.socket]
    events: queue.Queue
    tmp: tempfile.TemporaryDirectory

    def __init__(self):
        self.stopped = False
        self.events = queue.Queue()

    def __enter__(self):
        self.start()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.stop()

    def start(self):
        self.tmp = tempfile.TemporaryDirectory()
        self.socket_filename = Path(self.tmp.name) / "telemetry.sock"
        self.connections = set()
        self.server = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
        self.server.bind(self.socket_filename.as_posix())
        self.server.listen()
        listener = threading.Thread(target=self._listen, daemon=True)
        listener.start()

    def stop(self):
        self.stopped = True
        self.server.close()
        for conn in list(self.connections):
            conn.close()

    def _listen(self):
        while not self.stopped:
            try:
                conn, _ = self.server.accept()
                receiver = threading.Thread(target=self._recv, args=(conn,), daemon=True)
                receiver.start()
                self.connections.add(conn)
            except OSError as e:
                logging.debug(f"listener got exception, exiting: {e}")
                return

    def _recv(self, conn):
        try:
            buffer = bytearray()
            while not self.stopped:
                try:
                    data = conn.recv(1024)
                except OSError as e:
                    logging.debug(f"connection {conn} got exception, exiting: {e}")
                    return
                buffer.extend(data)
                while True:
                    if len(buffer) <= MESSAGE_HEADER_LEN:
                        break
                    message_len = unpack(MESSAGE_HEADER_FORMAT, buffer[0:MESSAGE_HEADER_LEN])[0]
                    if len(buffer) < message_len + MESSAGE_HEADER_LEN:
                        break
                    message_bytes = buffer[MESSAGE_HEADER_LEN : MESSAGE_HEADER_LEN + message_len]
                    buffer = buffer[MESSAGE_HEADER_LEN + message_len :]
                    message = message_bytes.decode("utf-8").strip()
                    message = json.loads(message)
                    self.events.put(message)
        finally:
            self.connections.remove(conn)


def test_import_tracing(monkeypatch):
    if not supported_platform():
        pytest.skip(f"unsupported platform: {sys.platform}")

    with TelemetryConsumer() as consumer, ImportInterceptor.connect(consumer.socket_filename.absolute().as_posix()):
        from .telemetry import tracing_module_1  # noqa

        expected_messages: list[dict[str, typing.Any]] = [
            {"event": "module_load_start", "attributes": {"name": "test.telemetry.tracing_module_1"}},
            {"event": "module_load_start", "attributes": {"name": "test.telemetry.tracing_module_2"}},
            {"event": "module_load_end", "attributes": {"name": "test.telemetry.tracing_module_2"}},
            {"event": "module_load_end", "attributes": {"name": "test.telemetry.tracing_module_1"}},
        ]

        for expected_message in expected_messages:
            m = consumer.events.get(timeout=30)
            # skip this test module - behavior seems to vary depending on timing and maybe python version etc
            while m["attributes"]["name"] == "test.telemetry":
                m = consumer.events.get(timeout=30)
            assert m["event"] == expected_message["event"]
            assert m["attributes"]["name"] == expected_message["attributes"]["name"]
            assert m["timestamp"] >= 0
            assert uuid.UUID(m["span_id"])
            if m["event"] == "module_load_end":
                assert m["attributes"]["latency"] >= 0


# For manual testing
def generate_import_telemetry(telemetry_socket):
    instrument_imports(telemetry_socket)
    t0 = time.monotonic()
    import kubernetes  # noqa

    return time.monotonic() - t0


# For manual testing
def main():
    telemetry_socket = os.environ.get("MODAL_TELEMETRY_SOCKET")
    if telemetry_socket:
        latency = generate_import_telemetry(telemetry_socket)
    else:
        with TelemetryConsumer() as consumer:
            latency = generate_import_telemetry(consumer.socket_filename.absolute().as_posix())

            while True:
                try:
                    m = consumer.events.get_nowait()
                    print(m)
                except queue.Empty:
                    break

    print(f"import kubernetes took {latency:.02}s")


if __name__ == "__main__":
    main()



================================================
FILE: test/token_flow_test.py
================================================
# Copyright Modal Labs 2023
import pytest

import aiohttp

from modal.token_flow import TokenFlow


@pytest.mark.asyncio
async def test_token_flow_server(servicer, client):
    tf = TokenFlow(client)
    async with tf.start() as (token_flow_id, _, _):
        # Make a request against the local web server and make sure it validates
        localhost_url = f"http://localhost:{servicer.token_flow_localhost_port}"
        async with aiohttp.ClientSession() as session:
            async with session.get(localhost_url) as resp:
                text = await resp.text()
                assert text == token_flow_id



================================================
FILE: test/traceback_test.py
================================================
# Copyright Modal Labs 2024
import pytest
import sys
import types
import typing
from pathlib import Path
from traceback import extract_tb

import grpclib
from grpclib import GRPCError, Status

import modal
from modal._traceback import (
    append_modal_tb,
    extract_traceback,
    reduce_traceback_to_user_code,
    traceback_contains_remote_call,
)
from modal._vendor import tblib
from modal.exception import NotFoundError

from .supports.raise_error import raise_error

SUPPORT_MODULE = "supports.raise_error"


def call_raise_error():
    raise_error()


def test_extract_traceback():
    task_id = "ta-123"
    try:
        call_raise_error()
    except Exception as exc:
        tb_dict, line_cache = extract_traceback(exc, task_id)

    test_path = Path(__file__)
    support_path = test_path.parent / (SUPPORT_MODULE.replace(".", "/") + ".py")

    frame = tb_dict["tb_frame"]
    assert tb_dict["tb_lineno"] == frame["f_lineno"] - 2
    assert frame["f_code"]["co_filename"] == f"<{task_id}>:{test_path}"
    assert frame["f_code"]["co_name"] == "test_extract_traceback"
    assert frame["f_globals"]["__file__"] == str(test_path)
    assert frame["f_globals"]["__name__"] == f"test.{test_path.name[:-3]}"
    assert frame["f_locals"] == {}

    frame = tb_dict["tb_next"]["tb_frame"]
    assert frame["f_code"]["co_filename"] == f"<{task_id}>:{test_path}"
    assert frame["f_code"]["co_name"] == "call_raise_error"
    assert frame["f_globals"]["__file__"] == str(test_path)
    assert frame["f_globals"]["__name__"] == f"test.{test_path.name[:-3]}"
    assert frame["f_locals"] == {}

    frame = tb_dict["tb_next"]["tb_next"]["tb_frame"]
    assert frame["f_code"]["co_filename"] == f"<{task_id}>:{support_path}"
    assert frame["f_code"]["co_name"] == "raise_error"
    assert frame["f_globals"]["__file__"] == str(support_path)
    assert frame["f_globals"]["__name__"] == f"test.{SUPPORT_MODULE}"
    assert frame["f_locals"] == {}

    assert tb_dict["tb_next"]["tb_next"]["tb_next"] is None

    line_cache_list = list(line_cache.items())
    assert line_cache_list[0][0][0] == str(test_path)
    assert line_cache_list[0][1] == "call_raise_error()"
    assert line_cache_list[1][0][0] == str(test_path)
    assert line_cache_list[1][1] == "raise_error()"
    assert line_cache_list[2][0][0] == str(support_path)
    assert line_cache_list[2][1] == 'raise RuntimeError("Boo!")'


def test_append_modal_tb():
    task_id = "ta-123"
    try:
        call_raise_error()
    except Exception as exc:
        tb_dict, line_cache = extract_traceback(exc, task_id)

    try:
        raise RuntimeError("Remote error")
    except Exception as exc:
        remote_exc = exc
        append_modal_tb(exc, tb_dict, line_cache)

    assert remote_exc.__line_cache__ == line_cache  # type: ignore
    frames = [f.name for f in extract_tb(remote_exc.__traceback__)]
    assert frames == ["test_append_modal_tb", "call_raise_error", "raise_error"]


def make_tb_stack(frames: list[tuple[str, str]]) -> list[dict]:
    """Given a minimal specification of (code filename, code name), return dict formatted for tblib."""
    tb_frames = []
    for lineno, (filename, name) in enumerate(frames):
        tb_frames.append(
            {
                "tb_lineno": lineno,
                "tb_frame": {
                    "f_lineno": lineno,
                    "f_globals": {},
                    "f_locals": {},
                    "f_code": {"co_filename": filename, "co_name": name},
                },
            }
        )
    return tb_frames


def tb_dict_from_stack_dicts(stack: list[dict]) -> dict:
    tb_root = tb = stack.pop(0)
    while stack:
        tb["tb_next"] = stack.pop(0)
        tb = tb["tb_next"]
    tb["tb_next"] = None
    return tb_root


@pytest.mark.parametrize("user_mode", ["script", "module"])
def test_reduce_traceback_to_user_code(user_mode):
    if user_mode == "script":
        user_source, user_filename, user_name = ("/root/user/ai.py", "/root/user/ai.py", "train")
    elif user_mode == "module":
        user_source, user_filename, user_name = ("ai.training", "/root/user/ai/training.py", "<module>")

    stack = [
        ("/modal/__main__.py", "main"),
        ("/modal/entrypoint.py", "run"),
        ("/site-packages/synchronicity/wizard.py", "magic"),
        (user_filename, user_name),
        ("/modal/function.py", "execute"),
        ("/site-packages/synchronicity/devil.py", "pitchfork"),
    ]

    tb_dict = tb_dict_from_stack_dicts(make_tb_stack(stack))
    tb = tblib.Traceback.from_dict(tb_dict)
    tb_out = reduce_traceback_to_user_code(tb, user_source)

    f = tb_out.tb_frame
    assert f.f_code.co_filename == user_filename
    assert f.f_code.co_name == user_name

    f = tb_out.tb_next.tb_frame
    assert f.f_code.co_filename == "/modal/function.py"
    assert f.f_code.co_name == "execute"

    assert tb_out.tb_next.tb_next is None


def test_traceback_contains_remote_call():
    stack = [
        ("/home/foobar/code/script.py", "f"),
        ("/usr/local/venv/modal.py", "local"),
    ]

    tb = tblib.Traceback.from_dict(tb_dict_from_stack_dicts(make_tb_stack(stack)))
    assert not traceback_contains_remote_call(tb)

    task_id = "ta-0123456789ABCDEFGHILJKMNOP"
    stack.extend(
        [
            (f"<{task_id}>:/usr/local/lib/python3.11/importlib/__init__.py", ""),
            ("/root/script.py", ""),
        ]
    )

    tb = tblib.Traceback.from_dict(tb_dict_from_stack_dicts(make_tb_stack(stack)))
    assert traceback_contains_remote_call(tb)


ModuleOrFilename = typing.Union[types.ModuleType, str]


def to_path(mof: ModuleOrFilename) -> Path:
    if isinstance(mof, str):
        return Path(mof)
    module_file = Path(mof.__file__)
    if module_file.name == "__init__.py":
        return module_file.parent
    return module_file


def assert_expected_traceback(traceback, expected_module_frames: list[tuple[ModuleOrFilename, str]]):
    failure = ""
    for i, frame in enumerate(traceback):
        if i >= len(expected_module_frames):
            failure = "(past end of expected traceback)"
        else:
            expected_path = to_path(expected_module_frames[i][0])
            expected_name = expected_module_frames[i][1]
            if expected_path != Path(frame.path) or expected_name != frame.name:
                failure = f"Expected: {str(expected_path)}, {expected_name}"

        if failure:
            if sys.version_info >= (3, 11):
                full_tb = "\n".join(f"{'>>>' if i == j else ''}{frame}" for j, frame in enumerate(traceback))
                raise AssertionError(f"Unexpected traceback frame:\n{full_tb}\n{failure}")
            else:
                # we don't skip these tests, since we still want to exercise that the related
                # traceback transforms don't completely break things on older Pythons
                print("Traceback assertion failed on non 3.11 Python - this is expected")


def test_internal_frame_suppression_graceful_error(set_env_client, servicer):
    # when converting a grpc error into a modal error, like modal.exceptions.NotFoundError
    with servicer.intercept() as ctx:

        async def QueueGetOrCreate(self, stream):
            raise GRPCError(Status.NOT_FOUND)

        ctx.set_responder("QueueGetOrCreate", QueueGetOrCreate)

        with pytest.raises(NotFoundError) as exc_info:
            modal.Queue.from_name("asdlfjkjalsdkf").get()

        assert_expected_traceback(
            exc_info.traceback,
            [
                (__file__, "test_internal_frame_suppression_graceful_error"),  # this frame
                (modal._object, "wrapped"),  # from @live_method calling .hydrate()
                (modal.queue, "_load"),
            ],
        )


def test_internal_frame_suppression_internal_error(set_env_client, servicer):
    # internal grpc errors that aren't wrapped
    with servicer.intercept() as ctx:

        async def QueueGetOrCreate(self, stream):
            raise GRPCError(status=Status.INTERNAL, message="kaboom")

        ctx.set_responder("QueueGetOrCreate", QueueGetOrCreate)
        with pytest.raises(GRPCError, match="kaboom") as exc_info:
            modal.Queue.from_name("asdlfjkjalsdkf").get()

        assert_expected_traceback(
            exc_info.traceback,
            [
                (__file__, "test_internal_frame_suppression_internal_error"),  # this frame
                (modal._object, "wrapped"),  # from @live_method calling .hydrate()
                (modal.queue, "_load"),
                (grpclib.client, "_raise_for_grpc_status"),  # raw status
            ],
        )



================================================
FILE: test/tunnel_test.py
================================================
# Copyright Modal Labs 2023

import pytest

from modal import forward
from modal.exception import InvalidError

from .supports.skip import skip_windows_unix_socket


def test_tunnel_outside_container(client):
    with pytest.raises(InvalidError):
        with forward(8000, client=client):
            pass


@skip_windows_unix_socket
def test_invalid_port_numbers(container_client):
    for port in (-1, 0, 65536):
        with pytest.raises(InvalidError):
            with forward(port, client=container_client):
                pass


@skip_windows_unix_socket
def test_create_tunnel(container_client):
    with forward(8000, client=container_client) as tunnel:
        assert tunnel.host == "8000.modal.test"
        assert tunnel.url == "https://8000.modal.test"



================================================
FILE: test/user_code_import_test.py
================================================
# Copyright Modal Labs 2024
from unittest.mock import MagicMock

from modal._runtime import user_code_imports
from modal._utils.async_utils import synchronizer
from modal.image import _Image
from modal_proto import api_pb2


def test_import_function(supports_dir, monkeypatch):
    monkeypatch.syspath_prepend(supports_dir)
    fun = api_pb2.Function(
        module_name="user_code_import_samples.func",
        function_name="f",
        supported_output_formats=[api_pb2.DATA_FORMAT_CBOR],
    )
    service = user_code_imports.import_single_function_service(
        fun,
        None,
    )
    assert len(service.service_deps) == 1
    assert type(service.service_deps[0]) is _Image
    assert service.app

    assert service.user_cls_instance is None

    # TODO (elias): shouldn't have to pass the function definition again!
    io_manager = MagicMock()  # shouldn't actually be used except by web endpoints - indicates some need for refactoring
    finalized_funcs = service.get_finalized_functions(fun, container_io_manager=io_manager)
    assert len(finalized_funcs) == 1
    finalized_func = finalized_funcs[""]
    assert finalized_func.is_async is False
    assert finalized_func.is_generator is False
    assert finalized_func.supported_output_formats == [api_pb2.DATA_FORMAT_CBOR]
    assert finalized_func.lifespan_manager is None
    container_callable = finalized_func.callable
    assert container_callable("world") == "hello world"


def test_import_function_undecorated(monkeypatch, supports_on_path):
    import user_code_import_samples.func

    fun = api_pb2.Function(
        module_name="user_code_import_samples.func",
        function_name="undecorated_f",
        app_name="user_code_import_samples_func_app",
    )
    service = user_code_imports.import_single_function_service(
        fun,
        None,
    )
    assert service.service_deps is None  # undecorated - can't get code deps
    # can't get app via the decorator attachment, falls back to checking global registry of apps/names
    assert service.app is synchronizer._translate_in(user_code_import_samples.func.app)


def test_import_class(monkeypatch, supports_dir, client):
    monkeypatch.syspath_prepend(supports_dir)
    function_def = api_pb2.Function(
        module_name="user_code_import_samples.cls",
        function_name="C.*",
        supported_output_formats=[api_pb2.DATA_FORMAT_PICKLE, api_pb2.DATA_FORMAT_CBOR],
    )
    service = user_code_imports.import_class_service(
        function_def,
        service_function_hydration_data=api_pb2.Object(
            object_id="fu-123",
        ),
        class_id="cs-123",
        client=client,
        ser_user_cls=None,
        cls_args=(),
        cls_kwargs={},
    )
    assert len(service.service_deps) == 1
    assert type(service.service_deps[0]) is _Image

    assert service.app

    from user_code_import_samples.cls import UndecoratedC  # type: ignore

    assert isinstance(service.user_cls_instance, UndecoratedC)

    # TODO (elias): shouldn't have to pass the function definition again!
    io_manager = MagicMock()  # shouldn't actually be used except by web endpoints - indicates some need for refactoring
    finalized_funcs = service.get_finalized_functions(function_def, container_io_manager=io_manager)
    io_manager.assert_not_called()
    assert len(finalized_funcs) == 4

    for finalized in finalized_funcs.values():
        assert finalized.is_async is False
        assert finalized.is_generator is False
        assert finalized.supported_output_formats == [api_pb2.DATA_FORMAT_PICKLE, api_pb2.DATA_FORMAT_CBOR]
        assert finalized.lifespan_manager is None

    finalized_1, finalized_2, self_ref = finalized_funcs["f"], finalized_funcs["f2"], finalized_funcs["self_ref"]
    assert finalized_1.callable("world") == "hello world"
    assert finalized_2.callable("world") == "other world"
    callable_self = self_ref.callable()
    assert isinstance(callable_self, UndecoratedC)  # Arguably this could/should be an Obj instead?


# TODO: add test cases for serialized functions, web endpoints, explicit/implicit generators etc.
#   with and without decorators in globals scope...



================================================
FILE: test/utils_test.py
================================================
# Copyright Modal Labs 2022
import asyncio
import hashlib
import io
import pytest

from modal._utils.bytes_io_segment_payload import BytesIOSegmentPayload
from modal._utils.name_utils import (
    check_object_name,
    is_valid_environment_name,
    is_valid_object_name,
    is_valid_subdomain_label,
    is_valid_tag,
)
from modal._utils.package_utils import parse_major_minor_version
from modal.exception import InvalidError


def test_subdomain_label():
    assert is_valid_subdomain_label("banana")
    assert is_valid_subdomain_label("foo-123-456")
    assert not is_valid_subdomain_label("BaNaNa")
    assert not is_valid_subdomain_label(" ")
    assert not is_valid_subdomain_label("ban/ana")


def test_object_name():
    assert is_valid_object_name("baNaNa")
    assert is_valid_object_name("foo-123_456")
    assert is_valid_object_name("a" * 64)
    assert not is_valid_object_name("hello world")
    assert not is_valid_object_name("a" * 65)
    assert not is_valid_object_name("ap-abcdefghABCDEFGH012345")
    with pytest.raises(InvalidError, match="Invalid Volume name: 'foo/bar'"):
        check_object_name("foo/bar", "Volume")


def test_environment_name():
    assert is_valid_object_name("a" * 64)
    assert not is_valid_object_name("a" * 65)
    assert not is_valid_environment_name("--help")
    assert not is_valid_environment_name(":env")
    assert not is_valid_environment_name("env:env")
    assert not is_valid_environment_name("/env")
    assert not is_valid_environment_name("env/env")
    assert not is_valid_environment_name("")


def test_tag():
    assert is_valid_tag("v1.0.0")
    assert is_valid_tag("a38298githash39920bk")
    assert not is_valid_tag("v1 .0.0-alpha")
    assert not is_valid_tag("$$$build")


@pytest.mark.asyncio
async def test_file_segment_payloads():
    data = io.BytesIO(b"abc123")
    data2 = io.BytesIO(data.getbuffer())

    class DummyOutput:  # AbstractStreamWriter
        def __init__(self):
            self.value = b""

        async def write(self, chunk: bytes):
            self.value += chunk

    out1 = DummyOutput()
    out2 = DummyOutput()
    p1 = BytesIOSegmentPayload(data, 0, 3)
    p2 = BytesIOSegmentPayload(data2, 3, 3)

    # "out of order" writes
    await p2.write(out2)  # type: ignore
    await p1.write(out1)  # type: ignore
    assert out1.value == b"abc"
    assert out2.value == b"123"
    assert p1.md5_checksum().digest() == hashlib.md5(b"abc").digest()
    assert p2.md5_checksum().digest() == hashlib.md5(b"123").digest()

    data = io.BytesIO(b"abc123")

    # test reset_on_error
    all_data = BytesIOSegmentPayload(data, 0, 6)

    class DummyExc(Exception):
        pass

    try:
        with all_data.reset_on_error():
            await all_data.write(DummyOutput())  # type: ignore
    except DummyExc:
        pass

    out = DummyOutput()
    await all_data.write(out)  # type: ignore
    assert out.value == b"abc123"


@pytest.mark.asyncio
async def test_file_segment_payloads_concurrency():
    data = io.BytesIO((b"123" * 1024 * 350)[: 1024 * 1024])  # 1 MiB
    data2 = io.BytesIO(data.getbuffer())

    class DummyOutput:  # AbstractStreamWriter
        def __init__(self):
            self.value = b""

        async def write(self, chunk: bytes):
            self.value += chunk

    out1 = DummyOutput()
    out2 = DummyOutput()
    p1 = BytesIOSegmentPayload(data, 0, len(data.getvalue()) // 2, chunk_size=100 * 1024)  # 100 KiB chunks
    p2 = BytesIOSegmentPayload(data2, len(data.getvalue()) // 2, len(data.getvalue()) // 2, chunk_size=100 * 1024)
    await asyncio.gather(p2.write(out2), p1.write(out1))  # type: ignore
    assert out1.value + out2.value == data.getvalue()


def test_parse_major_minor_version():
    assert parse_major_minor_version("3.11") == (3, 11)
    assert parse_major_minor_version("3.9.1") == (3, 9)
    assert parse_major_minor_version("3.10.1rc0") == (3, 10)
    with pytest.raises(ValueError, match="at least an 'X.Y' format"):
        parse_major_minor_version("123")
    with pytest.raises(ValueError, match="at least an 'X.Y' format with integral"):
        parse_major_minor_version("x.y")



================================================
FILE: test/version_test.py
================================================
# Copyright Modal Labs 2022
import pkg_resources

import modal


def test_version():
    mod_version = modal.__version__
    pkg_version = pkg_resources.require("modal")[0].version

    assert pkg_resources.parse_version(mod_version) > pkg_resources.parse_version("0.0.0")
    assert pkg_resources.parse_version(pkg_version) > pkg_resources.parse_version("0.0.0")

    assert mod_version == pkg_version



================================================
FILE: test/volume_test.py
================================================
# Copyright Modal Labs 2023
import asyncio
import io
import os
import platform
import pytest
import random
import re
import sys
import time
from difflib import ndiff
from pathlib import Path
from unittest import mock

import modal
from modal._utils.blob_utils import BLOCK_SIZE
from modal.exception import AlreadyExistsError, DeprecationError, InvalidError, NotFoundError, VolumeUploadTimeoutError
from modal.volume import _open_files_error_annotation
from modal_proto import api_pb2

VERSIONS = [
    None,
    api_pb2.VOLUME_FS_VERSION_V2,
]


def dummy():
    pass


def assert_eq_large(left, right):
    assert len(left) == len(right)
    if left != right:
        raise AssertionError(ndiff(left.splitlines(), right.splitlines()))


def test_volume_info(servicer, client):
    name = "super-important-data"
    vol = modal.Volume.from_name(name, create_if_missing=True)
    assert vol.name == name

    vol.hydrate(client)
    info = vol.info()
    assert info.name == name
    assert info.created_by == servicer.default_username


@pytest.mark.parametrize("read_only", [True, False])
@pytest.mark.parametrize("version", VERSIONS)
def test_volume_mount(client, servicer, version, read_only):
    app = modal.App()

    vol = modal.Volume.from_name("xyz", create_if_missing=True, version=version)
    if read_only:
        vol = vol.read_only()

    _ = app.function(volumes={"/root/foo": vol})(dummy)

    with servicer.intercept() as ctx:
        with app.run(client=client):
            req = ctx.pop_request("FunctionCreate")
            assert req.function.volume_mounts[0].read_only == read_only


def test_volume_bad_paths():
    app = modal.App()
    vol = modal.Volume.from_name("xyz")

    with pytest.raises(InvalidError):
        app.function(volumes={"/root/../../foo": vol})(dummy)

    with pytest.raises(InvalidError):
        app.function(volumes={"/": vol})(dummy)

    with pytest.raises(InvalidError):
        app.function(volumes={"/tmp/": vol})(dummy)


def test_volume_mount_read_only_error(client):
    read_only_vol = modal.Volume.from_name("xyz", create_if_missing=True).read_only()
    read_only_vol_hydrated = read_only_vol.hydrate(client)

    with pytest.raises(InvalidError):
        read_only_vol_hydrated.batch_upload()

    with pytest.raises(InvalidError):
        read_only_vol_hydrated.remove_file("file1.txt")

    with pytest.raises(InvalidError):
        read_only_vol_hydrated.copy_files(["file1.txt"], "bar2")


def test_volume_duplicate_mount():
    app = modal.App()
    vol = modal.Volume.from_name("xyz")

    with pytest.raises(InvalidError):
        app.function(volumes={"/foo": vol, "/bar": vol})(dummy)


@pytest.mark.parametrize("skip_reload", [False, True])
def test_volume_commit(client, servicer, skip_reload):
    with servicer.intercept() as ctx:
        ctx.add_response("VolumeCommit", api_pb2.VolumeCommitResponse(skip_reload=skip_reload))
        ctx.add_response("VolumeCommit", api_pb2.VolumeCommitResponse(skip_reload=skip_reload))

        with modal.Volume.ephemeral(client=client) as vol:
            # Note that in practice this will not work unless run in a task.
            vol.commit()

            # Make sure we can commit through the provider too
            vol.commit()

            assert ctx.pop_request("VolumeCommit").volume_id == vol.object_id
            assert ctx.pop_request("VolumeCommit").volume_id == vol.object_id

            # commit should implicitly reload on successful commit if skip_reload=False
            assert servicer.volume_reloads[vol.object_id] == 0 if skip_reload else 2


@pytest.mark.asyncio
@pytest.mark.skip(reason="TODO(dflemstr) this test has started flaking at a high rate recently")
@pytest.mark.parametrize("version", VERSIONS)
@pytest.mark.parametrize("file_contents_size", [100, 8 * 1024 * 1024, 16 * 1024 * 1024, 32 * 1024 * 1024 + 4711])
async def test_volume_get(servicer, client, tmp_path, version, file_contents_size):
    await modal.Volume.objects.create.aio("my-vol", client=client, version=version)
    vol = await modal.Volume.from_name("my-vol").hydrate.aio(client=client)

    file_contents = random.randbytes(file_contents_size)
    file_path = "foo.bin"
    local_file_path = tmp_path / file_path
    local_file_path.write_bytes(file_contents)

    async with vol.batch_upload() as batch:
        batch.put_file(local_file_path, file_path)

    data = b""
    for chunk in vol.read_file(file_path):
        data += chunk

    # Faster assert to avoid huge error when there are large content differences:
    assert len(data) == file_contents_size
    assert data == file_contents

    output = io.BytesIO()
    vol.read_file_into_fileobj(file_path, output)

    # Faster assert to avoid huge error when there are large content differences:
    assert len(output.getvalue()) == file_contents_size
    assert output.getvalue() == file_contents

    with pytest.raises(FileNotFoundError):
        for _ in vol.read_file("/abc/def/i-dont-exist-at-all"):
            ...


def test_volume_reload(client, servicer):
    with modal.Volume.ephemeral(client=client) as vol:
        # Note that in practice this will not work unless run in a task.
        vol.reload()

        assert servicer.volume_reloads[vol.object_id] == 1


@pytest.mark.asyncio
@pytest.mark.parametrize("version", VERSIONS)
async def test_volume_batch_upload(servicer, client, tmp_path, version):
    local_file_path = tmp_path / "some_file"
    local_file_path.write_text("hello world")

    local_dir = tmp_path / "some_dir"
    local_dir.mkdir()
    (local_dir / "smol").write_text("###")

    subdir = local_dir / "subdir"
    subdir.mkdir()
    (subdir / "other").write_text("####")

    async with modal.Volume.ephemeral(client=client, version=version) as vol:
        with open(local_file_path, "rb") as fp:
            with vol.batch_upload() as batch:
                batch.put_file(local_file_path, "/some_file")
                batch.put_directory(local_dir, "/some_dir")
                batch.put_file(io.BytesIO(b"data from a file-like object"), "/filelike", mode=0o600)
                batch.put_directory(local_dir, "/non-recursive", recursive=False)
                batch.put_file(fp, "/filelike2")
        object_id = vol.object_id

    assert servicer.volumes[object_id].files.keys() == {
        "/some_file",
        "/some_dir/smol",
        "/some_dir/subdir/other",
        "/filelike",
        "/non-recursive/smol",
        "/filelike2",
    }
    assert servicer.volumes[object_id].files["/some_file"].data == b"hello world"
    assert servicer.volumes[object_id].files["/some_dir/smol"].data == b"###"
    assert servicer.volumes[object_id].files["/some_dir/subdir/other"].data == b"####"
    assert servicer.volumes[object_id].files["/filelike"].data == b"data from a file-like object"
    assert servicer.volumes[object_id].files["/filelike"].mode == 0o600
    assert servicer.volumes[object_id].files["/non-recursive/smol"].data == b"###"
    assert servicer.volumes[object_id].files["/filelike2"].data == b"hello world"
    assert servicer.volumes[object_id].files["/filelike2"].mode == 0o644


@pytest.mark.asyncio
@pytest.mark.parametrize("version", VERSIONS)
async def test_volume_batch_upload_bytesio(servicer, client, tmp_path, version):
    async with modal.Volume.ephemeral(client=client, version=version) as vol:
        with vol.batch_upload() as batch:
            batch.put_file(io.BytesIO(b"data from a file-like object"), "/filelike", mode=0o600)
        object_id = vol.object_id

    assert servicer.volumes[object_id].files.keys() == {
        "/filelike",
    }
    assert servicer.volumes[object_id].files["/filelike"].data == b"data from a file-like object"
    assert servicer.volumes[object_id].files["/filelike"].mode == 0o600


@pytest.mark.asyncio
@pytest.mark.parametrize("version", VERSIONS)
async def test_volume_batch_upload_opened_file(servicer, client, tmp_path, version):
    local_file_path = tmp_path / "some_file"
    local_file_path.write_text("hello world")

    async with modal.Volume.ephemeral(client=client, version=version) as vol:
        with open(local_file_path, "rb") as fp, vol.batch_upload() as batch:
            batch.put_file(fp, "/filelike2", mode=0o600)
        object_id = vol.object_id

    assert servicer.volumes[object_id].files.keys() == {
        "/filelike2",
    }
    assert servicer.volumes[object_id].files["/filelike2"].data == b"hello world"
    assert servicer.volumes[object_id].files["/filelike2"].mode == 0o600


@pytest.mark.asyncio
@pytest.mark.parametrize("version", VERSIONS)
async def test_volume_batch_upload_force(servicer, client, tmp_path, version):
    local_file_path = tmp_path / "some_file"
    local_file_path.write_text("hello world")

    local_file_path2 = tmp_path / "some_file2"
    local_file_path2.write_text("overwritten")

    async with modal.Volume.ephemeral(client=client, version=version) as vol:
        with servicer.intercept() as ctx:
            # Seed the volume
            with vol.batch_upload() as batch:
                batch.put_file(local_file_path, "/some_file")

            if version == api_pb2.VOLUME_FS_VERSION_V2:
                # The batch should involve two calls; once with a missing_blocks response
                assert ctx.pop_request("VolumePutFiles2").disallow_overwrite_existing_files
                assert ctx.pop_request("VolumePutFiles2").disallow_overwrite_existing_files
            else:
                assert ctx.pop_request("VolumePutFiles").disallow_overwrite_existing_files

            # Attempting to overwrite the file with force=False should result in an error
            with pytest.raises(FileExistsError):
                with vol.batch_upload(force=False) as batch:
                    batch.put_file(local_file_path, "/some_file")

            if version == api_pb2.VOLUME_FS_VERSION_V2:
                # The batch should fail on the first call since force=False
                assert ctx.pop_request("VolumePutFiles2").disallow_overwrite_existing_files
            else:
                assert ctx.pop_request("VolumePutFiles").disallow_overwrite_existing_files

            assert servicer.volumes[vol.object_id].files["/some_file"].data == b"hello world"

            # Overwriting should work with force=True
            with vol.batch_upload(force=True) as batch:
                batch.put_file(local_file_path2, "/some_file")

            if version == api_pb2.VOLUME_FS_VERSION_V2:
                # The batch should involve two calls; once with a missing_blocks response
                assert not ctx.pop_request("VolumePutFiles2").disallow_overwrite_existing_files
                assert not ctx.pop_request("VolumePutFiles2").disallow_overwrite_existing_files
            else:
                assert not ctx.pop_request("VolumePutFiles").disallow_overwrite_existing_files

            assert servicer.volumes[vol.object_id].files["/some_file"].data == b"overwritten"


@pytest.mark.asyncio
@pytest.mark.parametrize("version", VERSIONS)
async def test_volume_upload_removed_file(servicer, client, tmp_path, version):
    local_file_path = tmp_path / "some_file"
    local_file_path.write_text("hello world")

    async with modal.Volume.ephemeral(client=client, version=version) as vol:
        with pytest.raises(FileNotFoundError):
            with vol.batch_upload() as batch:
                batch.put_file(local_file_path, "/dest")
                local_file_path.unlink()


@pytest.mark.asyncio
async def test_volume_upload_large_file(client, tmp_path, servicer, blob_server):
    with mock.patch("modal._utils.blob_utils.LARGE_FILE_LIMIT", 10):
        local_file_path = tmp_path / "bigfile"
        local_file_path.write_text("hello world, this is a lot of text")

        async with modal.Volume.ephemeral(client=client) as vol:
            async with vol.batch_upload() as batch:
                batch.put_file(local_file_path, "/a")
            object_id = vol.object_id

        assert servicer.volumes[object_id].files.keys() == {"/a"}
        assert servicer.volumes[object_id].files["/a"].data == b""
        assert servicer.volumes[object_id].files["/a"].data_blob_id == "bl-1"

        _, blobs, _, _ = blob_server
        assert blobs["bl-1"] == b"hello world, this is a lot of text"


@pytest.mark.asyncio
async def test_volume2_upload_large_file(client, tmp_path, servicer, blob_server):
    # Volumes version 2 don't use `modal._utils.blob_utils.LARGE_FILE_LIMIT`
    # Instead, we need to go over 8MiB to trigger different behavior (ie spilling into multiple blocks), but in this
    # unit test context there isn't much of a semantic difference.

    # Create a byte buffer that is larger than 8MiB
    data = b"hello world, this is a lot of text" * 250_000
    assert len(data) > BLOCK_SIZE

    local_file_path = tmp_path / "bigfile"
    local_file_path.write_bytes(data)

    async with modal.Volume.ephemeral(client=client, version=api_pb2.VOLUME_FS_VERSION_V2) as vol:
        async with vol.batch_upload() as batch:
            batch.put_file(local_file_path, "/a")
        object_id = vol.object_id

    assert servicer.volumes[object_id].files.keys() == {"/a"}
    # Volumes version 2 don't use blob entities
    assert_eq_large(servicer.volumes[object_id].files["/a"].data, data)


@pytest.mark.asyncio
async def test_volume2_upload_large_blank_file(client, tmp_path, servicer, blob_server):
    # Volumes version 2 don't use `modal._utils.blob_utils.LARGE_FILE_LIMIT`
    # Instead, we need to go over 8MiB to trigger different behavior (ie spilling into multiple blocks), but in this
    # unit test context there isn't much of a semantic difference.

    # Create a byte buffer that is larger than 8MiB. Each block starts with b"a" followed by zeroes until the next
    # block boundary, except the last block that just contains b"cdef"
    data = (b"a" + (b"\0" * (8 * 1024 * 1024 - 1))) * 2 + b"cdef"
    assert len(data) > BLOCK_SIZE

    local_file_path = tmp_path / "bigfile"
    local_file_path.write_bytes(data)

    async with modal.Volume.ephemeral(client=client, version=api_pb2.VOLUME_FS_VERSION_V2) as vol:
        async with vol.batch_upload() as batch:
            batch.put_file(local_file_path, "/a")
        object_id = vol.object_id

    assert servicer.volumes[object_id].files.keys() == {"/a"}
    # Volumes version 2 don't use blob entities
    assert_eq_large(servicer.volumes[object_id].files["/a"].data, data)


@pytest.mark.asyncio
async def test_volume_upload_large_stream(client, servicer, blob_server):
    with mock.patch("modal._utils.blob_utils.LARGE_FILE_LIMIT", 10):
        stream = io.BytesIO(b"hello world, this is a lot of text")

        async with modal.Volume.ephemeral(client=client) as vol:
            async with vol.batch_upload() as batch:
                batch.put_file(stream, "/a")
            object_id = vol.object_id

        assert servicer.volumes[object_id].files.keys() == {"/a"}
        assert servicer.volumes[object_id].files["/a"].data == b""
        assert servicer.volumes[object_id].files["/a"].data_blob_id == "bl-1"

        _, blobs, _, _ = blob_server
        assert blobs["bl-1"] == b"hello world, this is a lot of text"


@pytest.mark.asyncio
async def test_volume2_upload_large_stream(client, servicer, blob_server):
    # Volumes version 2 don't use `modal._utils.blob_utils.LARGE_FILE_LIMIT`
    # Instead, we need to go over 8MiB to trigger different behavior (ie spilling into multiple blocks), but in this
    # unit test context there isn't much of a semantic difference.

    # Create a byte buffer that is larger than 8MiB
    data = b"hello world, this is a lot of text" * 250_000
    assert len(data) > BLOCK_SIZE

    stream = io.BytesIO(data)

    async with modal.Volume.ephemeral(client=client, version=api_pb2.VOLUME_FS_VERSION_V2) as vol:
        async with vol.batch_upload() as batch:
            batch.put_file(stream, "/a")
        object_id = vol.object_id

    assert servicer.volumes[object_id].files.keys() == {"/a"}
    assert servicer.volumes[object_id].files["/a"].data == data


@pytest.mark.asyncio
async def test_volume_upload_file_timeout(client, tmp_path, servicer, blob_server, *args):
    call_count = 0

    async def mount_put_file(self, stream):
        await stream.recv_message()
        nonlocal call_count
        call_count += 1
        await stream.send_message(api_pb2.MountPutFileResponse(exists=False))

    with servicer.intercept() as ctx:
        ctx.set_responder("MountPutFile", mount_put_file)
        with mock.patch("modal._utils.blob_utils.LARGE_FILE_LIMIT", 10):
            with mock.patch("modal.volume.VOLUME_PUT_FILE_CLIENT_TIMEOUT", 0.5):
                local_file_path = tmp_path / "bigfile"
                local_file_path.write_text("hello world, this is a lot of text")

                async with modal.Volume.ephemeral(client=client) as vol:
                    with pytest.raises(VolumeUploadTimeoutError):
                        async with vol.batch_upload() as batch:
                            batch.put_file(local_file_path, "/dest")

                assert call_count > 2


@pytest.mark.asyncio
@pytest.mark.parametrize("version", VERSIONS)
async def test_volume_copy_1(client, tmp_path, servicer, version):
    ## test 1: copy src path to dst path ##
    src_path = "original.txt"
    dst_path = "copied.txt"
    local_file_path = tmp_path / src_path
    local_file_path.write_text("test copy")

    async with modal.Volume.ephemeral(client=client, version=version) as vol:
        # add local file to volume
        async with vol.batch_upload() as batch:
            batch.put_file(local_file_path, src_path)
        object_id = vol.object_id

        # copy file from src_path to dst_path
        vol.copy_files([src_path], dst_path, False)

    assert servicer.volumes[object_id].files.keys() == {src_path, dst_path}

    assert servicer.volumes[object_id].files[src_path].data == b"test copy"
    assert servicer.volumes[object_id].files[dst_path].data == b"test copy"


@pytest.mark.asyncio
@pytest.mark.parametrize("version", VERSIONS)
async def test_volume_copy_2(client, tmp_path, servicer, version):
    ## test 2: copy multiple files into a directory ##
    file_paths = ["file1.txt", "file2.txt"]

    async with modal.Volume.ephemeral(client=client, version=version) as vol:
        for file_path in file_paths:
            local_file_path = tmp_path / file_path
            local_file_path.write_text("test copy")
            async with vol.batch_upload() as batch:
                batch.put_file(local_file_path, file_path)
            object_id = vol.object_id

        vol.copy_files(file_paths, "test_dir", False)

    returned_volume_files = [Path(file) for file in servicer.volumes[object_id].files.keys()]
    expected_volume_files = [
        Path(file) for file in ["file1.txt", "file2.txt", "test_dir/file1.txt", "test_dir/file2.txt"]
    ]

    assert returned_volume_files == expected_volume_files

    returned_file_data = {
        Path(entry): servicer.volumes[object_id].files[entry] for entry in servicer.volumes[object_id].files
    }
    assert returned_file_data[Path("test_dir/file1.txt")].data == b"test copy"
    assert returned_file_data[Path("test_dir/file2.txt")].data == b"test copy"


@pytest.mark.parametrize("version", VERSIONS)
def test_from_name(servicer, client, version):
    # Lookup should fail since it doesn't exist
    with pytest.raises(NotFoundError):
        modal.Volume.from_name("xyz", version=version).hydrate(client)

    # Create it
    modal.Volume.from_name("xyz", create_if_missing=True, version=version).hydrate(client)

    # Lookup should succeed now
    modal.Volume.from_name("xyz", version=version).hydrate(client)

    modal.Volume.objects.delete("xyz", client=client)
    # Lookup should fail again
    with pytest.raises(NotFoundError):
        modal.Volume.from_name("xyz", version=version).hydrate(client)
    modal.Volume.objects.delete("xyz", client=client, allow_missing=True)


def test_ephemeral(servicer, client):
    assert servicer.n_vol_heartbeats == 0
    with modal.Volume.ephemeral(client=client, _heartbeat_sleep=1) as vol:
        assert vol.listdir("/") == []
        # TODO(erikbern): perform some operations
        time.sleep(1.5)  # Make time for 2 heartbeats
    assert servicer.n_vol_heartbeats == 2


def test_lazy_hydration_from_named(set_env_client):
    vol = modal.Volume.from_name("my-vol", create_if_missing=True)
    assert vol.listdir("/") == []


@pytest.mark.skipif(platform.system() != "Linux", reason="needs /proc")
@pytest.mark.asyncio
async def test_open_files_error_annotation(tmp_path):
    assert _open_files_error_annotation(tmp_path) is None

    # Current process keeps file open
    with (tmp_path / "foo.txt").open("w") as _f:
        assert _open_files_error_annotation(tmp_path) == "path foo.txt is open"

    # cwd of current process is inside volume
    cwd = os.getcwd()
    os.chdir(tmp_path)
    assert _open_files_error_annotation(tmp_path) == "cwd is inside volume"
    os.chdir(cwd)

    # Subprocess keeps open file
    open_path = tmp_path / "bar.txt"
    open_path.write_text("")
    proc = await asyncio.create_subprocess_exec("tail", "-f", open_path.as_posix())
    await asyncio.sleep(0.01)  # Give process some time to start
    assert _open_files_error_annotation(tmp_path) == f"path bar.txt is open from 'tail -f {open_path.as_posix()}'"
    proc.kill()
    await proc.wait()
    assert _open_files_error_annotation(tmp_path) is None

    # Subprocess cwd inside volume
    proc = await asyncio.create_subprocess_exec(
        sys.executable, "-c", f"import time; import os; os.chdir('{tmp_path}'); time.sleep(60)"
    )
    # Wait for process to chdir
    for _ in range(100):
        if os.readlink(f"/proc/{proc.pid}/cwd") == tmp_path.as_posix():
            break
        await asyncio.sleep(0.05)
    assert re.match(f"^cwd of '{sys.executable} -c .*' is inside volume$", _open_files_error_annotation(tmp_path))
    proc.kill()
    await proc.wait()
    assert _open_files_error_annotation(tmp_path) is None


@pytest.mark.parametrize("name", ["has space", "has/slash", "a" * 65])
def test_invalid_name(name):
    with pytest.raises(InvalidError, match="Invalid Volume name"):
        modal.Volume.from_name(name)


@pytest.fixture()
def unset_main_thread_event_loop():
    try:
        event_loop = asyncio.get_event_loop()
    except RuntimeError:
        event_loop = None

    asyncio.set_event_loop(None)
    try:
        yield
    finally:
        asyncio.set_event_loop(event_loop)  # reset so we don't break other tests


@pytest.mark.usefixtures("unset_main_thread_event_loop")
def test_lock_is_py39_safe(set_env_client):
    vol = modal.Volume.from_name("my_vol", create_if_missing=True)
    vol.reload()


def test_volume_namespace_deprecated(servicer, client):
    # Test from_name with namespace parameter warns
    with pytest.warns(
        DeprecationError,
        match="The `namespace` parameter for `modal.Volume.from_name` is deprecated",
    ):
        modal.Volume.from_name("test-volume", namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE)

    # Test that from_name without namespace parameter doesn't warn about namespace
    import warnings

    with warnings.catch_warnings(record=True) as record:
        warnings.simplefilter("always")
        modal.Volume.from_name("test-volume")
    # Filter out any unrelated warnings
    namespace_warnings = [w for w in record if "namespace" in str(w.message).lower()]
    assert len(namespace_warnings) == 0


def test_remove_file_not_found(set_env_client):
    vol = modal.Volume.from_name("my_vol", create_if_missing=True)
    with pytest.raises(FileNotFoundError):
        vol.remove_file("a")


def test_volume_list(servicer, client):
    for i in range(5):
        modal.Volume.from_name(f"test-volume-{i}", create_if_missing=True).hydrate(client)
    if sys.platform == "win32":
        time.sleep(1 / 32)

    volume_list = modal.Volume.objects.list(client=client)
    assert len(volume_list) == 5
    assert all(v.name.startswith("test-volume-") for v in volume_list)
    assert all(v.info().created_by == servicer.default_username for v in volume_list)

    volume_list = modal.Volume.objects.list(max_objects=2, client=client)
    assert len(volume_list) == 2


def test_volume_create(servicer, client):
    modal.Volume.objects.create(name="test-volume-create", client=client)
    modal.Volume.from_name("test-volume-create").hydrate(client)
    with pytest.raises(AlreadyExistsError):
        modal.Volume.objects.create(name="test-volume-create", client=client)
    modal.Volume.objects.create(name="test-volume-create", allow_existing=True, client=client)
    with pytest.raises(InvalidError, match="Invalid Volume name"):
        modal.Volume.objects.create(name="has space", client=client)


def test_volume_create_version(servicer, client):
    for version in [1, 2]:
        modal.Volume.objects.create(name=f"should-be-v{version}", version=version, client=client)
        vol_id = servicer.deployed_volumes[(f"should-be-v{version}", "main")]
        assert servicer.volumes[vol_id].version == version

    with pytest.raises(InvalidError, match="VolumeFS version must be either 1 or 2"):
        modal.Volume.objects.create(name="should-be-v3", version=3, client=client)



================================================
FILE: test/watcher_test.py
================================================
# Copyright Modal Labs 2023
import pytest
import random
import string
from pathlib import Path

from watchfiles import Change

import modal
from modal._watcher import _watch_args_from_mounts
from modal.exception import ExecutionError
from modal.mount import _Mount


@pytest.mark.asyncio
async def test__watch_args_from_mounts(monkeypatch, test_dir):
    paths, watch_filter = _watch_args_from_mounts(
        mounts=[
            _Mount._from_local_file("/x/foo.py", remote_path="/foo.py"),
            _Mount._from_local_dir("/one/two/bucklemyshoe", remote_path="/"),
            _Mount._from_local_dir("/x/z", remote_path="/z"),
        ]
    )

    assert paths == {Path("/x").absolute(), Path("/one/two/bucklemyshoe").absolute(), Path("/x/z").absolute()}
    assert watch_filter(Change.modified, "/x/foo.py")
    assert not watch_filter(Change.modified, "/x/notwatched.py")
    assert not watch_filter(Change.modified, "/x/y/foo.py")
    assert watch_filter(Change.modified, "/x/z/bar.py")
    random_filename = "".join(random.choices(string.ascii_uppercase + string.digits, k=10))
    assert watch_filter(Change.modified, f"/one/two/bucklemyshoe/{random_filename}")
    assert not watch_filter(Change.modified, "/one/two/bucklemyshoe/.DS_Store")


def dummy():
    pass


def test_watch_mounts_requires_running_app():
    # Arguably a bit strange to test this, as the exception should never
    # happen unless there is a bug in the client, since _get_watch_mounts
    # is not a public function, and should only ever be called from "safe"
    # contexts...

    # requires running app to make sure the mounts have been loaded
    app = modal.App()
    with pytest.raises(ExecutionError):
        # _get_watch_mounts needs to be called on a hydrated app
        app._get_watch_mounts()


def test_watch_mounts_ignore_non_local(client, servicer):
    app = modal.App()

    # uses the published client mount - should not be included in watch items
    # serialized=True avoids auto-mounting the entrypoint
    @app.function(serialized=True)
    def dummy():
        pass

    with app.run(client=client):
        mounts = app._get_watch_mounts()

    assert len(mounts) == 0


def test_add_local_mount_included_in_serve_watchers(servicer, client, supports_on_path):
    deb_slim = modal.Image.debian_slim()
    img = deb_slim.add_local_python_source("pkg_a")
    app = modal.App()

    @app.function(serialized=True, image=img)
    def f():
        pass

    with app.run(client=client):
        watch_mounts = app._get_watch_mounts()
    assert len(watch_mounts) == 1



================================================
FILE: test/web_server_proxy_test.py
================================================
# Copyright Modal Labs 2024
import asyncio
import contextlib
import pytest
import socket
from dataclasses import dataclass
from typing import Any

import pytest_asyncio
from aiohttp.web import Application
from aiohttp.web_runner import AppRunner, SockSite

import modal._runtime.asgi

# TODO: add more tests


@dataclass
class DummyHttpServer:
    host: str
    port: int
    event: asyncio.Event
    assertion_log: list[str]


@contextlib.asynccontextmanager
async def run_temporary_http_server(app: Application):
    # Allocates a random port, runs a server in a context manager
    sock = socket.socket()
    host = "127.0.0.1"
    sock.bind((host, 0))
    port = sock.getsockname()[1]
    runner = AppRunner(app)
    await runner.setup()
    site = SockSite(runner, sock=sock)
    await site.start()
    try:
        yield host, port
    finally:
        await runner.cleanup()


@pytest_asyncio.fixture()
async def http_dummy_server():
    from aiohttp import web

    assertion_log = []
    event = asyncio.Event()

    async def hello(request):
        assertion_log.append("request")
        try:
            await request.read()
        except asyncio.CancelledError:
            assertion_log.append("cancelled")
            raise
        except OSError:
            # disconnect
            assertion_log.append("disconnect")
            event.set()
            return

        return web.Response(text="Hello, world")

    app = web.Application()
    app.add_routes([web.post("/", hello)])
    async with run_temporary_http_server(app) as (host, port):
        yield DummyHttpServer(host=host, port=port, event=event, assertion_log=assertion_log)


@contextlib.asynccontextmanager
async def lifespan_ctx_manager(asgi_app):
    state: dict[str, Any] = {}

    lm = modal._runtime.asgi.LifespanManager(asgi_app, state)
    t = asyncio.create_task(lm.background_task())
    await lm.lifespan_startup()
    yield state
    await lm.lifespan_shutdown()

    t.cancel()


@pytest.mark.asyncio
async def test_web_server_wrapper_immediate_disconnect(http_dummy_server: DummyHttpServer):
    proxy_asgi_app = modal._runtime.asgi.web_server_proxy(http_dummy_server.host, http_dummy_server.port)

    msgs = [{"type": "http.request", "body": b"a", "more_body": True}, {"type": "http.disconnect"}]

    async def recv():
        msg = msgs.pop(0)
        return msg

    async def send(msg):
        print("msg", msg)

    async with lifespan_ctx_manager(proxy_asgi_app) as state:
        scope = {"type": "http", "method": "POST", "path": "/", "headers": [], "state": state}
        await proxy_asgi_app(scope, recv, send)
        await http_dummy_server.event.wait()
        assert http_dummy_server.assertion_log == ["request", "disconnect"]


def test_add_forwarded_for_header():
    # case 1:
    # X-Forwarded-For already exist in headers and is the same as client IP
    # should do nothing
    original_scope = {"headers": [(b"X-Forwarded-For", b"1.2.3.4")], "client": ("1.2.3.4", 80)}
    expected_scope = {"headers": [(b"X-Forwarded-For", b"1.2.3.4")], "client": ("1.2.3.4", 80)}
    res = modal._runtime.asgi._add_forwarded_for_header(original_scope)
    assert res == expected_scope

    # case 2:
    # X-Forwarded-For already exist in headers but is not the same as client IP
    # should append client IP to X-Forwarded-For
    original_scope = {"headers": [(b"X-Forwarded-For", b"1.2.3.4")], "client": ("4.5.6.7", 80)}
    expected_scope = {"headers": [(b"X-Forwarded-For", b"4.5.6.7, 1.2.3.4")], "client": ("4.5.6.7", 80)}
    res = modal._runtime.asgi._add_forwarded_for_header(original_scope)
    assert res == expected_scope

    # case 3:
    # X-Forwarded-For does not exist in headers
    # should add X-Forwarded-For with client IP
    original_scope = {"headers": [], "client": ("4.5.6.7", 80)}
    expected_scope = {"headers": [(b"X-Forwarded-For", b"4.5.6.7")], "client": ("4.5.6.7", 80)}
    res = modal._runtime.asgi._add_forwarded_for_header(original_scope)
    assert res == expected_scope

    # case 4:
    # X-Forwarded-For exists multiple times in headers
    # but client IP is not in the list
    # should add client IP to the first one
    original_scope = {
        "headers": [(b"X-Forwarded-For", b"1.2.3.4"), (b"X-Forwarded-For", b"5.6.7.8")],
        "client": ("4.5.6.7", 80),
    }
    expected_scope = {
        "headers": [(b"X-Forwarded-For", b"4.5.6.7, 1.2.3.4"), (b"X-Forwarded-For", b"5.6.7.8")],
        "client": ("4.5.6.7", 80),
    }
    res = modal._runtime.asgi._add_forwarded_for_header(original_scope)
    assert res == expected_scope

    # case 5:
    # X-Forwarded-For exists multiple times in headers
    # but client IP is already in the list
    # should do nothing
    original_scope = {
        "headers": [(b"X-Forwarded-For", b"1.2.3.4"), (b"X-Forwarded-For", b"5.6.7.8, 4.5.6.7")],
        "client": ("4.5.6.7", 80),
    }
    expected_scope = {
        "headers": [(b"X-Forwarded-For", b"1.2.3.4"), (b"X-Forwarded-For", b"5.6.7.8, 4.5.6.7")],
        "client": ("4.5.6.7", 80),
    }
    res = modal._runtime.asgi._add_forwarded_for_header(original_scope)
    assert res == expected_scope, res



================================================
FILE: test/webhook_test.py
================================================
# Copyright Modal Labs 2022
import pathlib
import pytest
import subprocess
import sys

from fastapi.testclient import TestClient

import modal
from modal import App, asgi_app, fastapi_endpoint, wsgi_app
from modal._runtime.asgi import magic_fastapi_app
from modal.exception import InvalidError
from modal.functions import Function
from modal.running_app import RunningApp
from modal_proto import api_pb2

app = App()


@app.function(cpu=42)
@fastapi_endpoint(method="PATCH", docs=True)
async def f(x):
    return {"square": x**2}


@pytest.mark.asyncio
async def test_webhook(servicer, client, reset_container_app):
    async with app.run(client=client):
        assert f.get_web_url()

        assert servicer.app_functions["fu-1"].webhook_config.type == api_pb2.WEBHOOK_TYPE_FUNCTION
        assert servicer.app_functions["fu-1"].webhook_config.method == "PATCH"

        # Make sure we can call the webhooks
        # TODO: reinstate `.remote` check when direct webhook fn invocation is fixed.
        # assert await f.remote(10)
        assert await f.local(100) == {"square": 10000}

        # Make sure the container gets the app id as well
        container_app = RunningApp(app.app_id)
        app._init_container(client, container_app)
        assert isinstance(f, Function)
        assert f.get_web_url()


def test_webhook_cors():
    def handler():
        return {"message": "Hello, World!"}

    app = magic_fastapi_app(handler, method="GET", docs=False)
    client = TestClient(app)
    resp = client.options(
        "/",
        headers={
            "Origin": "http://example.com",
            "Access-Control-Request-Method": "POST",
        },
    )
    assert resp.headers["Access-Control-Allow-Origin"] == "http://example.com"

    assert client.get("/").json() == {"message": "Hello, World!"}
    assert client.post("/").status_code == 405  # Method Not Allowed


@pytest.mark.asyncio
async def test_webhook_no_docs():
    # FastAPI automatically sets docs URLs for apps, which we disable by default because it
    # can be unexpected for users who are unfamilar with FastAPI.
    #
    # https://fastapi.tiangolo.com/tutorial/metadata/#docs-urls

    def handler():
        return {"message": "Hello, World!"}

    app = magic_fastapi_app(handler, method="GET", docs=False)
    client = TestClient(app)
    assert client.get("/docs").status_code == 404
    assert client.get("/redoc").status_code == 404
    assert client.get("/openapi.json").status_code == 404


@pytest.mark.asyncio
async def test_webhook_docs():
    # By turning on docs, we should get three new routes: /docs, /redoc, and /openapi.json
    def handler():
        return {"message": "Hello, docs!"}

    app = magic_fastapi_app(handler, method="GET", docs=True)
    client = TestClient(app)
    assert client.get("/docs").status_code == 200
    assert client.get("/redoc").status_code == 200
    assert client.get("/openapi.json").status_code == 200


def test_webhook_generator():
    app = App()

    with pytest.raises(InvalidError) as excinfo:

        @app.function(serialized=True)
        @fastapi_endpoint()
        def web_gen():
            yield None

    assert "streaming" in str(excinfo.value).lower()


@pytest.mark.asyncio
async def test_webhook_forgot_function(servicer, client):
    lib_dir = pathlib.Path(__file__).parent.parent
    args = [sys.executable, "-m", "test.supports.webhook_forgot_function"]
    ret = subprocess.run(args, cwd=lib_dir, stderr=subprocess.PIPE)
    stderr = ret.stderr.decode()
    assert "absent_minded_function" in stderr
    assert "@app.function" in stderr


@pytest.mark.asyncio
@pytest.mark.parametrize("decorator", [fastapi_endpoint, asgi_app, wsgi_app])
async def test_webhook_decorator_in_wrong_order(decorator):
    app = App()

    with pytest.raises(InvalidError, match=decorator.__name__) as excinfo:

        @decorator()  # type: ignore
        @app.function(serialized=True)
        async def g(x):
            pass

    assert "swap the order" in str(excinfo.value).lower()


@pytest.mark.asyncio
@pytest.mark.parametrize("decorator", [fastapi_endpoint, asgi_app, wsgi_app])
async def test_webhook_decorator_on_class(decorator):
    app = App()

    with pytest.raises(InvalidError, match=decorator.__name__) as excinfo:

        @app.cls(serialized=True)
        @decorator()  # type: ignore
        class C:
            @modal.method()
            def f(self):
                pass

    assert "method instead" in str(excinfo.value).lower()


@pytest.mark.asyncio
async def test_asgi_wsgi(servicer, client):
    app = App()

    @app.function(serialized=True)
    @asgi_app()
    def my_asgi():
        pass

    @app.function(serialized=True)
    @wsgi_app()
    def my_wsgi():
        pass

    with pytest.raises(InvalidError, match="can't have parameters"):

        @app.function(serialized=True)
        @asgi_app()
        def my_invalid_asgi(x):
            pass

    with pytest.raises(InvalidError, match="can't have parameters"):

        @app.function(serialized=True)
        @wsgi_app()
        def my_invalid_wsgi(x):
            pass

    with pytest.raises(InvalidError, match="can't have parameters"):

        @app.function(serialized=True)
        @asgi_app()
        def my_deprecated_default_params_asgi(x=1):
            pass

    with pytest.raises(InvalidError, match="can't have parameters"):

        @app.function(serialized=True)
        @wsgi_app()
        def my_deprecated_default_params_wsgi(x=1):
            pass

    with pytest.raises(InvalidError, match="async function"):

        @app.function(serialized=True)
        @asgi_app()
        async def my_async_asgi_function():
            pass

    with pytest.raises(InvalidError, match="async function"):

        @app.function(serialized=True)
        @wsgi_app()
        async def my_async_wsgi_function():
            pass

    async with app.run(client=client):
        pass

    assert len(servicer.app_functions) == 2
    assert servicer.app_functions["fu-1"].webhook_config.type == api_pb2.WEBHOOK_TYPE_ASGI_APP
    assert servicer.app_functions["fu-2"].webhook_config.type == api_pb2.WEBHOOK_TYPE_WSGI_APP


def test_positional_method(servicer, client):
    with pytest.raises(InvalidError, match="method="):
        fastapi_endpoint("GET")
    with pytest.raises(InvalidError, match="label="):
        asgi_app("baz")
    with pytest.raises(InvalidError, match="label="):
        wsgi_app("baz")



================================================
FILE: test/mdmd_data/foo-expected.md
================================================
# foo

This module does cool stuff

## foo.Foo

```python
class Foo(object)
```

A class that foos

### bar

```python
def bar(self):
```

## foo.funky

```python
def funky():
```

funks the baz

**Usage**

```python
import foo
foo.funky()  # outputs something
```

Enjoy!



================================================
FILE: test/mdmd_data/foo.py
================================================
# Copyright Modal Labs 2023
"""This module does cool stuff"""

# global untyped objects are currently not documented
some_dict = {}  # type: ignore


class Foo:
    """A class that foos"""

    def bar(self):
        pass


def funky():
    """funks the baz

    **Usage**

    ```python
    import foo
    foo.funky()  # outputs something
    ```

    Enjoy!
    """
    pass


def hidden():
    """mdmd:hidden

    This is marked as hidden in docs and shouldn't be shown"""



================================================
FILE: test/supports/assert_package.py
================================================
# Copyright Modal Labs 2022
# See test in client_test/package_utils_test.py

name = "xyz"



================================================
FILE: test/supports/base_class.py
================================================
# Copyright Modal Labs 2022
from modal import enter, method


class BaseCls2:
    @enter()
    def enter(self):
        self.x = 2

    @method()
    def run(self, y):
        return self.x * y



================================================
FILE: test/supports/batching_config.py
================================================
# Copyright Modal Labs 2025
import modal

app = modal.App("batching-config")

CONFIG_VALS = {"MAX_SIZE": 100, "WAIT_MS": 1000}


@app.function()
@modal.batched(max_batch_size=CONFIG_VALS["MAX_SIZE"], wait_ms=CONFIG_VALS["WAIT_MS"])
def has_batch_config():
    pass


@app.cls()
class HasBatchConfig:
    @modal.batched(max_batch_size=CONFIG_VALS["MAX_SIZE"], wait_ms=CONFIG_VALS["WAIT_MS"])
    def is_batched(self):
        pass



================================================
FILE: test/supports/class_hierarchy.py
================================================
# Copyright Modal Labs 2024
import modal

app = modal.App("class-hierarchy")


class Base:
    @modal.method()
    def defined_on_base(self):
        print("base")

    @modal.method()
    def overridden_on_wrapped(self):
        raise NotImplementedError()


@app.cls()
class Wrapped(Base):
    @modal.method()
    def overridden_on_wrapped(self):
        print("wrapped")



================================================
FILE: test/supports/class_with_image.py
================================================
# Copyright Modal Labs 2024
import modal

image = modal.Image.debian_slim()
app = modal.App(image=image)


@app.cls()
class ClassWithImage:
    @modal.method()
    def image_is_hydrated(self):
        return image.is_hydrated



================================================
FILE: test/supports/common.py
================================================
# Copyright Modal Labs 2022
import modal

app = modal.App()


@app.function()
def f(x):
    # not actually used in test (servicer returns sum of square of all args)
    pass



================================================
FILE: test/supports/concurrency_config.py
================================================
# Copyright Modal Labs 2025
import modal

app = modal.App("concurrency-config")

CONFIG_VALS = {"OLD_MAX": 100, "NEW_MAX": 1000, "TARGET": 500}


@app.function(allow_concurrent_inputs=CONFIG_VALS["OLD_MAX"])
def has_old_config():
    pass


@app.function()
@modal.concurrent(max_inputs=CONFIG_VALS["NEW_MAX"], target_inputs=CONFIG_VALS["TARGET"])
def has_new_config():
    pass


@app.function()
@modal.concurrent(max_inputs=CONFIG_VALS["NEW_MAX"], target_inputs=CONFIG_VALS["TARGET"])
@modal.fastapi_endpoint()
def has_new_config_and_fastapi_endpoint():
    pass


@app.function()
@modal.fastapi_endpoint()
@modal.concurrent(max_inputs=CONFIG_VALS["NEW_MAX"], target_inputs=CONFIG_VALS["TARGET"])
def has_fastapi_endpoint_and_new_config():
    pass


@app.function()
def has_no_config():
    pass


@app.cls(allow_concurrent_inputs=CONFIG_VALS["OLD_MAX"])
class HasOldConfig:
    @modal.method()
    def method(self): ...


@app.cls()
@modal.concurrent(max_inputs=CONFIG_VALS["NEW_MAX"], target_inputs=CONFIG_VALS["TARGET"])
class HasNewConfig:
    @modal.method()
    def method(self): ...


@app.cls()
class HasNoConfig:
    @modal.method()
    def method(self): ...


@app.cls()
@modal.concurrent(max_inputs=CONFIG_VALS["NEW_MAX"], target_inputs=CONFIG_VALS["TARGET"])
class HasNewConfigAndFastapiEndpoint:
    @modal.fastapi_endpoint()
    def method(self): ...



================================================
FILE: test/supports/consumed_map.py
================================================
# Copyright Modal Labs 2022
from .common import app, f

if __name__ == "__main__":
    with app.run():
        for x in f.map([1, 2, 3]):  # type: ignore
            pass



================================================
FILE: test/supports/forking.py
================================================
# Copyright Modal Labs 2024
import os
import sys

from modal._utils.async_utils import synchronize_api
from modal.client import Client
from modal.config import config
from modal_proto import api_pb2


@synchronize_api
async def test_stub(stub):
    await stub.VolumeList(api_pb2.VolumeListRequest(environment_name="main"))
    print(os.getpid())


@synchronize_api
async def test_stub_method(volume_list_stub_method):
    await volume_list_stub_method(api_pb2.VolumeListRequest(environment_name="main"))
    print(os.getpid())


if __name__ == "__main__":
    client = Client.from_env()
    test = sys.argv[1]

    new_proc = False
    if test == "test_stub_method":
        # Test that a reference to a stub method can be used across forks
        rpc_method = client.stub.VolumeList
        test_stub_method(rpc_method)
        if not (fork_pid := os.fork()):
            new_proc = True
            test_stub_method(rpc_method)
        else:
            os.waitpid(fork_pid, 0)
    elif test == "test_stub_reference":
        # Test that a reference to a stub can be used across forks
        stub = client.get_stub(config["server_url"])
        test_stub(stub)
        if not (fork_pid := os.fork()):
            test_stub(stub)
        else:
            os.waitpid(fork_pid, 0)
    elif test == "test_default_stub":
        # Test that the default stub can be used across forks
        test_stub(client.stub)
        if not (fork_pid := os.fork()):
            test_stub(client.stub)
        else:
            os.waitpid(fork_pid, 0)
    elif test == "test_default_stub_reference":
        # Test that a reference to the  default stub can be used across forks
        stub = client.stub
        test_stub(stub)
        if not (fork_pid := os.fork()):
            test_stub(stub)
        else:
            os.waitpid(fork_pid, 0)
    else:
        raise ValueError(f"Unknown test: {test}")



================================================
FILE: test/supports/function_without_app.py
================================================
# Copyright Modal Labs 2024
from modal.app import App


def f(x):
    assert App._get_container_app()
    return 123



================================================
FILE: test/supports/functions.py
================================================
# Copyright Modal Labs 2022
import asyncio
import contextlib
import threading
import time
from typing import Sequence

import modal
from modal import (
    App,
    Sandbox,
    asgi_app,
    batched,
    concurrent,
    current_function_call_id,
    current_input_id,
    enter,
    exit,
    fastapi_endpoint,
    is_local,
    method,
    web_server,
    wsgi_app,
)
from modal._utils.deprecation import deprecation_warning
from modal.experimental import get_local_input_concurrency, set_local_input_concurrency
from modal.queue import Queue

SLEEP_DELAY = 0.1

app = App()


@app.function()
def square(x: int):
    return x * x


@app.function(_experimental_restrict_output=True)
def square_restrict_output(x: int):
    return x * x


@app.function(_experimental_restrict_output=True)
def cbor_incompatible_output(x: int):
    with Queue.ephemeral() as q:
        return q


@app.cls()
class SimpleCls:
    @method()
    def square(self, x: int) -> int:
        return x**2


@app.cls(_experimental_restrict_output=True)
class SimpleCbor:
    @method()
    def square(self, x: int) -> int:
        return x**2


@app.function()
def ident(x):
    return x


@app.function()
def delay(t):
    time.sleep(t)
    return t


@app.function()
async def delay_async(t):
    await asyncio.sleep(t)
    return t


@app.function()
async def async_cancel_doesnt_reraise(t):
    try:
        await asyncio.sleep(t)
    except asyncio.CancelledError:
        pass


@app.function()
async def square_async(x):
    await asyncio.sleep(SLEEP_DELAY)
    return x * x


@app.function()
def raises(x):
    raise Exception("Failure!")


@app.function()
def raises_sysexit(x):
    raise SystemExit(1)


@app.function()
def raises_keyboardinterrupt(x):
    raise KeyboardInterrupt()


@app.function()
def gen_n(n):
    for i in range(n):
        yield i**2


@app.function()
def gen_n_fail_on_m(n, m):
    for i in range(n):
        if i == m:
            raise Exception("bad")
        yield i**2


@app.function()
async def async_gen_n_fail_on_m(n, m):
    for i in range(n):
        if i == m:
            raise Exception("bad")
        yield i**2


def deprecated_function(x):
    deprecation_warning((2000, 1, 1), "This function is deprecated")
    return x**2


@app.function()
@fastapi_endpoint()
def webhook(arg="world"):
    return {"hello": arg}


@app.function(serialized=True)
@fastapi_endpoint()
def webhook_serialized(arg="world"):
    return f"Hello, {arg}"


def stream():
    for i in range(10):
        time.sleep(SLEEP_DELAY)
        yield f"{i}..."


@app.function()
@fastapi_endpoint()
def webhook_streaming():
    from fastapi.responses import StreamingResponse

    return StreamingResponse(stream())


async def stream_async():
    for i in range(10):
        await asyncio.sleep(SLEEP_DELAY)
        yield f"{i}..."


@app.function()
@fastapi_endpoint()
async def webhook_streaming_async():
    from fastapi.responses import StreamingResponse

    return StreamingResponse(stream_async())


if __name__ == "__main__":
    raise Exception("This line is not supposed to be reachable")


def gen(n):
    for i in range(n):
        yield i**2


@app.function(is_generator=True)
def fun_returning_gen(n):
    return gen(n)


@app.function()
@asgi_app()
def fastapi_app():
    from fastapi import FastAPI

    web_app = FastAPI()

    @web_app.get("/foo")
    async def foo(arg="world"):
        return {"hello": arg}

    return web_app


@app.function()
@web_server(8765, startup_timeout=1)
def non_blocking_web_server():
    import subprocess

    subprocess.Popen(["python", "-m", "http.server", "-b", "0.0.0.0", "8765"])


lifespan_global_asgi_app_func: list[str] = []


@app.function()
@asgi_app()
def fastapi_app_with_lifespan():
    from fastapi import FastAPI, Request

    assert len(lifespan_global_asgi_app_func) == 0

    @contextlib.asynccontextmanager
    async def lifespan(wapp: FastAPI):
        lifespan_global_asgi_app_func.append("enter")
        yield {"foo": "this was set from state"}
        lifespan_global_asgi_app_func.append("exit")

    web_app = FastAPI(lifespan=lifespan)

    @web_app.get("/")
    async def foo(request: Request):
        lifespan_global_asgi_app_func.append("foo")
        return request.state.foo

    return web_app


@app.function()
@asgi_app()
def fastapi_app_with_lifespan_failing_startup():
    from fastapi import FastAPI

    @contextlib.asynccontextmanager
    async def lifespan(wapp: FastAPI):
        print("enter")
        raise Exception("Error while setting up asgi app")
        yield
        print("exit")

    web_app = FastAPI(lifespan=lifespan)

    @web_app.get("/")
    async def foo():
        print("foo")
        return "bar"

    return web_app


@app.function()
@asgi_app()
def fastapi_app_with_lifespan_failing_shutdown():
    from fastapi import FastAPI

    @contextlib.asynccontextmanager
    async def lifespan(wapp: FastAPI):
        print("enter")
        yield
        raise Exception("Error while setting up asgi app")
        print("exit")

    web_app = FastAPI(lifespan=lifespan)

    @web_app.get("/")
    async def foo():
        print("foo")
        return "bar"

    return web_app


lifespan_global_asgi_app_cls: list[str] = []


@app.cls(scaledown_window=300, max_containers=1)
@concurrent(max_inputs=100)
class fastapi_class_multiple_asgi_apps_lifespans:
    @modal.enter()
    def enter_assertion(self):
        assert len(lifespan_global_asgi_app_cls) == 0

    @asgi_app()
    def my_app1(self):
        from fastapi import FastAPI

        @contextlib.asynccontextmanager
        async def lifespan1(wapp):
            lifespan_global_asgi_app_cls.append("enter1")
            yield
            lifespan_global_asgi_app_cls.append("exit1")

        web_app1 = FastAPI(lifespan=lifespan1)

        @web_app1.get("/")
        async def foo1():
            lifespan_global_asgi_app_cls.append("foo1")
            return "foo1"

        return web_app1

    @asgi_app()
    def my_app2(self):
        from fastapi import FastAPI

        @contextlib.asynccontextmanager
        async def lifespan2(wapp):
            lifespan_global_asgi_app_cls.append("enter2")
            yield
            lifespan_global_asgi_app_cls.append("exit2")

        web_app2 = FastAPI(lifespan=lifespan2)

        @web_app2.get("/")
        async def foo2():
            lifespan_global_asgi_app_cls.append("foo2")
            return "foo2"

        return web_app2

    @exit()
    def exit(self):
        lifespan_global_asgi_app_cls.append("exit")


lifespan_global_asgi_app_cls_fail: list[str] = []


@app.cls(scaledown_window=300, max_containers=1)
@concurrent(max_inputs=100)
class fastapi_class_lifespan_shutdown_failure:
    @modal.enter()
    def enter_assertion(self):
        assert len(lifespan_global_asgi_app_cls_fail) == 0

    @asgi_app()
    def my_app1(self):
        from fastapi import FastAPI

        @contextlib.asynccontextmanager
        async def lifespan1(wapp):
            lifespan_global_asgi_app_cls_fail.append("enter")
            yield
            raise

        web_app1 = FastAPI(lifespan=lifespan1)

        @web_app1.get("/")
        async def foo():
            lifespan_global_asgi_app_cls_fail.append("foo")
            return "foo"

        return web_app1

    @exit()
    def exit(self):
        lifespan_global_asgi_app_cls_fail.append("lifecycle exit")


@app.function()
@asgi_app()
def asgi_app_with_slow_lifespan_wind_down():
    async def _asgi_app(scope, receive, send):
        if scope["type"] == "lifespan":
            while True:
                message = await receive()
                if message["type"] == "lifespan.startup":
                    await send({"type": "lifespan.startup.complete"})
                elif message["type"] == "lifespan.shutdown":
                    await send({"type": "lifespan.shutdown.complete"})
                await asyncio.sleep(1)  # take some time to shut down - this should either be cancelled or awaited
        else:
            # dummy response to other requests
            await send({"type": "http.response.start", "status": 200})
            await send({"type": "http.response.body", "body": b'{"some_result":"foo"}'})

    return _asgi_app


@app.function()
@asgi_app()
def non_lifespan_asgi():
    async def app(scope, receive, send):
        if not scope["type"] == "http":
            return

        await send(
            {
                "type": "http.response.start",
                "status": 200,
                "headers": [
                    (b"content-type", b"application/json"),
                ],
            }
        )

        await send(
            {
                "type": "http.response.body",
                "body": b'"foo"',
            }
        )

    return app


@app.function()
@asgi_app()
def error_in_asgi_setup():
    raise Exception("Error while setting up asgi app")


@app.function()
@wsgi_app()
def basic_wsgi_app():
    def simple_app(environ, start_response):
        status = "200 OK"
        headers = [("Content-type", "text/plain; charset=utf-8")]
        body = environ["wsgi.input"].read()

        start_response(status, headers)
        yield b"got body: " + body

    return simple_app


@app.cls()
class LifecycleCls:
    """Ensures that {sync,async} lifecycle hooks work with {sync,async} functions."""

    print_at_exit: int = modal.parameter(default=0)
    sync_enter_duration: int = modal.parameter(default=0)
    async_enter_duration: int = modal.parameter(default=0)
    sync_exit_duration: int = modal.parameter(default=0)
    async_exit_duration: int = modal.parameter(default=0)

    @property
    def events(self) -> list[str]:
        return self.__dict__.setdefault("_events", [])

    def _print_at_exit(self):
        import atexit

        atexit.register(lambda: print("[events:" + ",".join(self.events) + "]"))

    @enter()
    def enter_sync(self):
        if self.print_at_exit:
            self._print_at_exit()
        self.events.append("enter_sync")
        time.sleep(self.sync_enter_duration)

    @enter()
    async def enter_async(self):
        self.events.append("enter_async")
        await asyncio.sleep(self.async_enter_duration)

    @exit()
    def exit_sync(self):
        self.events.append("exit_sync")
        time.sleep(self.sync_exit_duration)

    @exit()
    async def exit_async(self):
        self.events.append("exit_async")
        await asyncio.sleep(self.async_exit_duration)

    @method()
    def local(self):
        self.events.append("local")

    @method()
    def f_sync(self):
        self.events.append("f_sync")
        self.local.local()
        return self.events

    @method()
    async def f_async(self):
        self.events.append("f_async")
        self.local.local()
        return self.events

    @method()
    def delay(self, duration: float):
        self._print_at_exit()
        self.events.append("delay")
        time.sleep(duration)
        return self.events

    @method()
    async def delay_async(self, duration: float):
        self._print_at_exit()
        self.events.append("delay_async")
        await asyncio.sleep(duration)
        return self.events


@app.function()
@concurrent(max_inputs=6)
def sleep_700_sync(x):
    time.sleep(0.7)
    return x * x, current_input_id(), current_function_call_id()


@app.function()
@concurrent(max_inputs=6)
async def sleep_700_async(x):
    await asyncio.sleep(0.7)
    return x * x, current_input_id(), current_function_call_id()


@app.function()
@batched(max_batch_size=4, wait_ms=500)
def batch_function_sync(x: tuple[int], y: tuple[int]):
    outputs = []
    for x_i, y_i in zip(x, y):
        outputs.append(x_i / y_i)
    return outputs


@app.function()
@batched(max_batch_size=4, wait_ms=500)
def batch_function_cbor_tester(c_list: list[Sequence[str]]) -> Sequence[Sequence[str]]:
    # batch processing so we process all entries before returning:
    res = []
    for input_entry in c_list:
        if input_entry[0] == "error":
            raise Exception("custom error!")
        input_type = type(input_entry).__name__  # if input was cbor this becomes "list", if pickle then "tuple"
        res.append((input_type,))  # returns a tuple per input (gets transformed to list for cbor)

    return res


@app.function()
@batched(max_batch_size=500, wait_ms=500)
def batch_function_sync_large_batch(x: tuple[int], y: tuple[int]):
    return [x_i / y_i for x_i, y_i in zip(x, y)]


@app.function()
@batched(max_batch_size=4, wait_ms=500)
def batch_function_outputs_not_list(x: tuple[int], y: tuple[int]):
    return str(x)


@app.function()
@batched(max_batch_size=4, wait_ms=500)
def batch_function_outputs_wrong_len(x: tuple[int], y: tuple[int]):
    return list(x) + [0]


@app.function()
@batched(max_batch_size=4, wait_ms=500)
async def batch_function_async(x: tuple[int], y: tuple[int]):
    outputs = []
    for x_i, y_i in zip(x, y):
        outputs.append(x_i / y_i)
    await asyncio.sleep(0.1)
    return outputs


def unassociated_function(x):
    return 100 - x


class BaseCls:
    @enter()
    def enter(self):
        self.x = 2

    @method()
    def run(self, y):
        return self.x * y


@app.cls()
class DerivedCls(BaseCls):
    pass


@app.function()
def cube(x):
    # Note: this ends up calling the servicer fixture,
    # which always just returns the sum of the squares of the inputs,
    # regardless of the actual funtion.
    assert square.is_hydrated
    return square.remote(x) * x


@app.cls(enable_memory_snapshot=True)
class SnapshottingCls:
    @property
    def _vals(self) -> list[str]:
        return self.__dict__.setdefault("__vals", [])

    @enter(snap=True)
    def enter1(self):
        self._vals.append("A")

    @enter(snap=True)
    def enter2(self):
        self._vals.append("B")

    @enter()
    def enter3(self):
        self._vals.append("C")

    @method()
    def f(self, x):
        return "".join(self._vals) + x


@app.function(enable_memory_snapshot=True)
def snapshotting_square(x):
    return x * x


@app.cls()
class EventLoopCls:
    @enter()
    async def enter(self):
        self.loop = asyncio.get_running_loop()

    @method()
    async def f(self):
        return self.loop.is_running()


@app.function()
def sandbox_f(x):
    # TODO(erikbern): maybe inside containers, `app=app` should be automatic?
    sb = Sandbox.create("echo", str(x), app=app)
    return sb.object_id


@app.function()
def is_local_f(x):
    return is_local()


@app.function()
def raise_large_unicode_exception():
    byte_str = (b"k" * 120_000_000) + b"\x99"
    byte_str.decode("utf-8")


@app.function()
@modal.concurrent(target_inputs=2, max_inputs=10)
def get_input_concurrency(timeout: int):
    time.sleep(timeout)
    return get_local_input_concurrency()


@app.function()
@modal.concurrent(target_inputs=3, max_inputs=6)
def set_input_concurrency(start: float):
    set_local_input_concurrency(3)
    time.sleep(1)
    return time.time() - start


@app.function()
def check_container_app():
    # The container app should be associated with the app object
    assert App._get_container_app() == app


@app.function()
def get_running_loop(x):
    return asyncio.get_running_loop()


@app.function()
def is_main_thread_sync(x):
    return threading.main_thread() == threading.current_thread()


@app.function()
async def is_main_thread_async(x):
    return threading.main_thread() == threading.current_thread()


_import_thread_is_main_thread = threading.main_thread() == threading.current_thread()


@app.function()
def import_thread_is_main_thread(x):
    return _import_thread_is_main_thread


class CustomException(Exception):
    pass


@app.function()
def raises_custom_exception(x):
    raise CustomException("Failure!")


@app.cls()
class StopFetching:
    @enter()
    def init(self):
        self.counter = 0

    @method()
    def after_two(self, x):
        import modal.experimental

        self.counter += 1

        if self.counter >= 2:
            modal.experimental.stop_fetching_inputs()

        return x * x


@app.cls(serialized=True)
class SerializedCls:
    @enter()
    def enter(self):
        self.power = 5

    @method()
    def method(self, x):
        return x**self.power


@app.cls(serialized=True)
class SerializedClassWithParams:
    p: int = modal.parameter()

    @modal.method()
    def method(self):
        return "hello"


@app.function(
    volumes={
        "/var/foo": modal.Volume.from_name("foo", create_if_missing=True),
        "/var/bar": modal.Volume.from_name("bar", create_if_missing=True),
    }
)
def function_with_volumes(should_raise: bool):
    if should_raise:
        raise Exception("Failure!")
    return "success"


@app.cls(serialized=True)
class Foo:
    x: str = modal.parameter()

    @enter()
    def some_enter(self):
        self.x += "_enter"

    @method()
    def method_a(self, y):
        return self.x + f"_a_{y}"

    @method()
    def method_b(self, y):
        return self.x + f"_b_{y}"


@app.function(serialized=True)
def serialized_triple(x):
    return 3 * x



================================================
FILE: test/supports/hello.py
================================================
# Copyright Modal Labs 2023
import modal

app = modal.App()


@app.function()
def hello():
    print("hello")
    return "hello"


@app.function()
def other():
    return "other"



================================================
FILE: test/supports/image_run_function.py
================================================
# Copyright Modal Labs 2022
import modal

app = modal.App("a")
other = modal.App("b")


def builder_function():
    print("ran builder function")


image = modal.Image.debian_slim().run_function(builder_function)


@app.function(image=image)
def foo():
    pass



================================================
FILE: test/supports/import_and_filter_source.py
================================================
# Copyright Modal Labs 2025
from modal import App, asgi_app, fastapi_endpoint, method

app_with_one_web_function = App()


@app_with_one_web_function.function()
@fastapi_endpoint()
def web1():
    pass


app_with_one_function_one_web_endpoint = App()


@app_with_one_function_one_web_endpoint.function()
def f1():
    pass


@app_with_one_function_one_web_endpoint.function()
@fastapi_endpoint()
def web2():
    pass


app_with_one_web_method = App()


@app_with_one_web_method.cls()
class C1:
    @asgi_app()
    def web_3(self):
        pass


app_with_one_web_method_one_method = App()


@app_with_one_web_method_one_method.cls()
class C2:
    @asgi_app()
    def web_4(self):
        pass

    @method()
    def f2(self):
        pass


app_with_local_entrypoint_and_function = App()


@app_with_local_entrypoint_and_function.local_entrypoint()
def le_1():
    pass


@app_with_local_entrypoint_and_function.function()
def f3():
    pass



================================================
FILE: test/supports/import_modal_from_thread.py
================================================
# Copyright Modal Labs 2024
import threading

success = threading.Event()


def main():
    import modal  # noqa

    success.set()


if __name__ == "__main__":
    t = threading.Thread(target=main, daemon=True)
    t.start()
    was_success = success.wait(timeout=5)
    assert was_success



================================================
FILE: test/supports/imports_ast.py
================================================
# Copyright Modal Labs 2024
import ast  # noqa

import modal

app = modal.App("imports_ast")


@app.function()
def some_func():
    pass



================================================
FILE: test/supports/imports_six.py
================================================
# Copyright Modal Labs 2024
import six  # noqa

import modal

app = modal.App("imports_six")


@app.function()
def some_func():
    pass



================================================
FILE: test/supports/lazy_hydration.py
================================================
# Copyright Modal Labs 2024
from modal import App, Image, Queue, Volume

app = App()

image = Image.debian_slim().pip_install("xyz")
volume = Volume.from_name("my-vol")
queue = Queue.from_name("my-queue")


@app.function(image=image, volumes={"/tmp/xyz": volume})
def f(x):
    # These are hydrated by virtue of being dependencies
    assert image.is_hydrated
    assert volume.is_hydrated

    # This one should be hydrated lazily
    queue.put("123")
    assert queue.get() == "123"



================================================
FILE: test/supports/map_item_test_utils.py
================================================
# Copyright Modal Labs 2025

# This file contains helpers for map_item_context_test.py and map_item_manager_test.py

from dataclasses import dataclass
from typing import Union

from modal._utils.jwt_utils import DecodedJwt
from modal.parallel_map import _MapItemContext, _MapItemState
from modal_proto import api_pb2

result_success = api_pb2.GenericResult(status=api_pb2.GenericResult.GENERIC_STATUS_SUCCESS)
result_failure = api_pb2.GenericResult(status=api_pb2.GenericResult.GENERIC_STATUS_FAILURE)
result_internal_failure = api_pb2.GenericResult(status=api_pb2.GenericResult.GENERIC_STATUS_INTERNAL_FAILURE)


@dataclass
class InputJwtData:
    """
    A helper class that represents a decoded input jwt that contains a map idx and retry count.
    """

    idx: int
    retry_count: int

    @staticmethod
    def of(idx: int, retry_count: int) -> "InputJwtData":
        return InputJwtData(idx, retry_count)

    @staticmethod
    def from_jwt(jwt: str) -> "InputJwtData":
        decoded = DecodedJwt.decode_without_verification(jwt)
        return InputJwtData(decoded.payload["idx"], decoded.payload["retry_count"])

    def to_jwt(self) -> str:
        return DecodedJwt.encode_without_signature({"idx": self.idx, "retry_count": self.retry_count})


def assert_context_is(
    ctx: _MapItemContext,
    state: _MapItemState,
    retry_count: int,
    input_id: Union[str, None],
    input_jwt: Union[InputJwtData, None],
    input_args: bytes,
):
    assert ctx
    assert ctx.state == state
    assert ctx.input == api_pb2.FunctionInput(args=input_args)
    assert ctx.retry_manager.retry_count == retry_count
    if input_id:
        # We call result rather than await because we want to test that the result has been set already.
        assert ctx.input_id.result() == input_id
    else:
        assert not ctx.input_id.done()
    if input_jwt:
        assert InputJwtData.from_jwt(ctx.input_jwt.result()) == input_jwt
    else:
        assert not ctx.input_jwt.done()


def assert_retry_item_is(
    retry_item: api_pb2.FunctionRetryInputsItem, input_jwt: InputJwtData, retry_count: int, input_args: bytes
):
    assert InputJwtData.from_jwt(retry_item.input_jwt) == input_jwt
    assert retry_item.retry_count == retry_count
    assert retry_item.input == api_pb2.FunctionInput(args=input_args)



================================================
FILE: test/supports/missing_main_conditional.py
================================================
# Copyright Modal Labs 2022
import modal

app = modal.App()


@app.function()
def square(x):
    return x**2


# This should fail in a container
with app.run():
    print(square.remote(42))



================================================
FILE: test/supports/modal_run_from_function.py
================================================
# Copyright Modal Labs 2025
import modal

app = modal.App("app1")

app2 = modal.App("app2")


@app2.function()
def foo():
    pass


@app.function(image=modal.Image.debian_slim().pip_install("rich"))
def run_other_app():
    with modal.enable_output():
        with app2.run():
            foo.remote()



================================================
FILE: test/supports/module_1.py
================================================
# Copyright Modal Labs 2022
def square(x):
    return x**2



================================================
FILE: test/supports/module_2.py
================================================
# Copyright Modal Labs 2022
def square(x):
    return x**2



================================================
FILE: test/supports/mount_dedupe.py
================================================
# Copyright Modal Labs 2023

import modal

app = modal.App()


image_1 = modal.Image.debian_slim().add_local_python_source("pkg_a")  # this should be reused

# same as above, but different instance - should be app-deduplicated:
image_2 = (
    modal.Image.debian_slim()
    .add_local_python_source("pkg_a")  # identical to first explicit mount and auto mounts
    .add_local_python_source(
        # custom ignore condition, include normally_not_included.pyc (but skip __pycache__)
        "pkg_a",
        ignore=["**/__pycache__"],
    )
)


@app.function(image=image_1)
def foo():
    pass


@app.function(image=image_2)
def bar():
    pass



================================================
FILE: test/supports/multiapp.py
================================================
# Copyright Modal Labs 2023
import modal

a = modal.App()


@a.function()
def a_func(i):
    assert a_func.is_hydrated
    assert not b_func.is_hydrated
    assert modal.App._get_container_app() == a


b = modal.App()


@b.function()
def b_func(i):
    assert b_func.is_hydrated
    assert not a_func.is_hydrated
    assert modal.App._get_container_app() == b



================================================
FILE: test/supports/multiapp_privately_decorated.py
================================================
# Copyright Modal Labs 2023
import modal

app = modal.App()


def foo(i):
    return 1


foo_handle = app.function()(foo)  #  "privately" decorated, by not override the original function


other_app = modal.App()


@other_app.function()
def bar(i):
    return 2



================================================
FILE: test/supports/multiapp_privately_decorated_named_app.py
================================================
# Copyright Modal Labs 2023
import modal

app = modal.App("dummy")


def foo(i):
    return 1


foo_handle = app.function()(foo)


other_app = modal.App()


@other_app.function()
def bar(i):
    return 2



================================================
FILE: test/supports/multiapp_same_name.py
================================================
# Copyright Modal Labs 2023
import modal

app = modal.App("dummy")


def foo(i):
    return 1


foo_handle = app.function()(foo)


other_app = modal.App("dummy")


@other_app.function()
def bar(i):
    return 2



================================================
FILE: test/supports/multiapp_serialized_func.py
================================================
# Copyright Modal Labs 2023
import modal

app = modal.App()


def foo(x):
    return x


foo_handle = app.function(serialized=True)(foo)


other_app = modal.App()


@other_app.function()
def bar(i):
    return 2



================================================
FILE: test/supports/package_mount.py
================================================
# Copyright Modal Labs 2022
from modal import App, Image

app = App()

# just make sure that non-existing package doesn't cause this to crash in containers:
image = Image.debian_slim().add_local_python_source("non_existing_package_123154")


@app.function(image=image, serialized=True)
def dummy(_x):
    return 0



================================================
FILE: test/supports/progress_info.py
================================================
# Copyright Modal Labs 2022
from modal import enable_output

from .common import app, f

if __name__ == "__main__":
    with enable_output():
        with app.run():
            assert f.remote(2, 4) == 20  # type: ignore



================================================
FILE: test/supports/pyproject.toml
================================================
[foo]
bar = "baz"



================================================
FILE: test/supports/raise_error.py
================================================
# Copyright Modal Labs 2024
def raise_error():
    raise RuntimeError("Boo!")



================================================
FILE: test/supports/sandbox.py
================================================
# Copyright Modal Labs 2024
import modal

app = modal.App()


@app.function()
def spawn_sandbox(x):
    modal.Sandbox.create("bash", "-c", "echo bar")



================================================
FILE: test/supports/script.py
================================================
# Copyright Modal Labs 2022
from .common import app, f

if __name__ == "__main__":
    with app.run():
        assert f.remote(2, 4) == 20  # type: ignore



================================================
FILE: test/supports/serialize_class.py
================================================
# Copyright Modal Labs 2024
import sys

import modal
from modal import enter, fastapi_endpoint, method
from modal._serialization import serialize


class UserCls:
    @enter()
    def enter(self):
        pass

    @method()
    def method(self):
        return "a"

    @fastapi_endpoint()
    def fastapi_endpoint(self):
        pass


app = modal.App()
app.cls()(UserCls)  # avoid warnings about not turning methods into functions

sys.stdout.buffer.write(serialize(UserCls))



================================================
FILE: test/supports/sibling_hydration_app.py
================================================
# Copyright Modal Labs 2025
import modal
from modal import asgi_app, enter, fastapi_endpoint, method

app = modal.App()


@app.function()
def square(x):
    return x * x


@app.function()
@asgi_app()
def fastapi_app():
    return None


def gen():
    yield


@app.function(is_generator=True)
def fun_returning_gen():
    return gen()


@app.function()
def function_calling_method(x, y, z):
    obj = ParamCls(x=x, y=y)
    return obj.f.remote(z)


@app.function()
def check_sibling_hydration(x):
    assert square.is_hydrated
    assert fastapi_app.is_hydrated
    assert fastapi_app.get_web_url()
    assert fun_returning_gen.is_hydrated
    assert fun_returning_gen.is_generator

    # make sure the underlying service function for the class is hydrated:
    assert NonParamCls._get_class_service_function().is_hydrated  # type: ignore
    assert ParamCls._get_class_service_function().is_hydrated  # type: ignore

    # notably not hydrated at this point:
    # NonParamCls()  (instance of parameter-less class - note that hydration shouldn't require any roundtrips for this)
    # NonParamCls().f  (method of parameter-less class - note that hydration shouldn't require any roundtrips for this)
    # ParamCls(x=1, y=3)  (parameter-bound class instance)


@app.cls()
class NonParamCls:
    _k = 11  # not a parameter, just a static initial value

    @enter()
    def enter(self):
        self._k = 111

    @method()
    def f(self, x):
        return self._k * x

    @fastapi_endpoint()
    def web(self, arg):
        return {"ret": arg * self._k}

    @asgi_app()
    def asgi_web(self):
        from fastapi import FastAPI

        k_at_construction = self._k  # expected to be 111
        hydrated_at_contruction = square.is_hydrated
        web_app = FastAPI()

        @web_app.get("/")
        def k(arg: str):
            return {
                "at_construction": k_at_construction,
                "at_runtime": self._k,
                "arg": arg,
                "other_hydrated": hydrated_at_contruction,
            }

        return web_app

    def _generator(self, x):
        yield x**3

    @method(is_generator=True)
    def generator(self, x):
        return self._generator(x)


@app.cls()
class ParamCls:
    x: int = modal.parameter()
    y: str = modal.parameter()

    @method()
    def f(self, z: int):
        return f"{self.x} {self.y} {z}"

    @method()
    def g(self, z):
        return self.f.local(z)



================================================
FILE: test/supports/skip.py
================================================
# Copyright Modal Labs 2022
import os
import platform
import pytest
import sys


def skip_windows(msg: str):
    return pytest.mark.skipif(platform.system() == "Windows", reason=msg)


def skip_macos(msg: str):
    return pytest.mark.skipif(platform.system() == "Darwin", reason=msg)


skip_windows_unix_socket = skip_windows("Windows doesn't have UNIX sockets")


def skip_old_py(msg: str, min_version: tuple):
    return pytest.mark.skipif(sys.version_info < min_version, reason=msg)


skip_github_non_linux = pytest.mark.skipif(
    (os.environ.get("GITHUB_ACTIONS") == "true" and platform.system() != "Linux"),
    reason="containers only have to run on linux.",
)



================================================
FILE: test/supports/slow_dependencies_container.py
================================================
# Copyright Modal Labs 2024
import sys

import modal._container_entrypoint  # noqa

assert "modal" in sys.modules

# This is a very heavy dependency that takes about 70-80ms to import
# Let's make sure it doesn't get imported in global scope
assert "aiohttp" not in sys.modules



================================================
FILE: test/supports/slow_dependencies_local.py
================================================
# Copyright Modal Labs 2024
import sys

import modal  # noqa

assert "modal" in sys.modules

# This is a very heavy dependency that takes about 70-80ms to import
# Let's make sure it doesn't get imported in global scope
assert "aiohttp" not in sys.modules



================================================
FILE: test/supports/special_poetry.lock
================================================
[foo]
bar = "baz"



================================================
FILE: test/supports/standalone_file.py
================================================
# Copyright Modal Labs 2022



================================================
FILE: test/supports/startup_failure.py
================================================
# Copyright Modal Labs 2022
import modal

app = modal.App("hello-world")

if not modal.is_local():
    import nonexistent_package  # noqa


@app.function()
def f(i):
    pass



================================================
FILE: test/supports/startup_failure_bigexception.py
================================================
# Copyright Modal Labs 2022
import modal

app = modal.App("hello-world")


class BigException(Exception):
    def __init__(self, message):
        self.full_message = message
        self.message = message[:10]
        super().__init__(self.message)


if not modal.is_local():
    raise BigException("a" * 5_000_000)


@app.function()
def f(i):
    pass



================================================
FILE: test/supports/test-conda-environment.yml
================================================
name: env1
channels:
  - pytorch
  - defaults
dependencies:
  - python=3.12.5
  - foo=1.0
  - pip:
      - bar=2.1



================================================
FILE: test/supports/test-dockerfile
================================================
FROM python:3.10-slim-bullseye
RUN pip install numpy



================================================
FILE: test/supports/test-pyproject.toml
================================================
[build-system]
requires = ["setuptools", "wheel"]
build-backend = "setuptools.build_meta"

[tool.mypy]
python_version = "3.9"
exclude = "build"
ignore_missing_imports = true
check_untyped_defs = true
no_strict_optional = true
namespace_packages = true

[project]
name = "foo"
description = "bar"
requires-python = ">=3.9"
dependencies = ["banana >=1.2.0", "potato >=0.1.0"]

[project.optional-dependencies]
dev = ["linting-tool >=0.0.0"]
test = ["pytest >=1.2.0"]
doc = ["mkdocs >=1.4.2"]



================================================
FILE: test/supports/test-requirements.txt
================================================
# some comment
banana~=1.2.3
apple # another comment
blueberry~=0.0.0; python_version >= '3.7'
git+https://github.com/modal-com/synchronicity.git#egg=synchronicity


# more
# comments



================================================
FILE: test/supports/type_assertions.py
================================================
# Copyright Modal Labs 2024
import typing
from importlib.util import find_spec

from typing_extensions import assert_type

import modal
from modal.partial_function import method

app = modal.App()


@app.function()
def typed_func(a: str) -> float:
    return 0.0


@app.function()
def other_func() -> str:
    return "foo"


ret = typed_func.remote(a="hello")
assert_type(ret, float)

ret2 = modal.FunctionCall.gather(typed_func.spawn("bar"), other_func.spawn())
# This assertion doesn't work in mypy (it infers the more generic list[object]), but does work in pyright/vscode:
# assert_type(ret2, typing.List[typing.Union[float, str]])
mypy_compatible_ret: typing.Sequence[object] = ret2  # mypy infers to the broader "object" type instead


should_be_float = typed_func.remote(a="hello")
assert_type(should_be_float, float)


@app.function()
async def async_typed_func(b: bool) -> str:
    return ""


async_typed_func

should_be_str = async_typed_func.remote(False)  # should be blocking without aio
assert_type(should_be_str, str)


@app.cls()
class Cls:
    @method()
    def foo(self, a: str) -> int:
        return 1

    @method()
    async def bar(self, a: str) -> int:
        return 1


instance = Cls()
should_be_int = instance.foo.remote("foo")
assert_type(should_be_int, int)

should_be_int = instance.bar.remote("bar")
assert_type(should_be_int, int)


async def async_block() -> None:
    should_be_str_2 = await async_typed_func.remote.aio(True)
    assert_type(should_be_str_2, str)
    should_also_be_str = await async_typed_func.local(False)  # local should be the original return type (!)
    assert_type(should_also_be_str, str)
    should_be_int = await instance.bar.local("bar")
    assert_type(should_be_int, int)


# check sandboxes
sandbox = modal.Sandbox.create("dummy")
assert_type(sandbox.stdout.read(), str)

for line_str in sandbox.stdout:
    assert_type(line_str, str)

cmd = sandbox.exec("other")
assert_type(cmd.stdout.read(), str)

for line_str in cmd.stdout:
    assert_type(line_str, str)

cmd2 = sandbox.exec("other_bin", text=False)
assert_type(cmd2.stdout.read(), bytes)

for line_bytes in cmd2.stdout:
    assert_type(line_bytes, bytes)

# check file_io
file_io = sandbox.open("foo", "w")
assert_type(file_io.read(), str)
assert_type(file_io.readline(), str)
assert_type(file_io.readlines(), typing.Sequence[str])

file_io2 = sandbox.open("foo", "rb")
assert_type(file_io2.read(), bytes)
assert_type(file_io2.readline(), bytes)
assert_type(file_io2.readlines(), typing.Sequence[bytes])

# check secrets
secret = modal.Secret.from_name("foo")
assert_type(secret, modal.Secret)

secret = modal.Secret.from_dict({})
assert_type(secret, modal.Secret)

secret = modal.Secret.from_local_environ(["FOO"])
assert_type(secret, modal.Secret)

if find_spec("dotenv"):
    secret = modal.Secret.from_dotenv(filename="non-existing-dotenv")
    assert_type(secret, modal.Secret)



================================================
FILE: test/supports/type_assertions_negative.py
================================================
# Copyright Modal Labs 2024
import modal

app = modal.App()


@app.function()
def typed_func(a: str) -> float:
    return 0.0


typed_func.remote(b="hello")  # wrong arg name
typed_func.remote(a=10)  # wrong arg type

typed_func.local(c="hello")  # wrong arg name
typed_func.local(a=10)  # wrong arg type


async def aio_calls() -> None:
    await typed_func.remote.aio(e="hello")  # wrong arg name
    await typed_func.remote.aio(a=10.5)  # wrong arg type



================================================
FILE: test/supports/unconsumed_map.py
================================================
# Copyright Modal Labs 2022
from .common import app, f

if __name__ == "__main__":
    with app.run():
        f.map([1, 2, 3])  # type: ignore



================================================
FILE: test/supports/volume_local.py
================================================
# Copyright Modal Labs 2024
from modal import App, Volume

app2 = App()


@app2.function(volumes={"/foo": Volume.from_name("my-vol")})
def volume_func():
    pass


@app2.function()
def volume_func_outer():
    volume_func.local()



================================================
FILE: test/supports/webhook_forgot_function.py
================================================
# Copyright Modal Labs 2023
from modal import fastapi_endpoint


@fastapi_endpoint()
async def absent_minded_function(x):
    pass



================================================
FILE: test/supports/app_run_tests/app_with_lookups.py
================================================
# Copyright Modal Labs 2022
import modal

app = modal.App("my-app")

nfs = modal.NetworkFileSystem.from_name("volume_app").hydrate()


@app.function()
def foo():
    print("foo")



================================================
FILE: test/supports/app_run_tests/app_with_multiple_functions.py
================================================
# Copyright Modal Labs 2023
import modal

app = modal.App()


@app.function()
def foo():
    pass


@app.function()
def bar():
    pass



================================================
FILE: test/supports/app_run_tests/async_app.py
================================================
# Copyright Modal Labs 2022
import modal

app = modal.App()


@app.function()
async def foo():
    pass



================================================
FILE: test/supports/app_run_tests/cli_args.py
================================================
# Copyright Modal Labs 2022
from datetime import datetime
from typing import Optional, Union

from modal import App, method

app = App()


@app.local_entrypoint()
def dt_arg(dt: datetime):
    print(f"the day is {dt.day}")


@app.local_entrypoint()
def int_arg(i: int):
    print(repr(i), type(i))


@app.local_entrypoint()
def default_arg(i: int = 10):
    print(repr(i), type(i))


@app.local_entrypoint()
def unannotated_arg(i):
    print(repr(i), type(i))


@app.local_entrypoint()
def unannotated_default_arg(i=10):
    print(repr(i), type(i))


@app.function()
def int_arg_fn(i: int):
    print(repr(i), type(i))


@app.cls()
class ALifecycle:
    @method()
    def some_method(self, i):
        print(repr(i), type(i))

    @method()
    def some_method_int(self, i: int):
        print(repr(i), type(i))


@app.local_entrypoint()
def optional_arg(i: Optional[int] = None):
    print(repr(i), type(i))


@app.local_entrypoint()
def optional_arg_pep604(i: "int | None" = None):
    print(repr(i), type(i))


@app.local_entrypoint()
def optional_arg_postponed(i: "Optional[int]" = None):
    print(repr(i), type(i))


@app.function()
def optional_arg_fn(i: Optional[int] = None):
    print(repr(i), type(i))


@app.local_entrypoint()
def unparseable_annot(i: Union[int, str]):
    pass


@app.local_entrypoint()
def unevaluatable_annot(i: "no go"):  # type: ignore  # noqa
    pass



================================================
FILE: test/supports/app_run_tests/cls.py
================================================
# Copyright Modal Labs 2023
import modal

app = modal.App()


@app.cls()
class AParametrized:
    x: int = modal.parameter()

    @modal.method()
    def some_method(self, y: int): ...

    @modal.asgi_app()
    def other_method(self): ...



================================================
FILE: test/supports/app_run_tests/custom_app.py
================================================
# Copyright Modal Labs 2022
import modal

my_app = modal.App()


@my_app.function()
def foo():
    pass



================================================
FILE: test/supports/app_run_tests/default_app.py
================================================
# Copyright Modal Labs 2022
import modal

app = modal.App()


@app.function()
def foo():
    print("foo")



================================================
FILE: test/supports/app_run_tests/file_with_global_lookups.py
================================================
# Copyright Modal Labs 2025
import modal

remote_func = modal.Function.from_name("app", "some_func")
remote_cls = modal.Cls.from_name("app", "some_class")

app = modal.App()


@app.function()
def local_f():
    pass



================================================
FILE: test/supports/app_run_tests/generator.py
================================================
# Copyright Modal Labs 2022
import modal

app = modal.App()


@app.function()
def foo():
    yield "xyz"



================================================
FILE: test/supports/app_run_tests/local_entrypoint.py
================================================
# Copyright Modal Labs 2022

import modal

app = modal.App()


@app.function()
def foo():
    pass


@app.local_entrypoint()
def main():
    print("called locally")
    foo.remote()
    foo.remote()



================================================
FILE: test/supports/app_run_tests/local_entrypoint_async.py
================================================
# Copyright Modal Labs 2022

import modal

app = modal.App()


@app.function()
def foo():
    pass


@app.local_entrypoint()
async def main():
    print("called locally (async)")
    await foo.remote.aio()
    await foo.remote.aio()



================================================
FILE: test/supports/app_run_tests/local_entrypoint_invalid.py
================================================
# Copyright Modal Labs 2023
import modal

app = modal.App()


@app.function()
def foo():
    pass


@app.local_entrypoint()
def main():
    with app.run():  # should error here
        print("unreachable")
        foo.remote()  # should not get here



================================================
FILE: test/supports/app_run_tests/main_thread_assertion.py
================================================
# Copyright Modal Labs 2022
import pytest
import threading

import modal

assert threading.current_thread() == threading.main_thread()

# can be checked to ensure module is loaded at all
pytest._did_load_main_thread_assertion = True  # type: ignore

app = modal.App()


@app.function()
def dummy():
    pass



================================================
FILE: test/supports/app_run_tests/prints_desc_app.py
================================================
# Copyright Modal Labs 2022
import modal

app = modal.App()

# This is in module scope, so will show what the `description`
# value is at import time, which may be different if some code
# changes the `description` post-import.
print(f"app.description: {app.description}")


@app.function()
def foo():
    pass



================================================
FILE: test/supports/app_run_tests/raises_error.py
================================================
# Copyright Modal Labs 2024
import modal

app = modal.App()


@app.function(gpu="broken:gpu:string")
def f():
    pass



================================================
FILE: test/supports/app_run_tests/returns_data.py
================================================
# Copyright Modal Labs 2024
import modal

app = modal.App()


@app.local_entrypoint()
def returns_str() -> str:
    return "Hello!"


@app.local_entrypoint()
def returns_bytes() -> bytes:
    return b"Hello!"


@app.local_entrypoint()
def returns_int() -> int:
    return 42



================================================
FILE: test/supports/app_run_tests/uses_experimental_options.py
================================================
# Copyright Modal Labs 2025

import modal

app = modal.App()


@app.function(experimental_options={"warn_me": 1})
def gets_warning():
    print("Done!")



================================================
FILE: test/supports/app_run_tests/uses_with_options.py
================================================
# Copyright Modal Labs 2025

import modal

app = modal.App("uses-with-options")


@app.cls()
class C:
    @modal.method()
    def f(self):
        print("Done!")


C_with_gpu = C.with_options(gpu="H100")  # type: ignore  # Type masking is a problem for with_options



================================================
FILE: test/supports/app_run_tests/variadic_args.py
================================================
# Copyright Modal Labs 2022
from modal import App, method

app = App()


@app.cls()
class VaClass:
    @method()
    def va_method(self, *args):
        pass  # Set via @servicer.function_body

    @method()
    def va_method_invalid(self, x: int, *args):
        pass  # Set via @servicer.function_body


@app.function()
def va_function(*args):
    pass  # Set via @servicer.function_body


@app.function()
def va_function_invalid(x: int, *args):
    pass  # Set via @servicer.function_body


@app.local_entrypoint()
def va_entrypoint(*args):
    print(f"args: {args}")


@app.local_entrypoint()
def va_entrypoint_invalid(x: int, *args):
    print(f"args: {args}")



================================================
FILE: test/supports/app_run_tests/webhook.py
================================================
# Copyright Modal Labs 2022
from modal import App, fastapi_endpoint

app = App()


@app.function()
@fastapi_endpoint()
def foo():
    return {"bar": "baz"}



================================================
FILE: test/supports/app_run_tests/multifile/__init__.py
================================================
# Copyright Modal Labs 2025



================================================
FILE: test/supports/app_run_tests/multifile/main.py
================================================
# Copyright Modal Labs 2025
import modal

app = modal.App()


@app.local_entrypoint()
def some_main_entrypoint():
    print("main entrypoint")


def main_func():
    print("main func")



================================================
FILE: test/supports/app_run_tests/multifile/util.py
================================================
# Copyright Modal Labs 2025
from .main import app, main_func


@app.local_entrypoint()
def run_this():
    print("ran util")
    main_func()



================================================
FILE: test/supports/multifile_project/__init__.py
================================================
# Copyright Modal Labs 2025



================================================
FILE: test/supports/multifile_project/a.py
================================================
# Copyright Modal Labs 2024
import modal

from . import c

app = modal.App()

d = modal.Dict.from_name("my-queue", create_if_missing=True)


@app.function()
def a_func():
    d["foo"] = "bar"


app.include(c.app)



================================================
FILE: test/supports/multifile_project/b.py
================================================
# Copyright Modal Labs 2024
import modal

from . import c

app = modal.App()


@app.function(secrets=[modal.Secret.from_dict({"foo": "bar"})])
def b_func():
    pass


app.include(c.app)



================================================
FILE: test/supports/multifile_project/c.py
================================================
# Copyright Modal Labs 2024
import modal

app = modal.App("c")


@app.function()
def c_func():
    pass



================================================
FILE: test/supports/multifile_project/main.py
================================================
# Copyright Modal Labs 2024
import modal
from modal import enter, fastapi_endpoint, method

from . import a, b

app = modal.App().include(a.app).include(b.app)


@app.function()
def main_function():
    pass


@app.function()
@fastapi_endpoint()
def web():
    pass


other_app = modal.App()


@other_app.cls()
class Cls:
    @enter()
    def startup(self):
        pass

    @method()
    def method_on_other_app_class(self):
        pass

    @fastapi_endpoint()
    def web_endpoint_on_other_app(self):
        pass



================================================
FILE: test/supports/notebooks/simple.notebook.py
================================================
# ---
# jupyter:
#   jupytext:
#     text_representation:
#       extension: .py
#       format_name: light
#       format_version: '1.5'
#       jupytext_version: 1.14.1
#   kernelspec:
#     display_name: Python 3 (ipykernel)
#     language: python
#     name: python3
# ---

# + tags=["parameters"]
server_addr = None
# -

from modal.client import Client
from modal_proto import api_pb2

client = Client(server_addr, api_pb2.CLIENT_TYPE_CLIENT, ("foo-id", "foo-secret"))

# +
import modal

app = modal.App()


@app.function()
def hello():
    print("running")


# + tags=["main"]
with client:
    with app.run(client=client):
        hello.remote()
# -



================================================
FILE: test/supports/pkg_a/__init__.py
================================================
# Copyright Modal Labs 2022



================================================
FILE: test/supports/pkg_a/a.py
================================================
# Copyright Modal Labs 2022



================================================
FILE: test/supports/pkg_a/d.py
================================================
# Copyright Modal Labs 2022



================================================
FILE: test/supports/pkg_a/package.py
================================================
# Copyright Modal Labs 2022

import pkg_b.f  # noqa
import pkg_b.g.h  # noqa

import modal  # noqa

from .a import *  # noqa
from .b.c import *  # noqa


app = modal.App()


@app.function()
def f():
    pass



================================================
FILE: test/supports/pkg_a/script.py
================================================
# Copyright Modal Labs 2022
import a  # noqa
import b  # noqa
import b.c  # noqa
import pkg_b  # noqa
import six  # noqa

import modal  # noqa


app = modal.App()


@app.function()
def f():
    pass



================================================
FILE: test/supports/pkg_a/serialized_fn.py
================================================
# Copyright Modal Labs 2022
import modal

app = modal.App()


@app.function(serialized=True)
def f():
    pass



================================================
FILE: test/supports/pkg_a/b/c.py
================================================
# Copyright Modal Labs 2022



================================================
FILE: test/supports/pkg_a/b/e.py
================================================
# Copyright Modal Labs 2022



================================================
FILE: test/supports/pkg_b/__init__.py
================================================
# Copyright Modal Labs 2022



================================================
FILE: test/supports/pkg_b/f.py
================================================
# Copyright Modal Labs 2022



================================================
FILE: test/supports/pkg_b/g/h.py
================================================
# Copyright Modal Labs 2022



================================================
FILE: test/supports/pkg_c/__init__.py
================================================
# Copyright Modal Labs 2022



================================================
FILE: test/supports/pkg_c/i.py
================================================
# Copyright Modal Labs 2022



================================================
FILE: test/supports/pkg_c/j/k.py
================================================
# Copyright Modal Labs 2022



================================================
FILE: test/supports/pkg_d/__init__.py
================================================
# Copyright Modal Labs 2025



================================================
FILE: test/supports/pkg_d/main.py
================================================
# Copyright Modal Labs 2025
import os

import modal

app = modal.App()

image = modal.Image.debian_slim()

if os.environ.get("ADD_SOURCE") == "add":
    # intentionally makes add local not the last call, to make sure the added modules transfer to downstream layers
    image = image.add_local_python_source("pkg_a").add_local_file(__file__, "/tmp/blah")

elif os.environ.get("ADD_SOURCE") == "copy":
    # intentionally makes add local not the last call, to make sure the added modules transfer to downstream layers
    image = image.add_local_python_source("pkg_a", copy=True).run_commands("echo hello")


@app.function(image=image)
def f():
    pass



================================================
FILE: test/supports/pkg_d/sibling.py
================================================
# Copyright Modal Labs 2025



================================================
FILE: test/supports/user_code_import_samples/__init__.py
================================================
# Copyright Modal Labs 2024



================================================
FILE: test/supports/user_code_import_samples/cls.py
================================================
# Copyright Modal Labs 2024
import modal
from modal import App

app = App()


class C:
    @modal.method()
    def f(self, arg):
        return f"hello {arg}"

    @modal.method()
    def f2(self, arg):
        return f"other {arg}"

    @modal.method()
    def calls_f_remote(self, arg):
        return self.f.remote(arg)

    @modal.method()
    def self_ref(self):
        return self


UndecoratedC = C  # keep a reference to original class before overwriting

C = app.cls()(C)  # type: ignore[misc]   # "decorator" of C



================================================
FILE: test/supports/user_code_import_samples/func.py
================================================
# Copyright Modal Labs 2024

from modal import App

app = App(name="user_code_import_samples_func_app")


@app.function()
def f(arg):
    return f"hello {arg}"


def undecorated_f(arg):
    return f"hello {arg}"



================================================
FILE: test/supports/uv_lock_no_modal/pyproject.toml
================================================
[project]
name = "uv-lock-project"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.13"
dependencies = [
    "scipy",
]

[project.optional-dependencies]
extra1 = [
    "modal>=1.0.3",
]

[dependency-groups]
group1 = [
    "modal>=1.0.3",
]



================================================
FILE: test/supports/uv_lock_project/pyproject.toml
================================================
[project]
name = "uv-lock-project"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.13"
dependencies = [
    "modal>=1.0.3",
]

[project.optional-dependencies]
extra1 = [
    "ruff>=0.11.13",
]

[dependency-groups]
group1 = [
    "scipy>=1.15.3",
]



================================================
FILE: test/supports/uv_lock_workspace/pyproject.toml
================================================
[project]
name = "my-app"
version = "0.1.0"
description = "An example of uv workspace"
readme = "README.md"
requires-python = ">=3.12"
dependencies = ["modal"]

[tool.uv.sources]
my-app = { workspace = true }

[tool.uv.workspace]
members = ["packages/*"]



================================================
FILE: test/supports/uv_lock_workspace/packages/mod1/pyproject.toml
================================================
[project]
name = "mod1"
version = "0.1.0"
description = "An example of uv workspace"
readme = "README.md"
requires-python = ">=3.12"
dependencies = ["scipy"]

# [project.optional-dependencies]
# extra1 = ["scipy>=1.15.3"]

# [dependency-groups]
# group1 = ["pydantic>=2.11.5"]



================================================
FILE: test/supports/uv_lock_workspace/packages/mod2/pyproject.toml
================================================
[project]
name = "mod2"
version = "0.1.0"
description = "An example of uv workspace"
readme = "README.md"
requires-python = ">=3.12"
dependencies = ["scikit-learn"]

# [project.optional-dependencies]
# extra1 = ["scipy>=1.15.3"]

# [dependency-groups]
# group1 = ["pydantic>=2.11.5"]



================================================
FILE: test/supports/uv_sync_just_pyproject/pyproject.toml
================================================
[project]
name = "uv-lock-project"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.13"
dependencies = [
    "modal>=1.0.3",
]



================================================
FILE: test/telemetry/tracing_module_1.py
================================================
# Copyright Modal Labs 2022
from . import tracing_module_2  # noqa


def foo():
    pass



================================================
FILE: test/telemetry/tracing_module_2.py
================================================
# Copyright Modal Labs 2022
def bar():
    pass



================================================
FILE: .github/CONTRIBUTING.md
================================================
# Contributing

## Set Up Development Environment

1. Create and activate a Python virtual environment with your preferred tool. The Python version
   used for development is specified in `.github/workflows/ci-cd.yml` under
   `.github/actions/setup-cached-python`.
    1. If you use `uv`, run `uv venv .venv --python 3.11 --seed && source .venv/bin/activate`.
    1. If you use `pyenv`, run `pyenv virtualenv -p python3.11 3.11.12 modal-client && pyenv
       activate modal-client`.
1. Install development dependencies: `pip install -r requirements.dev.txt`
1. Compile protobuf files: `inv protoc`
1. Install the repo in editable mode: `pip install -e .`
1. Build type Python stubs and check types: `inv type-check`
1. Install pre-commit: `pre-commit install`



================================================
FILE: .github/pull_request_template.md
================================================
## Describe your changes

- _Provide Linear issue reference (e.g. CLI-1234) if available._

<details> <summary>Checklists</summary>

---

## Compatibility checklist

Check these boxes or delete any item (or this section) if not relevant for this PR.

- [ ] Client+Server: this change is compatible with old servers
- [ ] Client forward compatibility: this change ensures client can accept data intended for later versions of itself

Note on protobuf: protobuf message changes in one place may have impact to
multiple entities (client, server, worker, database). See points above.


---

## Release checklist

If you intend for this commit to trigger a full release to PyPI, please ensure that the following steps have been taken:

- [ ] Version file (`modal_version/__init__.py`) has been updated with the next logical version
- [ ] Changelog has been cleaned up and given an appropriate subhead

---

</details>

## Changelog

<!--
If relevant, include a brief user-facing description of what's new in this version.

Format the changelog updates using bullet points.
See https://modal.com/docs/reference/changelog for examples and try to use a consistent style.

Provide short code examples, indented under the relevant bullet point, if they would be helpful.
Cross-linking to relevant documentation is also encouraged.
-->



================================================
FILE: .github/actions/setup-cached-python/action.yml
================================================
name: setup-cached-python

inputs:
  version:
    description: Which Python version to install
    required: true
    default: "3.9"

runs:
  using: composite
  steps:
    - name: Install Python
      uses: actions/setup-python@8d9ed9ac5c53483de85588cdf95a591a75ab9f55 # v5
      with:
        python-version: ${{ inputs.version }}

    - name: Get cached python dependencies
      uses: actions/cache@5a3ec84eff668545956fd18022155c47e93e2684 # v4
      with:
        path: ${{ env.pythonLocation }}
        key: |
          ${{ runner.os }}-${{ env.pythonLocation }}-${{ hashFiles('**/setup.cfg', 'requirements.dev.txt', 'pyproject.toml') }}-v3

    - name: Install Python packages
      shell: bash
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.dev.txt



================================================
FILE: .github/ISSUE_TEMPLATE/1_bug_report.yaml
================================================
name: Bug report
description: Report an error or unexpected behavior
labels: ["bug"]
body:
  - type: markdown
    attributes:
      value: |
        Thanks for helping to improve Modal!
        We generally prefer to receive bug reports over [Slack](https://modal.com/slack), which is monitored by a larger support team and plugs into our internal ticketing system.
        If that is not an option, you can file a bug report here, although we may be slower to respond.

  - type: textarea
    attributes:
      label: Summary
      description: |
        A clear and concise description of the bug, ideally including a minimal reproducible example (e.g. a script that we can `modal run` to observe the defective behavior).
    validations:
      required: true

  - type: input
    attributes:
      label: Version
      description: The version of the modal client you are using (`modal --version`)
      placeholder: e.g., 0.70.123
    validations:
      required: true

  - type: input
    attributes:
      label: App ID
      description: If the bug pertains to an existing App, please share the ID. Sharing an ID implies permission for Modal engineers to view the App logs.
      placeholder: e.g., ap-123abc567xyz
    validations:
      required: false



================================================
FILE: .github/ISSUE_TEMPLATE/config.yml
================================================
blank_issues_enabled: false
contact_links:
  - name: User Support
    url: https://modal.com/slack
    about: Please ask for support using Modal on Slack (#general)
  - name: Feature Requests
    url: https://modal.com/slack
    about: Please reach out with feature requests on Slack (#feature-requests)




================================================
FILE: .github/workflows/check.yml
================================================
name: Check

on:
  push:
    branches:
      - main
  pull_request:

# Cancel previous runs of the same PR but do not cancel previous runs on main
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: ${{ github.ref != 'refs/heads/main' }}

jobs:
  lint:
    name: Ruff linting
    runs-on: ubuntu-24.04

    steps:
      - uses: actions/checkout@f43a0e5ff2bd294095638e18286ca9a3d1956744 # v3

      - uses: ./.github/actions/setup-cached-python
        with:
          version: "3.10"

      - run: inv lint

      - run: inv lint-protos

  type_check:
    name: Static type checks
    runs-on: ubuntu-24.04

    steps:
      - uses: actions/checkout@f43a0e5ff2bd294095638e18286ca9a3d1956744 # v3

      - uses: ./.github/actions/setup-cached-python
        with:
          version: "3.10"

      - run: inv protoc

      - run: pip install -e .  # gets all dependencies and the package itself into python env

      - name: Build type stubs
        run: inv type-stubs

      - run: inv type-check

  check-copyright:
    name: Check copyright
    runs-on: ubuntu-24.04

    steps:
      - uses: actions/checkout@f43a0e5ff2bd294095638e18286ca9a3d1956744 # v3

      - uses: ./.github/actions/setup-cached-python
        with:
          version: "3.10"

      - run: inv check-copyright



================================================
FILE: .github/workflows/ci-cd.yml
================================================
name: CI/CD

on:
  push:
    branches:
      - main
  pull_request:

# Cancel previous runs of the same PR but do not cancel previous runs on main
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: ${{ github.ref != 'refs/heads/main' }}

env:
  TERM: linux
  TERMINFO: /etc/terminfo
  PYTHONIOENCODING: utf-8

jobs:
  client-versioning:
    if: github.ref == 'refs/heads/main'
    name: Update changelog and client version
    concurrency: client-versioning
    runs-on: ubuntu-24.04
    env:
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
    outputs:
      release-hash: ${{ steps.release-data.outputs.hash }}

    steps:
      - name: Generate token for Github PR Bot
        id: generate_token
        uses: tibdex/github-app-token@32691ba7c9e7063bd457bd8f2a5703138591fa58 # v1
        with:
          app_id: ${{ secrets.GH_PRBOT_APP_ID }}
          private_key: ${{ secrets.GH_PRBOT_APP_PRIVATE_KEY }}

      - uses: actions/checkout@f43a0e5ff2bd294095638e18286ca9a3d1956744 # v3
        with:
          token: ${{ steps.generate_token.outputs.token }}
          fetch-depth: 2 # Needed so that we can check whether the version file was modified

      - uses: ./.github/actions/setup-cached-python
        with:
          version: "3.10"

      - name: Update the version
        run: inv bump-dev-version

      - name: Update the changelog
        run: inv update-changelog --sha=$GITHUB_SHA

      - name: Get release tag
        id: tag
        run: echo "release_tag=`inv get-release-tag`" >> "$GITHUB_OUTPUT"

      - uses: EndBug/add-and-commit@a94899bca583c204427a224a7af87c02f9b325d5 # v9
        with:
          pull: "--rebase --autostash"
          add: modal_version/__init__.py CHANGELOG.md
          commit: "--allow-empty"
          message: "[auto-commit] [skip ci] Bump the build number"
          tag: ${{ steps.tag.outputs.release_tag }}
          default_author: github_actions

      - name: Get release hash
        id: release-data
        run: echo "hash=`git rev-parse HEAD`" >> "$GITHUB_OUTPUT"

      - name: Install the client
        run: |
          inv protoc
          pip install .

      - name: Publish client mount
        env:
          MODAL_ENVIRONMENT: main
          MODAL_LOGLEVEL: DEBUG
          MODAL_TOKEN_ID: ${{ secrets.MODAL_TOKEN_ID }}
          MODAL_TOKEN_SECRET: ${{ secrets.MODAL_TOKEN_SECRET }}
        run: python -m modal_global_objects.mounts.modal_client_package

  client-test:
    name: Unit tests on ${{ matrix.python-version }} and ${{ matrix.os }} (protobuf=${{ matrix.proto-version }})
    timeout-minutes: 30

    strategy:
      fail-fast: false # run all variants across python versions/os to completion
      matrix:
        python-version: ["3.9", "3.10", "3.11", "3.12", "3.13"]
        os: ["ubuntu-24.04"]
        proto-version: ["latest"]
        include:
          - os: "macos-13" # x86-64
            python-version: "3.10"
            proto-version: "latest"
          - os: "macos-14" # ARM64 (M1)
            python-version: "3.10"
            proto-version: "latest"
          - os: "windows-latest"
            python-version: "3.10"
            proto-version: "latest"
          - os: "ubuntu-24.04"
            python-version: "3.9"
            proto-version: "3.19"

    runs-on: ${{ matrix.os }}
    steps:
      - uses: actions/checkout@f43a0e5ff2bd294095638e18286ca9a3d1956744 # v3

      - uses: ./.github/actions/setup-cached-python
        with:
          version: ${{ matrix.python-version }}

      - if: matrix.proto-version != 'latest'
        name: Install protobuf
        run: pip install protobuf==${{ matrix.proto-version }}

      - name: Build protobuf
        run: inv protoc

      - name: Build client package (installs all dependencies)
        run: pip install -e .

      - name: Run client tests
        run: inv test

      - name: Run docstring tests
        if: github.event.pull_request.head.repo.fork == false
        env:
          MODAL_ENVIRONMENT: client-doc-tests
          MODAL_TOKEN_ID: ${{ secrets.MODAL_TOKEN_ID }}
          MODAL_TOKEN_SECRET: ${{ secrets.MODAL_TOKEN_SECRET }}
        run: pytest -v --markdown-docs -m markdown-docs modal

  container-dependencies:
    name: Check minimal container dependencies for ${{ matrix.python-version }} / ${{ matrix.image-builder-version }}
    runs-on: ubuntu-24.04
    timeout-minutes: 4
    strategy:
      matrix:
        include:
          - image-builder-version: "2024.04"
            python-version: "3.9"
          - image-builder-version: "2024.04"
            python-version: "3.12"
          - image-builder-version: "2024.10"
            python-version: "3.9"
          - image-builder-version: "2024.10"
            python-version: "3.13"
          - image-builder-version: "2025.06"
            python-version: "3.9"
          - image-builder-version: "2025.06"
            python-version: "3.13"

    steps:
      - uses: actions/checkout@f43a0e5ff2bd294095638e18286ca9a3d1956744 # v3

      - uses: actions/setup-python@8d9ed9ac5c53483de85588cdf95a591a75ab9f55 # v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install dependencies
        run: |
          pip install -r modal/builder/${{ matrix.image-builder-version }}.txt
          pip install synchronicity

      - name: Compile protos
        run: |
          python -m venv venv
          source venv/bin/activate
          if [ "${{ matrix.python-version }}" == "3.9" ]; then
            pip install grpcio-tools==1.48.2 grpclib==0.4.7;
          elif [ "${{ matrix.python-version }}" == "3.12" ]; then
            pip install grpcio-tools==1.59.2 grpclib==0.4.7;
          elif [ "${{ matrix.python-version }}" == "3.13" ]; then
            pip install grpcio-tools==1.66.2 grpclib==0.4.7;
          fi
          python -m grpc_tools.protoc --python_out=. --grpclib_python_out=. --grpc_python_out=. -I . modal_proto/api.proto modal_proto/options.proto
          python -m grpc_tools.protoc --plugin=protoc-gen-modal-grpclib-python=protoc_plugin/plugin.py --modal-grpclib-python_out=. -I . modal_proto/api.proto modal_proto/options.proto
          deactivate

      - name: Check entrypoint import
        run: |
          python -c 'import modal._container_entrypoint; import modal._runtime.asgi'
          if [ "${{ matrix.image-builder-version }}" == "2024.04" ]; then python -c 'import fastapi'; fi

  publish-client:
    name: Publish client package
    if: github.ref == 'refs/heads/main'
    needs: [client-versioning, client-test]
    runs-on: ubuntu-24.04
    concurrency: publish-client
    timeout-minutes: 5
    steps:
      - uses: actions/checkout@f43a0e5ff2bd294095638e18286ca9a3d1956744 # v3
        with:
          # Check out the commit that bumped the version, after the merge that triggered the workflow
          ref: ${{ needs.client-versioning.outputs.release-hash }}

      - uses: ./.github/actions/setup-cached-python
        with:
          # This is load-bearing because it determines the version of the protobuf compiler
          # used for the gencode, which should be the *minimal* version that we support.
          version: "3.10"

      - name: Build protobuf
        run: inv protoc

      - name: Install all dependencies
        run: pip install -e .

      - name: Build type stubs
        run: inv type-stubs

      - name: Install build
        run: pip install build

      - name: Build package distributions (wheel and source)
        run: |
          python -m build

      - name: Upload to PyPI
        env:
          TWINE_USERNAME: __token__
          TWINE_PASSWORD: ${{ secrets.PYPI_API_TOKEN }}
        run: twine upload dist/* --non-interactive

  publish-python-standalone:
    name: Publish Python standalone mounts
    if: github.ref == 'refs/heads/main'
    needs: [client-versioning, client-test, publish-client]
    runs-on: ubuntu-24.04
    timeout-minutes: 5
    env:
      MODAL_LOGLEVEL: DEBUG
      MODAL_TOKEN_ID: ${{ secrets.MODAL_TOKEN_ID }}
      MODAL_TOKEN_SECRET: ${{ secrets.MODAL_TOKEN_SECRET }}

    steps:
      - uses: actions/checkout@f43a0e5ff2bd294095638e18286ca9a3d1956744 # v3
        with:
          # Check out the commit that bumped the version, after the merge that triggered the workflow
          ref: ${{ needs.client-versioning.outputs.release-hash }}

      - uses: ./.github/actions/setup-cached-python
        with:
          version: "3.11"

      - name: Build protobuf
        run: inv protoc

      - name: Build client package (installs all dependencies)
        run: pip install -e .

      - name: Publish mounts
        run: python -m modal_global_objects.mounts.python_standalone

  publish-client-dependency-mounts:
    name: Publish client dependency mounts
    if: github.ref == 'refs/heads/main'
    needs: [client-versioning, client-test, publish-client]
    runs-on: ubuntu-24.04
    timeout-minutes: 5
    env:
      MODAL_LOGLEVEL: DEBUG
      MODAL_TOKEN_ID: ${{ secrets.MODAL_TOKEN_ID }}
      MODAL_TOKEN_SECRET: ${{ secrets.MODAL_TOKEN_SECRET }}

    steps:
      - uses: actions/checkout@f43a0e5ff2bd294095638e18286ca9a3d1956744 # v3
        with:
          # Check out the commit that bumped the version, after the merge that triggered the workflow
          ref: ${{ needs.client-versioning.outputs.release-hash }}

      - uses: ./.github/actions/setup-cached-python
        with:
          version: "3.11"

      - name: Build protobuf
        run: inv protoc

      - name: Build client package (installs all dependencies)
        run: pip install -e .

      - name: Install uv
        run: pip install uv

      - name: Publish mounts
        run: python -m modal_global_objects.mounts.modal_client_dependencies

  publish-base-images:
    name: |
      Publish base images for ${{ matrix.image-name }} ${{ matrix.image-builder-version }}
    if: github.ref == 'refs/heads/main'
    needs: [client-versioning, client-test, publish-client]
    runs-on: ubuntu-24.04
    timeout-minutes: 5
    env:
      MODAL_LOGLEVEL: DEBUG
      MODAL_TOKEN_ID: ${{ secrets.MODAL_TOKEN_ID }}
      MODAL_TOKEN_SECRET: ${{ secrets.MODAL_TOKEN_SECRET }}
    strategy:
      matrix:
        image-builder-version: ["2023.12", "2024.04", "2024.10", "2025.06"]
        image-name: ["debian_slim", "micromamba"]

    steps:
      - uses: actions/checkout@f43a0e5ff2bd294095638e18286ca9a3d1956744 # v3
        with:
          # Check out the commit that bumped the version, after the merge that triggered the workflow
          ref: ${{ needs.client-versioning.outputs.release-hash }}

      - uses: ./.github/actions/setup-cached-python
        with:
          version: "3.11"

      - name: Build protobuf
        run: inv protoc

      - name: Build client package (installs all dependencies)
        run: pip install -e .

      - name: Set the Modal environment
        run: modal config set-environment main

      - name: Publish base images
        run: inv publish-base-images ${{ matrix.image-name }}
          --builder-version ${{ matrix.image-builder-version }}
          --allow-global-deployment --no-confirm



================================================
FILE: .github/workflows/docs.yml
================================================
name: Docs

on:
  push:
    branches:
      - main
  pull_request:

# Cancel previous runs of the same PR but do not cancel previous runs on main
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: ${{ github.ref != 'refs/heads/main' }}

jobs:
  doc-test:
    name: Doc generation tests
    timeout-minutes: 5
    runs-on: ubuntu-24.04
    steps:
      - uses: actions/checkout@f43a0e5ff2bd294095638e18286ca9a3d1956744 # v3

      - uses: ./.github/actions/setup-cached-python

      - name: Build protobuf
        run: inv protoc

      - name: Install package + deps
        run: pip install -e .  # Makes sure doc generation doesn't break on client imports etc.

      - name: Generate reference docs
        run: python -m modal_docs.gen_reference_docs reference_docs_output

      - name: Generate CLI docs
        run: python -m modal_docs.gen_cli_docs cli_docs



================================================
FILE: .github/workflows/sast-codeql.yml
================================================
# For most projects, this workflow file will not need changing; you simply need
# to commit it to your repository.
#
# You may wish to alter this file to override the set of languages analyzed,
# or to provide custom queries or build logic.
#
# ******** NOTE ********
# We have attempted to detect the languages in your repository. Please check
# the `language` matrix defined below to confirm you have the correct set of
# supported CodeQL languages.
#
name: "CodeQL"

on:
  push:
    branches: [ "main" ]
  pull_request:
    # The branches below must be a subset of the branches above
    branches: [ "main" ]
  schedule:
    - cron: '43 3 * * 1'

# Cancel previous runs of the same PR but do not cancel previous runs on main
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: ${{ github.ref != 'refs/heads/main' }}

jobs:
  analyze:
    name: Analyze
    runs-on: ${{ (matrix.language == 'swift' && 'macos-latest') || 'ubuntu-latest' }}
    permissions:
      actions: read
      contents: read
      security-events: write

    strategy:
      fail-fast: false
      matrix:
        language: [ 'python' ]
        # CodeQL supports [ 'cpp', 'csharp', 'go', 'java', 'javascript', 'python', 'ruby' ]
        # Use only 'java' to analyze code written in Java, Kotlin or both
        # Use only 'javascript' to analyze code written in JavaScript, TypeScript or both
        # Learn more about CodeQL language support at https://aka.ms/codeql-docs/language-support

    steps:
    - name: Checkout repository
      uses: actions/checkout@f43a0e5ff2bd294095638e18286ca9a3d1956744 # v3

    # Initializes the CodeQL tools for scanning.
    - name: Initialize CodeQL
      uses: github/codeql-action/init@b8d3b6e8af63cde30bdc382c0bc28114f4346c88 # v2
      with:
        languages: ${{ matrix.language }}
        # If you wish to specify custom queries, you can do so here or in a config file.
        # By default, queries listed here will override any specified in a config file.
        # Prefix the list here with "+" to use these queries and those in the config file.

        # For more details on CodeQL's query packs, refer to: https://docs.github.com/en/code-security/code-scanning/automatically-scanning-your-code-for-vulnerabilities-and-errors/configuring-code-scanning#using-queries-in-ql-packs
        # queries: security-extended,security-and-quality


    # Autobuild attempts to build any compiled languages  (C/C++, C#, Go, or Java).
    # If this step fails, then you should remove it and run the build manually (see below)
    - name: Autobuild
      uses: github/codeql-action/autobuild@b8d3b6e8af63cde30bdc382c0bc28114f4346c88 # v2

    # â„¹ï¸ Command-line programs to run using the OS shell.
    # ðŸ“š See https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions#jobsjob_idstepsrun

    #   If the Autobuild fails above, remove it and uncomment the following three lines.
    #   modify them (or add more) to build your code if your project, please refer to the EXAMPLE below for guidance.

    # - run: |
    #     echo "Run, Build Application using script"
    #     ./location_of_script_within_repo/buildscript.sh

    - name: Perform CodeQL Analysis
      uses: github/codeql-action/analyze@b8d3b6e8af63cde30bdc382c0bc28114f4346c88 # v2
      with:
        category: "/language:${{matrix.language}}"


