Directory structure:
└── nolar-kopf/
    ├── README.md
    ├── _importlinter_conditional.py
    ├── CODE_OF_CONDUCT.md
    ├── CONTRIBUTING.md
    ├── CONTRIBUTORS.md
    ├── DEVELOPMENT.md
    ├── LICENSE
    ├── MAINTAINERS
    ├── mypy.ini
    ├── peering.yaml
    ├── pytest.ini
    ├── requirements.txt
    ├── SECURITY.md
    ├── setup.py
    ├── .codecov.yml
    ├── .importlinter
    ├── .isort.cfg
    ├── .pre-commit-config.yaml
    ├── .readthedocs.yaml
    ├── docs/
    │   ├── admission.rst
    │   ├── alternatives.rst
    │   ├── architecture-layers.xml
    │   ├── architecture.rst
    │   ├── async.rst
    │   ├── authentication.rst
    │   ├── cli.rst
    │   ├── concepts.rst
    │   ├── conf.py
    │   ├── configuration.rst
    │   ├── continuity.rst
    │   ├── contributing.rst
    │   ├── daemons.rst
    │   ├── deployment-depl.yaml
    │   ├── deployment-rbac.yaml
    │   ├── deployment.rst
    │   ├── embedding.rst
    │   ├── errors.rst
    │   ├── events.rst
    │   ├── filters.rst
    │   ├── handlers.rst
    │   ├── hierarchies.rst
    │   ├── idempotence.rst
    │   ├── index.rst
    │   ├── indexing.rst
    │   ├── install.rst
    │   ├── kwargs.rst
    │   ├── loading.rst
    │   ├── memos.rst
    │   ├── minikube.rst
    │   ├── naming.rst
    │   ├── peering.rst
    │   ├── probing.rst
    │   ├── reconciliation.rst
    │   ├── requirements.txt
    │   ├── resources.rst
    │   ├── results.rst
    │   ├── scopes.rst
    │   ├── shutdown.rst
    │   ├── startup.rst
    │   ├── testing.rst
    │   ├── timers.rst
    │   ├── tips-and-tricks.rst
    │   ├── troubleshooting.rst
    │   ├── vision.rst
    │   └── walkthrough/
    │       ├── cleanup.rst
    │       ├── creation.rst
    │       ├── deletion.rst
    │       ├── diffs.rst
    │       ├── prerequisites.rst
    │       ├── problem.rst
    │       ├── resources.rst
    │       ├── starting.rst
    │       └── updates.rst
    ├── examples/
    │   ├── README.md
    │   ├── crd.yaml
    │   ├── obj.yaml
    │   ├── requirements.txt
    │   ├── .isort.cfg
    │   ├── 01-minimal/
    │   │   ├── README.md
    │   │   └── example.py
    │   ├── 02-children/
    │   │   ├── README.md
    │   │   └── example.py
    │   ├── 03-exceptions/
    │   │   ├── README.md
    │   │   └── example.py
    │   ├── 04-events/
    │   │   ├── README.md
    │   │   └── example.py
    │   ├── 05-handlers/
    │   │   ├── README.md
    │   │   └── example.py
    │   ├── 06-peering/
    │   │   ├── README.md
    │   │   └── example.py
    │   ├── 07-subhandlers/
    │   │   ├── README.md
    │   │   └── example.py
    │   ├── 08-events/
    │   │   ├── README.md
    │   │   └── example.py
    │   ├── 09-testing/
    │   │   ├── README.md
    │   │   ├── example.py
    │   │   └── test_example_09.py
    │   ├── 10-builtins/
    │   │   ├── README.md
    │   │   ├── example.py
    │   │   └── test_example_10.py
    │   ├── 11-filtering-handlers/
    │   │   ├── README.md
    │   │   ├── example.py
    │   │   └── test_example_11.py
    │   ├── 12-embedded/
    │   │   ├── README.md
    │   │   ├── example.py
    │   │   └── test_nothing.py
    │   ├── 13-hooks/
    │   │   ├── README.md
    │   │   └── example.py
    │   ├── 14-daemons/
    │   │   └── example.py
    │   ├── 15-timers/
    │   │   └── example.py
    │   ├── 16-indexing/
    │   │   └── example.py
    │   ├── 17-admission/
    │   │   └── example.py
    │   └── 99-all-at-once/
    │       ├── README.md
    │       └── example.py
    ├── kopf/
    │   ├── __init__.py
    │   ├── __main__.py
    │   ├── cli.py
    │   ├── on.py
    │   ├── py.typed
    │   ├── testing.py
    │   ├── _cogs/
    │   │   ├── __init__.py
    │   │   ├── aiokits/
    │   │   │   ├── __init__.py
    │   │   │   ├── aioadapters.py
    │   │   │   ├── aiobindings.py
    │   │   │   ├── aioenums.py
    │   │   │   ├── aiotasks.py
    │   │   │   ├── aiotime.py
    │   │   │   ├── aiotoggles.py
    │   │   │   └── aiovalues.py
    │   │   ├── clients/
    │   │   │   ├── __init__.py
    │   │   │   ├── api.py
    │   │   │   ├── auth.py
    │   │   │   ├── creating.py
    │   │   │   ├── errors.py
    │   │   │   ├── events.py
    │   │   │   ├── fetching.py
    │   │   │   ├── patching.py
    │   │   │   ├── scanning.py
    │   │   │   └── watching.py
    │   │   ├── configs/
    │   │   │   ├── __init__.py
    │   │   │   ├── configuration.py
    │   │   │   ├── conventions.py
    │   │   │   ├── diffbase.py
    │   │   │   └── progress.py
    │   │   ├── helpers/
    │   │   │   ├── __init__.py
    │   │   │   ├── hostnames.py
    │   │   │   ├── loaders.py
    │   │   │   ├── thirdparty.py
    │   │   │   ├── typedefs.py
    │   │   │   └── versions.py
    │   │   └── structs/
    │   │       ├── __init__.py
    │   │       ├── bodies.py
    │   │       ├── credentials.py
    │   │       ├── dicts.py
    │   │       ├── diffs.py
    │   │       ├── ephemera.py
    │   │       ├── finalizers.py
    │   │       ├── ids.py
    │   │       ├── patches.py
    │   │       ├── references.py
    │   │       └── reviews.py
    │   ├── _core/
    │   │   ├── __init__.py
    │   │   ├── actions/
    │   │   │   ├── __init__.py
    │   │   │   ├── application.py
    │   │   │   ├── execution.py
    │   │   │   ├── invocation.py
    │   │   │   ├── lifecycles.py
    │   │   │   ├── loggers.py
    │   │   │   ├── progression.py
    │   │   │   └── throttlers.py
    │   │   ├── engines/
    │   │   │   ├── __init__.py
    │   │   │   ├── activities.py
    │   │   │   ├── admission.py
    │   │   │   ├── daemons.py
    │   │   │   ├── indexing.py
    │   │   │   ├── peering.py
    │   │   │   ├── posting.py
    │   │   │   └── probing.py
    │   │   ├── intents/
    │   │   │   ├── __init__.py
    │   │   │   ├── callbacks.py
    │   │   │   ├── causes.py
    │   │   │   ├── filters.py
    │   │   │   ├── handlers.py
    │   │   │   ├── piggybacking.py
    │   │   │   ├── registries.py
    │   │   │   └── stoppers.py
    │   │   └── reactor/
    │   │       ├── __init__.py
    │   │       ├── inventory.py
    │   │       ├── observation.py
    │   │       ├── orchestration.py
    │   │       ├── processing.py
    │   │       ├── queueing.py
    │   │       ├── running.py
    │   │       └── subhandling.py
    │   └── _kits/
    │       ├── __init__.py
    │       ├── hierarchies.py
    │       ├── loops.py
    │       ├── runner.py
    │       ├── webhacks.py
    │       └── webhooks.py
    ├── tests/
    │   ├── conftest.py
    │   ├── test_absent_modules.py
    │   ├── test_async.py
    │   ├── test_filtering_helpers.py
    │   ├── test_finalizers.py
    │   ├── test_it.py
    │   ├── test_liveness.py
    │   ├── test_thirdparty.py
    │   ├── test_versions.py
    │   ├── admission/
    │   │   ├── conftest.py
    │   │   ├── test_admission_manager.py
    │   │   ├── test_admission_server.py
    │   │   ├── test_certificates.py
    │   │   ├── test_jsonpatch.py
    │   │   ├── test_managed_webhooks.py
    │   │   ├── test_serving_ephemeral_memos.py
    │   │   ├── test_serving_handler_selection.py
    │   │   ├── test_serving_kwargs_passthrough.py
    │   │   ├── test_serving_responses.py
    │   │   ├── test_webhook_detection.py
    │   │   ├── test_webhook_ngrok.py
    │   │   └── test_webhook_server.py
    │   ├── apis/
    │   │   ├── test_api_requests.py
    │   │   ├── test_default_namespace.py
    │   │   ├── test_error_retries.py
    │   │   └── test_iterjsonlines.py
    │   ├── authentication/
    │   │   ├── test_authentication.py
    │   │   ├── test_connectioninfo.py
    │   │   ├── test_credentials.py
    │   │   ├── test_login_kubeconfig.py
    │   │   ├── test_login_serviceaccount.py
    │   │   ├── test_reauthentication.py
    │   │   ├── test_tempfiles.py
    │   │   └── test_vault.py
    │   ├── basic-structs/
    │   │   ├── test_causes.py
    │   │   ├── test_handlers.py
    │   │   ├── test_memories.py
    │   │   ├── test_memos.py
    │   │   └── test_resource.py
    │   ├── causation/
    │   │   ├── test_detection.py
    │   │   └── test_kwargs.py
    │   ├── cli/
    │   │   ├── conftest.py
    │   │   ├── test_help.py
    │   │   ├── test_logging.py
    │   │   ├── test_options.py
    │   │   └── test_preloading.py
    │   ├── dicts/
    │   │   ├── test_cherrypicking.py
    │   │   ├── test_dictviews.py
    │   │   ├── test_ensuring.py
    │   │   ├── test_parsing.py
    │   │   ├── test_removing.py
    │   │   ├── test_resolving.py
    │   │   └── test_walking.py
    │   ├── diffs/
    │   │   ├── test_calculation.py
    │   │   ├── test_protocols.py
    │   │   └── test_reduction.py
    │   ├── e2e/
    │   │   ├── conftest.py
    │   │   └── test_examples.py
    │   ├── handling/
    │   │   ├── conftest.py
    │   │   ├── test_activity_triggering.py
    │   │   ├── test_cause_handling.py
    │   │   ├── test_cause_logging.py
    │   │   ├── test_delays.py
    │   │   ├── test_error_handling.py
    │   │   ├── test_event_handling.py
    │   │   ├── test_multistep.py
    │   │   ├── test_no_handlers.py
    │   │   ├── test_parametrization.py
    │   │   ├── test_retrying_limits.py
    │   │   ├── test_timing_consistency.py
    │   │   ├── daemons/
    │   │   │   ├── conftest.py
    │   │   │   ├── test_daemon_errors.py
    │   │   │   ├── test_daemon_filtration.py
    │   │   │   ├── test_daemon_rematching.py
    │   │   │   ├── test_daemon_spawning.py
    │   │   │   ├── test_daemon_termination.py
    │   │   │   ├── test_timer_errors.py
    │   │   │   ├── test_timer_filtration.py
    │   │   │   ├── test_timer_intervals.py
    │   │   │   └── test_timer_triggering.py
    │   │   ├── indexing/
    │   │   │   ├── conftest.py
    │   │   │   ├── test_blocking_until_indexed.py
    │   │   │   ├── test_index_exclusion.py
    │   │   │   └── test_index_population.py
    │   │   └── subhandling/
    │   │       └── test_subhandling.py
    │   ├── hierarchies/
    │   │   ├── conftest.py
    │   │   ├── test_contextual_owner.py
    │   │   ├── test_labelling.py
    │   │   ├── test_name_harmonizing.py
    │   │   ├── test_namespace_adjusting.py
    │   │   ├── test_owner_referencing.py
    │   │   └── test_type_validation.py
    │   ├── invocations/
    │   │   └── test_callbacks.py
    │   ├── k8s/
    │   │   ├── conftest.py
    │   │   ├── test_creating.py
    │   │   ├── test_errors.py
    │   │   ├── test_events.py
    │   │   ├── test_list_objs.py
    │   │   ├── test_patching.py
    │   │   ├── test_scanning.py
    │   │   ├── test_watching_bookmarks.py
    │   │   ├── test_watching_continuously.py
    │   │   ├── test_watching_infinitely.py
    │   │   └── test_watching_with_freezes.py
    │   ├── lifecycles/
    │   │   ├── conftest.py
    │   │   ├── test_global_defaults.py
    │   │   ├── test_handler_selection.py
    │   │   └── test_real_invocation.py
    │   ├── logging/
    │   │   ├── conftest.py
    │   │   ├── test_configuration.py
    │   │   ├── test_formatters.py
    │   │   └── test_loggers.py
    │   ├── observation/
    │   │   ├── test_processing_of_namespaces.py
    │   │   ├── test_processing_of_resources.py
    │   │   ├── test_revision_of_namespaces.py
    │   │   └── test_revision_of_resources.py
    │   ├── orchestration/
    │   │   └── test_task_adjustments.py
    │   ├── peering/
    │   │   ├── conftest.py
    │   │   ├── test_freeze_mode.py
    │   │   ├── test_id_generation.py
    │   │   ├── test_keepalive.py
    │   │   ├── test_peer_patching.py
    │   │   ├── test_peers.py
    │   │   └── test_resource_guessing.py
    │   ├── persistence/
    │   │   ├── test_annotations_hashing.py
    │   │   ├── test_essences.py
    │   │   ├── test_outcomes.py
    │   │   ├── test_states.py
    │   │   ├── test_storing_of_diffbase.py
    │   │   └── test_storing_of_progress.py
    │   ├── posting/
    │   │   ├── conftest.py
    │   │   ├── test_log2k8s.py
    │   │   ├── test_poster.py
    │   │   └── test_threadsafety.py
    │   ├── primitives/
    │   │   ├── test_conditions.py
    │   │   ├── test_containers.py
    │   │   ├── test_flags.py
    │   │   ├── test_toggles.py
    │   │   └── test_togglesets.py
    │   ├── reactor/
    │   │   ├── conftest.py
    │   │   ├── test_patching_inconsistencies.py
    │   │   ├── test_queueing.py
    │   │   └── test_uids.py
    │   ├── references/
    │   │   ├── test_backbone.py
    │   │   ├── test_namespace_matching.py
    │   │   ├── test_namespace_selection.py
    │   │   ├── test_selector_matching.py
    │   │   ├── test_selector_parsing.py
    │   │   └── test_selector_properties.py
    │   ├── registries/
    │   │   ├── conftest.py
    │   │   ├── test_creation.py
    │   │   ├── test_decorators.py
    │   │   ├── test_default_registry.py
    │   │   ├── test_handler_getting.py
    │   │   ├── test_id_detection.py
    │   │   ├── test_matching_for_changing.py
    │   │   ├── test_matching_for_indexing.py
    │   │   ├── test_matching_for_spawning.py
    │   │   ├── test_matching_for_watching.py
    │   │   ├── test_matching_of_callbacks.py
    │   │   ├── test_matching_of_resources.py
    │   │   ├── test_requires_finalizer.py
    │   │   ├── test_resumes_mixed_in.py
    │   │   └── test_subhandlers_ids.py
    │   ├── settings/
    │   │   ├── test_defaults.py
    │   │   └── test_executor.py
    │   ├── testing/
    │   │   └── test_runner.py
    │   ├── timing/
    │   │   ├── test_sleeping.py
    │   │   └── test_throttling.py
    │   └── utilities/
    │       └── aiotasks/
    │           ├── test_coro_cancellation.py
    │           ├── test_scheduler.py
    │           ├── test_task_guarding.py
    │           ├── test_task_selection.py
    │           ├── test_task_stopping.py
    │           └── test_task_waiting.py
    ├── tools/
    │   ├── install-kind.sh
    │   ├── install-kubectl.sh
    │   └── install-minikube.sh
    └── .github/
        ├── CODEOWNERS
        ├── FUNDING.yml
        ├── ISSUE_TEMPLATE.md
        ├── PULL_REQUEST_TEMPLATE.md
        ├── ISSUE_TEMPLATE/
        │   ├── bug-report.yaml
        │   ├── config.yaml
        │   ├── feature-request.yaml
        │   └── question.yaml
        └── workflows/
            ├── ci.yaml
            ├── codeql.yml
            ├── publish.yaml
            └── thorough.yaml

================================================
FILE: README.md
================================================
# Kubernetes Operator Pythonic Framework (Kopf)

[![GitHub](https://img.shields.io/github/stars/nolar/kopf?style=flat&label=GitHub%E2%AD%90%EF%B8%8F)](https://github.com/nolar/kopf)
[![CI](https://github.com/nolar/kopf/actions/workflows/thorough.yaml/badge.svg)](https://github.com/nolar/kopf/actions/workflows/thorough.yaml)
[![Supported Python versions](https://img.shields.io/pypi/pyversions/kopf.svg)](https://pypi.org/project/kopf/)
[![codecov](https://codecov.io/gh/nolar/kopf/branch/main/graph/badge.svg)](https://codecov.io/gh/nolar/kopf)
[![Coverage Status](https://coveralls.io/repos/github/nolar/kopf/badge.svg?branch=main)](https://coveralls.io/github/nolar/kopf?branch=main)

**Kopf** —Kubernetes Operator Pythonic Framework— is a framework and a library
to make Kubernetes operators development easier, just in a few lines of Python code.

The main goal is to bring the Domain-Driven Design to the infrastructure level,
with Kubernetes being an orchestrator/database of the domain objects (custom resources),
and the operators containing the domain logic (with no or minimal infrastructure logic).

The project was originally started as `zalando-incubator/kopf` in March 2019,
and then forked as `nolar/kopf` in August 2020: but it is the same codebase,
the same packages, the same developer(s).

As of now, the project is in maintenance mode since approximately mid-2021:
Python, Kubernetes, CI tooling, dependencies are upgraded, new bugs are fixed,
new versions are released from time to time, but no new big features are added
— there is nothing to add to this project without exploding its scope
beyond the "operator framework" definition (ideas are welcome!).


## Documentation

* https://kopf.readthedocs.io/


## Features

* Simple, but powerful:
  * A full-featured operator in just 2 files: a `Dockerfile` + a Python file (*).
  * Handling functions registered via decorators with a declarative approach.
  * No infrastructure boilerplate code with K8s API communication.
  * Both sync and async handlers, with sync ones being threaded under the hood.
  * Detailed documentation with examples.
* Intuitive mapping of Python concepts to Kubernetes concepts and back:
  * Marshalling of resources' data to the handlers' kwargs.
  * Marshalling of handlers' results to the resources' statuses.
  * Publishing of logging messages as Kubernetes events linked to the resources.
* Support anything that exists in K8s:
  * Custom K8s resources.
  * Builtin K8s resources (pods, namespaces, etc).
  * Multiple resource types in one operator.
  * Both cluster and namespaced operators.
* All the ways of handling that a developer can wish for:
  * Low-level handlers for events received from K8s APIs "as is" (an equivalent of _informers_).
  * High-level handlers for detected causes of changes (creation, updates with diffs, deletion).
  * Handling of selected fields only instead of the whole objects (if needed).
  * Dynamically generated or conditional sub-handlers (an advanced feature).
  * Timers that tick as long as the resource exists, optionally with a delay since the last change.
  * Daemons that run as long as the resource exists (in threads or asyncio-tasks).
  * Validating and mutating admission webhook (with dev-mode tunneling).
  * Live in-memory indexing of resources or their excerpts.
  * Filtering with stealth mode (no logging): by arbitrary filtering functions,
    by labels/annotations with values, presence/absence, or dynamic callbacks.
  * In-memory all-purpose containers to store non-serializable objects for individual resources.
* Eventual consistency of handling:
  * Retrying the handlers in case of arbitrary errors until they succeed.
  * Special exceptions to request a special retry or to never retry again.
  * Custom limits for the number of attempts or the time.
  * Implicit persistence of the progress that survives the operator restarts.
  * Tolerance to restarts and lengthy downtimes: handles the changes afterwards.
* Awareness of other Kopf-based operators:
  * Configurable identities for different Kopf-based operators for the same resource kinds.
  * Avoiding double-processing due to cross-pod awareness of the same operator ("peering").
  * Pausing of a deployed operator when a dev-mode operator runs outside of the cluster.
* Extra toolkits and integrations:
  * Some limited support for object hierarchies with name/labels propagation.
  * Friendly to any K8s client libraries (and is client agnostic).
  * Startup/cleanup operator-level handlers.
  * Liveness probing endpoints and rudimentary metrics exports.
  * Basic testing toolkit for in-memory per-test operator running.
  * Embeddable into other Python applications.
* Highly configurable (to some reasonable extent).

(*) _Small font: two files of the operator itself, plus some amount of
deployment files like RBAC roles, bindings, service accounts, network policies
— everything needed to deploy an application in your specific infrastructure._


## Examples

See [examples](https://github.com/nolar/kopf/tree/main/examples)
for the examples of the typical use-cases.

A minimalistic operator can look like this:

```python
import kopf

@kopf.on.create('kopfexamples')
def create_fn(spec, name, meta, status, **kwargs):
    print(f"And here we are! Created {name} with spec: {spec}")
```

Numerous kwargs are available, such as `body`, `meta`, `spec`, `status`,
`name`, `namespace`, `retry`, `diff`, `old`, `new`, `logger`, etc:
see [Arguments](https://kopf.readthedocs.io/en/latest/kwargs/)

To run a never-exiting function for every resource as long as it exists:

```python
import time
import kopf

@kopf.daemon('kopfexamples')
def my_daemon(spec, stopped, **kwargs):
    while not stopped:
        print(f"Object's spec: {spec}")
        time.sleep(1)
```

Or the same with the timers:

```python
import kopf

@kopf.timer('kopfexamples', interval=1)
def my_timer(spec, **kwargs):
    print(f"Object's spec: {spec}")
```

That easy! For more features, see the [documentation](https://kopf.readthedocs.io/).


## Usage

Python 3.9+ is required:
[CPython](https://www.python.org/) and [PyPy](https://www.pypy.org/)
are officially supported and tested; other Python implementations can work too.

We assume that when the operator is executed in the cluster, it must be packaged
into a docker image with a CI/CD tool of your preference.

```dockerfile
FROM python:3.13
ADD . /src
RUN pip install kopf
CMD kopf run /src/handlers.py --verbose
```

Where `handlers.py` is your Python script with the handlers
(see `examples/*/example.py` for the examples).

See `kopf run --help` for other ways of attaching the handlers.


## Contributing

Please read [CONTRIBUTING.md](https://github.com/nolar/kopf/blob/main/CONTRIBUTING.md)
for details on our process for submitting pull requests to us, and please ensure
you follow the [CODE_OF_CONDUCT.md](https://github.com/nolar/kopf/blob/main/CODE_OF_CONDUCT.md).

To install the environment for the local development,
read [DEVELOPMENT.md](https://github.com/nolar/kopf/blob/main/DEVELOPMENT.md).


## Versioning

We use [SemVer](http://semver.org/) for versioning. For the versions available,
see the [releases on this repository](https://github.com/nolar/kopf/releases).


## License

This project is licensed under the MIT License —
see the [LICENSE](https://github.com/nolar/kopf/blob/main/LICENSE) file for details.


## Acknowledgments

* Thanks to Zalando for starting this project in Zalando's Open-Source Incubator
  in the first place.
* Thanks to [@side8](https://github.com/side8) and their [k8s-operator](https://github.com/side8/k8s-operator)
  for inspiration.



================================================
FILE: _importlinter_conditional.py
================================================
"""
A contract for the import linter to secure 3rd-party clients importing.

Wrong::

    import kubernetes

Right::

    try:
        import kubernetes
    except ImportError:
        ...

https://import-linter.readthedocs.io/en/stable/custom_contract_types.html
"""
import os.path

import astpath
from grimp import ImportGraph
from importlinter import Contract, ContractCheck, fields, output


class ConditionalImportContract(Contract):
    """
    Contract that defines a single forbidden import between
    two modules.
    """
    source_modules = fields.ListField(subfield=fields.ModuleField())
    conditional_modules = fields.ListField(subfield=fields.ModuleField())

    def check(self, graph: ImportGraph, verbose: bool) -> ContractCheck:
        failed_details = []

        # Combine all source x all target (secured) modules.
        conditional_modules = [m for m in self.conditional_modules if m.name in graph.modules]
        for source_module in self.source_modules:
            for conditional_module in conditional_modules:

                # For every pair of source & target, find all import chains.
                chains = graph.find_shortest_chains(
                    importer=source_module.name,
                    imported=conditional_module.name,
                )
                for chain in chains:
                    # Of each chain, we only need the tail for our analysis.
                    # A sample chain: ('kopf.on', 'kopf._core.intents.registries', 'pykube')
                    importer, imported = chain[-2:]
                    details = graph.get_import_details(
                        importer=importer,
                        imported=imported
                    )

                    # For each import (possible several per file), get its line number and check it.
                    for detail in details:
                        ok = self._check_secured_import(detail['importer'], detail['line_number'])
                        if not ok:
                            failed_details.append(detail)

        return ContractCheck(
            kept=not failed_details,
            metadata={'failed_details': failed_details},
        )

    def render_broken_contract(self, check):
        for detail in check.metadata['failed_details']:
            importer = detail['importer']
            imported = detail['imported']
            line_number = detail['line_number']
            line_contents = detail['line_contents']
            output.print_error(
                f'{importer} is not allowed to import {imported} without try-except-ImportError:',
                bold=True,
            )
            output.new_line()
            output.indent_cursor()
            output.print_error(f'{importer}:{line_number}: {line_contents}')

    def _check_secured_import(self, mod: str, lno: int) -> bool:

        # Some hard-coded heuristics because importlib fails on circular imports.
        # TODO: switch to: importlib.util.find_spec(mod)?.origin
        path = os.path.join(os.path.dirname(__file__), mod.replace('.', '/')) + '.py'
        with open(path, encoding='utf-8') as f:
            text = f.read()
            xtree = astpath.file_contents_to_xml_ast(text)

        # For every "import" of interest, find any surrounding "try-except-ImportError" clauses.
        for node in xtree.xpath(f'''//Import[@lineno={lno!r}]'''):
            tries = node.xpath('''../parent::Try[//ExceptHandler/type/Name/@id="ImportError"]''')
            if not tries:
                return False
        return True



================================================
FILE: CODE_OF_CONDUCT.md
================================================
# Contributor Covenant Code of Conduct

## Our Pledge

In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.

## Our Standards

Examples of behavior that contributes to creating a positive environment include:

* Using welcoming and inclusive language
* Being respectful of differing viewpoints and experiences
* Gracefully accepting constructive criticism
* Focusing on what is best for the community
* Showing empathy towards other community members

Examples of unacceptable behavior by participants include:

* The use of sexualized language or imagery and unwelcome sexual attention or advances
* Trolling, insulting/derogatory comments, and personal or political attacks
* Public or private harassment
* Publishing others' private information, such as a physical or electronic address, without explicit permission
* Other conduct which could reasonably be considered inappropriate in a professional setting

## Our Responsibilities

Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.

Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.

## Scope

This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at nolar@nolar.info. The project team will review and investigate all complaints, and will respond in a way that it deems appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.

Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.

## Attribution

This Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4, available at [http://contributor-covenant.org/version/1/4][version]

[homepage]: http://contributor-covenant.org
[version]: http://contributor-covenant.org/version/1/4/



================================================
FILE: CONTRIBUTING.md
================================================
# Contributing to Kopf

**Thank you for your interest in Kopf. Your contributions are highly welcome.**

There are multiple ways of getting involved:

- [Report a bug](#report-a-bug)
- [Suggest a feature](#suggest-a-feature)
- [Contribute code](#contribute-code)

Below are a few guidelines we would like you to follow.
If you need help, please reach out to us by opening an issue.


## Report a bug

Reporting bugs is one of the best ways to contribute. Before creating a bug report, please check that an [issue](/issues) reporting the same problem does not already exist. If there is such an issue, you may add your information as a comment.

To report a new bug you should open an issue that summarizes the bug and set the label to "bug".

If you want to provide a fix along with your bug report: That is great! In this case please send us a pull request as described in section [Contribute Code](#contribute-code).


## Suggest a feature

To request a new feature you should open an [issue](../../issues/new) and summarize the desired functionality and its use case. Set the issue label to "feature".


## Contribute code

Check the list of open [issues](../../issues).
Either assign an existing issue to yourself, or create a new one
that you would like work on and discuss your ideas and use cases.
It is always best to discuss your plans beforehand,
to ensure that your contribution is in line with our goals.

Read https://kopf.readthedocs.io/en/stable/contributing/
for detailed information, conventions and guidelines.

Thanks for your contributions!

**Have fun, and happy hacking!**



================================================
FILE: CONTRIBUTORS.md
================================================
# Project Contributors

All external contributors to the project, we are grateful for all their help.

For the detailed information on who did what,
see [GitHub Contributors](https://github.com/nolar/kopf/graphs/contributors)
and [Historic GitHub Contributors](https://github.com/zalando-incubator/kopf/graphs/contributors).

## Contributors sorted alphabetically

- [Anthony Nash](https://github.com/nashant)
- [Daniel Middlecote](https://github.com/dlmiddlecote)
- [Henning Jacobs](https://github.com/hjacobs)
- [Clement Liaw](https://github.com/iexalt)
- [Ismail Kaboubi](https://github.com/smileisak)
- [Michael Narodovitch](https://github.com/michaelnarodovitch)
- [Rodrigo Tavares](https://github.com/tavaresrodrigo)
- [Sergey Vasilyev](https://github.com/nolar)
- [Soroosh Sarabadani](https://github.com/psycho-ir)
- [Trond Hindenes](https://github.com/trondhindenes)
- [Vennamaneni Sai Narasimha](https://github.com/thevennamaneni)
- [Cliff Burdick](https://github.com/cliffburdick)
- [CJ Baar](https://github.com/cjbaar)



================================================
FILE: DEVELOPMENT.md
================================================
# Bootstrap the development environment

## Minikube cluster

To develop the framework and the operators in an isolated Kubernetes cluster,
use [minikube](https://github.com/kubernetes/minikube):

MacOS:

```bash
brew install minikube
brew install hyperkit

minikube config set driver hyperkit
```

Start the minikube cluster:

```bash
minikube start
minikube dashboard
```

It will automatically create and activate the kubectl context named `minikube`.
If not, or if you have multiple clusters, activate it explicitly:

```bash
kubectl config get-contexts
kubectl config current-context
kubectl config use-context minikube
```


## Cluster setup

Apply the framework's peering resource definition (for neighbourhood awareness):

```bash
kubectl apply -f peering.yaml
```

Apply the custom resource definitions of your application
(here, we use an example application and resource):

```bash
kubectl apply -f examples/crd.yaml
```


## Runtime setup

Install the operator to your virtualenv in the editable mode
(and all its dependencies):

```bash
pip install -e .
kopf --help
```

Run the operator in the background console/terminal tab:

```bash
kopf run examples/01-minimal/example.py --verbose
```

Create and delete a sample object (just an example here).
Observe how the operator reacts and prints the logs,
and how the handling progress is reported on the object's events.

```bash
kubectl apply -f examples/obj.yaml
kubectl describe -f examples/obj.yaml
kubectl delete -f examples/obj.yaml
```

## PyCharm & IDEs

If you use PyCharm, create a Run/Debug Configuration as follows:

* Mode: `module name`
* Module name: `kopf`
* Arguments: `run examples/01-minimal/example.py --verbose`
* Python Interpreter: anything with Python>=3.9

Stop the console operator, and start the IDE debug session.
Put a breakpoint in the used operator script on the first line of the function.
Repeat the object creation, and ensure the IDE stops at the breakpoint.

Congratulations! You are ready to develop and debug your own operator.


## Real cluster

**WARNING:** Running the operator against a real cluster can influence
the real applications in the ways not expected by other team members.
The dev-mode operator's logs will not be visible in the central loggging,
as there are not sent there. Use the real clusters only when you have
the strong reasons to do so, such as the system resource requests
(CPU, RAM, PVC), which are not achievable in the minikube's VMs.

**WARNING:** Running multiple operators for the same cluster without isolation
can cause infinite loops, conflicting changes, and duplicated side effects
(such as the children object creation, e.g. jobs, pods, etc).
It is your responsibility to design the deployment in such a way that
the operators do not collide. The framework helps by providing the `--peering`
and `--namespace` CLI options, but does not prevent the mis-configurations.

To run against the real cluster, use the dev-mode of the framework.
This will set the operator's priority to 666 (just a high number),
and will pause all other running operators (the default priority is 0)
for the runtime, so that they do not collide with each other:

```bash
kopf run examples/01-minimal/example.py --verbose --dev
```

Alternatively, explicitly pause/resume all other operators,
and it will pause them even if your operator is not running
(e.g., for 2 hours):

```bash
kopf freeze --lifetime $((2*60*60))
kopf resume
```


## Cleanup

To cleanup the cluster from the framework-related objects:

```bash
kubectl delete -f peering.yaml
kubectl delete -f examples/obj.yaml
kubectl delete -f examples/crd.yaml
```

For the minikube cleanup (to release the CPU/RAM/disk resources):

```bash
minikube stop
minikube delete
```



================================================
FILE: LICENSE
================================================
MIT License

Copyright (c) 2020 Sergey Vasilyev <nolar@nolar.info>
Copyright (c) 2019-2020 Zalando SE

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.



================================================
FILE: MAINTAINERS
================================================
Sergey Vasilyev <nolar@nolar.info>



================================================
FILE: mypy.ini
================================================
[mypy]
warn_unused_configs = True
ignore_missing_imports = True



================================================
FILE: peering.yaml
================================================
---
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: clusterkopfpeerings.kopf.dev
spec:
  scope: Cluster
  group: kopf.dev
  names:
    kind: ClusterKopfPeering
    plural: clusterkopfpeerings
    singular: clusterkopfpeering
  versions:
    - name: v1
      served: true
      storage: true
      schema:
        openAPIV3Schema:
          type: object
          properties:
            status:
              type: object
              x-kubernetes-preserve-unknown-fields: true
---
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: kopfpeerings.kopf.dev
spec:
  scope: Namespaced
  group: kopf.dev
  names:
    kind: KopfPeering
    plural: kopfpeerings
    singular: kopfpeering
  versions:
    - name: v1
      served: true
      storage: true
      schema:
        openAPIV3Schema:
          type: object
          properties:
            status:
              type: object
              x-kubernetes-preserve-unknown-fields: true
---
apiVersion: kopf.dev/v1
kind: ClusterKopfPeering
metadata:
  name: default
---
apiVersion: kopf.dev/v1
kind: KopfPeering
metadata:
  namespace: default
  name: default
---



================================================
FILE: pytest.ini
================================================
[pytest]
asyncio_mode = auto
asyncio_default_fixture_loop_scope = function
addopts =
    --strict-markers



================================================
FILE: requirements.txt
================================================
# Everything needed to develop (test, debug) the framework.
# The runtime dependencies of the framework, as if via `pip install kopf`.
-e .
aresponses
astpath[xpath]
certbuilder
certvalidator
codecov
coverage
coveralls
freezegun

# Enforce the hotfix for Ubuntu 24.04 in CI. Otherwise, we are stuck in Ubuntu 22.04.
# The bugfix is merged but not released: https://github.com/wbond/oscrypto/issues/78
# Pinning this in the end operators is the decision of the operator developers,
# including the protocols of accessing the repo or vendoring the dependency code.
# The dev-mode dependency is used ONLY with an temporary & insecure self-signed CA for simplicity,
# and ONLY with Ubuntu 24.04+. Therefore, it is not pinned in setup.py (e.g., works fine in 22.04).
# In the worst case, configure the operator with a self-signed CA made in the OpenSSL CLI manually.
git+https://github.com/wbond/oscrypto.git@1547f535001ba568b239b8797465536759c742a3

import-linter
isort
lxml
mypy==1.15.0
pre-commit
pyngrok
pytest>=6.0.0
pytest-aiohttp
pytest-asyncio
pytest-cov
pytest-mock
pytest-timeout
types-PyYAML
types-setuptools



================================================
FILE: SECURITY.md
================================================
We acknowledge that every line of code that we write may potentially contain security issues.
We are trying to deal with it responsibly and provide patches as quickly as possible.
In case you detect any security issues, contact nolar@nolar.info.



================================================
FILE: setup.py
================================================
import os.path

from setuptools import find_packages, setup

with open(os.path.join(os.path.dirname(__file__), 'README.md')) as f:
    LONG_DESCRIPTION = f.read()
    DESCRIPTION = LONG_DESCRIPTION.splitlines()[0].lstrip('#').strip()

PROJECT_URLS = {
    'Documentation': 'https://kopf.readthedocs.io',
    'Bug Tracker': 'https://github.com/nolar/kopf/issues',
    'Source Code': 'https://github.com/nolar/kopf',
}

setup(
    name='kopf',
    use_scm_version=True,

    url=PROJECT_URLS['Source Code'],
    project_urls=PROJECT_URLS,
    description=DESCRIPTION,
    long_description=LONG_DESCRIPTION,
    long_description_content_type='text/markdown',
    author='Sergey Vasilyev',
    author_email='nolar@nolar.info',
    maintainer='Sergey Vasilyev',
    maintainer_email='nolar@nolar.info',
    keywords=['kubernetes', 'operator', 'framework', 'python', 'k8s'],
    license='MIT',
    classifiers = [
        'Intended Audience :: Developers',
        'License :: OSI Approved :: MIT License',
        'Operating System :: OS Independent',
        'Programming Language :: Python',
        'Programming Language :: Python :: 3',
        'Programming Language :: Python :: 3.9',
        'Programming Language :: Python :: 3.10',
        'Programming Language :: Python :: 3.11',
        'Programming Language :: Python :: 3.12',
        'Programming Language :: Python :: 3.13',
        'Programming Language :: Python :: 3 :: Only',
        'Programming Language :: Python :: Implementation :: CPython',
        'Programming Language :: Python :: Implementation :: PyPy',
        'Topic :: Software Development :: Libraries',
    ],

    zip_safe=True,
    packages=find_packages(),
    include_package_data=True,
    entry_points={
        'console_scripts': [
            'kopf = kopf.cli:main',
        ],
    },

    python_requires='>=3.9',
    setup_requires=[
        'setuptools_scm',
    ],
    install_requires=[
        'python-json-logger',   # 0.05 MB
        'iso8601',              # 0.07 MB
        'click',                # 0.60 MB
        'aiohttp',              # 7.80 MB
        'aiohttp>=3.9.0; python_version>="3.12"',
        'pyyaml',               # 0.90 MB
    ],
    extras_require={
        'full-auth': [
            'pykube-ng',        # 4.90 MB
            'kubernetes',       # 40.0 MB (!)
        ],
        'uvloop': [
            'uvloop',           # 9.00 MB
            'uvloop>=0.18.0; python_version>="3.12"',
        ],
        'dev': [
            # NB: oscrypto is pinned for Ubuntu 24.04+ in requirements.txt - read the details there.
            'oscrypto',         # 2.80 MB (smaller than cryptography: 8.7 MB)
            'certbuilder',      # +0.1 MB (2.90 MB if alone)
            'certvalidator',    # +0.1 MB (2.90 MB if alone)
            'pyngrok',          # 1.00 MB + downloaded binary
        ],
    },
    package_data={"kopf": ["py.typed"]},
)



================================================
FILE: .codecov.yml
================================================
comment: off



================================================
FILE: .importlinter
================================================
; Importing constraints for layered layout of modules and packages.
; The goal is higher cohesion and lower coupling of components.
; https://import-linter.readthedocs.io/en/stable/contract_types.html
[importlinter]
root_package = kopf
include_external_packages = True
contract_types =
    conditional: _importlinter_conditional.ConditionalImportContract

[importlinter:contract:root-layers]
name = The root framework modules must be layered
type = layers
layers =
    kopf.on
    kopf._kits
    kopf._core
    kopf._cogs

[importlinter:contract:core-layers]
name = The internal core must be layered
type = layers
layers =
    kopf._core.reactor
    kopf._core.engines
    kopf._core.intents
    kopf._core.actions

[importlinter:contract:cogs-layers]
name = The internal cogs must be layered
type = layers
layers =
    kopf._cogs.clients
    kopf._cogs.configs
    kopf._cogs.structs
    kopf._cogs.aiokits
    kopf._cogs.helpers

[importlinter:contract:progress-storage]
name = Progress storages must be persistence settings
type = layers
layers =
    kopf._cogs.configs.configuration
    kopf._cogs.configs.progress
    kopf._cogs.configs.conventions

[importlinter:contract:diffbase-storage]
name = Diffbase storages must be persistence settings
type = layers
layers =
    kopf._cogs.configs.configuration
    kopf._cogs.configs.diffbase
    kopf._cogs.configs.conventions

[importlinter:contract:independent-storages]
name = Storage types must be unaware of each other
type = independence
modules =
    kopf._cogs.configs.diffbase
    kopf._cogs.configs.progress

[importlinter:contract:independent-aiokits]
name = Most asyncio kits must be unaware of each other
type = independence
modules =
    kopf._cogs.aiokits.aioadapters
    kopf._cogs.aiokits.aiobindings
    kopf._cogs.aiokits.aioenums
    kopf._cogs.aiokits.aiotoggles
    kopf._cogs.aiokits.aiovalues
    ; but not aiotasks & aiotime!

[importlinter:contract:ban-toolkits]
name = The internals must be unaware of user-facing toolkits
type = forbidden
source_modules =
    kopf._cogs
    kopf._core
forbidden_modules =
    kopf._kits

[importlinter:contract:indenpendent-toolkits]
name = The user-facing toolkits must be unaware of each other
type = independence
modules =
    kopf._kits.hierarchies
    kopf._kits.runner
    kopf._kits.webhooks

[importlinter:contract:allow-3rd-party]
name = 3rd-party clients must be explicitly allowed
type = forbidden
source_modules =
    kopf
forbidden_modules =
    pykube
    kubernetes
ignore_imports =
    kopf._core.intents.piggybacking -> pykube
    kopf._core.intents.piggybacking -> kubernetes
    kopf._cogs.helpers.thirdparty -> pykube
    kopf._cogs.helpers.thirdparty -> kubernetes

[importlinter:contract:secure-3rd-party]
name = 3rd-party clients must be secured by conditional imports
type = conditional
source_modules =
    kopf
conditional_modules =
    pykube
    kubernetes



================================================
FILE: .isort.cfg
================================================
[settings]
line_length = 100
multi_line_output = 11
balanced_wrapping = true
combine_as_imports = true
case_sensitive = true

known_first_party = kopf

filter_files = true
skip_glob = examples/*.py



================================================
FILE: .pre-commit-config.yaml
================================================
# See https://pre-commit.com for more information
# See https://pre-commit.com/hooks.html for more hooks
exclude: |
  (?x)^(
    docs/.*\.xml|
    docs/.*\.png
  )
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v5.0.0
    hooks:
      - id: check-ast
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: fix-byte-order-marker
      - id: check-xml
      - id: check-toml
      - id: check-yaml
        args: [--allow-multiple-documents]
      - id: check-json
      - id: pretty-format-json
      - id: check-added-large-files
      - id: check-builtin-literals
      - id: check-case-conflict
      - id: check-executables-have-shebangs
      - id: check-vcs-permalinks
      - id: check-docstring-first
      - id: check-merge-conflict
      - id: check-symlinks
      - id: destroyed-symlinks
      - id: debug-statements
      - id: detect-aws-credentials
        args: [--allow-missing-credentials]
      - id: detect-private-key
        exclude: ^tests/authentication/test_credentials.py$
      - id: fix-encoding-pragma
        args: [--remove]
      - id: forbid-new-submodules
      - id: mixed-line-ending
        args: [--fix=auto]
      - id: name-tests-test
        args: [--django]
      - id: requirements-txt-fixer

      # Intentionally disabled:
      # - id: double-quote-string-fixer

  - repo: https://github.com/pre-commit/pygrep-hooks
    rev: v1.10.0
    hooks:
      - id: python-check-blanket-noqa
      - id: python-check-mock-methods
      - id: python-no-eval
      - id: python-use-type-annotations
      - id: rst-backticks
      - id: rst-directive-colons
      - id: rst-inline-touching-normal
      - id: text-unicode-replacement-char

      # Intentionally disabled:
      # - id: python-no-log-warn  # overreacts to `kopf.warn()`.

  - repo: https://github.com/seddonym/import-linter
    rev: v2.2
    hooks:
      - id: import-linter
        additional_dependencies:
          - astpath[xpath]

  - repo: https://github.com/PyCQA/isort
    rev: 6.0.1
    hooks:
      - id: isort
        name: isort-source-code

  - repo: https://github.com/PyCQA/isort
    rev: 6.0.1
    hooks:
      - id: isort
        name: isort-examples
        args: [--settings=examples]
        files: '^examples/'

  - repo: https://github.com/asottile/pyupgrade
    rev: v3.19.1
    hooks:
    -   id: pyupgrade
        args: [--py39-plus, --keep-mock]



================================================
FILE: .readthedocs.yaml
================================================
# Read the Docs configuration file
# See https://docs.readthedocs.io/en/stable/config-file/v2.html for details
version: 2
formats: all
build:
  os: ubuntu-24.04
  tools:
    python: "3"
python:
  install:
    - requirements: requirements.txt
    - requirements: docs/requirements.txt
sphinx:
  configuration: docs/conf.py
  builder: "dirhtml"
#  fail_on_warning: true



================================================
FILE: docs/admission.rst
================================================
=================
Admission control
=================

Admission hooks are callbacks from Kubernetes to the operator before
the resources are created or modified. There are two types of hooks:

* Validating admission webhooks.
* Mutating admission webhooks.

For more information on the admission webhooks,
see the Kubernetes documentation: `Dynamic Admission Control`__.

__ https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/


Dependencies
============

To minimize Kopf's footprint in production systems,
it does not include heavy-weight dependencies needed only for development,
such as SSL cryptography and certificate generation libraries.
For example, Kopf's footprint with critical dependencies is 8.8 MB,
while ``cryptography`` would add 8.7 MB; ``certbuilder`` adds "only" 2.9 MB.

To use all features of development-mode admission webhook servers and tunnels,
you have to install Kopf with an extra:

.. code-block:: bash

    pip install kopf[dev]

If this extra is not installed, Kopf will not generate self-signed certificates
and will run either with HTTP only or with externally provided certificates.

Also, without this extra, Kopf will not be able to establish Ngrok tunnels.
Though, it will be able to use K3d & Minikube servers with magic hostnames.

Any attempt to run it in a mode with self-signed certificates or tunnels
will raise a startup-time error with an explanation and suggested actions.


Validation handlers
===================

.. code-block:: python

    import kopf

    @kopf.on.validate('kopfexamples')
    def say_hello(warnings: list[str], **_):
        warnings.append("Verified with the operator's hook.")

    @kopf.on.validate('kopfexamples')
    def check_numbers(spec, **_):
        if not isinstance(spec.get('numbers', []), list):
            raise kopf.AdmissionError("Numbers must be a list if present.")

    @kopf.on.validate('kopfexamples')
    def convertible_numbers(spec, warnings, **_):
        if isinstance(spec.get('numbers', []), list):
            for val in spec.get('numbers', []):
                if not isinstance(val, float):
                    try:
                        float(val)
                    except ValueError:
                        raise kopf.AdmissionError(f"Cannot convert {val!r} to a number.")
                    else:
                        warnings.append(f"{val!r} is not a number but can be converted.")

    @kopf.on.validate('kopfexamples')
    def numbers_range(spec, **_):
        if isinstance(spec.get('numbers', []), list):
            if not all(0 <= float(val) <= 100 for val in spec.get('numbers', [])):
                raise kopf.AdmissionError("Numbers must be below 0..100.", code=499)

Each handler is mapped to its dedicated admission webhook and an endpoint
so that all handlers are executed in parallel independently of each other.
They must not expect that other checks are already performed by other handlers;
if such logic is needed, make it as one handler with a sequential execution.


Mutation handlers
=================

To mutate the object, modify the :kwarg:`patch`. Changes to :kwarg:`body`,
:kwarg:`spec`, etc, will not be remembered (and are not possible):

.. code-block:: python

    import kopf

    @kopf.on.mutate('kopfexamples')
    def ensure_default_numbers(spec, patch, **_):
        if 'numbers' not in spec:
            patch.spec['numbers'] = [1, 2, 3]

    @kopf.on.mutate('kopfexamples')
    def convert_numbers_if_possible(spec, patch, **_):
        if 'numbers' in spec and isinstance(spec.get('numbers'), list):
            patch.spec['numbers'] = [_maybe_number(v) for v in spec['numbers']]

    def _maybe_number(v):
        try:
            return float(v)
        except ValueError:
            return v

The semantics is the same or as close as possible to the Kubernetes API's one.
``None`` values will remove the relevant keys.

Under the hood, the patch object will remember each change
and will return a JSONPatch structure to Kubernetes.


Handler options
===============

Handlers have a limited capability to inform Kubernetes about its behaviour.
The following options are supported:

``persistent`` (``bool``) webhooks will not be removed from the managed
configurations on exit; non-persisted webhooks will be removed if possible.
Such webhooks will prevent all admissions even when the operator is down.
This option has no effect if there is no managed configuration.
The webhook cleanup only happens on graceful exits; on forced exits, even
non-persisted webhooks might be persisted and block the admissions.

``operation`` (``str``) will configure this handler/webhook to be called only
for a specific operation. For multiple operations, add several decorators.
Possible values are ``"CREATE"``, ``"UPDATE"``, ``"DELETE"``, ``"CONNECT"``.
The default is ``None``, i.e. all operations (equivalent to ``"*"``).

``subresource`` (``str``) will only react when to the specified subresource.
Usually it is ``"status"`` or ``"scale"``, but can be anything else.
The value ``None`` means that only the main resource body will be checked.
The value ``"*"`` means that both the main body and any subresource are checked.
The default is ``None``, i.e. only the main body to be checked.

``side_effects`` (``bool``) tells Kubernetes that the handler can have side
effects in non-dry-run mode. In dry-run mode, it must have no side effects.
The dry-run mode is passed to the handler as a :kwarg:`dryrun` kwarg.
The default is ``False``, i.e. the handler has no side effects.

``ignore_failures`` (``bool``) marks the webhook as tolerant to errors.
This includes errors of the handler itself (disproved admissions),
so as HTTP/TCP communication errors when apiservers talk to the webhook server.
By default, an inaccessible or rejecting webhook blocks the admission.

The developers can use regular :doc:`/filters`. In particular, the ``labels``
will be passed to the webhook configuration as ``.webhooks.*.objectSelector``
for optimization purposes: so that admissions are not even sent to the webhook
server if it is known that they will be filtered out and therefore allowed.

Server-side filtering supports everything except callbacks:
i.e., ``"strings"``, ``kopf.PRESENT`` and ``kopf.ABSENT`` markers.
The callbacks will be evaluated after the admission review request is received.

.. warning::

    Be careful with the builtin resources and admission hooks.
    If a handler is broken or misconfigured, it can prevent creating
    those resources, e.g. pods, in the whole cluster. This will render
    the cluster unusable until the configuration is manually removed.

    Start the development in local clusters, validating/mutating the custom
    resources first, and enable ``ignore_errors`` initially.
    Enable the strict mode of the handlers only when stabilised.


In-memory containers
====================

Kopf provides :doc:`/memos` for each resource. However, webhooks can happen
before a resource is created. This affects how the memos work.

For update and deletion requests, the actual memos of the resources are used.

For the admission requests on resource creation, a memo is created and discarded
immediately. It means that the creation's memos are useless at the moment.

This can change in the future: the memos of resource creation attempts
will be preserved for a limited but short time (configurable),
so that the values could be shared between the admission and the handling, but
so that there are no memory leaks if the resource never succeeds in admission.


Admission warnings
==================

Starting with Kubernetes 1.19 (check with ``kubectl version``),
admission warnings can be returned from admission handlers.

To populate warnings, accept a **mutable** :kwarg:`warnings` (``list[str]``)
and add strings to it:

.. code-block:: python

    import kopf

    @kopf.on.validate('kopfexamples')
    def ensure_default_numbers(spec, warnings: list[str], **_):
        if spec.get('field') == 'value':
            warnings.append("The default value is used. It is okay but worth changing.")

The admission warnings look like this (requires kubectl 1.19+):

.. code-block:: none

    $ kubectl create -f examples/obj.yaml
    Warning: The default value is used. It is okay but worth changing.
    kopfexample.kopf.dev/kopf-example-1 created

.. note::

    Despite Kopf's intention to utilise Python's native features that
    semantically map to Kubernetes's or operators' features,
    Python StdLib's :mod:`warnings` is not used for admission warnings
    (the initial idea was to catch `UserWarning` and ``warnings.warn("...")``
    calls and return them as admission warnings).

    The StdLib's module is documented as thread-unsafe (therefore, task-unsafe)
    and requires hacking the global state which might affect other threads
    and/or tasks -- there is no clear way to do this consistently.

    This may be revised in the future and provided as an additional feature.


Admission errors
================

Unlike with regular handlers and their error handling logic (:doc:`/errors`),
the webhooks cannot do retries or backoffs. So, the ``backoff=``, ``errors=``,
``retries=``, ``timeout=`` options are not accepted on the admission handlers.

A special exception :class:`kopf.AdmissionError` is provided to customize
the status code and the message of the admission review response.

All other exceptions,
including :class:`kopf.PermanentError` and :class:`kopf.TemporaryError`,
equally fail the admission (be that validating or mutating admission).
However, they return the general HTTP code 500 (non-customisable).

One and only one error is returned to the user who make an API request.
In cases when Kubernetes makes several parallel requests to several webhooks
(typically with managed webhook configurations, the fastest error is used).
Within Kopf (usually with custom webhook servers/tunnels or self-made
non-managed webhook configurations), errors are prioritised: first, admission
errors, then permanent errors, then temporary errors, then arbitrary errors
are used to select the only error to report in the admission review response.

.. code-block:: python

    @kopf.on.validate('kopfexamples')
    def validate1(spec, **_):
        if spec.get('field') == 'value':
            raise kopf.AdmissionError("Meh! I don't like it. Change the field.", code=400)

The admission errors look like this (manually indented for readability):

.. code-block:: none

    $ kubectl create -f examples/obj.yaml
    Error from server: error when creating "examples/obj.yaml":
        admission webhook "validate1.auto.kopf.dev" denied the request:
            Meh! I don't like it. Change the field.

Note that Kubernetes executes multiple webhooks in parallel.
The first one to return the result is the one and the only shown;
other webhooks are not shown even if they fail with useful messages.
With multiple failing admissions, the message will be varying on each attempt.


Webhook management
==================

Admission (both for validation and for mutation) only works when the cluster
has special resources created: either ``kind: ValidatingWebhookConfiguration``
or ``kind: MutatingWebhookConfiguration`` or both.
Kopf can automatically manage the webhook configuration resources
in the cluster if it is given RBAC permissions to do so.

To manage the validating/mutating webhook configurations, Kopf requires
the following RBAC permissions in its service account (see :doc:`/deployment`):

.. code-block:: yaml

    apiVersion: rbac.authorization.k8s.io/v1beta1
    kind: ClusterRole
    rules:
      - apiGroups: [admissionregistration.k8s.io/v1, admissionregistration.k8s.io/v1beta1]
        resources: [validatingwebhookconfigurations, mutatingwebhookconfigurations]
        verbs: [create, patch]

By default, configuration management is disabled (for safety and stability).
To enable, set the name of the managed configuration objects:

.. code-block:: python

    @kopf.on.startup()
    def configure(settings: kopf.OperatorSettings, **_):
        settings.admission.managed = 'auto.kopf.dev'

Multiple records for webhooks will be added or removed for multiple resources
to those configuration objects as needed. Existing records will be overwritten.
If the configuration resource is absent, it will be created
(but at most one for validating and one for mutating configurations).

Kopf manages the webhook configurations according to how Kopf itself believes
it is sufficient to achieve the goal. Many available Kubernetes features
are not covered by this management. To use these features and control
the configuration with precision, operator developers can disable
the automated management and take care of the configuration manually.


Servers and tunnels
===================

Kubernetes admission webhooks are designed to be passive rather than active
(from the operator's point of view; vice versa from Kubernetes's point of view).
It means, the webhooks must passively wait for requests via an HTTPS endpoint.
There is currently no official way how an operator can actively pull or poll
the admission requests and send the responses back
(as it is done for all other resource changes streamed via the Kubernetes API).

It is typically non-trivial to forward the requests from a remote or isolated
cluster to a local host machine where the operator is running for development.

However, one of Kopf's main promises is to work the same way both in-cluster
and on the developers' machines. It cannot be made "the same way" for webhooks,
but Kopf attempts to make these modes similar to each other code-wise.

To fulfil its promise, Kopf delegates this task to webhook servers and tunnels,
which are capable of receiving the webhook requests, marshalling them
to the handler callbacks, and then returning the results to Kubernetes.

Due to numerous ways of how the development and production environments can be
configured, Kopf does not provide a default configuration for a webhook server,
so it must be set by the developer:

.. code-block:: python

    @kopf.on.startup()
    def configure(settings: kopf.OperatorSettings, **_):
        if os.environ.get('ENVIRONMENT') is None:
            # Only as an example:
            settings.admission.server = kopf.WebhookK3dServer(port=54321)
            settings.admission.managed = 'auto.kopf.dev'
        else:
            # Assuming that the configuration is done manually:
            settings.admission.server = kopf.WebhookServer(addr='0.0.0.0', port=8080)
            settings.admission.managed = 'auto.kopf.dev'

If there are admission handlers present and no webhook server/tunnel configured,
the operator will fail at startup with an explanatory message.

Kopf provides several webhook servers and tunnels out of the box,
each with its configuration parameters (see their descriptions):

*Webhook servers* listen on an HTTPS port locally and handle requests.

* :class:`kopf.WebhookServer` is helpful for local development and ``curl`` and
  a Kubernetes cluster that runs directly on the host machine and can access it.
  It is also used internally by most tunnels for a local target endpoint.
* :class:`kopf.WebhookK3dServer` is for local K3d/K3s clusters (even in a VM),
  accessing the server via a magical hostname ``host.k3d.internal``.
* :class:`kopf.WebhookMinikubeServer` for local Minikube clusters (even in VMs),
  accessing the server via a magical hostname ``host.minikube.internal``.
* :class:`kopf.WebhookDockerDesktopServer` for the DockerDesktop cluster,
  accessing the server via a magical hostname ``host.docker.internal``.

*Webhook tunnels* forward the webhook requests through external endpoints
usually to a locally running *webhook server*.

* :class:`kopf.WebhookNgrokTunnel` established a tunnel through ngrok_.

.. _ngrok: https://ngrok.com/

For ease of use, the cluster type can be recognised automatically in some cases:

* :class:`kopf.WebhookAutoServer` runs locally, detects Minikube & K3s, and
  uses them via their special hostnames. If it cannot detect the cluster type,
  it runs a simple local webhook server. The auto-server never tunnels.
* :class:`kopf.WebhookAutoTunnel` attempts to use an auto-server if possible.
  If not, it uses one of the available tunnels (currently, only ngrok).
  This is the most universal way to make any environment work.

.. note::
    External tunnelling services usually limit the number of requests.
    For example, ngrok has a limit of 40 requests per minute on a free plan.

    The services also usually provide paid subscriptions to overcome that limit.
    It might be a wise idea to support the service you rely on with some money.
    If that is not an option, you can implement free tunnelling your way.

.. note::
    A reminder: using development-mode tunnels and self-signed certificates
    requires extra dependencies: ``pip install kopf[dev]``.


Authenticate apiservers
=======================

There are many ways how webhook clients (Kubernetes's apiservers)
can authenticate against webhook servers (the operator's webhooks),
and even more ways to validate the supplied credentials.

More on that, apiservers cannot be configured to authenticate against
webhooks dynamically at runtime, as `this requires control-plane configs`__,
which are out of reach of Kopf.

__ https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#authenticate-apiservers

For simplicity, Kopf does not authenticate webhook clients.

However, Kopf's built-in webhook servers & tunnels extract the very basic
request information and pass it to the admission handlers
for additional verification and possibly for authentification:

* :kwarg:`headers` (``Mapping[str, str]``) contains all HTTPS headers,
  including ``Authorization: Basic ...``, ``Authorization: Bearer ...``.
* :kwarg:`sslpeer` (``Mapping[str, Any]``) contains the SSL peer information
  as returned by :func:`ssl.SSLSocket.getpeercert` or ``None`` if no proper SSL
  certificate is provided by a client (i.e. by apiservers talking to webhooks).

An example of headers:

.. code-block:: python

    {'Host': 'localhost:54321',
     'Authorization': 'Basic dXNzc2VyOnBhc3Nzdw==',  # base64("ussser:passsw")
     'Content-Length': '844',
     'Content-Type': 'application/x-www-form-urlencoded'}

An example of a self-signed peer certificate presented to ``sslpeer``:

.. code-block:: python

    {'subject': ((('commonName', 'Example Common Name'),),
                 (('emailAddress', 'example@kopf.dev'),)),
     'issuer': ((('commonName', 'Example Common Name'),),
                (('emailAddress', 'example@kopf.dev'),)),
     'version': 1,
     'serialNumber': 'F01984716829537E',
     'notBefore': 'Mar  7 17:12:20 2021 GMT',
     'notAfter': 'Mar  7 17:12:20 2022 GMT'}

To reproduce these examples without configuring the Kubernetes apiservers
but only Kopf & CLI tools, do the following:

Step 1: Generate a self-signed ceritificate to be used as a client certificate:

.. code-block:: bash

    openssl req -x509 -newkey rsa:2048 -keyout client-key.pem -out client-cert.pem -days 365 -nodes
    # Country Name (2 letter code) []:
    # State or Province Name (full name) []:
    # Locality Name (eg, city) []:
    # Organization Name (eg, company) []:
    # Organizational Unit Name (eg, section) []:
    # Common Name (eg, fully qualified host name) []:Example Common Name
    # Email Address []:example@kopf.dev

Step 2: Start an operator with the certificate as a CA (for simplicity;
in normal setups, there is a separate CA, which signs the client certificates;
explaining this topic is beyond the scope of this framework's documentation):

.. code-block:: python

    import kopf

    @kopf.on.startup()
    def config(settings: kopf.OperatorSettings, **_):
        settings.admission.managed = 'auto.kopf.dev'
        settings.admission.server = kopf.WebhookServer(cafile='client-cert.pem')

    @kopf.on.validate('kex')
    def show_auth(headers, sslpeer, **_):
        print(f'{headers=}')
        print(f'{sslpeer=}')

Step 3: Save the admission review payload into a local file:

.. code-block:: bash

    cat >review.json << EOF
    {
      "kind": "AdmissionReview",
      "apiVersion": "admission.k8s.io/v1",
      "request": {
        "uid": "1ca13837-ad60-4c9e-abb8-86f29d6c0e84",
        "kind": {"group": "kopf.dev", "version": "v1", "kind": "KopfExample"},
        "resource": {"group": "kopf.dev", "version": "v1", "resource": "kopfexamples"},
        "requestKind": {"group": "kopf.dev", "version": "v1", "kind": "KopfExample"},
        "requestResource": {"group": "kopf.dev", "version": "v1", "resource": "kopfexamples"},
        "name": "kopf-example-1",
        "namespace": "default",
        "operation": "CREATE",
        "userInfo": {"username": "admin", "uid": "admin", "groups": ["system:masters", "system:authenticated"]},
        "object": {
          "apiVersion": "kopf.dev/v1",
          "kind": "KopfExample",
          "metadata": {"name": "kopf-example-1", "namespace": "default"}
        },
        "oldObject": null,
        "dryRun": true
      }
    }
    EOF

Step 4: Send the admission review payload to the operator's webhook server
using the generated client certificate, observe the client identity printed
to stdout by the webhook server and returned in the warnings:

.. code-block:: bash

    curl --insecure --cert client-cert.pem --key client-key.pem https://ussser:passsw@localhost:54321 -d @review.json
    # {"apiVersion": "admission.k8s.io/v1", "kind": "AdmissionReview",
    #  "response": {"uid": "1ca13837-ad60-4c9e-abb8-86f29d6c0e84",
    #               "allowed": true,
    #               "warnings": ["SSL peer is Example Common Name."]}}

Using this data, operator developers can implement servers/tunnels
with custom authentication methods when and if needed.


Debugging with SSL
==================

Kubernetes requires that the webhook URLs are always HTTPS, never HTTP.
For this reason, Kopf runs the webhook servers/tunnels with HTTPS by default.

If a webhook server is configured without a server certificate,
a self-signed certificate is generated at startup, and only HTTPS is served.

.. code-block:: python

    @kopf.on.startup()
    def config(settings: kopf.OperatorSettings, **_):
        settings.admission.server = kopf.WebhookServer()

That endpoint can be accessed directly with ``curl``:

.. code-block:: bash

    curl --insecure https://localhost:54321 -d @review.json

It is possible to store the generated certificate itself and use as a CA:

.. code-block:: python

    @kopf.on.startup()
    def config(settings: kopf.OperatorSettings, **_):
        settings.admission.server = kopf.WebhookServer(cadump='selfsigned.pem')

.. code-block:: bash

    curl --cacert selfsigned.pem https://localhost:54321 -d @review.json

For production, a properly generated certificate should be used.
The CA, if not specified, is assumed to be in the default trust chain.
This applies to all servers:
:class:`kopf.WebhookServer`, :class:`kopf.WebhookK3dServer`, etc.

.. code-block:: python

    @kopf.on.startup()
    def config(settings: kopf.OperatorSettings, **_):
        settings.admission.server = kopf.WebhookServer(
            cafile='ca.pem',        # or cadata, or capath.
            certfile='cert.pem',
            pkeyfile='pkey.pem',
            password='...')         # for the private key, if used.

.. note::
    ``cadump`` (output) can be used together with ``cafile``/``cadata`` (input),
    though it will be the exact copy of the CA and does not add any benefit.

As a last resort, if SSL is still a problem, it can be disabled and an insecure
HTTP server can be used. This does not work with Kubernetes but can be used
for direct access during development; it is also used by some tunnels that
do not support HTTPS tunnelling (or require paid subscriptions):

.. code-block:: python

    @kopf.on.startup()
    def config(settings: kopf.OperatorSettings, **_):
        settings.admission.server = kopf.WebhookServer(insecure=True)


Custom servers/tunnels
======================

Operator developers can provide their custom servers and tunnels by implementing
an async iterator over client configs (`kopf.WebhookClientConfig`).
There are two ways to implement servers/tunnels.

One is a simple but non-configurable coroutine:

.. code-block:: python

    async def mytunnel(fn: kopf.WebhookFn) -> AsyncIterator[kopf.WebhookClientConfig]:
        ...
        yield client_config
        await asyncio.Event().wait()

    @kopf.on.startup()
    def configure(settings: kopf.OperatorSettings, **_):
        settings.admission.server = mytunnel  # no arguments!

Another one is a slightly more complex but configurable class:

.. code-block:: python

    class MyTunnel:
        async def __call__(self, fn: kopf.WebhookFn) -> AsyncIterator[kopf.WebhookClientConfig]:
            ...
            yield client_config
            await asyncio.Event().wait()

    @kopf.on.startup()
    def configure(settings: kopf.OperatorSettings, **_):
        settings.admission.server = MyTunnel()  # arguments are possible.

The iterator MUST accept a positional argument of type :class:`kopf.WebhookFn`
and call it with the JSON-parsed payload when a review request is received;
then, it MUST ``await`` the result and JSON-serialize it as a review response:

.. code-block:: python

    response = await fn(request)

Optionally (though highly recommended), several keyword arguments can be passed
to extend the request data (if not passed, they all use ``None`` by default):

* ``webhook`` (``str``) -- to execute only one specific handler/webhook.
  The id usually comes from the URL, which the framework injects automatically.
  It is highly recommended to provide at least this hint:
  otherwise, all admission handlers are executed, with mutating and validating
  handlers mixed, which can lead to mutating patches returned for validation
  requests, which in turn will fail the admission on the Kubernetes side.
* ``headers`` (``Mapping[str, str]``) -- the HTTPS headers of the request
  are passed to handlers as :kwarg:`headers` and can be used for authentication.
* ``sslpeer`` (``Mapping[str, Any]``) -- the SSL peer information taken from
  the client certificate (if provided and if verified); it is passed
  to handlers as :kwarg:`sslpeer` and can be used for authentication.

.. code-block:: python

    response = await fn(request, webhook=handler_id, headers=headers, sslpeer=sslpeer)

There is no guarantee on what is happening in the callback and how it works.
The exact implementation can be changed in the future without warning: e.g.,
the framework can either invoke the admission handlers directly in the callback
or queue the request for a background execution and return an awaitable future.

The iterator must yield one or more client configs. Configs are dictionaries
that go to the managed webhook configurations as ``.webhooks.*.clientConfig``.

Regardless of how the client config is created, the framework extends the URLs
in the ``url`` and ``service.path`` fields with the handler/webhook ids,
so that a URL ``https://myhost/path`` becomes ``https://myhost/path/handler1``,
``https://myhost/path/handler2``, so on.

Remember: Kubernetes prohibits using query parameters and fragments in the URLs.

In most cases, only one yielded config is enough if the server is going
to serve the requests at the same endpoint.
In rare cases when the endpoint changes over time (e.g. for dynamic tunnels),
the server/tunnel should yield a new config every time the endpoint changes,
and the webhook manager will reconfigure all managed webhooks accordingly.

The server/tunnel must hold control by running the server or by sleeping.
To sleep forever, use ``await asyncio.Event().wait()``. If the server/tunnel
exits unexpectedly, this causes the whole operator to exit.

If the goal is to implement a tunnel only, but not a custom webhook server,
it is highly advised to inherit from or directly use :class:`kopf.WebhookServer`
to run a locally listening endpoint. This server implements all URL parsing
and request handling logic well-aligned with the rest of the framework:

.. code-block:: python

    # Inheritance:
    class MyTunnel1(kopf.WebhookServer):
        async def __call__(self, fn: kopf.WebhookFn) -> AsyncIterator[kopf.WebhookClientConfig]:
            ...
            for client_config in super().__call__(fn):
                ...  # renew a tunnel, adjust the config
                yield client_config

    # Composition:
    class MyTunnel2:
        async def __call__(self, fn: kopf.WebhookFn) -> AsyncIterator[kopf.WebhookClientConfig]:
            server = kopf.WebhookServer(...)
            for client_config in server(fn):
                ...  # renew a tunnel, adjust the config
                yield client_config


System resource cleanup
=======================

It is advised that custom servers/tunnels cleanup the system resources
they allocate at runtime. The easiest way is the ``try-finally`` block --
the cleanup will happen on the garbage collection of the generator object
(beware: it can be postponed in some environments, e.g. in PyPy).

For explicit cleanup of system resources, the servers/tunnels can implement
the asynchronous context manager protocol:

.. code-block:: python

    class MyServer:
        def __init__(self):
            super().__init__()
            self._resource = None

        async def __aenter__(self) -> "MyServer":
            self._resource = PotentiallyLeakableResource()
            return self

        async def __aexit__(self, exc_type, exc_val, exc_tb) -> bool:
            self._resource.cleanup()
            self._resource = None

        async def __call__(self, fn: kopf.WebhookFn) -> AsyncIterator[kopf.WebhookClientConfig]:
            for client_config in super().__call__(fn):
                yield client_config

The context manager should usually return ``self``, but it can return
a substitute webhook server/tunnel object, which will actually be used.
That way, the context manager turns into a factory of webhook server(s).

Keep in mind that the webhook server/tunnel is used only once per
the operator's lifetime; once it exits, the whole operator stops.
It makes no practical sense in making the webhook servers/tunnels reentrant.

.. note::

    **An implementation note:** webhook servers and tunnels provided by Kopf
    use a little hack to keep them usable with the simple protocol
    (a callable that yields the client configs) while also supporting
    the optional context manager protocol for system resource safety:
    when the context manager is exited, it force-closes the generators
    that yield the client configs as if they were garbage-collected.
    Users' final webhook servers/tunnels do not need this level of complication.

.. seealso::
    For reference implementations of servers and tunnels,
    see the `provided webhooks`__.

__ https://github.com/nolar/kopf/blob/master/kopf/toolkits/webhooks.py



================================================
FILE: docs/alternatives.rst
================================================
============
Alternatives
============

Metacontroller
==============

The closest equivalent of Kopf is Metacontroller_.
It targets the same goal as Kopf does:
to make the development of Kubernetes operators easy,
with no need for in-depth knowledge of Kubernetes or Go.

However, it does that in a different way than Kopf does:
with a few YAML files describing the structure of your operator
(besides the custom resource definition),
and by wrapping your core domain logic into the Function-as-a-Service
or into the in-cluster HTTP API deployments,
which in turn react to the changes in the custom resources.

An operator developer still has to implement the infrastructure
of the API calls in these HTTP APIs and/or Lambdas.
The APIs must be reachable from inside the cluster,
which means that they must be deployed there.

Kopf, on the other hand, attempts to keep things explicit
(as per the `Zen of Python`_: *explicit is better than implicit*),
keeping the whole operator's logic in one place, in one syntax (Python).

.. admonition:: And, by the way...

    Not only it is about "*explicit is better than implicit*",
    but also "*simple is better than complex*",
    "*flat is better than nested*", and "*readability counts*",
    which makes Kopf a *pythonic* framework in the first place,
    not just *written with Python*.

Kopf also makes the effort to keep the operator development human-friendly,
which means at least the ease of debugging (e.g. with the breakpoints,
running in a local IDE, not in the cloud), the readability of the logs,
and other little pleasant things.

And also Kopf allows to write *any* arbitrary domain logic of the resources,
especially if it spans over long periods (hours, days if needed),
and is not limited to the timeout restrictions of the HTTP APIs with their
expectation of nearly-immediate outcome (i.e. in seconds or milliseconds).

Metacontroller, however, is more mature, 1.5 years older than Kopf,
and is backed by Google, who originally developed Kubernetes itself.

Unlike Kopf, Metacontroller supports the domain logic in any languages
due to its language-agnostic nature of HTTP APIs.

.. _Metacontroller: https://metacontroller.github.io/metacontroller/
.. _Zen of Python: https://www.python.org/dev/peps/pep-0020/


Side8's k8s-operator
====================

Side8's k8s-operator_ is another direct equivalent.
It was the initial inspiration for writing Kopf.

Side8's k8s-operator is written with Python3 and allows to write
the domain logic in the apply/delete scripts in any language.
The scripts run locally on the same machine where the controller is running
(usually the same pod, or a developer's computer).

However, the interaction with the script relies on stdout output
and the environment variables as the input,
which is only good if the scripts are written in shell/bash.
Writing the complicated domain logic in bash can be troublesome.

The scripts in other languages, such as Python, are supported but require
the inner infrastructure logic to parse the input and to render the output
and to perform the logging properly:
e.g., so that no single byte of garbage output is ever printed to stdout,
or so that the resulting status is merged with the initial status, etc --
which kills the idea of pure domain logic and no infrastructure logic
in the operator codebase.

.. _k8s-operator: https://github.com/side8/k8s-operator


CoreOS Operator SDK & Framework
===============================

`CoreOS Operator SDK`_ is not an operator framework.
It is an SDK, i.e. a Software Development Kit,
which generates the skeleton code for the operators-to-be,
and users should enrich it with the domain logic code as needed.

`CoreOS Operator Framework`_, of which the abovementioned SDK is a part,
is a bigger, more powerful, but very complicated tool for writing operators.

Both are developed purely for Go-based operators.
No other languages are supported.

.. _CoreOS Operator SDK: https://github.com/operator-framework/operator-sdk
.. _CoreOS Operator Framework: https://coreos.com/operators/

From the CoreOS'es point of view, an operator is a method of packaging
and managing a Kubernetes-native application (presumably of any purpose,
such as MySQL, Postgres, Redis, ElasticSearch, etc) with Kubernetes APIs
(e.g. the custom resources of ConfigMaps) and ``kubectl`` tooling.
They refer to operators as
"*the runtime that manages this type of application on Kubernetes*."

Kopf uses a more generic approach,
where the operator *is* the application with the domain logic in it.
Managing other applications inside of Kubernetes is just one special case
of such a domain logic, but the operators could also be used to manage
the applications outside of Kubernetes (via their APIs), or to implement
the direct actions without any supplementary applications at all.

.. seealso::
    * https://coreos.com/operators
    * https://coreos.com/blog/introducing-operator-framework
    * https://enterprisersproject.com/article/2019/2/kubernetes-operators-plain-english



================================================
FILE: docs/architecture-layers.xml
================================================
<mxfile host="Electron" modified="2021-05-12T09:38:07.966Z" agent="5.0 (Macintosh; Intel Mac OS X 11_3_0) AppleWebKit/537.36 (KHTML, like Gecko) draw.io/14.5.1 Chrome/89.0.4389.82 Electron/12.0.1 Safari/537.36" etag="YBm4qNFwEGlutoZtVfiV" compressed="true" version="14.5.1" type="device"><diagram id="zTwttHN_1trKSXOq0u0y" name="Page-1">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</diagram></mxfile>


================================================
FILE: docs/architecture.rst
================================================
============
Architecture
============

Layered layout
==============

The framework is organized into several layers, and the layers are layered too.
The higher-level layers and modules can import the lower-level ones,
but not vice versa. The layering is checked and enforced by `import-linter`_.

.. _import-linter: https://github.com/seddonym/import-linter/

.. figure:: architecture-layers.png
   :align: center
   :width: 100%
   :alt: A layered module layout overview (described below).

   Note: only the essential module dependencies are shown, not all of them.
   All other numerous imports are represented by cross-layer dependencies.

.. Drawn with https://diagrams.net/ (ex-draw.io; desktop version).
.. The source is here nearby. Export as PNG, border width 0, scale 200%,
.. transparent background ON, include copy of the diagram OFF.


Root
----

At the topmost level, the framework consists of cogs, core, and kits,
and user-facing modules.

``kopf``, ``kopf.on``, ``kopf.testing`` are the public interface that can be
imported by operator developers. Only these public modules contain all public
promises on names and signatures. Everything else is an implementation detail.

The internal modules are intentionally hidden (by underscore-naming)
to protect against introducing the dependencies on the implementation details
that can change without warnings.

``cogs`` are utilities used throughout the framework in nearly all modules.
They do not represent the main functionality of operators but are needed
to make them work. Generally, the cogs are fully independent of each other
and of anything in the framework --- to the point that they can be extracted
as separate libraries (in theory; if anyone needs it).

``core`` is the main functionality used by a Kopf-based operator.
It brings the operators into motion. The core is the essence of the framework,
it cannot be extracted or replaced without redefining the framework.

``kits`` are utilities and specialised tools provided to operator developers
for some scenarios and/or settings. The framework itself does not use them.


Cogs
----

``helpers`` are system-level or language-enhancing adapters. E.g., hostname
identification, dynamic Python module importing, integrations with 3rd-party
libraries (such as pykube-ng or the official Kubernetes Python client).

``aiokits`` are asynchronous primitives and enhancements for ``asyncio``,
sufficiently abstracted from the framework and the Kubernetes/operator domains.

``structs`` are data structures and type declarations for Kubernetes models:
such as resource kinds, selectors, bodies or their parts (specs, statuses, etc),
admission reviews, so on. Besides, this includes some specialised structures,
such as authentication credentials -- also abstracted for the framework even
in case the clients and their authentication are replaced.

``configs`` are the settings mostly, and everything needed to define them:
e.g. persistence storage classes (for handling progress and diff bases).

``clients`` are the asynchronous adapters and wrappers for the Kubernetes API.
They abstract away how the framework communicates with the API to achieve
its goals (such as patching a resource or watching for its changes).
Currently, it is based on aiohttp_; previously, it was the official Kubernetes
client library and pykube-ng. Over time, the whole clients' implementation
can be replaced with another one --- while keeping the signatures for the rest
of the framework. Only the clients are allowed to talk to the Kubernetes API.

.. _aiohttp: https://github.com/aio-libs/aiohttp


Core
----

``actions`` is the lowest level in the core (but not in the framework).
It defines how the functions and handlers are invoked, which ones specifically,
how their errors are handled and retried (if at all), how the function results
and the patches are applied to the cluster, so on.

``intents`` are mostly data structures that store the declared handlers
of the operators, but also some logic to select/filter them when a reaction
is needed.

``engines`` are specialised aspects of the framework, i.e. its functionality.
Engines are usually independent of each other (though, this is not a rule).
For example, daemons and timers, validating/mutating admission requests,
in-memory indexing, operator activities (authentication, probing, etc),
peering, Kubernetes ``kind: Event`` delayed posting, etc.

``reactor`` is the topmost layer in the framework. It defines the entry points
for the CLI and operator embedding (see :doc:`/embedding`) and implements
the task orchestration for all the engines and internal machinery.
Besides, the reactor observes the cluster for resources and namespaces,
and dynamically spawns/stops the tasks to serve them.


Kits
----

``hierarchies`` are helper functions to manage the hierarchies of Kubernetes
objects, such as labelling them, adding/removing the owner references,
name generation, so on. They support raw Python dicts so as some selected
libraries: pykube-ng and the official Kubernetes client for Python
(see :doc:`/hierarchies`).

``webhooks`` are helper servers and tunnels to accept admission requests
from a Kubernetes cluster even if running locally on a developer's machines
(see :doc:`/admission`).

``runner`` is a helper to run an operator in a Python context manager,
mostly useful for testing (see :doc:`/testing`).



================================================
FILE: docs/async.rst
================================================
===========
Async/Await
===========

.. todo:: Fit this page into the walk-through sample story?

Kopf supports asynchronous handler functions::

    import asyncio
    import kopf

    @kopf.on.create('kopfexamples')
    async def create_fn(spec, **_):
        await asyncio.sleep(1.0)

Async functions have an additional benefit over the non-async ones
to make the full stack trace available when exceptions occur
or IDE breakpoints are used since the async functions are executed
directly inside of Kopf's event loop in the main thread.

Regular synchronous handlers, despite supported for convenience,
are executed in parallel threads (via the default executor of the loop),
and can only see the stack traces up to the thread entry point.

.. warning::
    As with any async coroutines, it is the developer's responsibility
    to make sure that all the internal function calls are either
    ``await``\s of other async coroutines (e.g. ``await asyncio.sleep()``),
    or the regular non-blocking functions calls.

    Calling a synchronous function (e.g. HTTP API calls or ``time.sleep()``)
    inside of an asynchronous function will block the whole operator process
    until the synchronous call if finished, i.e. even other resources
    processed in parallel, and the Kubernetes event-watching/-queueing cycles.

    This can come unnoticed in the development environment
    with only a few resources and no external timeouts,
    but can hit hard in the production environments with high load.



================================================
FILE: docs/authentication.rst
================================================
==============
Authentication
==============

To access a Kubernetes cluster, an endpoint and some credentials are needed.
They are usually taken either from the environment (environment variables),
or from the ``~/.kube/config`` file, or from external authentication services.

Kopf provides rudimentary authentication out of the box: it can authenticate
with the Kubernetes API either via the service account or raw kubeconfig data
(with no additional interpretation or parsing of those).

But this can be not enough in some setups and environments.
Kopf does not try to maintain all the authentication methods possible.
Instead, it allows the operator developers to implement their custom
authentication methods and "piggybacks" the existing Kubernetes clients.

The latter ones can implement some advanced authentication techniques,
such as the temporary token retrieval via the authentication services,
token rotation, etc.


Custom authentication
=====================

In most setups, the normal authentication from one of the API client libraries
is enough --- it works out of the box if those clients are installed
(see :ref:`auth-piggybacking` below). Custom authentication is only needed
if the normal authentication methods do not work for some reason, such as if
you have a specific and unusual cluster setup (e.g. your own auth tokens).

To implement a custom authentication method, one or a few login-handlers
can be added. The login handlers should either return nothing (``None``)
or an instance of :class:`kopf.ConnectionInfo`::

    import datetime
    import kopf

    @kopf.on.login()
    def login_fn(**kwargs):
        return kopf.ConnectionInfo(
            server='https://localhost',
            ca_path='/etc/ssl/ca.crt',
            ca_data=b'...',
            insecure=True,
            username='...',
            password='...',
            scheme='Bearer',
            token='...',
            certificate_path='~/.minikube/client.crt',
            private_key_path='~/.minikube/client.key',
            certificate_data=b'...',
            private_key_data=b'...',
            expiration=datetime.datetime(2099, 12, 31, 23, 59, 59),
        )

Both TZ-naive & TZ-aware expiration times are supported.
The TZ-naive timestamps are always treated as UTC.

As with any other handlers, the login handler can be async if the network
communication is needed and async mode is supported::

    import kopf

    @kopf.on.login()
    async def login_fn(**kwargs):
        pass

A :class:`kopf.ConnectionInfo` is a container to bring the parameters necessary
for making the API calls, but not the ways of retrieving them. Specifically:

* TCP server host & port.
* SSL verification/ignorance flag.
* SSL certificate authority.
* SSL client certificate and its private key.
* HTTP ``Authorization: Basic username:password``.
* HTTP ``Authorization: Bearer token`` (or other schemes: Bearer, Digest, etc).
* URL's default namespace for the cases when this is implied.

No matter how the endpoints or credentials are retrieved, they are directly
mapped to TCP/SSL/HTTPS protocols in the API clients. It is the responsibility
of the authentication handlers to ensure that the values are consistent
and valid (e.g. via internal verification calls). It is in theory possible
to mix all authentication methods at once or to have none of them at all.
If the credentials are inconsistent or invalid, there will be permanent
re-authentication happening.

Multiple handlers can be declared to retrieve different credentials
or the same credentials via different libraries. All of the retrieved
credentials will be used in random order with no specific priority.

.. _auth-piggybacking:

Piggybacking
============

In case no handlers are explicitly declared, Kopf attempts to authenticate
with the existing Kubernetes libraries if they are installed.
At the moment: pykube-ng_ and kubernetes_.
In the future, more libraries can be added for authentication piggybacking.

.. _pykube-ng: https://github.com/hjacobs/pykube
.. _kubernetes: https://github.com/kubernetes-client/python

.. note::

    Since ``kopf>=1.29``, ``pykube-ng`` is not pre-installed implicitly.
    If needed, install it explicitly as a dependency of the operator,
    or via ``kopf[full-auth]`` (see :doc:`install`).

*Piggybacking* means that the config parsing and authentication methods of these
libraries are used, and only the information needed for API calls is extracted.

If a few of the piggybacked libraries are installed,
all of them will be attempted (as if multiple handlers are installed),
and all the credentials will be utilised in random order.

If that is not the desired case, and only one of the libraries is needed,
declare a custom login handler explicitly, and use only the preferred library
by calling one of the piggybacking functions::

    import kopf

    @kopf.on.login()
    def login_fn(**kwargs):
        return kopf.login_via_pykube(**kwargs)

Or::

    import kopf

    @kopf.on.login()
    def login_fn(**kwargs):
        return kopf.login_via_client(**kwargs)

The same trick is also useful to limit the authentication attempts
by time or by number of retries (by default, it tries forever
until succeeded, returned nothing, or explicitly failed)::

    import kopf

    @kopf.on.login(retries=3)
    def login_fn(**kwargs):
        return kopf.login_via_pykube(**kwargs)

Similarly, if the libraries are installed and needed, but their credentials
are not desired, the rudimentary login functions can be used directly::

    import kopf

    @kopf.on.login()
    def login_fn(**kwargs):
        return kopf.login_with_service_account(**kwargs) or kopf.login_with_kubeconfig(**kwargs)

.. seealso::
    `kopf.login_via_pykube`, `kopf.login_via_client`,
    `kopf.login_with_kubeconfig`, `kopf.login_with_service_account`.


Credentials lifecycle
=====================

Internally, all the credentials are gathered from all the active handlers
(either the declared ones or all the fallback piggybacking ones)
in no particular order, and are fed into a *vault*.

The Kubernetes API calls then use random credentials from that *vault*.
The credentials that have reached their expiration are ignored and removed.
If the API call fails with an HTTP 401 error, these credentials are marked
invalid, excluded from further use, and the next random credentials are tried.

When the *vault* is fully depleted, it freezes all the API calls and triggers
the login handlers for re-authentication. Only the new credentials are used.
The credentials, which previously were known to be invalid, are ignored
to prevent a permanent never-ending re-authentication loop.

There is no validation of credentials by making fake API calls.
Instead, the real API calls validate the credentials by using them
and reporting them back to the *vault* as invalid (or keeping them as valid),
potentially causing new re-authentication activities.

In case the *vault* is depleted and no new credentials are provided
by the login handlers, the API calls fail, and so does the operator.

This internal logic is hidden from the operator developers, but it is worth
knowing how it works internally. See :class:`Vault`.

If the expiration is intended to be often (e.g. every few minutes),
you might want to disable the logging of re-authenication (whether this is
a good idea or not, you decide using the information about your system)::

    import logging

    logging.getLogger('kopf.activities.authentication').disabled = True
    logging.getLogger('kopf._core.engines.activities').disabled = True



================================================
FILE: docs/cli.rst
================================================
====================
Command-line options
====================

Most of the options relate to ``kopf run``, though some are shared by other
commands, such as ``kopf freeze`` and ``kopf resume``.


Scripting options
=================

.. option:: -m, --module

    A semantical equivalent to ``python -m`` --- which importable modules
    to import on startup.


Logging options
===============

.. option:: --quiet

    Be quiet: only show warnings and errors, but not the normal processing logs.

.. option:: --verbose

    Show what Kopf is doing, but hide the low-level asyncio & aiohttp logs.

.. option:: --debug

    Extremely verbose: log all the asyncio internals too, so as the API traffic.

.. option:: --log-format (plain|full|json)

    See more in :doc:`/configuration`.

.. option:: --log-prefix, --no-log-prefix

    Whether to prefix all object-related messages with the name of the object.
    By default, the prefixing is enabled.

.. option:: --log-refkey

    For JSON logs, under which top-level key to put the object-identifying
    information, such as its name, namespace, etc.


Scope options
=============

.. option:: -n, --namespace

    Serve this namespace or all namespaces mathing the pattern
    (or excluded from patterns). The option can be repeated multiple times.

    .. seealso::
        :doc:`/scopes` for the pattern syntax.

.. option:: -A, --all-namespaces

    Serve the whole cluster. This is different from ``--namespace *``:
    with ``--namespace *``, the namespaces are monitored, and every resource
    in every namespace is watched separately, starting and stopping as needed;
    with ``--all-namespaces``, the cluster endpoints of the Kubernetes API
    are used for resources, the namespaces are not monitored.


Probing options
===============

.. option:: --liveness

    The endpoint where to serve the probes and health-checks.
    E.g. ``http://0.0.0.0:1234/``. Only ``http://`` is currently supported.
    By default, the probing endpoint is not served.

.. seealso::
    :doc:`/probing`


Peering options
===============

.. option:: --standalone

    Disable any peering or auto-detection of peering. Run strictly as if
    this is the only instance of the operator.

.. option:: --peering

    The name of the peering object to use. Depending on the operator's scope
    (:option:`--all-namespaces` vs. :option:`--namespace`, see :doc:`/scopes`),
    it is either ``kind: KopfPeering`` or ``kind: ClusterKopfPeering``.

    If specified, the operator will not run until that peering exists
    (for the namespaced operators, until it exists in each served namespace).

    If not specified, the operator checks for the name "default" and uses it.
    If the "default" peering is absent, the operator runs in standalone mode.

.. option:: --priority

    Which priority to use for the operator. An operator with the highest
    priority wins the peering competitions and handlers the resources.

    The default priority is ``0``; :option:`--dev` sets it to ``666``.

.. seealso::
    :doc:`/peering`


Development mode
================

.. option:: --dev

    Run in the development mode. Currently, this implies ``--priority=666``.
    Other meanings can be added in the future, such as automatic reloading
    of the source code.



================================================
FILE: docs/concepts.rst
================================================
========
Concepts
========

**Kubernetes** is a container orchestrator.

It provides some basic primitives to orchestrate application deployments
on a low level ---such as the pods, jobs, deployments, services, ingresses,
persistent volumes and volume claims, secrets---
and allows a Kubernetes cluster to be extended with the arbitrary
custom resources and custom controllers.

On the top level, it consists of the Kubernetes API, through which the users
talk to Kubernetes, internal storage of the state of the objects (etcd),
and a collection of controllers. The command-line tooling (``kubectl``)
can also be considered as a part of the solution.

----

The **Kubernetes controller** is the logic (i.e. the behaviour) behind most
objects, both built-in and added as extensions of Kubernetes.
Examples of objects are ReplicaSet and Pods, created when a Deployment object
is created, with the rolling version upgrades, and so on.

The main purpose of any controller is to bring the actual state
of the cluster to the desired state, as expressed with the resources/object
specifications.

----

The **Kubernetes operator** is one kind of the controllers, which orchestrates
objects of a specific kind, with some domain logic implemented inside.

The essential difference between operators and the controllers
is that operators are domain-specific controllers,
but not all controllers are necessary operators:
for example, the built-in controllers for pods, deployments, services, etc,
so as the extensions of the object's life-cycles based on the labels/annotations,
are not operators, but just controllers.

The essential similarity is that they both implement the same pattern:
watching the objects and reacting to the objects' events (usually the changes).

----

**Kopf** is a framework to build Kubernetes operators in Python.

Like any framework, Kopf provides both the "outer" toolkit to run the operator,
to talk to the Kubernetes cluster, and to marshal the Kubernetes events
into the pure-Python functions of the Kopf-based operator,
and the "inner" libraries to assist with a limited set of common tasks
of manipulating the Kubernetes objects
(however, it is not yet another Kubernetes client library).

.. seealso::

    See :doc:`/architecture`
    to understand how Kopf works in detail, and what it does exactly.

    See :doc:`/vision` and :doc:`/alternatives`
    to understand Kopf's self-positioning in the world of Kubernetes.

.. seealso::
    * https://en.wikipedia.org/wiki/Kubernetes
    * https://coreos.com/operators/
    * https://stackoverflow.com/a/47857073
    * https://github.com/kubeflow/tf-operator/issues/300



================================================
FILE: docs/conf.py
================================================
# Configuration file for the Sphinx documentation builder.
# http://www.sphinx-doc.org/en/master/config
import os

import docutils.nodes

###############################################################################
# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#
# import os
# import sys
# sys.path.insert(0, os.path.abspath('.'))

project = 'Kopf'
copyright = '2020 Sergey Vasilyev; 2019-2020 Zalando SE'
author = 'Sergey Vasilyev'

extensions = [
    'sphinx.ext.autodoc',
    'sphinx_autodoc_typehints',
    'sphinx.ext.todo',
    'sphinx.ext.extlinks',
    'sphinx.ext.linkcode',
    'sphinx.ext.intersphinx',
]

templates_path = ['_templates']
exclude_patterns = ['_build', 'Thumbs.db', '.DS_Store']
html_static_path = ['_static']
html_theme = 'sphinx_rtd_theme'

default_role = 'py:obj'

autodoc_typehints = 'description'
autodoc_member_order = 'bysource'

todo_include_todos = False
todo_emit_warnings = True

extlinks = {
    'issue': ('https://github.com/nolar/kopf/issues/%s', 'issue '),
}

intersphinx_mapping = {
    'python': ('https://docs.python.org/3', None),
    'mypy': ('https://mypy.readthedocs.io/en/latest/', None),
}


def linkcode_resolve(domain, info):
    if domain != 'py':
        return None
    if not info['module']:
        return None
    filename = info['module'].replace('.', '/')
    return "https://github.com/nolar/kopf/blob/main/%s.py" % filename


###############################################################################
# Ensure the apidoc is always built as part of the build process,
# especially in ReadTheDocs build environment.
# See: https://github.com/rtfd/readthedocs.org/issues/1139
###############################################################################

def run_apidoc(_):
    ignore_paths = [
    ]

    docs_path = os.path.relpath(os.path.dirname(__file__))
    root_path = os.path.relpath(os.path.dirname(os.path.dirname(__file__)))

    argv = [
        '--force',
        '--no-toc',
        '--separate',
        '--module-first',
        '--output-dir', os.path.join(docs_path, 'packages'),
        os.path.join(root_path, 'kopf'),
    ] + ignore_paths

    try:
        # Sphinx 1.7+
        from sphinx.ext import apidoc
        apidoc.main(argv)
    except ImportError:
        # Sphinx 1.6 (and earlier)
        from sphinx import apidoc
        argv.insert(0, apidoc.__file__)
        apidoc.main(argv)


def setup(app):
    app.add_crossref_type('kwarg', 'kwarg', "pair: %s; kwarg", docutils.nodes.literal)
    app.connect('builder-inited', run_apidoc)



================================================
FILE: docs/configuration.rst
================================================
=============
Configuration
=============

It is possible to fine-tune some aspects of Kopf-based operators,
like timeouts, synchronous handler pool sizes, automatic Kubernetes Event
creation from object-related log messages, etc.


Startup configuration
=====================

Every operator has its settings (even if there is more than one operator
in the same processes, e.g. due to :doc:`embedding`). The settings affect
how the framework behaves in details.

The settings can be modified in the startup handlers (see :doc:`startup`):

.. code-block:: python

    import kopf
    import logging

    @kopf.on.startup()
    def configure(settings: kopf.OperatorSettings, **_):
        settings.posting.level = logging.WARNING
        settings.watching.connect_timeout = 1 * 60
        settings.watching.server_timeout = 10 * 60

All the settings have reasonable defaults, so the configuration should be used
only for fine-tuning when and if necessary.

For more settings, see :class:`kopf.OperatorSettings` and :kwarg:`settings`.


Logging formats and levels
==========================

The following log formats are supported on CLI:

* Full logs (the default) -- with timestamps, log levels, and logger names:

    .. code-block:: bash

        kopf run -v --log-format=full

    .. code-block:: console

        [2019-11-04 17:49:25,365] kopf.reactor.activit [INFO    ] Initial authentication has been initiated.
        [2019-11-04 17:49:25,650] kopf.objects         [DEBUG   ] [default/kopf-example-1] Resuming is in progress: ...

* Plain logs, with only the message:

    .. code-block:: bash

        kopf run -v --log-format=plain

    .. code-block:: console

        Initial authentication has been initiated.
        [default/kopf-example-1] Resuming is in progress: ...

  For non-JSON logs, the object prefix can be disabled to make the logs
  completely flat (as in JSON logs):

    .. code-block:: bash

        kopf run -v --log-format=plain --no-log-prefix

    .. code-block:: console

        Initial authentication has been initiated.
        Resuming is in progress: ...

* JSON logs, with only the message:

    .. code-block:: bash

        kopf run -v --log-format=json

    .. code-block:: console

        {"message": "Initial authentication has been initiated.", "severity": "info", "timestamp": "2020-12-31T23:59:59.123456"}
        {"message": "Resuming is in progress: ...", "object": {"apiVersion": "kopf.dev/v1", "kind": "KopfExample", "name": "kopf-example-1", "uid": "...", "namespace": "default"}, "severity": "debug", "timestamp": "2020-12-31T23:59:59.123456"}

  For JSON logs, the object reference key can be configured to match
  the log parsers (if used) -- instead of the default ``"object"``:

    .. code-block:: bash

        kopf run -v --log-format=json --log-refkey=k8s-obj

    .. code-block:: console

        {"message": "Initial authentication has been initiated.", "severity": "info", "timestamp": "2020-12-31T23:59:59.123456"}
        {"message": "Resuming is in progress: ...", "k8s-obj": {...}, "severity": "debug", "timestamp": "2020-12-31T23:59:59.123456"}

    Note that the object prefixing is disabled for JSON logs by default, as the
    identifying information is available in the ref-keys. The prefixing can be
    explicitly re-enabled if needed:

    .. code-block:: bash

        kopf run -v --log-format=json --log-prefix

    .. code-block:: console

        {"message": "Initial authentication has been initiated.", "severity": "info", "timestamp": "2020-12-31T23:59:59.123456"}
        {"message": "[default/kopf-example-1] Resuming is in progress: ...", "object": {...}, "severity": "debug", "timestamp": "2020-12-31T23:59:59.123456"}

.. note::

    Logging verbosity and formatting are only configured via CLI options,
    not via ``settings.logging`` as all other aspects of configuration.
    When the startup handlers happen for ``settings``, it is too late:
    some initial messages could be already logged in the existing formats,
    or not logged when they should be due to verbosity/quietness levels.


Logging events
==============

``settings.posting`` allows to control which log messages should be posted as
Kubernetes events. Use ``logging`` constants or integer values to set the level:
e.g., ``logging.WARNING``, ``logging.ERROR``, etc.
The default is ``logging`.INFO``.

.. code-block:: python

    import logging
    import kopf

    @kopf.on.startup()
    def configure(settings: kopf.OperatorSettings, **_):
        settings.posting.level = logging.ERROR

The event-posting can be disabled completely (the default is to be enabled):

.. code-block:: python

    import kopf

    @kopf.on.startup()
    def configure(settings: kopf.OperatorSettings, **_):
        settings.posting.enabled = False

.. note::

    These settings also affect `kopf.event` and related functions:
    `kopf.info`, `kopf.warn`, `kopf.exception`, etc --
    even if they are called explicitly in the code.

    To avoid these settings having an impact on your code, post events
    directly with an API client library instead of the Kopf-provided toolkit.


.. _configure-sync-handlers:

Synchronous handlers
====================

``settings.execution`` allows setting the number of synchronous workers used
by the operator for synchronous handlers, or replace the asyncio executor
with another one:

.. code-block:: python

    import kopf

    @kopf.on.startup()
    def configure(settings: kopf.OperatorSettings, **_):
        settings.execution.max_workers = 20


It is possible to replace the whole asyncio executor used
for synchronous handlers (see :doc:`async`).

Please note that the handlers that started in a previous executor, will be
continued and finished with their original executor. This includes the startup
handler itself. To avoid it, make the on-startup handler asynchronous:

.. code-block:: python

    import concurrent.futures
    import kopf

    @kopf.on.startup()
    async def configure(settings: kopf.OperatorSettings, **_):
        settings.execution.executor = concurrent.futures.ThreadPoolExecutor()

The same executor is used both for regular sync handlers and for sync daemons.
If you expect a large number of synchronous daemons (e.g. for large clusters),
make sure to pre-scale the executor accordingly
(the default in Python is 5x times the CPU cores):

.. code-block:: python

    import kopf

    @kopf.on.startup()
    async def configure(settings: kopf.OperatorSettings, **kwargs):
        settings.execution.max_workers = 1000


Networking timeouts
===================

Timeouts can be controlled when communicating with Kubernetes API:

``settings.networking.request_timeout`` (seconds) is how long a regular
request should take before failing. This applies to all atomic requests --
cluster scanning, resource patching, etc. -- except the watch-streams.
The default is 5 minutes (300 seconds).

``settings.networking.connect_timeout`` (seconds) is how long a TCP handshake
can take for regular requests before failing. There is no default (``None``),
meaning that there is no timeout specifically for this; however, the handshake
is limited by the overall time of the request.

``settings.watching.connect_timeout`` (seconds) is how long a TCP handshake
can take for watch-streams before failing. There is no default (``None``),
which means that ``settings.networking.connect_timeout`` is used if set.
If not set, then ``settings.networking.request_timeout`` is used.

.. note::

    With the current aiohttp-based implementation, both connection timeouts
    correspond to ``sock_connect=`` timeout, not to ``connect=`` timeout,
    which would also include the time for getting a connection from the pool.
    Kopf uses unlimited aiohttp pools, so this should not be a problem.

``settings.watching.server_timeout`` (seconds) is how long the session
with a watching request will exist before closing it from the **server** side.
This value is passed to the server-side in a query string, and the server
decides on how to follow it. The watch-stream is then gracefully closed.
The default is to use the server setup (``None``).

``settings.watching.client_timeout`` (seconds) is how long the session
with a watching request will exist before closing it from the **client** side.
This includes establishing the connection and event streaming.
The default is forever (``None``).

It makes no sense to set the client-side timeout shorter than the server-side
timeout, but it is given to the developers' responsibility to decide.

The server-side timeouts are unpredictable, they can be 10 seconds or
10 minutes. Yet, it feels wrong to assume any "good" values in a framework
(especially since it works without timeouts defined, just produces extra logs).

``settings.watching.reconnect_backoff`` (seconds) is a backoff interval between
watching requests -- to prevent API flooding in case of errors or disconnects.
The default is 0.1 seconds (nearly instant, but not flooding).

.. code-block:: python

    import kopf

    @kopf.on.startup()
    def configure(settings: kopf.OperatorSettings, **_):
        settings.networking.connect_timeout = 10
        settings.networking.request_timeout = 60
        settings.watching.server_timeout = 10 * 60


Finalizers
==========

A resource is blocked from deletion if the framework believes it is safer
to do so, e.g. if non-optional deletion handlers are present
or if daemons/timers are running at the moment.

For this, a finalizer_ is added to the object. It is removed when the framework
believes it is safe to release the object for actual deletion.

.. _finalizer: https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/#finalizers

The name of the finalizer can be configured:

.. code-block:: python

    import kopf

    @kopf.on.startup()
    def configure(settings: kopf.OperatorSettings, **_):
        settings.persistence.finalizer = 'my-operator.example.com/kopf-finalizer'

The default is the one that was hard-coded before:
``kopf.zalando.org/KopfFinalizerMarker``.


.. _progress-storing:

Handling progress
=================

To keep the handling state across multiple handling cycles, and to be resilient
to errors and tolerable to restarts and downtimes, the operator keeps its state
in a configured state storage. See more in :doc:`continuity`.

To store the state only in the annotations with a preferred prefix:

.. code-block:: python

    import kopf

    @kopf.on.startup()
    def configure(settings: kopf.OperatorSettings, **_):
        settings.persistence.progress_storage = kopf.AnnotationsProgressStorage(prefix='my-op.example.com')

To store the state only in the status or any other field:

.. code-block:: python

    import kopf

    @kopf.on.startup()
    def configure(settings: kopf.OperatorSettings, **_):
        settings.persistence.progress_storage = kopf.StatusProgressStorage(field='status.my-operator')

To store in multiple places (stored in sync, but the first found state will be
used when fetching, i.e. the first storage has precedence):

.. code-block:: python

    import kopf

    @kopf.on.startup()
    def configure(settings: kopf.OperatorSettings, **_):
        settings.persistence.progress_storage = kopf.MultiProgressStorage([
            kopf.AnnotationsProgressStorage(prefix='my-op.example.com'),
            kopf.StatusProgressStorage(field='status.my-operator'),
        ])

The default storage is at both annotations and status, with annotations having
precedence over the status (this is done as a transitioning solution
from status-only storage in the past to annotations-only storage in the future).
The annotations are ``kopf.zalando.org/{id}``,
the status fields are ``status.kopf.progress.{id}``.
It is an equivalent of:

.. code-block:: python

    import kopf

    @kopf.on.startup()
    def configure(settings: kopf.OperatorSettings, **_):
        settings.persistence.progress_storage = kopf.SmartProgressStorage()

It is also possible to implement custom state storage instead of storing
the state directly in the resource's fields -- e.g., in external databases.
For this, inherit from :class:`kopf.ProgressStorage` and implement its abstract
methods (``fetch()``, ``store()``, ``purge()``, optionally ``flush()``).

.. note::

    The legacy behavior is an equivalent of
    ``kopf.StatusProgressStorage(field='status.kopf.progress')``.

    Starting with Kubernetes 1.16, both custom and built-in resources have
    strict structural schemas with the pruning of unknown fields
    (more information is in `Future of CRDs: Structural Schemas`__).

    __ https://kubernetes.io/blog/2019/06/20/crd-structural-schema/

    Long story short, unknown fields are silently pruned by Kubernetes API.
    As a result, Kopf's status storage will not be able to store
    anything in the resource, as it will be instantly lost.
    (See `#321 <https://github.com/zalando-incubator/kopf/issues/321>`_.)

    To quickly fix this for custom resources, modify their definitions
    with ``x-kubernetes-preserve-unknown-fields: true``. For example:

    .. code-block:: yaml

        apiVersion: apiextensions.k8s.io/v1
        kind: CustomResourceDefinition
        spec:
          scope: ...
          group: ...
          names: ...
          versions:
            - name: v1
              served: true
              storage: true
              schema:
                openAPIV3Schema:
                  type: object
                  x-kubernetes-preserve-unknown-fields: true

    See a more verbose example in ``examples/crd.yaml``.

    For built-in resources, such as pods, namespaces, etc, the schemas cannot
    be modified, so a full switch to annotations storage is advised.

    The new default "smart" storage is supposed to ensure a smooth upgrade
    of Kopf-based operators to the new state location without special upgrade
    actions or conversions needed.


.. _diffbase-storing:

Change detection
================

For change-detecting handlers, Kopf keeps the last handled configuration --
i.e. the last state that has been successfully handled. New changes are compared
against the last handled configuration, and a diff list is formed.

The last-handled configuration is also used to detect if there were any
essential changes at all -- i.e. not just the system or status fields.

The last-handled configuration storage can be configured
with ``settings.persistence.diffbase_storage``.
The default is an equivalent of:

.. code-block:: python

    import kopf

    @kopf.on.startup()
    def configure(settings: kopf.OperatorSettings, **_):
        settings.persistence.diffbase_storage = kopf.AnnotationsDiffBaseStorage(
            prefix='kopf.zalando.org',
            key='last-handled-configuration',
        )

The stored content is a JSON-serialised essence of the object (i.e., only
the important fields, with system fields and status stanza removed).

It is generally not a good idea to override this store unless multiple
Kopf-based operators must handle the same resources, and they should not
collide with each other. In that case, they must take different names.


Storage transition
==================

.. warning::

    Changing a storage method for an existing operator with existing resources
    is dangerous: the operator will consider all those resources
    as not handled yet (due to absence of a diff-base key) or will loose
    their progress state (if some handlers are retried or slow). The operator
    will start handling each of them again -- which can lead to duplicated
    children or other side-effects.

To ensure a smooth transition, use a composite multi-storage, with the
new storage as a first child, and the old storage as the second child
(both are used for writing, the first found value is used for reading).

For example, to eventually switch from Kopf's annotations to a status field
for diff-base storage, apply this configuration:

.. code-block:: python

    import kopf

    @kopf.on.startup()
    def configure(settings: kopf.OperatorSettings, **_):
        settings.persistence.diffbase_storage = kopf.MultiDiffBaseStorage([
            kopf.StatusDiffBaseStorage(field='status.diff-base'),
            kopf.AnnotationsDiffBaseStorage(prefix='kopf.zalando.org', key='last-handled-configuration'),
        ])

Run the operator for some time. Let all resources change or force this:
e.g. by arbitrarily labelling them, so that a new diff-base is generated:

.. code-block:: shell

    kubectl label kex -l somelabel=somevalue  ping=pong

Then, switch to the new storage alone, without the transitional setup.


.. _api-retrying:

Retrying of API errors
======================

In some cases, the Kubernetes API servers might be not ready on startup
or occasionally at runtime; the network might have issues too. In most cases,
these issues are of temporary nature and heal themselves withing seconds.

The framework retries the TCP/SSL networking errors and the HTTP 5xx errors
("the server is wrong") --- i.e. everything that is presumed to be temporary;
other errors -- those presumed to be permanent, including HTTP 4xx errors
("the client is wrong") -- escalate immediately without retrying.

The setting ``settings.networking.error_backoffs`` controls for how many times
and with which backoff interval (in seconds) the retries are performed.

It is a sequence of back-offs between attempts (in seconds):

.. code-block:: python

    import kopf

    @kopf.on.startup()
    def configure(settings: kopf.OperatorSettings, **_):
        settings.networking.error_backoffs = [10, 20, 30]

Note that the number of attempts is one more than the number of back-off
intervals (because the back-offs happen inbetween the attempts).

A single integer or float value means a single backoff, i.e. 2 attempts:
``(1.0)`` is equivalent to ``(1.0,)`` or ``[1.0]`` for convenience.

To have a uniform back-off delay D with N+1 attempts, set to ``[D] * N``.

To disable retrying (on your own risk), set it to ``[]`` or ``()``.

The default value covers roughly a minute of attempts before giving up.

Once the retries are over (if disabled, immediately on error), the API errors
escalate and are then handled according to :ref:`error-throttling`.

This value can be an arbitrary collection or an iterable object (even infinite):
only ``iter()`` is called on every new retrying cycle, no other protocols
are required; however, make sure that it is re-iterable for multiple uses:

.. code-block:: python

    import kopf
    import random

    class InfiniteBackoffsWithJitter:
        def __iter__(self):
            while True:
                yield 10 + random.randint(-5, +5)

    @kopf.on.startup()
    def configure(settings: kopf.OperatorSettings, **_):
        settings.networking.error_backoffs = InfiniteBackoffsWithJitter()


Retrying an API error blocks the task or the object's worker in which
the API error happens. However, other objects and tasks run normally
in parallel (unless they hit the same error in the same cluster).

Every further consecutive error leads to the next, typically bigger backoff.
Every success resets the backoff intervals, and it goes from the beginning
on the next error.

.. note::

    The format is the same as for ``settings.batching.error_delays``.
    The only difference: if the API operation does not succeed by the end
    of the sequence, the error of the last attempt escalates instead of blocking
    and retrying forever with the last delay in the sequence.

.. seealso::
    These back-offs cover only the server-side and networking errors.
    For errors in handlers, see :doc:`/errors`.
    For errors in the framework, see :ref:`error-throttling`.


.. _error-throttling:

Throttling of unexpected errors
===============================

To prevent an uncontrollable flood of activities in case of errors that prevent
the resources being marked as handled, which could lead to the Kubernetes API
flooding, it is possible to throttle the activities on a per-resource basis:

.. code-block:: python

    import kopf

    @kopf.on.startup()
    def configure(settings: kopf.OperatorSettings, **_):
        settings.batching.error_delays = [10, 20, 30]

In that case, all unhandled errors in the framework or in the Kubernetes API
would be backed-off by 10s after the 1st error, then by 20s after the 2nd one,
and then by 30s after the 3rd, 4th, 5th errors and so on. On the first success,
the backoff intervals will be reset and re-used again on the next error.

Once the errors stop and the operator is back to work, it processes only
the latest event seen for that malfunctioning resource (due to event batching).

The default is a sequence of Fibonacci numbers from 1 second to 10 minutes.

The back-offs are not persisted, so they are lost on the operator restarts.

These back-offs do not cover errors in the handlers -- the handlers have their
own per-handler back-off intervals. These back-offs are for Kopf's own errors.

To disable throttling (on your own risk), set it to ``[]`` or ``()``.
Interpret it as: no throttling delays set --- no throttling sleeps done.

If needed, this value can be an arbitrary collection/iterator/object:
only ``iter()`` is called on every new throttling cycle, no other protocols
are required; but make sure that it is re-iterable for multiple uses.



================================================
FILE: docs/continuity.rst
================================================
==========
Continuity
==========

Persistence
===========

Kopf does not have any database. It stores all the information directly
on the objects in the Kubernetes cluster (which means ``etcd`` usually).
All information is retrieved and stored via the Kubernetes API.

Specifically:

* The cross-operator exchange is performed via peering objects of type
  ``KopfPeering`` or ``ClusterKopfPeering``
  (API versions: either ``kopf.dev/v1`` or ``zalando.org/v1``).
  See :doc:`peering` for more info.
* The last handled state of the object is stored in ``metadata.annotations``
  (the ``kopf.zalando.org/last-handled-configuration`` annotation).
  It is used to calculate diffs upon changes.
* The handlers' state (failures, successes, retries, delays) is stored
  in either ``metadata.annotations`` (``kopf.zalando.org/{id}`` keys),
  or in ``status.kopf.progress.{id}``, where ``{id}`` is the handler's id.

The persistent state locations can be configured to use different keys,
thus allowing multiple independent operators to handle the same resources
without overlapping with each other. The above-mentioned keys are the defaults.
See how to configure the stores in :doc:`configuration`
(at :ref:`progress-storing`, :ref:`diffbase-storing`).


Restarts
========

It is safe to kill the operator's pod (or process) and allow it to restart.

The handlers that succeeded previously will not be re-executed.
The handlers that did not execute yet, or were scheduled for retrying,
will be retried by a new operators pod/process from the point where
the old pod/process was terminated.

Restarting an operator will only affect the handlers currently being
executed in that operator at the moment of termination, as there is
no record that they have succeeded.


Downtime
========

If the operator is down and not running, any changes to the objects
are ignored and not handled. They will be handled when the operator starts:
every time a Kopf-based operator starts, it lists all objects of the
resource kind, and checks for their state; if the state has changed since
the object was last handled (no matter how long time ago),
a new handling cycle starts.

Only the last state is taken into account. All the intermediate changes
are accumulated and handled together.
This corresponds to Kubernetes's concept of eventual consistency
and level triggering (as opposed to edge triggering).

.. warning::
    If the operator is down, the objects may not be deleted,
    as they may contain the Kopf's finalizers in ``metadata.finalizers``,
    and Kubernetes blocks the deletion until all finalizers are removed.
    If the operator is not running, the finalizers will never be removed.
    See: :ref:`finalizers-blocking-deletion` for a work-around.



================================================
FILE: docs/contributing.rst
================================================
============
Contributing
============

.. highlight:: bash

In a nutshell, to contribute, follow this scenario:

* Fork the repo in GitHub.
* Clone the fork.
* Check out a feature branch.
* **Implement the changes.**
  * Lint with ``pre-commit run``.
  * Test with ``pytest``.
* Sign-off your commits.
* Create a pull request.
* Ensure all required checks are passed.
* Wait for a review by the project maintainers.


Git workflow
============

Kopf uses Git Forking Workflow. It means, all the development should happen
in the individual forks, not in the feature branches of the main repo.

The recommended setup:

* Fork a repo on GitHub and clone the fork (not the original repo).
* Configure the ``upstream`` remote in addition to ``origin``::

        git remote add upstream git@github.com:nolar/kopf.git
        git fetch upstream

* Sync your ``main`` branch with the upstream regularly::

        git checkout main
        git pull upstream main --ff
        git push origin main

Work in the feature branches of your fork, not in the upstream's branches:

* Create a feature branch in the fork::

        git checkout -b feature-x
        git push origin feature-x

* Once the feature is ready, create a pull request
  from your fork to the main repo.

.. seealso::

    * `Overview of the Forking Workflow. <https://gist.github.com/Chaser324/ce0505fbed06b947d962>`_
    * `GitHub's manual on forking <https://help.github.com/en/articles/fork-a-repo>`_
    * `GitHub's manual on syncing the fork <https://help.github.com/en/articles/syncing-a-fork>`_


Git conventions
===============

The more rules you have, the less they are followed.

Kopf tries to avoid any written rules and to follow human habits
and intuitive expectations where possible. Therefore:

* Write clear and explanatory commit messages and PR titles.
  Read `How to Write a Git Commit Message <https://chris.beams.io/posts/git-commit/>`_
  for examples.
* Avoid commits' or PRs' prefixes/suffixes with the issues or change types.
  In general, keep the git log clean -- this will later go to the changelogs.
* Sign-off your commits for DCO (see below).

No more other rules.


DCO sign-off
============

All contributions (including pull requests) must agree
to the Developer Certificate of Origin (DCO) version 1.1.
This is the same one created and used by the Linux kernel developers
and posted on http://developercertificate.org/.

This is a developer's certification that they have the right to submit
the patch for inclusion into the project.

Simply submitting a contribution implies this agreement.
However, please include a "Signed-off-by" tag in every patch
(this tag is a conventional way to confirm that you agree to the DCO):

The sign-off can be either written manually or added with ``git commit -s``.
If you contribute often, you can automate this in Kopf's repo with
a [Git hook](https://stackoverflow.com/a/46536244/857383).


Code style
==========

Common sense is the best code formatter.
Blend your code into the surrounding code style.

Kopf does not use and will never use strict code formatters
(at least until they acquire common sense and context awareness).
In case of doubt, adhere to PEP-8 and
[Google Python Style Guide](https://google.github.io/styleguide/pyguide.html).

The line length is 100 characters for code, 80 for docstrings and RsT files.
Long URLs can exceed this length.

For linting, minor code styling, import sorting, layered modules checks, run::

    pre-commit run


Tests
=====

If possible, run the unit-tests locally before submitting
(this will save you some time, but is not mandatory)::

    pytest

If possible, run the functional tests with a realistic local cluster
(for examples, with k3s/k3d on MacOS; Kind and Minikube are also fine)::

    brew install k3d
    k3d cluster create
    pytest --only-e2e

If not possible, create a PR draft instead of a PR,
and check the GitHub Actions' results for unit- & functional tests,
fix as needed, and promote the PR draft into a PR once everything is ready.


Reviews
=======

If possible, refer to an issue for which the PR is created in the PR's body.
You can use one of the existing or closed issues that match your topic best.

The PRs can be reviewed and commented by anyone,
but can be approved only by the project maintainers.



================================================
FILE: docs/daemons.rst
================================================
=======
Daemons
=======

Daemons are a special type of handlers for background logic that accompanies
the Kubernetes resources during their life cycle.

Unlike event-driven short-running handlers declared with ``@kopf.on``,
daemons are started for every individual object when it is created
(or when an operator is started/restarted while the object exists),
and are capable of running indefinitely (or infinitely) long.

The object's daemons are stopped when the object is deleted
or the whole operator is exiting/restarting.


Spawning
========

To have a daemon accompanying a resource of some kind, decorate a function
with ``@kopf.daemon`` and make it run for a long time or forever:

.. code-block:: python

    import asyncio
    import time
    import kopf

    @kopf.daemon('kopfexamples')
    async def monitor_kex_async(**kwargs):
        while True:
            ...  # check something
            await asyncio.sleep(10)

    @kopf.daemon('kopfexamples')
    def monitor_kex_sync(stopped, **kwargs):
        while not stopped:
            ...  # check something
            time.sleep(10)

Synchronous functions are executed in threads, asynchronous functions are
executed directly in the asyncio event loop of the operator -- same as with
regular handlers. See :doc:`async`.

The same executor is used both for regular sync handlers and for sync daemons.
If you expect a large number of synchronous daemons (e.g. for large clusters),
make sure to pre-scale the executor accordingly.
See :doc:`configuration` (:ref:`configure-sync-handlers`).


Termination
===========

The daemons are terminated when either their resource is marked for deletion,
or the operator itself is exiting.

In both cases, the daemons are requested to terminate gracefully by setting
the :kwarg:`stopped` kwarg. The synchronous daemons MUST_, and asynchronous
daemons SHOULD_ check for the value of this flag as often as possible:

.. code-block:: python

    import asyncio
    import kopf

    @kopf.daemon('kopfexamples')
    def monitor_kex(stopped, **kwargs):
        while not stopped:
            time.sleep(1.0)
        print("We are done. Bye.")

The asynchronous daemons can skip these checks if they define the cancellation
timeout. In that case, they can expect an :class:`asyncio.CancelledError`
to be raised at any point of their code (specifically, at any ``await`` clause):

.. code-block:: python

    import asyncio
    import kopf

    @kopf.daemon('kopfexamples', cancellation_timeout=1.0)
    async def monitor_kex(**kwargs):
        try:
            while True:
                await asyncio.sleep(10)
        except asyncio.CancelledError:
            print("We are done. Bye.")

With no cancellation timeout set, cancellation is not performed at all,
as it is unclear for how long should the coroutine be awaited. However,
it is cancelled when the operator exits and stops all "hung" left-over tasks
(not specifically daemons).

.. note::

    The MUST_ / SHOULD_ separation is due to Python having no way to terminate
    a thread unless the thread exits on its own. The :kwarg:`stopped` flag
    is a way to signal the thread it should exit. If :kwarg:`stopped` is not
    checked, the synchronous daemons will run forever or until an error happens.

.. _MUST: https://tools.ietf.org/rfc/rfc2119.txt
.. _SHOULD: https://tools.ietf.org/rfc/rfc2119.txt


Timeouts
========

The termination sequence parameters can be controlled when declaring a daemon:

.. code-block:: python

    import asyncio
    import kopf

    @kopf.daemon('kopfexamples',
                 cancellation_backoff=1.0, cancellation_timeout=3.0)
    async def monitor_kex(stopped, **kwargs):
        while not stopped:
            await asyncio.sleep(1)

There are three stages of how the daemon is terminated:

* 1. Graceful termination:
  * ``stopped`` is set immediately (unconditionally).
  * ``cancellation_backoff`` is awaited (if set).
* 2. Forced termination -- only if ``cancellation_timeout`` is set:
  * :class:`asyncio.CancelledError` is raised (for async daemons only).
  * ``cancellation_timeout`` is awaited (if set).
* 3a. Giving up and abandoning -- only if ``cancellation_timeout`` is set:
  * A :class:`ResourceWarning` is issued for potential OS resource leaks.
  * The finalizer is removed, and the object is released for potential deletion.
* 3b. Forever polling -- only if ``cancellation_timeout`` is not set:
  * The daemon awaiting continues forever, logging from time to time.
  * The finalizer is not removed and the object remains blocked from deletion.

The ``cancellation_timeout`` is measured from the point when the daemon
is cancelled (forced termination begins), not from where the termination
itself begins; i.e., since the moment when the cancellation backoff is over.
The total termination time is ``cancellation_backoff + cancellation_timeout``.

.. warning::

    When the operator is terminating, it has its timeout of 5 seconds
    for all "hung" tasks. This includes the daemons after they are requested
    to finish gracefully and all timeouts are reached.

    If the daemon termination takes longer than this for any reason,
    the daemon will be cancelled (by the operator, not by the daemon guard)
    regardless of the graceful timeout of the daemon. If this does not help,
    the operator will be waiting for all hung tasks until SIGKILL'ed.

.. warning::

    If the operator is running in a cluster, there can be timeouts set for a pod
    (``terminationGracePeriodSeconds``, the default is 30 seconds).

    If the daemon termination is longer than this timeout, the daemons will not
    be finished in full at the operator exit, as the pod will be SIGKILL'ed.

Kopf itself does not set any implicit timeouts for the daemons.
Either design the daemons to exit as fast as possible, or configure
``terminationGracePeriodSeconds`` and cancellation timeouts accordingly.


Safe sleep
==========

For synchronous daemons, it is recommended to use ``stopped.wait()``
instead of ``time.sleep()``: the wait will end when either the time is reached
(as with the sleep), or immediately when the stopped flag is set:

.. code-block:: python

    import kopf

    @kopf.daemon('kopfexamples')
    def monitor_kex(stopped, **kwargs):
        while not stopped:
            stopped.wait(10)

For asynchronous handlers, regular ``asyncio.sleep()`` should be sufficient,
as it is cancellable via :class:`asyncio.CancelledError`. If a cancellation
is neither configured nor desired, ``stopped.wait()`` can be used too
(with ``await``):

.. code-block:: python

    import kopf

    @kopf.daemon('kopfexamples')
    async def monitor_kex(stopped, **kwargs):
        while not stopped:
            await stopped.wait(10)

This way, the daemon will exit as soon as possible when the :kwarg:`stopped`
is set, not when the next sleep is over. Therefore, the sleeps can be of any
duration while the daemon remains terminable (leads to no OS resource leakage).

.. note::

    Synchronous and asynchronous daemons get different types of stop-checker:
    with synchronous and asynchronous interfaces respectively.
    Therefore, they should be used accordingly: without or with ``await``.


Postponing
==========

Normally, daemons are spawned immediately once resource becomes visible
to the operator: i.e. on resource creation or operator startup.

It is possible to postpone the daemon spawning:

.. code-block:: python

    import asyncio
    import kopf

    @kopf.daemon('kopfexamples', initial_delay=30)
    async def monitor_kex(stopped, **kwargs):
        while True:
            await asyncio.sleep(1.0)


The start of the daemon will be delayed by 30 seconds after the resource
creation (or operator startup). For example, this can be used to give some time
for regular event-driven handlers to finish without producing too much activity.


Restarting
==========

It is generally expected that daemons are designed to run forever.
However, a daemon can exit prematurely, i.e. before the resource is deleted
or the operator terminates.

In that case, the daemon will not be restarted again during the lifecycle
of this resource in this operator process (however, it will be spawned again
if the operator restarts). This way, it becomes a long-running equivalent
of on-creation/on-resuming handlers.

To simulate restarting, raise :class:`kopf.TemporaryError` with a delay set.

.. code-block:: python

    import asyncio
    import kopf

    @kopf.daemon('kopfexamples')
    async def monitor_kex(stopped, **kwargs):
        await asyncio.sleep(10.0)
        raise kopf.TemporaryError("Need to restart.", delay=10)

Same as with regular error handling, a delay of ``None`` means instant restart.

See also: :ref:`never-again-filters` to prevent daemons from spawning across
operator restarts.


Deletion prevention
===================

Normally, a finalizer is put on the resource if there are daemons running
for it -- to prevent its actual deletion until all the daemons are terminated.

Only after the daemons are terminated, the finalizer is removed to release
the object for actual deletion.

However, it is possible to have daemons that disobey the exiting signals
and continue running after the timeouts. In that case, the finalizer is
anyway removed, and the orphaned daemons are left to themselves.


Resource fields access
======================

The resource's current state is accessible at any time through regular kwargs
(see :doc:`kwargs`): :kwarg:`body`, :kwarg:`spec`, :kwarg:`meta`,
:kwarg:`status`, :kwarg:`uid`, :kwarg:`name`, :kwarg:`namespace`, etc.

The values are "live views" of the current state of the object as it is being
modified during its lifecycle (not frozen as in the event-driven handlers):

.. code-block:: python

    import random
    import time
    import kopf

    @kopf.daemon('kopfexamples')
    def monitor_kex(stopped, logger, body, spec, **kwargs):
        while not stopped:
            logger.info(f"FIELD={spec['field']}")
            time.sleep(1)

    @kopf.timer('kopfexamples', interval=2.5)
    def modify_kex_sometimes(patch, **kwargs):
        patch.spec['field'] = random.randint(0, 100)

Always access the fields through the provided kwargs, and do not store
them in local variables. Internally, Kopf substitutes the whole object's
body on every external change. Storing the field values to the variables
will remember their value as it was at that moment in time,
and will not be updated as the object changes.


Results delivery
================

As with any other handlers, the daemons can return arbitrary JSON-serializable
values to be put on the resource's status:

.. code-block:: python

    import asyncio
    import kopf

    @kopf.daemon('kopfexamples')
    async def monitor_kex(stopped, **kwargs):
        await asyncio.sleep(10.0)
        return {'finished': True}


Error handling
==============

The error handling is the same as for all other handlers: see :doc:`errors`:

.. code-block:: python

    @kopf.daemon('kopfexamples',
                 errors=kopf.ErrorsMode.TEMPORARY, backoff=1, retries=10)
    def monitor_kex(retry, **_):
        if retry < 3:
            raise kopf.TemporaryError("I'll be back!", delay=1)
        elif retry < 5:
            raise EnvironmentError("Something happened!")
        else:
            raise kopf.PermanentError("Bye-bye!")

If a permanent error is raised, the daemon will never be restarted again.
Same as when the daemon exits on its own (but this could be reconsidered
in the future).


Filtering
=========

It is also possible to use the existing :doc:`filters`
to only spawn daemons for specific resources:

.. code-block:: python

    import time
    import kopf

    @kopf.daemon('kopfexamples',
                 annotations={'some-annotation': 'some-value'},
                 labels={'some-label': 'some-value'},
                 when=lambda name, **_: 'some' in name)
    def monitor_selected_kexes(stopped, **kwargs):
        while not stopped:
            time.sleep(1)

Other (non-matching) resources of that kind will be ignored.

The daemons will be executed only while the filtering criteria are met.
Both the resource's state and the criteria can be highly dynamic (e.g.
due to ``when=`` callable filters or labels/annotations value callbacks).

Once the daemon stops matching the criteria (either because the resource
or the criteria have been changed (e.g. for ``when=`` callbacks)),
the daemon is stopped. Once it matches the criteria again, it is re-spawned.

The checking is done only when the resource changes (any watch-event arrives).
The criteria themselves are not re-evaluated if nothing changes.

.. warning::

    A daemon that is terminating is considered as still running, therefore
    it will not be re-spawned until it fully terminates. It will be re-spawned
    the next time a watch-event arrives after the daemon has truly exited.


System resources
================

.. warning::

    A separate OS thread or asyncio task is started
    for each resource and each handler.

    Having hundreds or thousands of OS threads or asyncio tasks can consume
    system resources significantly. Make sure you only have daemons and timers
    with appropriate filters (e.g., by labels, annotations, or so).

    For the same reason, prefer to use async handlers (with properly designed
    async/await code), since asyncio tasks are somewhat cheaper than threads.
    See :doc:`async` for details.



================================================
FILE: docs/deployment-depl.yaml
================================================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kopfexample-operator
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      application: kopfexample-operator
  template:
    metadata:
      labels:
        application: kopfexample-operator
    spec:
      serviceAccountName: kopfexample-account
      containers:
      - name: the-only-one
        image: nolar/kopf-operator



================================================
FILE: docs/deployment-rbac.yaml
================================================
---
apiVersion: v1
kind: ServiceAccount
metadata:
  namespace: "{{NAMESPACE}}"
  name: kopfexample-account
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: kopfexample-role-cluster
rules:

  # Framework: knowing which other operators are running (i.e. peering).
  - apiGroups: [kopf.dev]
    resources: [clusterkopfpeerings]
    verbs: [list, watch, patch, get]

  # Framework: runtime observation of namespaces & CRDs (addition/deletion).
  - apiGroups: [apiextensions.k8s.io]
    resources: [customresourcedefinitions]
    verbs: [list, watch]
  - apiGroups: [""]
    resources: [namespaces]
    verbs: [list, watch]

  # Framework: admission webhook configuration management.
  - apiGroups: [admissionregistration.k8s.io/v1, admissionregistration.k8s.io/v1beta1]
    resources: [validatingwebhookconfigurations, mutatingwebhookconfigurations]
    verbs: [create, patch]

  # Application: read-only access for watching cluster-wide.
  - apiGroups: [kopf.dev]
    resources: [kopfexamples]
    verbs: [list, watch]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: "{{NAMESPACE}}"
  name: kopfexample-role-namespaced
rules:

  # Framework: knowing which other operators are running (i.e. peering).
  - apiGroups: [kopf.dev]
    resources: [kopfpeerings]
    verbs: [list, watch, patch, get]

  # Framework: posting the events about the handlers progress/errors.
  - apiGroups: [""]
    resources: [events]
    verbs: [create]

  # Application: watching & handling for the custom resource we declare.
  - apiGroups: [kopf.dev]
    resources: [kopfexamples]
    verbs: [list, watch, patch]

  # Application: other resources it produces and manipulates.
  # Here, we create Jobs+PVCs+Pods, but we do not patch/update/delete them ever.
  - apiGroups: [batch, extensions]
    resources: [jobs]
    verbs: [create]
  - apiGroups: [""]
    resources: [pods, persistentvolumeclaims]
    verbs: [create]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: kopfexample-rolebinding-cluster
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: kopfexample-role-cluster
subjects:
  - kind: ServiceAccount
    name: kopfexample-account
    namespace: "{{NAMESPACE}}"
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  namespace: "{{NAMESPACE}}"
  name: kopfexample-rolebinding-namespaced
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: kopfexample-role-namespaced
subjects:
  - kind: ServiceAccount
    name: kopfexample-account



================================================
FILE: docs/deployment.rst
================================================
==========
Deployment
==========

Kopf can be executed out of the cluster, as long as the environment is
authenticated to access the Kubernetes API.
But normally, the operators are usually deployed directly to the clusters.


Docker image
============

First of all, the operator must be packaged as a docker image with Python 3.9 or newer:

.. code-block:: dockerfile
    :caption: Dockerfile
    :name: dockerfile

    FROM python:3.13
    RUN pip install kopf
    ADD . /src
    CMD kopf run /src/handlers.py --verbose

Build and push it to some repository of your choice.
Here, we will use DockerHub_
(with a personal account "nolar" -- replace it with your name or namespace;
you may also want to add the versioning tags instead of the implied "latest"):

.. code-block:: bash

    docker build -t nolar/kopf-operator .
    docker push nolar/kopf-operator

.. _DockerHub: https://hub.docker.com/

.. seealso::
    Read `DockerHub documentation <https://docs.docker.com/docker-hub/>`_
    for how to use it to push & pull the docker images.


Cluster deployment
==================

The best way to deploy the operator to the cluster is via the Deployment_
object: in that case, it will be properly maintained alive and the versions
will be properly upgraded on the re-deployments.

.. _Deployment: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/

For this, create the deployment file:

.. literalinclude:: deployment-depl.yaml
    :language: yaml
    :emphasize-lines: 6,18
    :caption: deployment.yaml
    :name: deployment-yaml

Please note that there is only one replica. Keep it so. If there will be
two or more operators running in the cluster for the same objects,
they will collide with each other and the consequences are unpredictable.
In case of pod restarts, only one pod should be running at a time too:
use ``.spec.strategy.type=Recreate`` (see the documentation_).

.. _documentation: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#recreate-deployment

Deploy it to the cluster:

.. code-block:: bash

    kubectl apply -f deployment.yaml

No services or ingresses are needed (unlike in the typical web-app examples),
as the operator is not listening for any incoming connections,
but only makes the outcoming calls to the Kubernetes API.


RBAC
====

The pod where the operator runs must have the permissions to access
and to manipulate the objects, both domain-specific and the built-in ones.
For the example operator, those are:

* ``kind: ClusterKopfPeering`` for the cross-operator awareness (cluster-wide).
* ``kind: KopfPeering`` for the cross-operator awareness (namespace-wide).
* ``kind: KopfExample`` for the example operator objects.
* ``kind: Pod/Job/PersistentVolumeClaim`` as the children objects.
* And others as needed.

For that, the RBAC_ (Role-Based Access Control) could be used
and attached to the operator's pod via a service account.

.. _RBAC: https://kubernetes.io/docs/reference/access-authn-authz/rbac/

Here is an example of what an RBAC config should look like
(remove the parts which are not needed: e.g. the cluster roles/bindings
for the strictly namespace-bound operator):

.. literalinclude:: deployment-rbac.yaml
    :caption: rbac.yaml
    :name: rbac-yaml
    :language: yaml

And the created service account is attached to the pods as follows:

.. literalinclude:: deployment-depl.yaml
    :language: yaml
    :lines: 1-2,5,12,16-20
    :emphasize-lines: 17
    :caption: deployment.yaml
    :name: deployment-service-account-yaml


Please note that the service accounts are always namespace-scoped.
There are no cluster-wide service accounts.
They must be created in the same namespace as the operator is going to run in
(even if it is going to serve the whole cluster).



================================================
FILE: docs/embedding.rst
================================================
=========
Embedding
=========

Kopf is designed to be embeddable into other applications, which require
watching over the Kubernetes resources (custom or built-in), and handling
the changes.
This can be used, for example, in desktop applications or web APIs/UIs
to keep the state of the cluster and its resources in memory.


Manual execution
================

Since Kopf is fully asynchronous, the best way to run Kopf is to provide
an event-loop in a separate thread, which is dedicated to Kopf,
while running the main application in the main thread:

.. code-block:: python

    import asyncio
    import threading

    import kopf

    @kopf.on.create('kopfexamples')
    def create_fn(**_):
        pass

    def kopf_thread():
        asyncio.run(kopf.operator())

    def main():
        thread = threading.Thread(target=kopf_thread)
        thread.start()
        # ...
        thread.join()

In the case of :command:`kopf run`, the main application is Kopf itself,
so its event-loop runs in the main thread.

.. note::
    When an asyncio task runs not in the main thread, it cannot set
    the OS signal handlers, so a developer should implement the termination
    themselves (cancellation of an operator task is enough).


Manual orchestration
====================

Alternatively, a developer can orchestrate the operator's tasks and sub-tasks
themselves. The example above is an equivalent of the following:

.. code-block:: python

    def kopf_thread():
        loop = asyncio.get_event_loop_policy().get_event_loop()
        tasks = loop.run_until_complete(kopf.spawn_tasks())
        loop.run_until_complete(kopf.run_tasks(tasks, return_when=asyncio.FIRST_COMPLETED))

Or, if proper cancellation and termination are not expected, of the following:

.. code-block:: python

    def kopf_thread():
        loop = asyncio.get_event_loop_policy().get_event_loop()
        tasks = loop.run_until_complete(kopf.spawn_tasks())
        loop.run_until_complete(asyncio.wait(tasks))

In all cases, make sure that asyncio event loops are properly used.
Specifically, :func:`asyncio.run` creates and finalises a new event loop
for a single call. Several calls cannot share the coroutines and tasks.
To make several calls, either create a new event loop, or get the event loop
of the current asyncio _context_ (by default, of the current thread).
See more on the asyncio event loops and _contexts_ in `Asyncio Policies`__.

__ https://docs.python.org/3/library/asyncio-policy.html

.. _custom-event-loops:


Custom event loops
==================

Kopf can run in any AsyncIO-compatible event loop. For example, uvloop `claims to be 2x–2.5x times faster`__ than asyncio. To run Kopf in uvloop, call it this way:

__ http://magic.io/blog/uvloop-blazing-fast-python-networking/

.. code-block:: python

    import kopf
    import uvloop

    def main():
        loop = uvloop.EventLoopPolicy().get_event_loop()
        loop.run(kopf.operator())

Or this way:

.. code-block:: python

    import kopf
    import uvloop

    def main():
        kopf.run(loop=uvloop.EventLoopPolicy().new_event_loop())

Or this way:

.. code-block:: python

    import kopf
    import uvloop

    def main():
        asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())
        kopf.run()

Or any other way the event loop prescribes in its documentation.

Kopf's CLI (i.e. :command:`kopf run`) will use uvloop by default if it is installed. To disable this implicit behaviour, either uninstall uvloop from Kopf's environment, or run Kopf explicitly from the code using the standard event loop.

For convenience, Kopf can be installed as ``pip install kopf[uvloop]`` to enable this mode automatically.

Kopf will never implicitly activate the custom event loops if it is called from the code, not from the CLI.


Multiple operators
==================

Kopf can handle multiple resources at a time, so only one instance should be
sufficient for most cases. However, it can be needed to run multiple isolated
operators in the same process.

It should be safe to run multiple operators in multiple isolated event-loops.
Despite Kopf's routines use the global state, all such a global state is stored
in :mod:`contextvars` containers with values isolated per-loop and per-task.

.. code-block:: python

    import asyncio
    import threading

    import kopf

    registry = kopf.OperatorRegistry()

    @kopf.on.create('kopfexamples', registry=registry)
    def create_fn(**_):
        pass

    def kopf_thread():
        asyncio.run(kopf.operator(
            registry=registry,
        ))

    def main():
        thread = threading.Thread(target=kopf_thread)
        thread.start()
        # ...
        thread.join()


.. warning::
    It is not recommended to run Kopf in the same event-loop as other routines
    or applications: it considers all tasks in the event-loop as spawned by its
    workers and handlers, and cancels them when it exits.

    There are some basic safety measures to not cancel tasks existing prior
    to the operator's startup, but that cannot be applied to the tasks spawned
    later due to asyncio implementation details.



================================================
FILE: docs/errors.rst
================================================
==============
Error handling
==============

Kopf tracks the status of the handlers (except for the low-level event handlers)
catches the exceptions, and processes them from each of the handlers.

The last (or the final) exception is stored in the object's status,
and reported via the object's events.

.. note::
    Keep in mind, the Kubernetes events are often garbage-collected fast,
    e.g. less than 1 hour, so they are visible only soon after they are added.
    For persistence, the errors are also stored on the object's status.


Temporary errors
================

If an exception raised inherits from :class:`kopf.TemporaryError`,
it will postpone the current handler for the next iteration,
which can happen either immediately, or after some delay::

    import kopf

    @kopf.on.create('kopfexamples')
    def create_fn(spec, **_):
        if not is_data_ready():
            raise kopf.TemporaryError("The data is not yet ready.", delay=60)

In that case, there is no need to sleep in the handler explicitly, thus blocking
any other events, causes, and generally any other handlers on the same object
from being handled (such as deletion or parallel handlers/sub-handlers).

.. note::
    The multiple handlers and the sub-handlers are implemented via this
    kind of errors: if there are handlers left after the current cycle,
    a special retriable error is raised, which marks the current cycle
    as to be retried immediately, where it continues with the remaining
    handlers.

    The only difference is that this special case produces fewer logs.


Permanent errors
================

If a raised exception inherits from :class:`kopf.PermanentError`, the handler
is considered as non-retriable and non-recoverable and completely failed.

Use this when the domain logic of the application means that there
is no need to retry over time, as it will not become better::

    import kopf

    @kopf.on.create('kopfexamples')
    def create_fn(spec, **_):
        valid_until = datetime.datetime.fromisoformat(spec['validUntil'])
        if valid_until <= datetime.datetime.now(datetime.timezone.utc):
            raise kopf.PermanentError("The object is not valid anymore.")

See also: :ref:`never-again-filters` to prevent handlers from being invoked
for the future change-sets even after the operator restarts.


Regular errors
==============

Kopf assumes that any arbitrary errors
(i.e. not :class:`kopf.TemporaryError` and not :class:`kopf.PermanentError`)
are the environment's issues and can self-resolve after some time.

As such, as default behaviour,
Kopf retries the handlers with arbitrary errors
infinitely until the handlers either succeed or fail permanently.

The reaction to the arbitrary errors can be configured::

    import kopf

    @kopf.on.create('kopfexamples', errors=kopf.ErrorsMode.PERMANENT)
    def create_fn(spec, **_):
        raise Exception()

Possible values of ``errors`` are:

* ``kopf.ErrorsMode.TEMPORARY`` (the default).
* ``kopf.ErrorsMode.PERMANENT`` (prevent retries).
* ``kopf.ErrorsMode.IGNORED`` (same as in the resource watching handlers).


Timeouts
========

The overall runtime of the handler can be limited::

    import kopf

    @kopf.on.create('kopfexamples', timeout=60*60)
    def create_fn(spec, **_):
        raise kopf.TemporaryError(delay=60)

If the handler is not succeeded within this time, it is considered
as fatally failed.

If the handler is an async coroutine and it is still running at the moment,
an :class:`asyncio.TimeoutError` is raised;
there is no equivalent way of terminating the synchronous functions by force.

By default, there is no timeout, so the retries continue forever.


Retries
=======

The number of retries can be limited too::

    import kopf

    @kopf.on.create('kopfexamples', retries=3)
    def create_fn(spec, **_):
        raise Exception()

Once the number of retries is reached, the handler fails permanently.

By default, there is no limit, so the retries continue forever.


Backoff
=======

The interval between retries on arbitrary errors, when an external environment
is supposed to recover and be able to succeed the handler execution,
can be configured::

    import kopf

    @kopf.on.create('kopfexamples', backoff=30)
    def create_fn(spec, **_):
        raise Exception()

The default is 60 seconds.

.. note::

    This only affects the arbitrary errors. When `TemporaryError`
    is explicitly used, the delay should be configured with ``delay=...``.



================================================
FILE: docs/events.rst
================================================
======
Events
======

.. warning::
    Kubernetes itself contains a terminology conflict:
    There are *events* when watching over the objects/resources,
    such as in ``kubectl get pod --watch``.
    And there are *events* as a built-in object kind,
    as shown in ``kubectl describe pod ...`` in the "Events" section.
    In this documentation, they are distinguished as "watch-events"
    and "k8s-events". This section describes k8s-events only.

Handled objects
===============

.. todo:: the ``body`` arg must be optional, meaning the currently handled object.

Kopf provides some tools to report arbitrary information
for the handled objects as Kubernetes events::

    import kopf

    @kopf.on.create('kopfexamples')
    def create_fn(body, **_):
        kopf.event(body,
                   type='SomeType',
                   reason='SomeReason',
                   message='Some message')

The type and reason are arbitrary and can be anything.
Some restrictions apply (e.g. no spaces).
The message is also arbitrary free-text.
However, newlines are not rendered nicely
(they break the whole output of ``kubectl``).

For convenience, a few shortcuts are provided to mimic the Python's ``logging``::

    import kopf

    @kopf.on.create('kopfexamples')
    def create_fn(body, **_):
        kopf.warn(body, reason='SomeReason', message='Some message')
        kopf.info(body, reason='SomeReason', message='Some message')
        try:
            raise RuntimeError("Exception text.")
        except:
            kopf.exception(body, reason="SomeReason", message="Some exception:")

These events are seen in the output of:

.. code-block:: bash

    kubectl describe kopfexample kopf-example-1

.. code-block:: none

    ...
    Events:
      Type      Reason      Age   From  Message
      ----      ------      ----  ----  -------
      Normal    SomeReason  5s    kopf  Some message
      Normal    Success     5s    kopf  Handler create_fn succeeded.
      SomeType  SomeReason  6s    kopf  Some message
      Normal    Finished    5s    kopf  All handlers succeeded.
      Error     SomeReason  5s    kopf  Some exception: Exception text.
      Warning   SomeReason  5s    kopf  Some message


Other objects
=============

.. todo:: kubernetes and pykube objects should be accepted natively, not only the dicts.

Events can be also attached to other objects, not only those handled
at the moment (and not event the children)::

    import kopf
    import kubernetes

    @kopf.on.create('kopfexamples')
    def create_fn(name, namespace, uid, **_):

        pod = kubernetes.client.V1Pod()
        api = kubernetes.client.CoreV1Api()
        obj = api.create_namespaced_pod(namespace, pod)

        msg = f"This pod is created by KopfExample {name}"
        kopf.info(obj.to_dict(), reason='SomeReason', message=msg)

.. note::
    Events are not persistent.
    They are usually garbage-collected after some time, e.g. one hour.
    All the reported information must be only for short-term use.


Events for events
=================

As a rule of thumb, it is impossible to create "events for events".

No error will be raised. The event creation will be silently skipped.

As the primary purpose, this is done to prevent "event explosions"
when handling the core v1 events, which creates new core v1 events,
causing more handling, so on (similar to "fork-bombs").
Such cases are possible, for example, when using ``kopf.EVERYTHING``
(globally or for the v1 API), or when explicitly handling the core v1 events.

As a side-effect, "events for events" are also silenced when manually created
via :func:`kopf.event`, :func:`kopf.info`, :func:`kopf.warn`, etc.



================================================
FILE: docs/filters.rst
================================================
=========
Filtering
=========

Handlers can be restricted to only the resources that match certain criteria.

Multiple criteria are joined with AND, i.e. they all must be satisfied.

Unless stated otherwise, the described filters are available for all handlers:
resuming, creation, deletion, updating, event-watching, timers, daemons,
or even to sub-handlers (thus eliminating some checks in its parent's code).

There are only a few kinds of checks:

* Specific values -- expressed with Python literals such as ``"a string"``.
* Presence of values -- with special markers ``kopf.PRESENT/kopf.ABSENT``.
* Per-value callbacks -- with anything callable which evaluates to true/false.
* Whole-body callbacks -- with anything callable which evaluates to true/false.

But there are multiple places where these checks can be applied,
each has its specifics.


Metadata filters
================

Metadata is the most commonly filtered aspect of the resources.

Match only when the resource's label or annotation has a specific value:

.. code-block:: python

    @kopf.on.create('kopfexamples',
                    labels={'some-label': 'somevalue'},
                    annotations={'some-annotation': 'somevalue'})
    def my_handler(spec, **_):
        pass

Match only when the resource has a label or an annotation with any value:

.. code-block:: python

    @kopf.on.create('kopfexamples',
                    labels={'some-label': kopf.PRESENT},
                    annotations={'some-annotation': kopf.PRESENT})
    def my_handler(spec, **_):
        pass

Match only when the resource has no label or annotation with that name:

.. code-block:: python

    @kopf.on.create('kopfexamples',
                    labels={'some-label': kopf.ABSENT},
                    annotations={'some-annotation': kopf.ABSENT})
    def my_handler(spec, **_):
        pass

Note that empty strings in labels and annotations are treated as regular values,
i.e. they are considered as present on the resource.


Field filters
=============

Specific fields can be checked for specific values or presence/absence,
similar to the metadata filters:

.. code-block:: python

    @kopf.on.create('kopfexamples', field='spec.field', value='world')
    def created_with_world_in_field(**_):
        pass

    @kopf.on.create('kopfexamples', field='spec.field', value=kopf.PRESENT)
    def created_with_field(**_):
        pass

    @kopf.on.create('kopfexamples', field='spec.no-field', value=kopf.ABSENT)
    def created_without_field(**_):
        pass

When the ``value=`` filter is not specified, but the ``field=`` filter is,
it is equivalent to ``value=kopf.PRESENT``, i.e. the field must be present
with any value (for update handlers: present before or after the change).

.. code-block:: python

    @kopf.on.create('kopfexamples', field='spec.field')
    def created_with_field(**_):
        pass

    @kopf.on.update('kopfexamples', field='spec.field')
    def field_is_affected(old, new, **_):
        pass


Since the field name is part of the handler id (e.g., ``"fn/spec.field"``),
multiple decorators can be defined to react to different fields with the same
function and it will be invoked multiple times with different old & new values
relevant to the specified fields, so as different values of :kwarg:`param`:

.. code-block:: python

    @kopf.on.update('kopfexamples', field='spec.field', param='fld')
    @kopf.on.update('kopfexamples', field='spec.items', param='itm')
    def one_of_the_fields_is_affected(old, new, **_):
        pass

However, different causes --mostly resuming + one of creation/update/deletion--
will not be distinguished, so e.g. resume+create pair with the same field
will be called only once.

Due to the special nature of update handlers (``@on.update``, ``@on.field``),
described in a note below, this filtering semantics is extended for them:

The ``field=`` filter restricts the update-handlers to cases when the specified
field is in any way affected: changed, added or removed to/from the resource.
When the specified field is not affected, but something else is changed,
such update-handlers are not invoked even if they do match the field criteria.

The ``value=`` filter applies to either the old or the new value:
i.e. if any of them satisfies the value criterion. This covers both sides
of the state transition: when the value criterion has just been satisfied
(though was not satisfied before), or when the value criterion was satisfied
before (but stopped being satisfied). For the latter case, it means that
the transitioning resource still satisfies the filter in its "old" state.

.. note::

    **Technically,** the update handlers are called after the change has already
    happened on the low level -- i.e. when the field already has the new value.

    **Semantically,** the update handlers are only initiated by this change,
    but are executed before the current (new) state is processed and persisted,
    thus marking the end of the change processing cycle -- i.e. they are called
    in-between the old and new states, and therefore belong to both of them.

    **In general,** the resource-changing handlers are an abstraction on top
    of the low-level K8s machinery for eventual processing of such state
    transitions, so their semantics can differ from K8s's low-level semantics.
    In most cases, this is not visible or important to the operator developers,
    except for such cases, where it might affect the semantics of e.g. filters.

For reacting to *unrelated* changes of other fields while this field
satisfies the criterion, use ``when=`` instead of ``field=/value=``.

For reacting to only the cases when the desired state is reached
but not when the desired state is lost, use ``new=`` with the same criterion;
similarly, for the cases when the desired state is only lost, use ``old=``.

For all other handlers with no concept of "updating" and being in-between of
two equally valid and applicable states, the ``field=/value=`` filters
check the resource in its current --and the only-- state.
The handlers are being invoked and the daemons are running
as long as the field and the value match the criterion.


Change filters
==============

The update handlers (specifically, ``@kopf.on.update`` and ``@kopf.on.field``)
check the ``value=`` filter against both old & new values,
which might be not what is intended.
For more precision on filtering, the old/new values
can be checked separately with the ``old=/new=`` filters
with the same filtering methods/markers as all other filters.

.. code-block:: python

    @kopf.on.update('kopfexamples', field='spec.field', old='x', new='y')
    def field_is_edited(**_):
        pass

    @kopf.on.update('kopfexamples', field='spec.field', old=kopf.ABSENT, new=kopf.PRESENT)
    def field_is_added(**_):
        pass

    @kopf.on.update('kopfexamples', field='spec.field', old=kopf.PRESENT, new=kopf.ABSENT)
    def field_is_removed(**_):
        pass

If one of ``old=`` or ``new=`` is not specified (or set to ``None``),
that part is not checked, but the other (specified) part is still checked:

*Match when the field reaches a specific value either by being edited/patched
to it or by adding it to the resource (i.e. regardless of the old value):*

.. code-block:: python

    @kopf.on.update('kopfexamples', field='spec.field', new='world')
    def hello_world(**_):
        pass

*Match when the field loses a specific value either by being edited/patched
to something else, or by removing the field from the resource:*

.. code-block:: python

    @kopf.on.update('kopfexamples', field='spec.field', old='world')
    def goodbye_world(**_):
        pass

Generally, the update handlers with ``old=/new=`` filters are invoked only when
the field's value is changed, and are not invoked when it remains the same.

For clarity, "a change" means not only an actual change of the value,
but also a change in the field's presence or absence in the resource.

If none of the ``old=/new=/value=`` filters is specified, the handler is invoked
if the field is affected in any way, i.e. if it was modified, added, or removed.
This is the same behaviour as with the unspecified ``value=`` filter.

.. note::

    ``value=`` is currently made to be mutually exclusive with ``old=/new=``:
    only one filtering method can be used; if both methods are used together,
    it would be ambiguous. This can be reconsidered in the future.


Value callbacks
===============

Instead of specific values or special markers, all the value-based filters can
use arbitrary per-value callbacks (as an advanced use-case for advanced logic).

The value callbacks must receive the same :doc:`keyword arguments <kwargs>`
as the respective handlers (with ``**kwargs/**_`` for forwards compatibility),
plus one *positional* (not keyword!) argument with the value being checked.
The passed value will be ``None`` if the value is absent in the resource.

.. code-block:: python

    def check_value(value, spec, **_):
        return value == 'some-value' and spec.get('field') is not None

    @kopf.on.create('kopfexamples',
                    labels={'some-label': check_value},
                    annotations={'some-annotation': check_value})
    def my_handler(spec, **_):
        pass


Callback filters
================

The resource callbacks must receive the same :doc:`keyword arguments <kwargs>`
as the respective handlers (with ``**kwargs/**_`` for forwards compatibility).

.. code-block:: python

    def is_good_enough(spec, **_):
        return spec.get('field') in spec.get('items', [])

    @kopf.on.create('kopfexamples', when=is_good_enough)
    def my_handler(spec, **_):
        pass

    @kopf.on.create('kopfexamples', when=lambda spec, **_: spec.get('field') in spec.get('items', []))
    def my_handler(spec, **_):
        pass

There is no need for the callback filters to only check the resource's content.
They can filter by any kwarg data, e.g. by a :kwarg:`reason` of this invocation,
remembered :kwarg:`memo` values, etc. However, it is highly recommended that
the filters do not modify the state of the operator -- keep it for handlers.


Callback helpers
================

Kopf provides several helpers to combine multiple callbacks into one
(the semantics is the same as for Python's built-in functions):

.. code-block:: python

    import kopf

    def whole_fn1(name, **_): return name.startswith('kopf-')
    def whole_fn2(spec, **_): return spec.get('field') == 'value'
    def value_fn1(value, **_): return value.startswith('some')
    def value_fn2(value, **_): return value.endswith('label')

    @kopf.on.create('kopfexamples',
                    when=kopf.all_([whole_fn1, whole_fn2]),
                    labels={'somelabel': kopf.all_([value_fn1, value_fn2])})
    def create_fn1(**_):
        pass

    @kopf.on.create('kopfexamples',
                    when=kopf.any_([whole_fn1, whole_fn2]),
                    labels={'somelabel': kopf.any_([value_fn1, value_fn2])})
    def create_fn2(**_):
        pass

The following wrappers are available:

* ``kopf.not_(fn)`` -- the function must return ``False`` to pass the filters.
* ``kopf.any_([...])`` -- at least one of the functions must return ``True``.
* ``kopf.all_([...])`` -- all of the functions must return ``True``.
* ``kopf.none_([...])`` -- all of the functions must return ``False``.


Stealth mode
============

.. note::

    Please note that if an object does not match any filters of any handlers
    for its resource kind, there will be no messages logged and no annotations
    stored on the object. Such objects are processed in the stealth mode
    even if the operator technically sees them in the watch-stream.

    As the result, when the object is updated to match the filters some time
    later (e.g. by putting labels/annotations on it, or changing its spec),
    this will not be considered as an update but as a creation.

    From the operator's point of view, the object has suddenly appeared
    in sight with no diff-base, which means that it is a newly created object;
    so, the on-creation handlers will be called instead of the on-update ones.

    This behaviour is correct and reasonable from the filtering logic side.
    If this is a problem, then create a dummy handler without filters
    (e.g. a field-handler for a non-existent field) --
    this will make all the objects always being in the scope of the operator,
    even if the operator did not react to their creation/update/deletion,
    and so the diff-base annotations ("last-handled-configuration", etc)
    will be always added on the actual object creation, not on scope changes.



================================================
FILE: docs/handlers.rst
================================================
========
Handlers
========

.. todo:: Multiple handlers per script.

Handlers are Python functions with the actual behaviour
of the custom resources.

They are called when any custom resource (within the scope of the operator)
is created, modified, or deleted.

Any operator built with Kopf is based on handlers.


Events & Causes
===============

Kubernetes only notifies when something is changed in the object,
but it does not clarify what was changed.

More on that, since Kopf stores the state of the handlers on the object itself,
these state changes also cause the events, which are seen by the operators
and any other watchers.

To hide the complexity of the state storing, Kopf provides a cause detection:
whenever an event happens for the object, the framework detects what happened
actually, as follows:

* Was the object just created?
* Was the object deleted (marked for deletion)?
* Was the object edited, and which fields specifically were edited,
  from what old values into what new values?

These causes, in turn, trigger the appropriate handlers, passing the detected
information to the keyword arguments.


Registering
===========

To register a handler for an event, use the ``@kopf.on`` decorator::

    import kopf

    @kopf.on.create('kopfexamples')
    def my_handler(spec, **_):
        pass

All available decorators are described below.

Kopf only supports simple functions and static methods as handlers.
Class and instance methods are not supported.
For explanation and rationale, see the discussion in `#849`__ (briefly:
the semantics of handlers is vague when multiple instances exist or
multiple sub-classes inherit from the class, thus inheriting the handlers).

__ https://github.com/nolar/kopf/issues/849

Would you still want to use classes for namespacing, register the handlers
by using Kopf's decorators explicitly for specific instances/sub-classes thus
resolving the mentioned vagueness and giving the meaning to ``self``/``cls``::

    import kopf

    class MyCls:
        def my_handler(self, spec, **kwargs):
            print(repr(self))

    instance = MyCls()
    kopf.on.create('kopfexamples')(instance.my_handler)


Event-watching handlers
=======================

Low-level events can be intercepted and handled silently, without
storing the handlers' status (errors, retries, successes) on the object.

This can be useful if the operator needs to watch over the objects
of another operator or controller, without adding its data.

The following event-handler is available::

    import kopf

    @kopf.on.event('kopfexamples')
    def my_handler(event, **_):
        pass

If the event handler fails, the error is logged to the operator's log,
and then ignored.


.. note::
    Please note that the event handlers are invoked for *every* event received
    from the watching stream. This also includes the first-time listing when
    the operator starts or restarts.

    It is the developer's responsibility to make the handlers idempotent
    (re-executable with no duplicating side-effects).


State-changing handlers
=======================

Kopf goes further and beyond: it detects the actual causes of these events,
i.e. what happened to the object:

* Was the object just created?
* Was the object deleted (marked for deletion)?
* Was the object edited, and which fields specifically were edited,
  from which old values to which new values?

.. note::
    Worth noting that Kopf stores the status of the handlers, such as their
    progress or errors or retries, in the object itself (``.status`` stanza),
    which triggers its low-level events, but these events are not detected
    as separate causes, as there is nothing changed *essentially*.

The following 3 core cause-handlers are available::

    import kopf

    @kopf.on.create('kopfexamples')
    def my_handler(spec, **_):
        pass

    @kopf.on.update('kopfexamples')
    def my_handler(spec, old, new, diff, **_):
        pass

    @kopf.on.delete('kopfexamples')
    def my_handler(spec, **_):
        pass

.. note::
    Kopf's finalizers will be added to the object when there are delete
    handlers specified. Finalizers block Kubernetes from fully deleting
    objects and Kubernetes will only actually delete objects when all
    finalizers are removed, i.e. only if the Kopf operator is running to
    remove them (check: :ref:`finalizers-blocking-deletion` for a workaround).
    If a delete handler is added but finalizers are not required to block the
    actual deletion, i.e. the handler is optional, the optional argument
    ``optional=True`` can be passed to the delete cause decorator.


Resuming handlers
=================

A special kind of handlers can be used for cases when the operator restarts
and detects an object that existed before::

    @kopf.on.resume('kopfexamples')
    def my_handler(spec, **_):
        pass

This handler can be used to start threads or asyncio tasks or to update
a global state to keep it consistent with the actual state of the cluster.
With the resuming handler in addition to creation/update/deletion handlers,
no object will be left unattended even if it does not change over time.

The resuming handlers are guaranteed to execute only once per operator
lifetime for each resource object (except if errors are retried).

Normally, the resume handlers are mixed-in to the creation and updating
handling cycles, and are executed in the order they are declared.

It is a common pattern to declare both creation and resuming handler
pointing to the same function, so that this function is called either
when an object is created ("started) while the operator is alive ("exists"), or
when the operator is started ("created") when the object is existent ("alive")::

    @kopf.on.resume('kopfexamples')
    @kopf.on.create('kopfexamples')
    def my_handler(spec, **_):
        pass

However, the resuming handlers are **not** called if the object has been deleted
during the operator downtime or restart, and the deletion handlers are now
being invoked.

This is done intentionally to prevent the cases when the resuming handlers start
threads/tasks or allocate the resources, and the deletion handlers stop/free
them: it can happen so that the resuming handlers would be executed after
the deletion handlers, thus starting threads/tasks and never stopping them.
For example::

    TASKS = {}

    @kopf.on.delete('kopfexamples')
    async def my_handler(spec, name, **_):
        if name in TASKS:
            TASKS[name].cancel()

    @kopf.on.resume('kopfexamples')
    @kopf.on.create('kopfexamples')
    def my_handler(spec, **_):
        if name not in TASKS:
            TASKS[name] = asyncio.create_task(some_coroutine(spec))

In this example, if the operator starts and notices an object that is marked
for deletion, the deletion handler will be called, but the resuming handler
is not called at all, despite the object was noticed to exist out there.
Otherwise, there would be a resource (e.g. memory) leak.

If the resume handlers are still desired during the deletion handling, they
can be explicitly marked as compatible with the deleted state of the object
with ``deleted=True`` option::

    @kopf.on.resume('kopfexamples', deleted=True)
    def my_handler(spec, **_):
        pass

In that case, both the deletion and resuming handlers will be invoked. It is
the developer's responsibility to ensure this does not lead to memory leaks.


Field handlers
==============

Specific fields can be handled instead of the whole object::

    import kopf

    @kopf.on.field('kopfexamples', field='spec.somefield')
    def somefield_changed(old, new, **_):
        pass

There is no special detection of the causes for the fields,
such as create/update/delete, so the field-handler is efficient
only when the object is updated.


.. _subhandlers:

Sub-handlers
============

.. warning::
    Sub-handlers are an advanced topic. Please, make sure you understand
    the regular handlers first, so as the handling cycle of the framework.

A common example for this feature are the lists defined in the spec,
each of which should be handled with a handler-like approach
rather than explicitly -- i.e. with the error tracking, retries, logging,
progress and status reporting, etc.

This can be used with dynamically created functions, such as lambdas,
partials (`functools.partial`), or the inner functions in the closures:

.. code-block:: yaml

    spec:
      items:
        - item1
        - item2

Sub-handlers can be implemented either imperatively
(where it requires :doc:`asynchronous handlers <async>` and ``async/await``)::

    import functools
    import kopf

    @kopf.on.create('kopfexamples')
    async def create_fn(spec, **_):
        fns = {}

        for item in spec.get('items', []):
            fns[item] = functools.partial(handle_item, item=item)

       await kopf.execute(fns=fns)

    def handle_item(item, *, spec, **_):
        pass

Or declaratively with decorators::

    import kopf

    @kopf.on.create('kopfexamples')
    def create_fn(spec, **_):

        for item in spec.get('items', []):

            @kopf.subhandler(id=item)
            def handle_item(item=item, **_):
                pass

Both of these ways are equivalent.
It is a matter of taste and preference which one to use.

The sub-handlers will be processed by all the standard rules and cycles
of the Kopf's handling cycle, as if they were the regular handlers
with the ids like ``create_fn/item1``, ``create_fn/item2``, etc.

.. warning::
    The sub-handler functions, their code or their arguments,
    are not remembered on the object between the handling cycles.

    Instead, their parent handler is considered as not finished,
    and it is called again and again to register the sub-handlers
    until all the sub-handlers of that parent handler are finished,
    so that the parent handler also becomes finished.

    As such, the parent handler SHOULD NOT produce any side-effects
    except as the read-only parsing of the inputs (e.g. :kwarg:`spec`),
    and generating the dynamic functions of the sub-handlers.



================================================
FILE: docs/hierarchies.rst
================================================
===========
Hierarchies
===========

One of the most common patterns of the operators is to create
children resources in the same Kubernetes cluster.
Kopf provides some tools to simplify connecting these resources
by manipulating their content before it is sent to the Kubernetes API.

.. note::

    Kopf is not a Kubernetes client library.
    It does not provide any means to manipulate the Kubernetes resources
    in the cluster or to directly talk to the Kubernetes API in any other way.
    Use any of the existing libraries for that purpose,
    such as the official `kubernetes client`_, pykorm_, or pykube-ng_.

.. _kubernetes client: https://github.com/kubernetes-client/python
.. _pykorm: https://github.com/Frankkkkk/pykorm
.. _pykube-ng: https://github.com/hjacobs/pykube

In all examples below, ``obj`` and ``objs`` are either a supported object type
(native or 3rd-party, see below) or a list/tuple/iterable with several objects.


Labels
======

To label the resources to be created, use :func:`kopf.label`:

.. code-block:: python

    @kopf.on.create('KopfExample')
    def create_fn(**_):
        objs = [{'kind': 'Job'}, {'kind': 'Deployment'}]
        kopf.label(objs, {'label1': 'value1', 'label2': 'value2'})
        print(objs)
        # [{'kind': 'Job',
        #   'metadata': {'labels': {'label1': 'value1', 'label2': 'value2'}}},
        #  {'kind': 'Deployment',
        #   'metadata': {'labels': {'label1': 'value1', 'label2': 'value2'}}}]


To label the specified resource(s) with the same labels as the resource being
processed at the moment, omit the labels or set them to ``None`` (note, it is
not the same as an empty dict ``{}`` -- which is equivalent to doing nothing):

.. code-block:: python

    @kopf.on.create('KopfExample')
    def create_fn(**_):
        objs = [{'kind': 'Job'}, {'kind': 'Deployment'}]
        kopf.label(objs)
        print(objs)
        # [{'kind': 'Job',
        #   'metadata': {'labels': {'somelabel': 'somevalue'}}},
        #  {'kind': 'Deployment',
        #   'metadata': {'labels': {'somelabel': 'somevalue'}}}]


By default, if some of the requested labels already exist, they will not
be overwritten. To overwrite labels, use ``forced=True``:

.. code-block:: python

    @kopf.on.create('KopfExample')
    def create_fn(**_):
        objs = [{'kind': 'Job'}, {'kind': 'Deployment'}]
        kopf.label(objs, {'label1': 'value1', 'somelabel': 'not-this'}, forced=True)
        kopf.label(objs, forced=True)
        print(objs)
        # [{'kind': 'Job',
        #   'metadata': {'labels': {'label1': 'value1', 'somelabel': 'somevalue'}}},
        #  {'kind': 'Deployment',
        #   'metadata': {'labels': {'label1': 'value1', 'somelabel': 'somevalue'}}}]


Nested labels
=============

For some resources, e.g. ``Job`` or ``Deployment``, additional fields have
to be modified to affect the double-nested children (``Pod`` in this case).

For this, their nested fields must be mentioned in a ``nested=[...]`` iterable.
If this is only one nested field, it can be passed directly as ``nested='...'``.

If the nested structures are absent in the target resources, they are ignored
and no labels are added. The labels are added only to pre-existing structures:

.. code-block:: python

    @kopf.on.create('KopfExample')
    def create_fn(**_):
        objs = [{'kind': 'Job'}, {'kind': 'Deployment', 'spec': {'template': {}}}]
        kopf.label(objs, {'label1': 'value1'}, nested='spec.template')
        kopf.label(objs, nested='spec.template')
        print(objs)
        # [{'kind': 'Job',
        #   'metadata': {'labels': {'label1': 'value1', 'somelabel': 'somevalue'}}},
        #  {'kind': 'Deployment',
        #   'metadata': {'labels': {'label1': 'value1', 'somelabel': 'somevalue'}},
        #   'spec': {'template': {'metadata': {'labels': {'label1': 'value1', 'somelabel': 'somevalue'}}}}}]

The nested structures are treated as if they were the root-level resources, i.e.
they are expected to have or automatically get the ``metadata`` structure added.

The nested resources are labelled *in addition* to the target resources.
To label only the nested resources without the root resource, pass them
to the function directly (e.g., ``kopf.label(obj['spec']['template'], ...)``).


Owner references
================

Kubernetes natively supports the owner references: a child resource
can be marked as "owned" by one or more other resources (owners or parents).
If the owner is deleted, its children will be deleted too, automatically,
and no additional handlers are needed.

To set the ownership, use :func:`kopf.append_owner_reference`.
To remove the ownership, use :func:`kopf.remove_owner_reference`:

.. code-block:: python

    kopf.append_owner_reference(objs, owner)
    kopf.remove_owner_reference(objs, owner)

To add/remove the ownership of the requested resource(s) by the resource being
processed at the moment, omit the explicit owner argument or set it to ``None``:

.. code-block:: python

    @kopf.on.create('KopfExample')
    def create_fn(**_):
        objs = [{'kind': 'Job'}, {'kind': 'Deployment'}]
        kopf.append_owner_reference(objs)
        print(objs)
        # [{'kind': 'Job',
        #   'metadata': {'ownerReferences': [{'controller': True,
        #      'blockOwnerDeletion': True,
        #      'apiVersion': 'kopf.dev/v1',
        #      'kind': 'KopfExample',
        #      'name': 'kopf-example-1',
        #      'uid': '6b931859-5d50-4b5c-956b-ea2fed0d1058'}]}},
        #  {'kind': 'Deployment',
        #   'metadata': {'ownerReferences': [{'controller': True,
        #      'blockOwnerDeletion': True,
        #      'apiVersion': 'kopf.dev/v1',
        #      'kind': 'KopfExample',
        #      'name': 'kopf-example-1',
        #      'uid': '6b931859-5d50-4b5c-956b-ea2fed0d1058'}]}}]

To set an owner to not be a controller or not block owner deletion:

.. code-block:: python

    kopf.append_owner_reference(objs, controller=False, block_owner_deletion=False)

Both of the above are True by default

.. seealso::
    :doc:`walkthrough/deletion`.


Names
=====

It is common to name the children resources after the parent resource:
either strictly as the parent, or with a random suffix.

To give the resource(s) a name, use :func:`kopf.harmonize_naming`.
If the resource has its ``metadata.name`` field set, that name will be used.
If it does not, the specified name will be used.
It can be enforced with ``forced=True``:

.. code-block:: python

    kopf.harmonize_naming(objs, 'some-name')
    kopf.harmonize_naming(objs, 'some-name', forced=True)

By default, the specified name is used as a prefix, and a random suffix
is requested from Kubernetes (via ``metadata.generateName``). This is the
most widely used mode with multiple children resource of the same kind.
To ensure the exact name for single-child cases, pass ``strict=True``:

.. code-block:: python

    kopf.harmonize_naming(objs, 'some-name', strict=True)
    kopf.harmonize_naming(objs, 'some-name', strict=True, forced=True)

To align the name of the target resource(s) with the name of the resource
being processed at the moment, omit the name or set it to ``None``
(both ``strict=True`` and ``forced=True`` are supported in this form too):

.. code-block:: python

    @kopf.on.create('KopfExample')
    def create_fn(**_):
        objs = [{'kind': 'Job'}, {'kind': 'Deployment'}]
        kopf.harmonize_naming(objs, forced=True, strict=True)
        print(objs)
        # [{'kind': 'Job', 'metadata': {'name': 'kopf-example-1'}},
        #  {'kind': 'Deployment', 'metadata': {'name': 'kopf-example-1'}}]

Alternatively, the operator can request Kubernetes to generate a name
with the specified prefix and a random suffix (via ``metadata.generateName``).
The actual name will be known only after the resource is created:

.. code-block:: python

    @kopf.on.create('KopfExample')
    def create_fn(**_):
        objs = [{'kind': 'Job'}, {'kind': 'Deployment'}]
        kopf.harmonize_naming(objs)
        print(objs)
        # [{'kind': 'Job', 'metadata': {'generateName': 'kopf-example-1-'}},
        #  {'kind': 'Deployment', 'metadata': {'generateName': 'kopf-example-1-'}}]

Both ways are commonly used for parent resources that orchestrate multiple
children resources of the same kind (e.g., pods in the deployment).


Namespaces
==========

Usually, it is expected that the children resources are created in the same
namespace as their parent (unless there are strong reasons to do differently).

To set the desired namespace, use :func:`kopf.adjust_namespace`:

.. code-block:: python

    kopf.adjust_namespace(objs, 'namespace')

If the namespace is already set, it will not be overwritten.
To overwrite, pass ``forced=True``:

.. code-block:: python

    kopf.adjust_namespace(objs, 'namespace', forced=True)

To align the namespace of the specified resource(s) with the namespace
of the resource being processed, omit the namespace or set it to ``None``:

.. code-block:: python

    @kopf.on.create('KopfExample')
    def create_fn(**_):
        objs = [{'kind': 'Job'}, {'kind': 'Deployment'}]
        kopf.adjust_namespace(objs, forced=True)
        print(objs)
        # [{'kind': 'Job', 'metadata': {'namespace': 'default'}},
        #  {'kind': 'Deployment', 'metadata': {'namespace': 'default'}}]


Adopting
========

All of the above can be done in one call with :func:`kopf.adopt`; ``forced``,
``strict``, ``nested`` flags are passed to all functions that support them:

.. code-block:: python

    @kopf.on.create('KopfExample')
    def create_fn(**_):
        objs = [{'kind': 'Job'}, {'kind': 'Deployment'}]
        kopf.adopt(objs, strict=True, forced=True, nested='spec.template')
        print(objs)
        # [{'kind': 'Job',
        #   'metadata': {'ownerReferences': [{'controller': True,
        #      'blockOwnerDeletion': True,
        #      'apiVersion': 'kopf.dev/v1',
        #      'kind': 'KopfExample',
        #      'name': 'kopf-example-1',
        #      'uid': '4a15f2c2-d558-4b6e-8cf0-00585d823511'}],
        #    'name': 'kopf-example-1',
        #    'namespace': 'default',
        #    'labels': {'somelabel': 'somevalue'}}},
        #  {'kind': 'Deployment',
        #   'metadata': {'ownerReferences': [{'controller': True,
        #      'blockOwnerDeletion': True,
        #      'apiVersion': 'kopf.dev/v1',
        #      'kind': 'KopfExample',
        #      'name': 'kopf-example-1',
        #      'uid': '4a15f2c2-d558-4b6e-8cf0-00585d823511'}],
        #    'name': 'kopf-example-1',
        #    'namespace': 'default',
        #    'labels': {'somelabel': 'somevalue'}}}]


3rd-party libraries
===================

All described methods support resource-related classes of selected libraries
the same way as the native Python dictionaries (or any mutable mappings).
Currently, that is `pykube-ng`_ (classes based on ``pykube.objects.APIObject``)
and `kubernetes client`_ (resource models from ``kubernetes.client.models``).

.. code-block:: python

    import kopf
    import pykube

    @kopf.on.create('KopfExample')
    def create_fn(**_):
        api = pykube.HTTPClient(pykube.KubeConfig.from_env())
        pod = pykube.objects.Pod(api, {})
        kopf.adopt(pod)

.. code-block:: python

    import kopf
    import kubernetes.client

    @kopf.on.create('KopfExample')
    def create_fn(**_):
        pod = kubernetes.client.V1Pod()
        kopf.adopt(pod)
        print(pod)
        # {'api_version': None,
        #  'kind': None,
        #  'metadata': {'annotations': None,
        #               'cluster_name': None,
        #               'creation_timestamp': None,
        #               'deletion_grace_period_seconds': None,
        #               'deletion_timestamp': None,
        #               'finalizers': None,
        #               'generate_name': 'kopf-example-1-',
        #               'generation': None,
        #               'labels': {'somelabel': 'somevalue'},
        #               'managed_fields': None,
        #               'name': None,
        #               'namespace': 'default',
        #               'owner_references': [{'api_version': 'kopf.dev/v1',
        #                                     'block_owner_deletion': True,
        #                                     'controller': True,
        #                                     'kind': 'KopfExample',
        #                                     'name': 'kopf-example-1',
        #                                     'uid': 'a114fa89-e696-4e84-9b80-b29fbccc460c'}],
        #               'resource_version': None,
        #               'self_link': None,
        #               'uid': None},
        #  'spec': None,
        #  'status': None}



================================================
FILE: docs/idempotence.rst
================================================
===========
Idempotence
===========

Kopf provides tools to make the handlers idempotent.

The :func:`kopf.register` function and the :func:`kopf.subhandler` decorator
allow to schedule arbitrary sub-handlers for the execution in the current cycle.

:func:`kopf.execute` coroutine executes arbitrary sub-handlers
directly in the place of invocation, and returns when all they have succeeded.

Every one of the sub-handlers is tracked by Kopf, and will not be executed
twice within one handling cycle.

.. code-block:: python

    import functools
    import kopf

    @kopf.on.create('kopfexamples')
    async def create(spec, namespace, **kwargs):
        print("Entering create()!")  # executed ~7 times.
        await kopf.execute(fns={
            'a': create_a,
            'b': create_b,
        })
        print("Leaving create()!")  # executed 1 time only.

    async def create_a(retry, **kwargs):
        if retry < 2:
            raise kopf.TemporaryError("Not ready yet.", delay=10)

    async def create_b(retry, **kwargs):
        if retry < 6:
            raise kopf.TemporaryError("Not ready yet.", delay=10)

In this example, both ``create_a`` & ``create_b`` are submitted to Kopf
as the sub-handlers of ``create`` on every attempt to execute it.
It means, every ~10 seconds until both of the sub-handlers succeed,
and the main handler succeeds too.

The first one, ``create_a``, will succeed on the 3rd attempt after ~20s.
The second one, ``create_b``, will succeed only on the 7th attempt after ~60s.

However, despite ``create_a`` will be submitted whenever ``create``
and ``create_b`` are retried, it will not be executed in the 20s..60s range,
as it has succeeded already, and the record about this is stored on the object.

This approach can be used to perform operations, which needs protection
from double-execution, such as the children object creation with randomly
generated names (e.g. Pods, Jobs, PersistentVolumeClaims, etc).

.. seealso::
    :ref:`persistence`, :ref:`subhandlers`.



================================================
FILE: docs/index.rst
================================================
Kopf: Kubernetes Operators Framework
====================================

.. toctree::
   :maxdepth: 2
   :caption: First steps:

   install

.. toctree::
   :maxdepth: 2
   :caption: Tutorial:

   concepts
   walkthrough/problem
   walkthrough/prerequisites
   walkthrough/resources
   walkthrough/starting
   walkthrough/creation
   walkthrough/updates
   walkthrough/diffs
   walkthrough/deletion
   walkthrough/cleanup

.. toctree::
   :maxdepth: 2
   :caption: Resource handling:

   handlers
   daemons
   timers
   kwargs
   async
   loading
   resources
   filters
   results
   errors
   scopes
   memos
   indexing
   admission

.. toctree::
   :maxdepth: 2
   :caption: Operator handling:

   startup
   shutdown
   probing
   authentication
   configuration
   peering
   cli

.. toctree::
   :maxdepth: 2
   :caption: Toolkits:

   events
   hierarchies
   testing
   embedding

.. toctree::
   :maxdepth: 2
   :caption: Recipes:

   deployment
   continuity
   idempotence
   reconciliation
   tips-and-tricks
   troubleshooting

.. toctree::
   :maxdepth: 2
   :caption: Developer Manual:

   minikube
   contributing
   architecture
   packages/kopf

.. toctree::
   :maxdepth: 2
   :caption: About Kopf:

   vision
   naming
   alternatives


Indices and tables
==================

* :ref:`genindex`
* :ref:`modindex`
* :ref:`search`



================================================
FILE: docs/indexing.rst
================================================
==================
In-memory indexing
==================

Indexers automatically maintain in-memory overviews of resources (indices),
grouped by keys that are usually calculated based on these resources.

The indices can be used for cross-resource awareness:
e.g., when a resource of kind X is changed, it can get all the information
about all resources of kind Y without talking to the Kubernetes API.
Under the hood, the centralised watch-streams ---one per resource kind--- are
more efficient in gathering the information than individual listing requests.


Index declaration
=================

Indices are declared with a ``@kopf.index`` decorator on an indexing function
(all standard filters are supported --- see :doc:`filters`):

.. code-block:: python

    import kopf

    @kopf.index('pods')
    def my_idx(**_):
        ...

The name of the function or its ``id=`` option is the index's name.

The indices are then available to all resource- and operator-level handlers
as the direct kwargs named the same as the index (type hints are optional):

.. code-block:: python

    import kopf

    # ... continued from previous examples:
    @kopf.timer('KopfExample', interval=5)
    def tick(my_idx: kopf.Index, **_):
        ...

    @kopf.on.probe()
    def metric(my_idx: kopf.Index, **_):
        ...

When a resource is created or starts matching the filters, it is processed
by all relevant indexing functions, and the result is put into the indices.

When a previously indexed resource is deleted or stops matching the filters,
all associated values are removed (so are all empty collections after this
--- to keep the indices clean).

.. seealso::
    :doc:`/probing` for probing handlers in the example above.


Index structure
===============

An index is always a read-only *mapping* of type :class:`kopf.Index`
with arbitrary keys leading to *collections* of type :class:`kopf.Store`,
which in turn contain arbitrary values generated by the indexing functions.
The index is initially empty. The collections are never empty
(empty collections are removed when the last item in them is removed).

For example, if several individual resources return the following results
from the same indexing function, then the index gets the following structure
(shown in the comment below the code):

.. code-block:: python

    return {'key1': 'valueA'}  # 1st
    return {'key1': 'valueB'}  # 2nd
    return {'key2': 'valueC'}  # 3rd
    # {'key1': ['valueA', 'valueB'],
    #  'key2': ['valueC']}

The indices are not nested. The 2nd-level mapping in the result
is stored as a regular value:

.. code-block:: python

    return {'key1': 'valueA'}  # 1st
    return {'key1': 'valueB'}  # 2nd
    return {'key2': {'key3': 'valueC'}}  # 3rd
    # {'key1': ['valueA', 'valueB'],
    #  'key2': [{'key3': 'valueC'}]}


Index content
=============

When an indexing function returns a ``dict`` (strictly ``dict``! not a generic
mapping, not even a descendant of ``dict``, such as :class:`kopf.Memo`),
it is merged into the index under the key taken from the result:

.. code-block:: python

    import kopf

    @kopf.index('pods')
    def string_keys(namespace, name, **_):
        return {namespace: name}
        # {'namespace1': ['pod1a', 'pod1b', ...],
        #  'namespace2': ['pod2a', 'pod2b', ...],
        #   ...]

Multi-value keys are possible with e.g. tuples or other hashable types:

.. code-block:: python

    import kopf

    @kopf.index('pods')
    def tuple_keys(namespace, name, **_):
        return {(namespace, name): 'hello'}
        # {('namespace1', 'pod1a'): ['hello'],
        #  ('namespace1', 'pod1b'): ['hello'],
        #  ('namespace2': 'pod2a'): ['hello'],
        #  ('namespace2', 'pod2b'): ['hello'],
        #   ...}

Multiple keys can be returned at once for a single resource.
They are all merged into their relevant places in the index:

.. code-block:: python

    import kopf

    @kopf.index('pods')
    def by_label(labels, name, **_):
        return {(label, value): name for label, value in labels.items()}
        # {('label1', 'value1a'): ['pod1', 'pod2', ...],
        #  ('label1', 'value1b'): ['pod3', 'pod4', ...],
        #  ('label2', 'value2a'): ['pod5', 'pod6', ...],
        #  ('label2', 'value2b'): ['pod1', 'pod3', ...],
        #   ...}

    @kopf.timer('kex', interval=5)
    def tick(by_label: kopf.Index, **_):
        print(list(by_label.get(('label2', 'value2b'), [])))
        # ['pod1', 'pod3']
        for podname in by_label.get(('label2', 'value2b'), []):
            print(f"==> {podname}")
        # ==> pod1
        # ==> pod3

Note the multiple occurrences of some pods because they have two or more labels.
But they never repeat within the same label --- labels can have only one value.


Recipes
=======

Unindexed collections
---------------------

When an indexing function returns a non-``dict`` --- i.e. strings, numbers,
tuples, lists, sets, memos, arbitrary objects except ``dict`` --- then the key
is assumed to be ``None`` and a flat index with only one key is constructed.
The resources are not indexed, but rather collected under the same key
(which is still considered as indexing):

.. code-block:: python

    import kopf

    @kopf.index('pods')
    def pod_names(name: str, **_):
        return name
        # {None: ['pod1', 'pod2', ...]}

Other types and complex objects returned from the indexing function are stored
"as is" (i.e. with no special treatment):

.. code-block:: python

    import kopf

    @kopf.index('pods')
    def container_names(spec: kopf.Spec, **_):
        return {container['name'] for container in spec.get('containers', [])}
        # {None: [{'main1', 'sidecar2'}, {'main2'}, ...]}


Enumerating resources
---------------------

If the goal is not to store any payload but to only list the existing resources,
then index the resources' identities (usually, their namespaces and names).

One way is to only collect their identities in a flat collection -- in case
you need mostly to iterate over all of them without key lookups:

.. code-block:: python

    import kopf

    @kopf.index('pods')
    def pods_list(namespace, name, **_):
        return namespace, name
        # {None: [('namespace1', 'pod1a'),
        #         ('namespace1', 'pod1b'),
        #         ('namespace2', 'pod2a'),
        #         ('namespace2', 'pod2b'),
        #           ...]}

    @kopf.timer('kopfexamples', interval=5)
    def tick_list(pods_list: kopf.Index, **_):
        for ns, name in pods_list.get(None, []):
            print(f"{ns}::{name}")

Another way is to index them by keys --- when index lookups are going to happen
more often than index iterations:

.. code-block:: python

    import kopf

    @kopf.index('pods')
    def pods_dict(namespace, name, **_):
        return {(namespace, name): None}
        # {('namespace1', 'pod1a'): [None],
        #  ('namespace1', 'pod1b'): [None],
        #  ('namespace2', 'pod2a'): [None],
        #  ('namespace2', 'pod2b'): [None],
        #   ...}

    @kopf.timer('kopfexamples', interval=5)
    def tick_dict(pods_dict: kopf.Index, spec: kopf.Spec, namespace: str, **_):
        monitored_namespace = spec.get('monitoredNamespace', namespace)
        for ns, name in pods_dict:
            if ns == monitored_namespace:
                print(f"in {ns}: {name}")


Mirroring resources
-------------------

To store the whole resource or its essential parts, return them explicitly:

.. code-block:: python

    import kopf

    @kopf.index('deployments')
    def whole_deployments(name: str, namespace: str, body: kopf.Body, **_):
        return {(namespace, name): body}

    @kopf.timer('kopfexamples', interval=5)
    def tick(whole_deployments: kopf.Index, **_):
        deployment, *_ = whole_deployments[('kube-system', 'coredns')]
        actual = deployment.status.get('replicas')
        desired = deployment.spec.get('replicas')
        print(f"{deployment.meta.name}: {actual}/{desired}")

.. note::

    Mind the memory consumption on large clusters and/or overly verbose objects.
    Especially mind the memory consumption for "managed fields"
    (see `kubernetes/kubernetes#90066`__).

    __ https://github.com/kubernetes/kubernetes/issues/90066


Indices of indices
------------------

Iterating over all keys of the index can be slow (especially if there are many
keys: e.g. with thousands of pods). For that case, an index of an index
can be built: with one primary indexing containing the real values to be used,
while the other secondary index only contains the keys of the primary index
(full or partial).

By looking up a single key in the secondary index, the operator can directly
get or indirectly reconstruct all the necessary keys in the primary index
instead of iterating over the primary index with filtering.

For example, we want to get all container names of all pods in a namespace.
In that case, the primary index will index containers by pods' namespaces+names,
while the secondary index will index pods' names by namespaces only:

.. code-block:: python

    import kopf

    @kopf.index('pods')
    def primary(namespace, name, spec, **_):
        container_names = {container['name'] for container in spec['containers']}
        return {(namespace, name): container_names}
        # {('namespace1', 'pod1a'): [{'main'}],
        #  ('namespace1', 'pod1b'): [{'main', 'sidecar'}],
        #  ('namespace2', 'pod2a'): [{'main'}],
        #  ('namespace2', 'pod2b'): [{'the-only-one'}],
        #   ...}

    @kopf.index('pods')
    def secondary(namespace, name, **_):
        return {namespace: name}
        # {'namespace1': ['pod1a', 'pod1b'],
        #  'namespace2': ['pod2a', 'pod2b'],
        #   ...}

    @kopf.timer('kopfexamples', interval=5)
    def tick(primary: kopf.Index, secondary: kopf.Index, spec: kopf.Spec, **_):
        namespace_containers = set()
        monitored_namespace = spec.get('monitoredNamespace', 'default')
        for pod_name in secondary.get(monitored_namespace, []):
            reconstructed_key = (monitored_namespace, pod_name)
            pod_containers, *_ = primary[reconstructed_key]
            namespace_containers |= pod_containers
        print(f"containers in {monitored_namespace}: {namespace_containers}")
        # containers in namespace1: {'main', 'sidecar'}
        # containers in namespace2: {'main', 'the-only-one'}

However, such complicated structures and such performance requirements are rare.
For simplicity and performance, nested indices are not directly provided by
the framework as a feature, only as this tip based on other official features.


Conditional indexing
====================

Besides the usual filters (see :doc:`/filters`), the resources can be skipped
from indexing by returning ``None`` (Python's default for no-result functions).

If the indexing function returns ``None`` or does not return anything,
its result is ignored and not indexed. The existing values in the index
are preserved as they are (this is also the case when unexpected errors
happen in the indexing function with the errors mode set to ``IGNORED``):

.. code-block:: python

    import kopf

    @kopf.index('pods')
    def empty_index(**_):
        pass
        # {}

However, if the indexing function returns a dict with ``None`` as values,
such values are indexed as usually (they are not ignored). ``None`` values
can be used as placeholders when only the keys are sufficient; otherwise,
indices and collections with no values left in them are removed from the index:

.. code-block:: python

    import kopf

    @kopf.index('pods')
    def index_of_nones(**_):
        return {'key': None}
        # {'key': [None, None, ...]}


Errors in indexing
==================

The indexing functions are supposed to be fast and non-blocking,
as they are capable of delaying the operator startup and resource processing.
For this reason, in case of errors in handlers, the handlers are never retried.

Arbitrary exceptions with ``errors=IGNORED`` (the default) make the framework
ignore the error and keep the existing indexed values (which are now stale).
It means that the new values are expected to appear soon, but the old values
are good enough meanwhile (which is usually highly probable). This is the same
as returning ``None``, except that the exception's stack trace is logged too:

.. code-block:: python

    import kopf

    @kopf.index('pods', errors=kopf.ErrorsMode.IGNORED)  # the default
    def fn1(**_):
        raise Exception("Keep the stale values, if any.")

:class:`kopf.PermanentError` and arbitrary exceptions with ``errors=PERMANENT``
remove any existing indexed values and the resource's keys from the index,
and exclude the failed resource from indexing by this index in the future
(so that even the indexing function is not invoked for them):

.. code-block:: python

    import kopf

    @kopf.index('pods', errors=kopf.ErrorsMode.PERMANENT)
    def fn1(**_):
        raise Exception("Excluded forever.")

    @kopf.index('pods')
    def fn2(**_):
        raise kopf.PermamentError("Excluded forever.")

:class:`kopf.TemporaryError` and arbitrary exceptions with ``errors=TEMPORARY``
remove any existing indexed values and the resource's keys from the index,
and exclude the failed resource from indexing for the specified duration
(via the error's ``delay`` option; set to ``0`` or ``None`` for no delay).
It is expected that the resource could be reindexed in the future,
but right now, problems are preventing this from happening:

.. code-block:: python

    import kopf

    @kopf.index('pods', errors=kopf.ErrorsMode.TEMPORARY)
    def fn1(**_):
        raise Exception("Excluded for 60s.")

    @kopf.index('pods')
    def fn2(**_):
        raise kopf.TemporaryError("Excluded for 30s.", delay=30)

In the "temporary" mode, the decorator's options for error handling are used:
the ``backoff=`` is a default delay before the resource can be re-indexed
(the default is 60 seconds; for no delay, use ``0`` explicitly);
the ``retries=`` and ``timeout=`` are the limit of retries and the overall
duration since the first failure until the resource will be marked
as permanently excluded from indexing (unless it succeeds at some point).

The handler's kwargs :kwarg:`retry`, :kwarg:`started`, :kwarg:`runtime`
report the retrying attempts since the first indexing failure.
Successful indexing resets all the counters/timeouts and the retrying state
is not stored (to save memory).

The same as with regular handlers (:doc:`errors`),
Kopf's error classes (expected errors) only log a short message,
while arbitrary exceptions (unexpected errors) also dump their stack traces.

This matches the semantics of regular handlers but with in-memory specifics.

.. warning::

    **There is no good out-of-the-box default mode for error handling:**
    any kind of errors in the indexing functions means that the index becomes
    inconsistent with the actual state of the cluster and its resources:
    the entries for matching resources are either "lost" (permanent or temporary
    errors), or contain possibly outdated/stale values (ignored errors) ---
    all of these cases are misinformation about the actual state of the cluster.

    The default mode is chosen to reduce the index changes and reindexing
    in case of frequent errors --- by not making any changes to the index.
    Besides, the stale values can still be relevant and useful to some extent.

    For two other cases, the operator developers have to explicitly accept the
    risks by setting ``errors=`` if the operator can afford to lose the keys.


Kwargs safety
=============

Indices that are injected into kwargs, overwrite any kwargs of the framework,
existing and those to be added later. This guarantees that the new framework
versions will not break an operator if new kwargs are added with the same name
as the existing indices.

In this case, the trade-off is that the handlers cannot use the new features
until their indices are renamed to something else. Since the new features are
new, the old operator's code does not use them, so it is backwards compatible.

To reduce the probability of name collisions, keep these conventions in mind
when naming indices (they are fully optional and for convenience only):

* System kwargs are usually one-word; name your indices with 2+ words.
* System kwargs are usually singular (not always); name the indices as plurals.
* System kwargs are usually nouns; using abbreviations or prefixes/suffixes
  (e.g. ``cnames``, ``rpods``) would reduce the probability of collisions.


Performance
===========

Indexing can be a CPU- & RAM-consuming operation.
The data structures behind indices are chosen to be as efficient as possible:

* The index's lookups are O(1) --- as in Python's ``dict``.
* The store's updates/deletions are O(1) -- a ``dict`` is used internally.
* The overall updates/deletions are O(k), where "k" is the number of keys
  per object (not of all keys!), which is fixed in most cases, so it is O(1).

Neither the number of values stored in the index nor the overall amount of keys
affect its performance (in theory).

Some performance can be lost on additional method calls of the user-facing
mappings/collections made to hide the internal ``dict`` structures.
It is assumed to be negligible compared to the overall code overhead.


Guarantees
==========

If an index is declared, there is no need to additionally pre-check for its
existence --- the index exists immediately even if it contains no resources.

The indices are guaranteed to be fully pre-populated before any other
resource-related handlers are invoked in the operator.
As such, even the on-creation handlers or raw event handlers are guaranteed
to have the complete indexed overview of the cluster,
not just partially populated to the moment when they happened to be triggered.

There is no such guarantee for the operator handlers, such as startup/cleanup,
authentication, health probing, and for the indexing functions themselves:
the indices are available in kwargs but can be empty or partially populated
in the operator's startup and index pre-population stage. This can affect
the cleanup/login/probe handlers if they are invoked at that stage.

Though, the indices are safe to be passed to threads/tasks for later processing
if such threads/tasks are started from the before-mentioned startup handlers.


Limitations
===========

All in-memory values are lost on operator restarts; there is no persistence.
In particular, the indices are fully recalculated on operator restarts during
the initial listing of the resources (equivalent to ``@kopf.on.event``).

On large clusters with thousands of resources, the initial index population
can take time, so the operator's processing will be delayed regardless of
whether the handlers do use the indices or they do not (the framework cannot
know this for sure).

.. seealso::

    :doc:`/memos` --- other in-memory structures with similar limitations.

.. seealso::

    Indexers and indices are conceptually similar to `client-go's indexers`__
    -- with all the underlying components implemented inside of the framework
    ("batteries included").

    __ https://github.com/kubernetes/sample-controller/blob/master/docs/controller-client-go.md



================================================
FILE: docs/install.rst
================================================
============
Installation
============

.. highlight:: bash

Prerequisites:

* Python >= 3.9 (CPython and PyPy are officially tested and supported).

To install Kopf::

    pip install kopf

If you use some of the managed Kubernetes services which require a sophisticated
authentication beyond username+password, fixed tokens, or client SSL certs
(also see :ref:`authentication piggy-backing <auth-piggybacking>`)::

    pip install kopf[full-auth]

If you want extra i/o performance under the hood, install it as (also see :ref:`custom-event-loops`)::

    pip install kopf[uvloop]

Unless you use the standalone mode,
create a few Kopf-specific custom resources in the cluster::

    kubectl apply -f https://github.com/nolar/kopf/raw/main/peering.yaml

Optionally, if you are going to use the examples or the code snippets::

    kubectl apply -f https://github.com/nolar/kopf/raw/main/examples/crd.yaml

.. todo:: RBAC objects! kubectl apply -f rbac.yaml

You are ready to go::

    kopf --help
    kopf run --help
    kopf run examples/01-minimal/example.py



================================================
FILE: docs/kwargs.rst
================================================
=========
Arguments
=========


.. kwarg:: kwargs

Forward compatibility kwargs
============================

``**kwargs`` is required in all handlers for the forward compatibility:
the framework can add new keywords in the future, and the existing handlers
should accept them without breaking, even if they do not use them.

It can be named ``**_`` to prevent the "unused variable" warnings by linters.


.. kwarg:: retry
.. kwarg:: started
.. kwarg:: runtime

Retrying and timing
===================

Most (but not all) of the handlers -- such as resource change detection,
resource daemons and timers, and activity handlers -- are capable of retrying
their execution in case of errors (see also: :doc:`errors`).
They provide kwargs regarding the retrying process:

``retry`` (``int``) is the sequential number of retry of this handler.
For the first attempt, it is ``0``, so it can be used in expressions
like ``if not retry: ...``.

``started`` (`datetime.datetime`) is the start time of the handler,
in case of retries & errors -- i.e. of the first attempt.

``runtime`` (`datetime.timedelta`) is the duration of the handler run,
in case of retries & errors -- i.e. since the first attempt.


.. kwarg:: param

Parametrization
===============

``param`` (any type, defaults to ``None``) is a value passed from the same-named
handler option ``param=``. It can be helpful if there are multiple decorators,
possibly with multiple different selectors & filters, for one handler function:

.. code-block:: python

    import kopf

    @kopf.on.create('KopfExample', param=1000)
    @kopf.on.resume('KopfExample', param=100)
    @kopf.on.update('KopfExample', param=10, field='spec.field')
    @kopf.on.update('KopfExample', param=1, field='spec.items')
    def count_updates(param, patch, **_):
        patch.status['counter'] = body.status.get('counter', 0) + param

    @kopf.on.update('Child1', param='first', field='status.done', new=True)
    @kopf.on.update('Child2', param='second', field='status.done', new=True)
    def child_updated(param, patch, **_):
        patch_parent({'status': {param: {'done': True}}})

Note that Kopf deduplicates the handlers to execute on one single occasion by
their underlying function and its id, which includes the field name by default.

In this example below with overlapping criteria, if ``spec.field`` is updated,
the handler will be called twice: one time -- for ``spec`` as a whole,
another time -- for ``spec.field`` in particular;
each time with the proper values of old/new/diff/param kwargs for those fields:

.. code-block:: python

    import kopf

    @kopf.on.update('KopfExample', param=10, field='spec.field')
    @kopf.on.update('KopfExample', param=1, field='spec')
    def fn(param, **_):
        pass


.. kwarg:: settings

Operator configuration
======================

``settings`` is passed to activity handlers (but not to resource handlers).

It is an object with a predefined nested structure of containers with values,
which defines the operator's behaviour. See: :class:`kopf.OperatorSettings`.

It can be modified if needed (usually in the startup handlers). Every operator
(if there are more than one in the same process) has its config.

See also: :doc:`configuration`.


Resource-related kwargs
=======================

.. kwarg:: resource
.. kwarg:: body
.. kwarg:: spec
.. kwarg:: meta
.. kwarg:: status
.. kwarg:: uid
.. kwarg:: name
.. kwarg:: namespace
.. kwarg:: labels
.. kwarg:: annotations

Body parts
----------

``resource`` (:class:`kopf.Resource`) is the actual resource being served
as retrieved from the cluster during the initial discovery.
Please note that it is not necessary the same selector as used in the decorator,
as one selector can match multiple actual resources.

``body`` is the handled object's body, a read-only mapping (dict).
It might look like this as an example:

.. code-block:: python

    {
        'apiVersion': 'kopf.dev/v1',
        'kind': 'KopfExample',
        'metadata': {
            'name': 'kopf-example-1',
            'namespace': 'default',
            'uid': '1234-5678-...',
        },
        'spec': {
            'field': 'value',
        },
        'status': {
            ...
        },
    }

``spec``, ``meta``, ``status`` are aliases for relevant stanzas, and are
live-views into ``body['spec']``, ``body['metadata']``, ``body['status']``.

``namespace``, ``name``, ``uid`` can be used to identify the object being
handled, and are aliases for the respective fields in ``body['metadata']``.
If the values are not present for any reason (e.g. namespaced for cluster-scoped
objects), the fields are ``None`` -- unlike accessing the same fields by key,
when a ``KeyError`` is raised.

``labels`` and ``annotations`` are equivalents of ``body['metadata']['labels']``
and ``body['metadata']['annotations']`` if they exist. If not, these two behave
as empty dicts.


.. kwarg:: logger

Logging
-------

``logger`` is a per-object logger, with the messages prefixed with the object's
namespace/name.

Some of the log messages are also sent as Kubernetes events according to the
log-level configuration (default is INFO, WARNINGs, ERRORs).


.. kwarg:: patch

Patching
--------

``patch`` is a mutable mapping (dict) with the object changes to be applied
after the handler. It is actively used internally by the framework itself,
and is shared to the handlers for convenience _(since patching happens anyway
in the framework, why make separate API calls for patching?)_.

.. note::
    Currently, it is just a dictionary, and the changes are applied
    as ``application/merge-patch+json``: ``None`` values delete the fields,
    other values override, dicts are merged.

    In the future, at discretion of this framework, it can be converted
    to JSON-patch (a list of add/change/remove operation), while keeping
    the same Python mutable mapping protocol and remembering the changes
    in the order they were made.


.. kwarg:: memo

In-memory container
-------------------

``memo`` is an in-memory container for arbitrary runtime-only keys-values.
The values can be accessed as either object attributes or dictionary keys.

For resource handlers, ``memo`` is shared by all handlers of the same
individual resource (not of the resource kind, but of the resource object).
For operator handlers, ``memo`` is shared by all handlers of the same operator
and later used to populate the resources' ``memo`` containers.

.. seealso::
    :doc:`memos` and :class:`kopf.Memo`.


.. kwarg:: indices
.. kwarg:: indexes

In-memory indices
-----------------

Indices are in-memory overviews of matching resources in the cluster.
They are populated according to ``@kopf.index`` handlers and their filters.

Each index is exposed in kwargs under its name (function name)
or id (if overridden with ``id=``). There is no global structure to access
all indices at once. If needed, use ``**kwargs`` itself.

Indices are available for all operator-level and all resource-level handlers.
For resource handlers, they are guaranteed to be populated before any handlers
are invoked. For operator handlers, there is no such guarantee.

.. seealso::
    :doc:`/indexing`.


Resource-watching kwargs
========================

For the resource watching handlers, an extra kwarg is provided:


.. kwarg:: event

API event
---------

``event`` is a raw JSON-decoded message received from Kubernetes API;
it is a dict with ``['type']`` & ``['object']`` keys.


Resource-changing kwargs
========================

Kopf provides functionality for change detection and triggers the handlers
for those changes (not for every event coming from the Kubernetes API).
A few extra kwargs are provided for these handlers, exposing the changes:


.. kwarg:: reason

Causation
---------

``reason`` is a type of change detection (creation, update, deletion, resuming).
It is generally reflected in the handler decorator used, but can be useful for
the multi-purpose handlers pointing to the same function
(e.g. for ``@kopf.on.create`` + ``@kopf.on.resume`` pairs).


.. kwarg:: old
.. kwarg:: new
.. kwarg:: diff

Diffing
-------

``old`` & ``new`` are the old & new state of the object or a field within
the detected changes. The new state usually corresponds to :kwarg:`body`.

For the whole-object handlers, ``new`` is an equivalent of :kwarg:`body`.
For the field handlers, it is the value of that field specifically.

``diff`` is a list of changes of the object between old & new states.

The diff highlights which keys were added, changed, or removed
in the dictionary, with old & new values being selectable,
and generally ignores all other fields that were not changed.

Due to specifics of Kubernetes, ``None`` is interpreted as absence
of the value/field, not as a value of its own kind. In case of diffs,
it means that the value did not exist before, or will not exist after
the changes (for the old & new value positions respectively):


Resource daemon kwargs
======================


.. kwarg:: stopped

Stop-flag
---------

Daemons also have ``stopped``. It is a flag object for sync & async daemons
(mostly, sync) to check if they should stop. See also: :class:`DaemonStopped`.

To check, ``.is_set()`` method can be called, or the object itself can be used
as a boolean expression: e.g. ``while not stopped: ...``.

Its ``.wait()`` method can be used to replace ``time.sleep()``
or ``asyncio.sleep()`` for faster (instant) termination on resource deletion.

See more: :doc:`daemons`.


Resource admission kwargs
=========================

.. kwarg:: dryrun

Dry run
-------

Admission handlers, both validating and mutating, must skip any side effects
if ``dryrun`` is ``True``. It is ``True`` when a dry-run API request is made,
e.g. with ``kubectl --dry-run=server ...``.

Regardless of ``dryrun``, the handlers must not make any side effects
unless they declare themselves as ``side_effects=True``.

See more: :doc:`admission`.


.. kwarg:: subresource

Subresources
------------

``subresource`` (``str|None``) is the name of a subresource being checked.
``None`` means that the main body of the resource is being checked.
Otherwise, it is usually ``"status"`` or ``"scale"``; other values are possible.
(The value is never ``"*"``, as the star mask is used only for handler filters.)

See more: :doc:`admission`.


.. kwarg:: warnings

Admission warnings
------------------

``warnings`` (``list[str]``) is a **mutable** list of string used as warnings.
The admission webhook handlers can populate the list with warnings (strings),
and the webhook servers/tunnels return them to Kubernetes, which shows them
to ``kubectl``.

See more: :doc:`admission`.


.. kwarg:: userinfo

User information
----------------

``userinfo`` (``Mapping[str, Any]``) is an information about a user that
sends the API request to Kubernetes.

It usually contains the keys ``'username'``, ``'uid'``, ``'groups'``,
but this might change in the future. The information is provided exactly
as Kubernetes sends it in the admission request.

See more: :doc:`admission`.


.. kwarg:: headers
.. kwarg:: sslpeer

Request credentials
-------------------

For rudimentary authentication and authorization, Kopf passes the information
from the admission requests to the admission handlers as is,
without additional interpretation of it.

``headers`` (``Mapping[str, str]``) contains all HTTPS request headers,
including ``Authorization: Basic ...``, ``Authorization: Bearer ...``.

``sslpeer`` (``Mapping[str, Any]``) contains the SSL peer information
as returned by :func:`ssl.SSLSocket.getpeercert`. It is ``None`` if no proper
SSL client certificate was provided (i.e. by apiservers talking to webhooks),
or if the SSL protocol could not verify the provided certificate with its CA.

.. note::
    This is an identity of the apiservers that send the admission request,
    not of the user or an app that sends the API request to Kubernetes.
    For the user's identity, use :kwarg:`userinfo`.

See more: :doc:`admission`.



================================================
FILE: docs/loading.rst
================================================
Loading and importing
=====================

Kopf requires the source files with the handlers on the command line.
It does not do any attempts to guess the user's intentions
or to introduce any conventions (at least, now).

There are two ways to specify them (both mimicking the Python interpreter):

* Direct script files::

    kopf run file1.py file2.py

* Importable modules::

    kopf run -m package1.module1 -m package2.module2

* Or mixed::

    kopf run file1.py file2.py -m package1.module1 -m package2.module2

Which way to use depends on how the source code is structured,
and is out of the scope of Kopf.

Each of the mentioned files and modules will be imported.
The handlers should be registered during the import.
This is usually done by using the function decorators --- see :doc:`/handlers`.



================================================
FILE: docs/memos.rst
================================================
====================
In-memory containers
====================

Kopf provides several ways of storing and exchanging the data in-memory
between handlers and operators.


Resource memos
==============

Every resource handler gets a :kwarg:`memo` kwarg of type :class:`kopf.Memo`.
It is an in-memory container for arbitrary runtime-only keys-values.
The values can be accessed as either object attributes or dictionary keys.

The memo is shared by all handlers of the same individual resource
(not of the resource kind, but a resource object).
If the resource is deleted and re-created with the same name,
the memo is also re-created (technically, it is a new resource).

.. code-block:: python

    import kopf

    @kopf.on.event('KopfExample')
    def pinged(memo: kopf.Memo, **_):
        memo.counter = memo.get('counter', 0) + 1

    @kopf.timer('KopfExample', interval=10)
    def tick(memo: kopf.Memo, logger, **_):
        logger.info(f"{memo.counter} events have been received in 10 seconds.")
        memo.counter = 0


Operator memos
==============

In the operator handlers, such as the operator startup/cleanup, liveness probes,
credentials retrieval, and everything else not specific to resources,
:kwarg:`memo` points to the operator's global container for arbitrary values.

The per-operator container can be either populated in the startup handlers,
or passed from outside of the operator when :doc:`embedding` is used, or both:

.. code-block:: python

    import kopf
    import queue
    import threading

    @kopf.on.startup()
    def start_background_worker(memo: kopf.Memo, **_):
        memo.my_queue = queue.Queue()
        memo.my_thread = threading.Thread(target=background, args=(memo.my_queue,))
        memo.my_thread.start()

    @kopf.on.cleanup()
    def stop_background_worker(memo: kopf.Memo, **_):
        memo['my_queue'].put(None)
        memo['my_thread'].join()

    def background(queue: queue.Queue):
        while True:
            item = queue.get()
            if item is None:
                break
            else:
                print(item)

.. note::

    For code quality and style consistency, it is recommended to use
    the same approach when accessing the stored values.
    The mixed style here is for demonstration purposes only.

The operator's memo is later used to populate the per-resource memos.
All keys & values are shallow-copied into each resource's memo,
where they can be mixed with the per-resource values:

.. code-block:: python

    # ... continued from the previous example.
    @kopf.on.event('KopfExample')
    def pinged(memo: kopf.Memo, namespace: str, name: str, **_):
        if not memo.get('is_seen'):
            memo.my_queue.put(f"{namespace}/{name}")
            memo.is_seen = True

Any changes to the operator's container since the first appearance
of the resource are **not** replicated to the existing resources' containers,
and are not guaranteed to be seen by the new resources (even if they are now).

However, due to shallow copying, the mutable objects (lists, dicts, and even
custom instances of :class:`kopf.Memo` itself) in the operator's container
can be modified from outside, and these changes will be seen in all individual
resource handlers & daemons which use their per-resource containers.


Custom memo classes
===================

For embedded operators (:doc:`/embedding`), it is possible to use any class
for memos. It is not even required to inherit from :class:`kopf.Memo`.

There are 2 strict requirements:

* The class must be supported by all involved handlers that use it.
* The class must support shallow copying via :func:`copy.copy` (``__copy__()``).

The latter is used to create per-resource memos from the operator's memo.
To have one global memo for all individual resources, redefine the class
to return ``self`` when requested to make a copy, as shown below:

.. code-block:: python

    import asyncio
    import dataclasses
    import kopf

    @dataclasses.dataclass()
    class CustomContext:
        create_tpl: str
        delete_tpl: str

        def __copy__(self) -> "CustomContext":
            return self

    @kopf.on.create('kopfexamples')
    def create_fn(memo: CustomContext, **kwargs):
        print(memo.create_tpl.format(**kwargs))

    @kopf.on.delete('kopfexamples')
    def delete_fn(memo: CustomContext, **kwargs):
        print(memo.delete_tpl.format(**kwargs))

    if __name__ == '__main__':
        kopf.configure(verbose=True)
        asyncio.run(kopf.operator(
            memo=CustomContext(
                create_tpl="Hello, {name}!",
                delete_tpl="Good bye, {name}!",
            ),
        ))

In all other regards, the framework does not use memos for its own needs
and passes them through the call stack to the handlers and daemons "as is".

This advanced feature is not available for operators executed via ``kopf run``.


Limitations
===========

All in-memory values are lost on operator restarts; there is no persistence.

The in-memory containers are recommended only for ephemeral objects scoped
to the process lifetime, such as concurrency primitives: locks, tasks, threads…
For persistent values, use the status stanza or annotations of the resources.

Essentially, the operator's memo is not much different from global variables
(unless 2+ embedded operator tasks are running there) or asyncio contextvars,
except that it provides the same interface as for the per-resource memos.

.. seealso::

    :doc:`/indexing` --- other in-memory structures with similar limitations.



================================================
FILE: docs/minikube.rst
================================================
========
Minikube
========

.. highlight:: bash

To develop the framework and the operators in an isolated Kubernetes cluster,
use minikube_.

.. _minikube: https://github.com/kubernetes/minikube

MacOS::

    brew install minikube
    brew install hyperkit

    minikube start --driver=hyperkit
    minikube config set driver hyperkit

Start the minikube cluster::

    minikube start
    minikube dashboard

It automatically creates and activates the kubectl context named ``minikube``.
If not, or if you have multiple clusters, activate it explicitly::

    kubectl config get-contexts
    kubectl config current-context
    kubectl config use-context minikube

For the minikube cleanup (to release the CPU/RAM/disk resources)::

    minikube stop
    minikube delete

.. seealso::
    For even more information, read the `Minikube installation manual`__.

__ https://kubernetes.io/docs/tasks/tools/install-minikube/



================================================
FILE: docs/naming.rst
================================================
======
Naming
======

Kopf is an abbreviation either for
**K**\ubernetes **O**\perator **P**\ythonic **F**\ramework, or for
**K**\ubernetes **OP**\erator **F**\ramework --- whatever you like more.

"Kopf" also means "head" in German.

It is capitalised in natural language texts:

    *I like using Kopf to manage my domain in Kubernetes.*

It is lower-cased in all system and code references::

    pip install kopf
    import kopf



================================================
FILE: docs/peering.rst
================================================
=======
Peering
=======

All running operators communicate with each other via peering objects
(additional kind of custom resources), so they know about each other.


Priorities
==========

Each operator has a priority (the default is 0). Whenever the operator
notices that other operators start with a higher priority, it pauses
its operation until those operators stop working.

This is done to prevent collisions of multiple operators handling
the same objects. If two operators runs with the same priority  all operators
issue a warning and freeze, so that the cluster becomes not served anymore.

To set the operator's priority, use :option:`--priority`:

.. code-block:: bash

    kopf run --priority=100 ...

Or:

.. code-block:: python

    import kopf

    @kopf.on.startup()
    def configure(settings: kopf.OperatorSettings, **_):
        settings.peering.priority = 100

As a shortcut, there is a :option:`--dev` option, which sets
the priority to ``666``, and is intended for the development mode.


Scopes
======

There are two types of custom resources used for peering:

* ``ClusterKopfPeering`` for the cluster-scoped operators.
* ``KopfPeering`` for the namespace-scoped operators.

Kopf automatically chooses which one to use, depending on whether
the operator is restricted to a namespace with :option:`--namespace`,
or it is running cluster-wide with :option:`--all-namespaces`.

Create a peering object as needed with one of:

.. code-block:: yaml

    apiVersion: kopf.dev/v1
    kind: ClusterKopfPeering
    metadata:
      name: example

.. code-block:: yaml

    apiVersion: kopf.dev/v1
    kind: KopfPeering
    metadata:
      namespace: default
      name: example

.. note::

    In ``kopf<0.11`` (until May 2019), ``KopfPeering`` was the only CRD,
    and it was cluster-scoped. In ``kopf>=0.11,<1.29`` (until Dec 2020),
    this mode was deprecated but supported if the old CRD existed.
    Since ``kopf>=1.29`` (Jan 2021), it is not supported anymore.
    To upgrade, delete and re-create the peering CRDs to the new ones.

.. note::

    In ``kopf<1.29``, all peering CRDs used the API group ``kopf.zalando.org``.
    Since ``kopf>=1.29`` (Jan'2021), they belong to the API group ``kopf.dev``.

    At runtime, both API groups are supported. However, these resources
    of different API groups are mutually exclusive and cannot co-exist
    in the same cluster since they use the same names. Whenever possible,
    re-create them with the new API group after the operator/framework upgrade.


Custom peering
==============

The operator can be instructed to use alternative peering objects::

    kopf run --peering=example ...
    kopf run --peering=example --namespace=some-ns ...

Or:

.. code-block:: python

    import kopf

    @kopf.on.startup()
    def configure(settings: kopf.OperatorSettings, **_):
        settings.peering.name = "example"
        settings.peering.mandatory = True

Depending on :option:`--namespace` or :option:`--all-namespaces`,
either ``ClusterKopfPeering`` or ``KopfPeering`` will be used automatically.

If the peering object does not exist, the operator will pause at the start.
Using :option:`--peering` assumes that the peering is mandatory.

Please note that in the startup handler, this is not the same:
the mandatory mode must be set explicitly. Otherwise, the operator will try
to auto-detect the presence of the custom peering object, but will not pause
if it is absent -- unlike with the ``--peering=`` CLI option.

The operators from different peering objects do not see each other.

This is especially useful for the cluster-scoped operators for different
resource kinds, which should not worry about other operators for other kinds.


Standalone mode
===============

To prevent an operator from peering and talking to other operators,
the standalone mode can be enabled::

    kopf run --standalone ...

Or:

.. code-block:: python

    import kopf

    @kopf.on.startup()
    def configure(settings: kopf.OperatorSettings, **_):
        settings.peering.standalone = True

In that case, the operator will not pause if other operators with
the higher priority will start handling the objects, which may lead
to the conflicting changes and reactions from multiple operators
for the same events.


Automatic peering
=================

If there is a peering object detected with the name ``default``
(either cluster-scoped or namespace-scoped),
then it is used by default as the peering object.

Otherwise, Kopf will run the operator in the standalone mode.


Multi-pod operators
===================

Usually, one and only one operator instance should be deployed for the resource.
If that operator's pod dies, the handling of the resource of this type
will stop until the operator's pod is restarted (and if restarted at all).

To start multiple operator pods, they must be distinctly prioritised.
In that case, only one operator will be active --- the one with the highest
priority. All other operators will pause and wait until this operator exits.
Once it dies, the second-highest priority operator will come into play.
And so on.

For this, assign a monotonically growing or random priority to each
operator in the deployment or replicaset:

.. code-block:: bash

    kopf run --priority=$RANDOM ...

Or:

.. code-block:: python

    import random
    import kopf

    @kopf.on.startup()
    def configure(settings: kopf.OperatorSettings, **_):
        settings.peering.priority = random.randint(0, 32767)

``$RANDOM`` is a feature of bash
(if you use another shell, see its man page for an equivalent).
It returns a random integer in the range 0..32767.
With high probability, 2-3 pods will get their unique priorities.

You can also use the pod's IP address in its numeric form as the priority,
or any other source of integers.


Stealth keep-alive
==================

Every few seconds (60 by default), the operator will send a keep-alive update
to the chosen peering, showing that it is still functioning. Other operators
will notice that and make decisions on their pausing or resuming.

The operator also logs a keep-alive activity to its logs. This can be
distracting. To disable:

.. code-block:: python

    import random
    import kopf

    @kopf.on.startup()
    def configure(settings: kopf.OperatorSettings, **_):
        settings.peering.stealth = True

There is no equivalent CLI option for that.

Please note that it only affects logging. The keep-alive is sent anyway.



================================================
FILE: docs/probing.rst
================================================
=============
Health-checks
=============

Kopf provides a minimalistic HTTP server to report its health status.


Liveness endpoints
==================

By default, no endpoint is configured, and no health is reported.
To specify an endpoint to listen for probes, use :option:`--liveness`:

.. code-block:: bash

    kopf run --liveness=http://0.0.0.0:8080/healthz --verbose handlers.py

Currently, only HTTP is supported.
Other protocols (TCP, HTTPS) can be added in the future.


Kubernetes probing
==================

This port and path can be used in a liveness probe of the operator's deployment.
If the operator does not respond for any reason, Kubernetes will restart it.

.. code-block:: yaml

   apiVersion: apps/v1
   kind: Deployment
   spec:
     template:
       spec:
         containers:
         - name: the-only-one
           image: ...
           livenessProbe:
             httpGet:
               path: /healthz
               port: 8080

.. seealso::

    Kubernetes manual on `liveness and readiness probes`__.

__ https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/

.. seealso::

    Please be aware of the readiness vs. liveness probing.
    In the case of operators, readiness probing makes no practical sense,
    as operators do not serve traffic under the load balancing or with services.
    Liveness probing can help in disastrous cases (e.g. the operator is stuck),
    but will not help in case of partial failures (one of the API calls stuck).
    You can read more here:
    https://srcco.de/posts/kubernetes-liveness-probes-are-dangerous.html

.. warning::

    Make sure that one and only one pod of an operator is running at a time,
    especially during the restarts --- see :doc:`deployment`.


Probe handlers
==============

The content of the response is empty by default. It can be populated with
probing handlers:

.. code-block:: python

    import datetime
    import kopf
    import random

    @kopf.on.probe(id='now')
    def get_current_timestamp(**kwargs):
        return datetime.datetime.now(datetime.timezone.utc).isoformat()

    @kopf.on.probe(id='random')
    def get_random_value(**kwargs):
        return random.randint(0, 1_000_000)

The probe handlers will be executed on the requests to the liveness URL,
and cached for a reasonable time to prevent overloading
by mass-requesting the status.

The handler results will be reported as the content of the liveness response:

.. code-block:: console

    $ curl http://localhost:8080/healthz
    {"now": "2019-11-07T18:03:52.513803+00:00", "random": 765846}

.. note::
    The liveness status report is simplistic and minimalistic at the moment.
    It only reports success if the health-reporting task runs at all.
    It can happen so that some of the operator's tasks, threads, or streams
    do break, freeze, or become unresponsive, while the health-reporting task
    continues to run. The probability of such a case is low, but not zero.

    There are no checks that the operator operates anything
    (unless they are implemented explicitly with the probe-handlers),
    as there are no reliable criteria for that -- total absence of handled
    resources or events can be an expected state of the cluster.



================================================
FILE: docs/reconciliation.rst
================================================
==============
Reconciliation
==============

Reconciliation is, in plain words, bringing the *actual state* of a system
to a *desired state* as expressed by the Kubernetes resources.
For example, starting as many pods, as it is declared in a deployment,
especially when this declaration changes due to resource updates.

Kopf is not an operator, it is a framework to make operators.
Therefore, it knows nothing about the *desired state* or *actual state*
(or any *state* at all).

Kopf-based operators must implement the checks and reactions to the changes,
so that both states are synchronised according to the operator's concepts.

Kopf only provides a few ways and tools for achieving this easily.


Event-driven reactions
======================

Normally, Kopf triggers the on-creation/on-update/on-deletion handlers
every time anything changes on the object, as reported by Kubernetes API.
It provides both the current state of the object and a diff list
with the last handled state.

The event-driven approach is the best, as it saves system resources (RAM & CPU),
and does not trigger any activity when it is not needed and does not consume
memory for keeping the object's last known state permanently in memory.

But it is more difficult to develop, and is not suitable for some cases:
e.g., when an external non-Kubernetes system is monitored via its API.

.. seealso::
    :doc:`handlers`


Regularly scheduled timers
==========================

Timers are triggered on a regular schedule, regardless of whether anything
changes or does not change in the resource itself. This can be used to
verify both the resource's body, and the state of other related resources
through API calls, and update the original resource's status/content.

.. seealso::
    :doc:`timers`


Permanently running daemons
===========================

As a last resort, a developer can implement a background task,
which checks the status of the system and reacts when the "actual" state
diverts from the "desired" state.

.. seealso::
    :doc:`daemons`


What to use when?
=================

As a rule of thumb _(recommended, but not insisted)_, the following guidelines
can be used to decide which way of reconciliation to use in which cases:

* In the first place, try the event-driven approach by watching
  for the children resources (those belonging to the "actual" state).

  If there are many children resources for one parent resource,
  store their brief statuses on the parent's ``status.children.{id}``
  from every individual child, and react to the changes of ``status.children``
  in the parent resource.

* If the "desired" state can be queried with blocking waits
  (e.g. by running a ``GET`` query on a remote job/task/activity via an API,
  which blocks until the requested condition is reached),
  then use daemons to poll for the status, and process it as soon as it changes.

* If the "desired" state is not Kubernetes-related, maybe it is an external
  system accessed by an API, or if delays in reconciliation are acceptable,
  then use the timers.

* Only as the last resort, use the daemons with a ``while True`` cycle
  and explicit sleep.



================================================
FILE: docs/requirements.txt
================================================
# Everything needed to build the docs.
# The runtime dependencies of the framework, as if via `pip install kopf`.
-e .
sphinx>=2.0.0
sphinx-autobuild
sphinx-autodoc-typehints
sphinx_rtd_theme>=0.5



================================================
FILE: docs/resources.rst
================================================
======================
Resource specification
======================

The following notations are supported to specify the resources to be handled.
As a rule of thumb, they are designed so that the intentions of a developer
are guessed the best way possible, and similar to ``kubectl`` semantics.

The resource name is always expected in the first place as the rightmost value.
The remaining parts are considered as an API group and an API version
of the resource -- given as either two separate strings, or as one,
but separated with a slash:

.. code-block:: python

    @kopf.on.event('kopf.dev', 'v1', 'kopfexamples')
    @kopf.on.event('kopf.dev/v1', 'kopfexamples')
    @kopf.on.event('apps', 'v1', 'deployments')
    @kopf.on.event('apps/v1', 'deployments')
    @kopf.on.event('', 'v1', 'pods')
    def fn(**_):
        pass

If only one API specification is given (except for ``v1``), it is treated
as an API group, and the preferred API version of that API group is used:

.. code-block:: python

    @kopf.on.event('kopf.dev', 'kopfexamples')
    @kopf.on.event('apps', 'deployments')
    def fn(**_):
        pass

It is also possible to specify the resources with ``kubectl``'s semantics:

.. code-block:: python

    @kopf.on.event('kopfexamples.kopf.dev')
    @kopf.on.event('deployments.apps')
    def fn(**_):
        pass

One exceptional case is ``v1`` as the API specification: it corresponds
to K8s's legacy core API (before API groups appeared), and is equivalent
to an empty API group name. The following specifications are equivalent:

.. code-block:: python

    @kopf.on.event('v1', 'pods')
    @kopf.on.event('', 'v1', 'pods')
    def fn(**_):
        pass

If neither the API group nor the API version is specified,
all resources with that name would match regardless of the API groups/versions.
However, it is reasonable to expect only one:

.. code-block:: python

    @kopf.on.event('kopfexamples')
    @kopf.on.event('deployments')
    @kopf.on.event('pods')
    def fn(**_):
        pass

In all examples above, where the resource identifier is expected, it can be
any name: plural, singular, kind, or a short name. As it is impossible to guess
which one is which, the name is remembered as is, and is later checked for all
possible names of the specific resources once those are discovered:

.. code-block:: python

    @kopf.on.event('kopfexamples')
    @kopf.on.event('kopfexample')
    @kopf.on.event('KopfExample')
    @kopf.on.event('kex')
    @kopf.on.event('StatefulSet')
    @kopf.on.event('deployments')
    @kopf.on.event('pod')
    def fn(**_):
        pass

The resource specification can be more specific on which name to match:

.. code-block:: python

    @kopf.on.event(kind='KopfExample')
    @kopf.on.event(plural='kopfexamples')
    @kopf.on.event(singular='kopfexample')
    @kopf.on.event(shortcut='kex')
    def fn(**_):
        pass

The whole categories of resources can be served, but they must be explicitly
specified to avoid unintended consequences:

.. code-block:: python

    @kopf.on.event(category='all')
    def fn(**_):
        pass

Note that the conventional category ``all`` does not really mean all resources,
but only those explicitly added to this category; some built-in resources
are excluded (e.g. ingresses, secrets).

To handle all resources in an API group/version, use a special marker instead
of the mandatory resource name:

.. code-block:: python

    @kopf.on.event('kopf.dev', 'v1', kopf.EVERYTHING)
    @kopf.on.event('kopf.dev/v1', kopf.EVERYTHING)
    @kopf.on.event('kopf.dev', kopf.EVERYTHING)
    def fn(**_):
        pass

As a consequence of the above, to handle every resource in the cluster
-- which might be not the best idea per se, but is technically possible --
omit the API group/version, and use the marker only:

.. code-block:: python

    @kopf.on.event(kopf.EVERYTHING)
    def fn(**_):
        pass

Serving everything is better when it is used with filters:

.. code-block:: python

    @kopf.on.event(kopf.EVERYTHING, labels={'only-this': kopf.PRESENT})
    def fn(**_):
        pass

.. note::

    Core v1 events are excluded from ``EVERYTHING``: they are created during
    handling of other resources in the implicit :doc:`events` from log messages,
    so they would cause unnecessary handling cycles for every essential change.

    To handle core v1 events, they must be named explicitly, e.g. like this:

    .. code-block:: python

        @kopf.on.event('v1', 'events')
        def fn(**_):
            pass

The resource specifications do not support multiple values, masks or globs.
To handle multiple independent resources, add multiple decorators
to the same handler function -- as shown above.
The handlers are deduplicated by the underlying function and its handler id
(which, in turn, equals to the function's name by default unless overridden),
so one function will never be triggered multiple times for the same resource
if there are some accidental overlaps in the specifications.

.. warning::

    Kopf tries to make it easy to specify resources a la ``kubectl``.
    However, some things cannot be made that easy. If resources are specified
    ambiguously, i.e. if 2+ resources of different API groups match the same
    resource specification, neither of them will be served, and a warning
    will be issued.

    This only applies to resource specifications where it is intended to have
    a specific resource by its name; specifications with intentional
    multi-resource mode are served as usually (e.g. by categories).

    However, ``v1`` resources have priority over all other resources. This
    resolves the conflict of ``pods.v1`` vs. ``pods.v1beta1.metrics.k8s.io``,
    so just ``"pods"`` can be specified and the intention will be understood.

    This mimics the behaviour of ``kubectl``, where such API priorities
    are `hard-coded`__.

    __ https://github.com/kubernetes/kubernetes/blob/323f34858de18b862d43c40b2cced65ad8e24052/staging/src/k8s.io/client-go/restmapper/discovery.go#L47-L49

    While it might be convenient to write short forms of resource names,
    the proper way is to always add at least an API group:

    .. code-block:: python

        import kopf

        @kopf.on.event('pods')  # NOT SO GOOD, ambiguous, though works
        @kopf.on.event('pods.v1')  # GOOD, specific
        @kopf.on.event('v1', 'pods')  # GOOD, specific
        @kopf.on.event('pods.metrics.k8s.io')  # GOOD, specific
        @kopf.on.event('metrics.k8s.io', 'pods')  # GOOD, specific
        def fn(**_):
            pass

    Keep the short forms only for prototyping and experimentation mode,
    and for ad-hoc operators with custom resources (not reusable and running
    in controlled clusters where no other similar resources can be defined).

.. warning::

    Some API groups are served by API extensions: e.g. ``metrics.k8s.io``.
    If the extension's deployment/service/pods are down, such a group will
    not be scannable (failing with "HTTP 503 Service Unavailable")
    and will block scanning the whole cluster if resources are specified
    with no group name (e.g. ``('pods')`` instead of ``('v1', 'pods')``).

    To avoid scanning the whole cluster and all (even unused) API groups,
    it is recommended to specify at least the group names for all resources,
    especially in reusable and publicly distributed operators.



================================================
FILE: docs/results.rst
================================================
================
Results delivery
================

All handlers can return arbitrary JSON-serializable values.
These values are then put to the resource status under the name of the handler:

.. code-block:: python

    import kopf

    @kopf.on.create('kopfexamples')
    def create_kex_1(**_):
        return 100

    @kopf.on.create('kopfexamples')
    def create_kex_2(uid, **_):
        return {'r1': random.randint(0, 100), 'r2': random.randint(100, 999)}

These results can be seen in the object's content:

.. code-block:: console

    $ kubectl get -o yaml kex kopf-example-1

.. code-block:: none

    ...
    status:
      create_kex_1: 100
      create_kex_2:
        r1: 66
        r2: 666

The function results can be used to communicate between handlers through
resource itself, assuming that handlers do not know in which order they
will be invoked (due to error handling and retrying), and to be able to
restore in case of operator failures & restarts:

.. code-block:: python

    import kopf
    import pykube

    @kopf.on.create('kopfexamples')
    def create_job(status, **_):
        if not status.get('create_pvc', {}):
            raise kopf.TemporaryError("PVC is not created yet.", delay=10)

        pvc_name = status['create_pvc']['name']

        api = pykube.HTTPClient(pykube.KubeConfig.from_env())
        obj = pykube.Job(api, {...})  # use pvc_name here
        obj.create()
        return {'name': obj.name}

    @kopf.on.create('kopfexamples')
    def create_pvc(**_):
        api = pykube.HTTPClient(pykube.KubeConfig.from_env())
        obj = pykube.PersistentVolumeClaim(api, {...})
        obj.create()
        return {'name': obj.name}

.. note::

    In this example, the handlers are *intentionally* put in such an order
    that the first handler always fails on the first attempt. Having them
    in the proper order (PVC first, Job afterwards) will make it work smoothly
    for most of the cases, until PVC creation fails for any temporary reason
    and has to be retried. The whole thing will eventually succeed anyway in
    1-2 additional retries, just with less friendly messages and stack traces.



================================================
FILE: docs/scopes.rst
================================================
======
Scopes
======

Namespaces
==========

An operator can be restricted to handle custom resources in one namespace only::

    kopf run --namespace=some-namespace ...
    kopf run -n some-namespace ...

Multiple namespaces can be served::

    kopf run --namespace=some-namespace --namespace=another-namespace ...
    kopf run -n some-namespace -n another-namespace ...

Namespace globs with ``*`` and ``?`` characters can be used too::

    kopf run --namespace=*-pr-123-* ...
    kopf run -n *-pr-123-* ...

Namespaces can be negated: all namespaces are served except those excluded::

    kopf run --namespace=!*-pr-123-* ...
    kopf run -n !*-pr-123-* ...

Multiple globs can be used in one pattern. The rightmost matching one wins.
The first glob is decisive: if a namespace does not match it, it does not match
the whole pattern regardless of what is there (other globs are not checked).
If the first glob is a negation, it is implied that initially, all namespaces
do match (as if preceded by ``*``), and then the negated ones are excluded.

In this artificial example, ``myapp-live`` will match, ``myapp-pr-456`` will
not match, but ``myapp-pr-123`` will match; ``otherapp-live`` will not match;
even ``otherapp-pr-123`` will not match despite the ``-pr-123`` suffix in it
because it does not match the initial decisive glob::

    kopf run --namespace=myapp-*,!*-pr-*,*-pr-123 ...

In all cases, the operator monitors the namespaces that exist at the startup
or are created/deleted at runtime, and starts/stops serving them accordingly.

If there are no permissions to list/watch the namespaces, the operator falls
back to the list of provided namespaces "as is", assuming they exist.
Namespace patterns do not work in this case; only the specific namespaces do
(which means, all namespaces with the ``,*?!`` characters are excluded).

If a namespace does not exist, `Kubernetes permits watching over it anyway`__.
The only difference is when the resource watching starts: if the permissions
are sufficient, the watching starts only after the namespace is created;
if not sufficient, the watching starts immediately (for an unexistent namespace)
and the resources will be served once that namespace is created.

__ https://github.com/kubernetes/kubernetes/issues/75537


Cluster-wide
============

To serve the resources in the whole cluster::

    kopf run --all-namespaces ...
    kopf run -A ...

In that case, the operator does not monitor the namespaces in the cluster,
and uses different K8s API URLs to list/watch the objects cluster-wide.



================================================
FILE: docs/shutdown.rst
================================================
========
Shutdown
========

The cleanup handlers are executed when the operator exits
either by a signal (e.g. SIGTERM) or by catching an exception,
or by raising the stop-flag, or by cancelling the operator's task
(for :doc:`embedded operators </embedding>`)::

    import kopf

    @kopf.on.cleanup()
    async def cleanup_fn(logger, **kwargs):
        pass

The cleanup handlers are not guaranteed to be fully executed if they take
too long -- due to a limited graceful period or non-graceful termination.

Similarly, the clean up handlers are not executed if the operator
is force-killed with no possibility to react (e.g. by SIGKILL).

.. note::

    If the operator is running in a Kubernetes cluster, there can be
    timeouts set for graceful termination of a pod
    (``terminationGracePeriodSeconds``, the default is 30 seconds).

    If the cleanup takes longer than that in total (e.g. due to retries),
    the activity will not be finished in full,
    as the pod will be SIGKILL'ed by Kubernetes.

    Either design the cleanup activities to be as fast as possible,
    or configure ``terminationGracePeriodSeconds`` accordingly.

    Kopf itself does not set any implicit timeouts for the cleanup activity,
    and it can continue forever (unless explicitly limited).



================================================
FILE: docs/startup.rst
================================================
=======
Startup
=======

The startup handlers are slightly different from the module-level code:
the actual tasks (e.g. API calls for resource watching) are not started
until all the startup handlers succeed.

The handlers run inside of the operator's event loop, so they can initialise
the loop-bound variables -- which is impossible in the module-level code::

    import asyncio
    import kopf

    LOCK: asyncio.Lock

    @kopf.on.startup()
    async def startup_fn(logger, **kwargs):
        global LOCK
        LOCK = asyncio.Lock()  # uses the running asyncio loop by default

If any of the startup handlers fail, the operator fails to start
without making any external API calls.

.. note::

    If the operator is running in a Kubernetes cluster, there can be
    timeouts set for liveness/readiness checks of a pod.

    If the startup takes too long in total (e.g. due to retries),
    the pod can be killed by Kubernetes as not responding to the probes.

    Either design the startup activities to be as fast as possible,
    or configure the liveness/readiness probes accordingly.

    Kopf itself does not set any implicit timeouts for the startup activity,
    and it can continue forever (unless explicitly limited).



================================================
FILE: docs/testing.rst
================================================
================
Operator testing
================

Kopf provides some tools to test the Kopf-based operators
via :mod:`kopf.testing` module (requires explicit importing).


Background runner
=================

:class:`kopf.testing.KopfRunner` runs an arbitrary operator in the background,
while the original testing thread does the object manipulation and assertions:

When the ``with`` block exits, the operator stops, and its exceptions,
exit code and output are available to the test (for additional assertions).

.. code-block:: python
    :caption: test_example_operator.py

    import time
    import subprocess
    from kopf.testing import KopfRunner

    def test_operator():
        with KopfRunner(['run', '-A', '--verbose', 'examples/01-minimal/example.py']) as runner:
            # do something while the operator is running.

            subprocess.run("kubectl apply -f examples/obj.yaml", shell=True, check=True)
            time.sleep(1)  # give it some time to react and to sleep and to retry

            subprocess.run("kubectl delete -f examples/obj.yaml", shell=True, check=True)
            time.sleep(1)  # give it some time to react

        assert runner.exit_code == 0
        assert runner.exception is None
        assert 'And here we are!' in runner.output
        assert 'Deleted, really deleted' in runner.output

.. note::
    The operator runs against the cluster which is currently authenticated ---
    same as if would be executed with `kopf run`.



================================================
FILE: docs/timers.rst
================================================
======
Timers
======

Timers are schedules of regular handler execution as long as the object exists,
no matter if there were any changes or not -- unlike the regular handlers,
which are event-driven and are triggered only when something changes.


Intervals
=========

The interval defines how often to trigger the handler (in seconds):

.. code-block:: python

    import asyncio
    import time
    import kopf

    @kopf.timer('kopfexamples', interval=1.0)
    def ping_kex(spec, **kwargs):
        pass


Sharpness
=========

Usually (by default), the timers are invoked with the specified interval
between each call. The time taken by the handler itself is not taken into
account. It is possible to define timers with a sharp schedule: i.e. invoked
every number of seconds sharp, no matter how long it takes to execute it:

.. code-block:: python

    import asyncio
    import time
    import kopf

    @kopf.timer('kopfexamples', interval=1.0, sharp=True)
    def ping_kex(spec, **kwargs):
        time.sleep(0.3)

In this example, the timer takes 0.3 seconds to execute. The actual interval
between the timers will be 0.7 seconds in the sharp mode: whatever is left
of the declared interval of 1.0 seconds minus the execution time.


Idling
======

Timers can be defined to idle if the resource changes too often, and only
be invoked when it is stable for some time:

.. code-block:: python

    import asyncio
    import kopf

    @kopf.timer('kopfexamples', idle=10)
    def ping_kex(spec, **kwargs):
        print(f"FIELD={spec['field']}")

The creation of a resource is considered as a change, so idling also shifts
the very first invocation by that time.

The default is to have no idle time, just the intervals.

It is possible to have a timer with both idling and interval. In that case,
the timer will be invoked only if there were no changes in the resource
for the specified duration (idle time),
and every N seconds after that (interval) as long as the object does not change.
Once changed, the timer will stop and wait for the new idling time:

.. code-block:: python

    import asyncio
    import kopf

    @kopf.timer('kopfexamples', idle=10, interval=1)
    def ping_kex(spec, **kwargs):
        print(f"FIELD={spec['field']}")


Postponing
==========

Normally, timers are invoked immediately once resource becomes visible
to the operator (unless idling is declared).

It is possible to postpone the invocations:

.. code-block:: python

    import asyncio
    import time
    import kopf

    @kopf.timer('kopfexamples', interval=1, initial_delay=5)
    def ping_kex(spec, **kwargs):
        print(f"FIELD={spec['field']}")

This is similar to idling, except that it is applied only once per
resource/operator lifecycle in the very beginning.


Combined timing
===============

It is possible to combine all scheduled intervals to achieve the desired effect.
For example, to give an operator 1 minute for warming up, and then pinging
the resources every 10 seconds if they are unmodified for 10 minutes:

.. code-block:: python

    import kopf

    @kopf.timer('kopfexamples',
                initial_delay=60, interval=10, idle=600)
    def ping_kex(spec, **kwargs):
        pass


Errors in timers
================

The timers follow the standard :doc:`error handling <errors>` protocol:
:class:`TemporaryError` and arbitrary exceptions are treated according to
the ``errors``, ``timeout``, ``retries``, ``backoff`` options of the handler.
The kwargs :kwarg:`retry`, :kwarg:`started`, :kwarg:`runtime` are provided too.

The default behaviour is to retry arbitrary error
(similar to the regular resource handlers).

When an error happens, its delay overrides the timer's schedule or life cycle:

* For arbitrary exceptions, the timer's ``backoff=...`` option is used.
* For :class:`kopf.TemporaryError`, the error's ``delay=...`` option is used.
* For :class:`kopf.PermanentError`, the timer stops forever and is not retried.

The timer's own interval is only used if the function exits successfully.

For example, if the handler fails 3 times with a back-off time set to 5 seconds
and the interval set to 10 seconds, it will take 25 seconds (``3*5+10``)
from the first execution to the end of the retrying cycle:

.. code-block:: python

    import kopf

    @kopf.timer('kopfexamples',
                errors=kopf.ErrorsMode.TEMPORARY, interval=10, backoff=5)
    def monitor_kex_by_time(name, retry, **kwargs):
        if retry < 3:
            raise Exception()

It will be executed in that order:

* A new cycle begins:
  * 1st execution attempt fails (``retry == 0``).
  * Waits for 5 seconds (``backoff``).
  * 2nd execution attempt fails (``retry == 1``).
  * Waits for 5 seconds (``backoff``).
  * 3rd execution attempt fails (``retry == 2``).
  * Waits for 5 seconds (``backoff``).
  * 4th execution attempt succeeds (``retry == 3``).
  * Waits for 10 seconds (``interval``).
* A new cycle begins:
  * 5th execution attempt fails (``retry == 0``).

The timer never overlaps with itself. Though, multiple timers with
different interval settings and execution schedules can eventually overlap
with each other and with event-driven handlers.


Results delivery
================

The timers follow the standard :doc:`results delivery <results>` protocol:
the returned values are put on the object's status under the handler's id
as a key.

.. code-block:: python

    import random
    import kopf

    @kopf.timer('kopfexamples', interval=10)
    def ping_kex(spec, **kwargs):
        return random.randint(0, 100)

.. note::

    Whenever a resulting value is serialised and put on the resource's status,
    it modifies the resource, which, in turn, resets the idle timer.
    Use carefully with both idling & returned results.


Filtering
=========

It is also possible to use the existing :doc:`filters`:

.. code-block:: python

    import kopf

    @kopf.timer('kopfexamples', interval=10,
                annotations={'some-annotation': 'some-value'},
                labels={'some-label': 'some-value'},
                when=lambda name, **_: 'some' in name)
    def ping_kex(spec, **kwargs):
        pass


System resources
================

.. warning::

    Timers are implemented the same way as asynchronous daemons
    (see :doc:`daemons`) — via asyncio tasks for every resource & handler.

    Despite OS threads are not involved until the synchronous functions
    are invoked (through the asyncio executors), this can lead to significant
    OS resource usage on large clusters with thousands of resources.

    Make sure you only have daemons and timers with appropriate filters
    (e.g., by labels, annotations, or so).



================================================
FILE: docs/tips-and-tricks.rst
================================================
=============
Tips & Tricks
=============


.. _never-again-filters:

Excluding handlers forever
==========================

Both successful executions and permanent errors of change-detecting handlers
only exclude these handlers from the current handling cycle, which is scoped
to the current change-set (i.e. one diff of an object).
On the next change, the handlers will be invoked again, regardless of their
previous permanent error.

The same is valid for the daemons: they will be spawned on the next operator
restart (assuming that one operator process is one handling cycle for daemons).

To prevent handlers or daemons from being invoked for a specific resource
ever again, even after the operator restarts, use annotations and filters
(or the same for labels or arbitrary fields with ``when=`` callback filtering):

.. code-block:: python

    import kopf

    @kopf.on.update('kopfexamples', annotations={'update-fn-never-again': kopf.ABSENT})
    def update_fn(patch, **_):
        patch.metadata.annotations['update-fn-never-again'] = 'yes'
        raise kopf.PermanentError("Never call update-fn again.")

    @kopf.daemon('kopfexamples', annotations={'monitor-never-again': kopf.ABSENT})
    async def monitor_kex(patch, **kwargs):
        patch.metadata.annotations['monitor-never-again'] = 'yes'

Such a never-again exclusion might be implemented as a feature of Kopf one day,
but it is not available now -- if not done explicitly as shown above.



================================================
FILE: docs/troubleshooting.rst
================================================
===============
Troubleshooting
===============

.. _finalizers-blocking-deletion:

``kubectl`` freezes on object deletion
======================================

This can happen if the operator is down at the moment of deletion.

The operator puts the finalizers on the objects as soon as it notices
them for the first time. When the objects are *requested for deletion*,
Kopf calls the deletion handlers and removes the finalizers,
thus releasing the object for the *actual deletion* by Kubernetes.

If the object has to be deleted without the operator starting again,
you can remove the finalizers manually:

.. code-block:: bash

    kubectl patch kopfexample kopf-example-1 -p '{"metadata": {"finalizers": []}}' --type merge

The object will be removed by Kubernetes immediately.

Alternatively, restart the operator, and allow it to remove the finalizers.



================================================
FILE: docs/vision.rst
================================================
======
Vision
======

Kubernetes `has become a standard de facto`__ for the enterprise infrastructure
management, especially for microservice-based infrastructures.

__ https://www.google.com/search?q=kubernetes+standard+de+facto&oq=kuerbenetes+standard+de+facto

Kubernetes operators have become a common way to extend Kubernetes
with domain objects and domain logic.

At the moment (2018-2019), operators are mostly written in Go
and require advanced knowledge both of Go and Kubernetes internals.
This raises the entry barrier to the operator development field.

In a perfect world of Kopf, Kubernetes operators are a commodity,
used to build the domain logic on top of Kubernetes fast and with ease,
requiring little or no skills in infrastructure management.

For this, Kopf hides the low-level infrastructure details from the user
(i.e. the operator developer),
exposing only the APIs and DSLs needed to express the user's domain.

Besides, Kopf does this in one of the widely used, easy to learn
programming languages: Python.

But Kopf does not go too far in abstracting the Kubernetes internals away:
it avoids the introduction of extra entities and controlling structures
(`Occam's Razor`_, `KISS`_), and most likely it will never have
a mapping of Python classes to Kubernetes resources
(like in the ORMs for the relational databases).

.. _Occam's Razor: https://en.wikipedia.org/wiki/Occam%27s_razor
.. _KISS: https://en.wikipedia.org/wiki/KISS_principle



================================================
FILE: docs/walkthrough/cleanup.rst
================================================
=======
Cleanup
=======

To clean up the cluster after all the experiments are finished:

.. code-block:: bash

    kubectl delete -f obj.yaml
    kubectl delete -f crd.yaml

Alternatively, Minikube can be reset for the full cluster cleanup.



================================================
FILE: docs/walkthrough/creation.rst
================================================
====================
Creating the objects
====================

Previously (:doc:`starting`),
we have created a skeleton operator and learned to start it and see the logs.
Now, let's add a few meaningful reactions to solve our problem (:doc:`problem`).

We want to create a real ``PersistentVolumeClaim`` object
immediately when an ``EphemeralVolumeClaim`` is created this way:

.. code-block:: yaml
    :name: evc
    :caption: evc.yaml

    apiVersion: kopf.dev/v1
    kind: EphemeralVolumeClaim
    metadata:
      name: my-claim
    spec:
      size: 1G

.. code-block:: bash

    kubectl apply -f evc.yaml

First, let's define a template of the persistent volume claim
(with the Python template string, so that no extra template engines are needed):

.. code-block:: yaml
    :name: pvc
    :caption: pvc.yaml

    apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      name: "{name}"
      annotations:
        volume.beta.kubernetes.io/storage-class: standard
    spec:
      accessModes:
        - ReadWriteOnce
      resources:
        requests:
          storage: "{size}"


Let's extend our only handler.
We will use the official Kubernetes client library (``pip install kubernetes``):

.. code-block:: python
    :name: creation
    :caption: ephemeral.py

    import os
    import kopf
    import kubernetes
    import yaml

    @kopf.on.create('ephemeralvolumeclaims')
    def create_fn(spec, name, namespace, logger, **kwargs):

        size = spec.get('size')
        if not size:
            raise kopf.PermanentError(f"Size must be set. Got {size!r}.")

        path = os.path.join(os.path.dirname(__file__), 'pvc.yaml')
        tmpl = open(path, 'rt').read()
        text = tmpl.format(name=name, size=size)
        data = yaml.safe_load(text)

        api = kubernetes.client.CoreV1Api()
        obj = api.create_namespaced_persistent_volume_claim(
            namespace=namespace,
            body=data,
        )

        logger.info(f"PVC child is created: {obj}")

And let us try it in action (assuming the operator is running in the background):

.. code-block:: bash

    kubectl apply -f evc.yaml

Wait 1-2 seconds, and take a look:

.. code-block:: bash

    kubectl get pvc

Now, the PVC can be attached to the pods by the same name, as EVC is named.

.. note::
    If you have to re-run the operator and hit an HTTP 409 error saying
    "persistentvolumeclaims "my-claim" already exists",
    then remove it manually:

    .. code-block:: bash

        kubectl delete pvc my-claim

.. seealso::
    See also :doc:`/handlers`, :doc:`/errors`, :doc:`/hierarchies`.



================================================
FILE: docs/walkthrough/deletion.rst
================================================
=================
Cascaded deletion
=================

Previously (:doc:`creation` & :doc:`updates` & :doc:`diffs`),
we have implemented the creation of a ``PersistentVolumeClaim`` (PVC)
every time an ``EphemeralVolumeClaim`` (EVC) is created,
and cascaded updates of the size and labels when they are changed.

What will happen if the ``EphemeralVolumeClaim`` is deleted?

.. code-block:: bash

    kubectl delete evc my-claim
    kubectl delete -f evc.yaml

By default, from the Kubernetes point of view, the PVC & EVC are not connected.
Hence, the PVC will continue to exist even if its parent EVC is deleted.
Hopefully, some other controller (e.g. the garbage collector) will delete it.
Or maybe not.

We want to make sure the child PVC is deleted when the parent EVC is deleted.

The straightforward way would be to implement a deletion handler
with ``@kopf.on.delete``. But we will go another way, and use the
built-in feature of Kubernetes: `the owner references`__.

__ https://kubernetes.io/docs/concepts/workloads/controllers/garbage-collection/

Let's extend the creation handler:

.. code-block:: python
    :name: adopting
    :caption: ephemeral.py
    :emphasize-lines: 18

    import os
    import kopf
    import kubernetes
    import yaml

    @kopf.on.create('ephemeralvolumeclaims')
    def create_fn(spec, name, namespace, logger, body, **kwargs):

        size = spec.get('size')
        if not size:
            raise kopf.PermanentError(f"Size must be set. Got {size!r}.")

        path = os.path.join(os.path.dirname(__file__), 'pvc.yaml')
        tmpl = open(path, 'rt').read()
        text = tmpl.format(name=name, size=size)
        data = yaml.safe_load(text)

        kopf.adopt(data)

        api = kubernetes.client.CoreV1Api()
        obj = api.create_namespaced_persistent_volume_claim(
            namespace=namespace,
            body=data,
        )

        logger.info(f"PVC child is created: {obj}")

        return {'pvc-name': obj.metadata.name}

With this one line, :func:`kopf.adopt` marks the PVC as a child of EVC.
This includes the name auto-generation (if absent), the label propagation,
the namespace assignment to the parent's object namespace,
and, finally, the owner referencing.

The PVC is now "owned" by the EVC, i.e. it has an owner reference.
When the parent EVC object is deleted,
the child PVC will also be deleted (and terminated in case of pods),
so that we do not need to control this ourselves.



================================================
FILE: docs/walkthrough/diffs.rst
================================================
==================
Diffing the fields
==================

Previously (:doc:`updates`), we have set the size of PVC to be updated
every time the size of EVC is updated, i.e. the cascaded updates.

What will happen if the user re-labels the EVC?

.. code-block:: bash

    kubectl label evc my-claim application=some-app owner=me

Nothing.
The EVC update handler will be called, but it only uses the size field.
Other fields are ignored.

Let's re-label the PVC with the labels of its EVC, and keep them in sync.
The sync is one-way: re-labelling the child PVC does not affect the parent EVC.


Old & New
=========

It can be done the same way as the size update handlers,
but we will use another feature of Kopf to track one specific field only:

.. code-block:: python
    :name: with-new
    :caption: ephemeral.py
    :emphasize-lines: 1, 5

    @kopf.on.field('ephemeralvolumeclaims', field='metadata.labels')
    def relabel(old, new, status, namespace, **kwargs):

        pvc_name = status['create_fn']['pvc-name']
        pvc_patch = {'metadata': {'labels': new}}

        api = kubernetes.client.CoreV1Api()
        obj = api.patch_namespaced_persistent_volume_claim(
            namespace=namespace,
            name=pvc_name,
            body=pvc_patch,
        )

The :kwarg:`old` & :kwarg:`new` kwargs contain the old & new values of the field
(or of the whole object for the object handlers).

It will work as expected when the user adds new labels and changes the existing
labels, but not when the user deletes the labels from the EVC.

Why? Because of how patching works in Kubernetes API:
it *merges* the dictionaries (with some exceptions).
To delete a field from the object, it should be set to ``None``
in the patch object.

So, we should know which fields were deleted from EVC.
Natively, Kubernetes does not provide this information for the object events,
since Kubernetes notifies the operators only with the newest state of the object
-- as seen in :kwarg:`body`/:kwarg:`meta` kwargs.


Diffs
=====

Kopf tracks the state of the objects and calculates the diffs.
The diffs are provided as the :kwarg:`diff` kwarg; the old & new states
of the object or field -- as the :kwarg:`old` & :kwarg:`new` kwargs.

A diff-object has this structure::

    ((action, n-tuple of object or field path, old, new),)

with example::

    (('add', ('metadata', 'labels', 'label1'), None, 'new-value'),
     ('change', ('metadata', 'labels', 'label2'), 'old-value', 'new-value'),
     ('remove', ('metadata', 'labels', 'label3'), 'old-value', None),
     ('change', ('spec', 'size'), '1G', '2G'))

For the field-handlers, it will be the same,
but the field path will be relative to the handled field,
and unrelated fields will be filtered out.
For example, if the field is ``metadata.labels``::

    (('add', ('label1',), None, 'new-value'),
     ('change', ('label2',), 'old-value', 'new-value'),
     ('remove', ('label3',), 'old-value', None))

Now, let's use this feature to explicitly react to the re-labelling of the EVCs.
Note that the ``new`` value for the removed dict key is ``None``,
exactly as needed for the patch object (i.e. the field is present there):

.. code-block:: python
    :name: with-diff
    :caption: ephemeral.py
    :emphasize-lines: 4

    @kopf.on.field('ephemeralvolumeclaims', field='metadata.labels')
    def relabel(diff, status, namespace, **kwargs):

        labels_patch = {field[0]: new for op, field, old, new in diff}
        pvc_name = status['create_fn']['pvc-name']
        pvc_patch = {'metadata': {'labels': labels_patch}}

        api = kubernetes.client.CoreV1Api()
        obj = api.patch_namespaced_persistent_volume_claim(
            namespace=namespace,
            name=pvc_name,
            body=pvc_patch,
        )

Note that the unrelated labels that were put on the PVC ---e.g., manually,
from the template, by other controllers/operators, beside the labels
coming from the parent EVC--- are persisted and never touched
(unless the same-named label is applied to EVC and propagated to the PVC).

.. code-block:: bash

    kubectl describe pvc my-claim

.. code-block:: none

    Name:          my-claim
    Namespace:     default
    StorageClass:  standard
    Status:        Bound
    Labels:        application=some-app
                   owner=me



================================================
FILE: docs/walkthrough/prerequisites.rst
================================================
=================
Environment Setup
=================

We need a running Kubernetes cluster and some tools for our experiments.
If you have a cluster already preconfigured, you can skip this section.
Otherwise, let's install minikube locally (e.g. for MacOS):

* Python >= 3.9 (running in a venv is recommended, though is not necessary).
* `Install kubectl <https://kubernetes.io/docs/tasks/tools/install-kubectl/>`_
* :doc:`Install minikube </minikube>` (a local Kubernetes cluster)
* :doc:`Install Kopf </install>`

.. warning::
    Unfortunately, Minikube cannot handle the PVC/PV resizing,
    as it uses the HostPath provider internally.
    You can either skip the :doc:`updates` step of this tutorial
    (where the sizes of the volumes are changed),
    or you can use an external Kubernetes cluster
    with real dynamically sized volumes.



================================================
FILE: docs/walkthrough/problem.rst
================================================
==============
Sample Problem
==============

Throughout this user documentation, we try to solve
a little real-world problem with Kopf, step by step,
presenting and explaining all the Kopf features one by one.


Problem Statement
=================

In Kubernetes, there are no ephemeral volumes of big sizes, e.g. 500 GB.
By ephemeral, it means that the volume does not persist after it is used.
Such volumes can be used as a workspace for large data-crunching jobs.

There is `Local Ephemeral Storage`__, which allocates some space on a node's
root partition shared with the docker images and other containers,
but it is often limited in size depending on the node/cluster config:

__ https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#local-ephemeral-storage

.. code-block:: yaml

    kind: Pod
    spec:
      containers:
        - name: main
          resources:
            requests:
              ephemeral-storage: 1G
            limits:
              ephemeral-storage: 1G

There is a `PersistentVolumeClaim`__ resource kind, but it is persistent,
i.e. not deleted after they are created (only manually deletable).

__ https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims

There is `StatefulSet`__, which has the volume claim template,
but the volume claim is again persistent,
and the set does not follow the same flow as the Jobs do, more like the Deployments.

__ https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/


Problem Solution
================

We will implement the ``EphemeralVolumeClaim`` object kind,
which will be directly equivalent to ``PersistentVolumeClaim``
(and will use it internally), but with a little extension:

It will be *designated* for a pod or pods with specific selection criteria.

Once used, and all those pods are gone and are not going to be restarted,
the ephemeral volume claim will be deleted after a *grace period*.

For safety, there will be an *expiry period* for the cases when the claim
was not used: e.g. if the pod could not start for some reasons
so that the claim does not remain stale forever.

The lifecycle of an ``EphemeralVolumeClaim`` is this:

* Created by a user with a template of ``PersistentVolumeClaim``
  and a designated pod selector (by labels).

* Waits until the claim is used at least once.

  * At least for N seconds of the safe time to allow the pods to start.

  * At most for M seconds for the case when the pod has failed to start,
    but the claim was created.

* Deletes the ``PersistentVolumeClaim`` after either the pod is finished,
  or the wait time has elapsed.

.. seealso::
    This documentation only highlights the main patterns & tricks of Kopf,
    but does not dive deep into the implementation of the operator's domain.
    The fully functional solution for ``EphemeralVolumeClaim`` resources,
    which is used for this documentation, is available at the following link:

    * https://github.com/nolar/ephemeral-volume-claims



================================================
FILE: docs/walkthrough/resources.rst
================================================
================
Custom Resources
================

Custom Resource Definition
==========================

Let us define a CRD (custom resource definition) for our object:

.. code-block:: yaml
    :caption: crd.yaml
    :name: crd-yaml

    apiVersion: apiextensions.k8s.io/v1
    kind: CustomResourceDefinition
    metadata:
      name: ephemeralvolumeclaims.kopf.dev
    spec:
      scope: Namespaced
      group: kopf.dev
      names:
        kind: EphemeralVolumeClaim
        plural: ephemeralvolumeclaims
        singular: ephemeralvolumeclaim
        shortNames:
          - evcs
          - evc
      versions:
        - name: v1
          served: true
          storage: true
          schema:
            openAPIV3Schema:
              type: object
              properties:
                spec:
                  type: object
                  x-kubernetes-preserve-unknown-fields: true
                status:
                  type: object
                  x-kubernetes-preserve-unknown-fields: true

Note the short names: they can be used as the aliases on the command line,
when getting a list or an object of that kind.

And apply the definition to the cluster:

.. code-block:: bash

    kubectl apply -f crd.yaml

If you want to revert this operation (e.g., to try it again):

.. code-block:: bash

    kubectl delete crd ephemeralvolumeclaims.kopf.dev
    kubectl delete -f crd.yaml


Custom Resource Objects
=======================

Now, we can already create the objects of this kind, apply it to the cluster,
modify and delete them. Nothing will happen, since there is no implemented
logic behind the objects yet.

Let's make a sample object:

.. code-block:: yaml
    :caption: obj.yaml
    :name: obj-yaml

    apiVersion: kopf.dev/v1
    kind: EphemeralVolumeClaim
    metadata:
      name: my-claim

This is the minimal yaml file needed, with no spec or fields inside.
We will add them later.

Apply it to the cluster:

.. code-block:: bash

    kubectl apply -f obj.yaml

Get a list of the existing objects of this kind with one of the commands:

.. code-block:: bash

    kubectl get EphemeralVolumeClaim
    kubectl get ephemeralvolumeclaims
    kubectl get ephemeralvolumeclaim
    kubectl get evcs
    kubectl get evc

Please note that we can use the short names as specified
on the custom resource definition.

.. seealso::
    * kubectl imperative style (create/edit/patch/delete)
    * kubectl declarative style (apply)



================================================
FILE: docs/walkthrough/starting.rst
================================================
=====================
Starting the operator
=====================

Previously, we have defined a :doc:`problem <problem>` that we are solving,
and created the :doc:`custom resource definitions <resources>`
for the ephemeral volume claims.

Now, we are ready to write some logic for this kind of objects.
Let's start with an operator skeleton that does nothing useful --
just to see how it can be started.

.. code-block:: python
    :name: skeleton
    :caption: ephemeral.py

    import kopf
    import logging

    @kopf.on.create('ephemeralvolumeclaims')
    def create_fn(body, **kwargs):
        logging.info(f"A handler is called with body: {body}")

.. note::
    Despite an obvious desire, do not name the file as ``operator.py``,
    since there is a built-in module in Python 3 with this name,
    and there could be potential conflicts on the imports.

Let's run the operator and see what will happen:

.. code-block:: bash

    kopf run ephemeral.py --verbose


The output looks like this:

.. code-block:: none

    [2019-05-31 10:42:11,870] kopf.config          [DEBUG   ] configured via kubeconfig file
    [2019-05-31 10:42:11,913] kopf.reactor.peering [WARNING ] Default peering object is not found, falling back to the standalone mode.
    [2019-05-31 10:42:12,037] kopf.reactor.handlin [DEBUG   ] [default/my-claim] First appearance: {'apiVersion': 'kopf.dev/v1', 'kind': 'EphemeralVolumeClaim', 'metadata': {'annotations': {'kubectl.kubernetes.io/last-applied-configuration': '{"apiVersion":"kopf.dev/v1","kind":"EphemeralVolumeClaim","metadata":{"annotations":{},"name":"my-claim","namespace":"default"}}\n'}, 'creationTimestamp': '2019-05-29T00:41:57Z', 'generation': 1, 'name': 'my-claim', 'namespace': 'default', 'resourceVersion': '47720', 'selfLink': '/apis/kopf.dev/v1/namespaces/default/ephemeralvolumeclaims/my-claim', 'uid': '904c2b9b-81aa-11e9-a202-a6e6b278a294'}}
    [2019-05-31 10:42:12,038] kopf.reactor.handlin [DEBUG   ] [default/my-claim] Adding the finalizer, thus preventing the actual deletion.
    [2019-05-31 10:42:12,038] kopf.reactor.handlin [DEBUG   ] [default/my-claim] Patching with: {'metadata': {'finalizers': ['KopfFinalizerMarker']}}
    [2019-05-31 10:42:12,165] kopf.reactor.handlin [DEBUG   ] [default/my-claim] Creation is in progress: {'apiVersion': 'kopf.dev/v1', 'kind': 'EphemeralVolumeClaim', 'metadata': {'annotations': {'kubectl.kubernetes.io/last-applied-configuration': '{"apiVersion":"kopf.dev/v1","kind":"EphemeralVolumeClaim","metadata":{"annotations":{},"name":"my-claim","namespace":"default"}}\n'}, 'creationTimestamp': '2019-05-29T00:41:57Z', 'finalizers': ['KopfFinalizerMarker'], 'generation': 1, 'name': 'my-claim', 'namespace': 'default', 'resourceVersion': '47732', 'selfLink': '/apis/kopf.dev/v1/namespaces/default/ephemeralvolumeclaims/my-claim', 'uid': '904c2b9b-81aa-11e9-a202-a6e6b278a294'}}
    [2019-05-31 10:42:12,166] root                 [INFO    ] A handler is called with body: {'apiVersion': 'kopf.dev/v1', 'kind': 'EphemeralVolumeClaim', 'metadata': {'annotations': {'kubectl.kubernetes.io/last-applied-configuration': '{"apiVersion":"kopf.dev/v1","kind":"EphemeralVolumeClaim","metadata":{"annotations":{},"name":"my-claim","namespace":"default"}}\n'}, 'creationTimestamp': '2019-05-29T00:41:57Z', 'finalizers': ['KopfFinalizerMarker'], 'generation': 1, 'name': 'my-claim', 'namespace': 'default', 'resourceVersion': '47732', 'selfLink': '/apis/kopf.dev/v1/namespaces/default/ephemeralvolumeclaims/my-claim', 'uid': '904c2b9b-81aa-11e9-a202-a6e6b278a294'}, 'spec': {}, 'status': {}}
    [2019-05-31 10:42:12,168] kopf.reactor.handlin [DEBUG   ] [default/my-claim] Invoking handler 'create_fn'.
    [2019-05-31 10:42:12,173] kopf.reactor.handlin [INFO    ] [default/my-claim] Handler 'create_fn' succeeded.
    [2019-05-31 10:42:12,210] kopf.reactor.handlin [INFO    ] [default/my-claim] All handlers succeeded for creation.
    [2019-05-31 10:42:12,223] kopf.reactor.handlin [DEBUG   ] [default/my-claim] Patching with: {'status': {'kopf': {'progress': None}}, 'metadata': {'annotations': {'kopf.zalando.org/last-handled-configuration': '{"apiVersion": "kopf.dev/v1", "kind": "EphemeralVolumeClaim", "metadata": {"name": "my-claim", "namespace": "default"}, "spec": {}}'}}}
    [2019-05-31 10:42:12,342] kopf.reactor.handlin [DEBUG   ] [default/my-claim] Updating is in progress: {'apiVersion': 'kopf.dev/v1', 'kind': 'EphemeralVolumeClaim', 'metadata': {'annotations': {'kopf.zalando.org/last-handled-configuration': '{"apiVersion": "kopf.dev/v1", "kind": "EphemeralVolumeClaim", "metadata": {"name": "my-claim", "namespace": "default"}, "spec": {}}', 'kubectl.kubernetes.io/last-applied-configuration': '{"apiVersion":"kopf.dev/v1","kind":"EphemeralVolumeClaim","metadata":{"annotations":{},"name":"my-claim","namespace":"default"}}\n'}, 'creationTimestamp': '2019-05-29T00:41:57Z', 'finalizers': ['KopfFinalizerMarker'], 'generation': 2, 'name': 'my-claim', 'namespace': 'default', 'resourceVersion': '47735', 'selfLink': '/apis/kopf.dev/v1/namespaces/default/ephemeralvolumeclaims/my-claim', 'uid': '904c2b9b-81aa-11e9-a202-a6e6b278a294'}, 'status': {'kopf': {}}}
    [2019-05-31 10:42:12,343] kopf.reactor.handlin [INFO    ] [default/my-claim] All handlers succeeded for update.
    [2019-05-31 10:42:12,362] kopf.reactor.handlin [DEBUG   ] [default/my-claim] Patching with: {'status': {'kopf': {'progress': None}}, 'metadata': {'annotations': {'kopf.zalando.org/last-handled-configuration': '{"apiVersion": "kopf.dev/v1", "kind": "EphemeralVolumeClaim", "metadata": {"name": "my-claim", "namespace": "default"}, "spec": {}}'}}}

Note that the operator has noticed an object created before the operator
was even started, and handled the object because it was not handled before.

Now, you can stop the operator with Ctrl-C (twice), and start it again:

.. code-block:: bash

    kopf run ephemeral.py --verbose

The operator will not handle the object, as now it is already successfully
handled. This is important in case the operator is restarted if it runs
in a normally deployed pod, or when you restart the operator for debugging.

Let's delete and re-create the same object to see the operator reacting:

.. code-block:: bash

    kubectl delete -f obj.yaml
    kubectl apply -f obj.yaml



================================================
FILE: docs/walkthrough/updates.rst
================================================
====================
Updating the objects
====================

.. warning::
    Unfortunately, Minikube cannot handle the PVC/PV resizing,
    as it uses the HostPath provider internally.
    You can either skip this step of the tutorial,
    or you can use an external Kubernetes cluster
    with real dynamically sized volumes.

Previously (:doc:`creation`),
we have implemented a handler for the creation of an ``EphemeralVolumeClaim`` (EVC),
and created the corresponding ``PersistantVolumeClaim`` (PVC).

What will happen if we change the size of the EVC when it already exists?
The PVC must be updated accordingly to match its parent EVC.

First, we have to remember the name of the created PVC:
Let's extend the creation handler we already have from the previous step
with one additional line:

.. code-block:: python
    :caption: ephemeral.py
    :emphasize-lines: 21

    @kopf.on.create('ephemeralvolumeclaims')
    def create_fn(spec, name, namespace, logger, **kwargs):

        size = spec.get('size')
        if not size:
            raise kopf.PermanentError(f"Size must be set. Got {size!r}.")

        path = os.path.join(os.path.dirname(__file__), 'pvc.yaml')
        tmpl = open(path, 'rt').read()
        text = tmpl.format(size=size, name=name)
        data = yaml.safe_load(text)

        api = kubernetes.client.CoreV1Api()
        obj = api.create_namespaced_persistent_volume_claim(
            namespace=namespace,
            body=data,
        )

        logger.info(f"PVC child is created: {obj}")

        return {'pvc-name': obj.metadata.name}

Whatever is returned from any handler, is stored in the object's status
under that handler id (which is the function name by default).
We can see that with kubectl:

.. code-block:: bash

    kubectl get -o yaml evc my-claim

.. code-block:: yaml

    spec:
      size: 1G
    status:
      create_fn:
        pvc-name: my-claim
      kopf: {}

.. note::
    If the above change causes ``Patching failed with inconsistencies``
    debug warnings and/or your EVC YAML doesn't show a ``.status`` field,
    make sure you have set the ``x-kubernetes-preserve-unknown-fields: true``
    field in your CRD on either the entire object or just the ``.status`` field
    as detailed in :doc:`resources`.
    Without setting this field, Kubernetes will prune the ``.status`` field
    when Kopf tries to update it. For more info on field pruning,
    see `the Kubernetes docs
    <https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#field-pruning>`_.

Let's add a yet another handler, but for the "update" cause.
This handler gets this stored PVC name from the creation handler,
and patches the PVC with the new size from the EVC::

    @kopf.on.update('ephemeralvolumeclaims')
    def update_fn(spec, status, namespace, logger, **kwargs):

        size = spec.get('size', None)
        if not size:
            raise kopf.PermanentError(f"Size must be set. Got {size!r}.")

        pvc_name = status['create_fn']['pvc-name']
        pvc_patch = {'spec': {'resources': {'requests': {'storage': size}}}}

        api = kubernetes.client.CoreV1Api()
        obj = api.patch_namespaced_persistent_volume_claim(
            namespace=namespace,
            name=pvc_name,
            body=pvc_patch,
        )

        logger.info(f"PVC child is updated: {obj}")

Now, let's change the EVC's size:

.. code-block:: bash

    kubectl edit evc my-claim

Or by patching it:

.. code-block:: bash

    kubectl patch evc my-claim --type merge -p '{"spec": {"size": "2G"}}'

Keep in mind the PVC size can only be increased, never decreased.

Give the operator a few seconds to handle the change.

Check the size of the actual PV behind the PVC, which is now increased:

.. code-block:: bash

    kubectl get pv

.. code-block:: none

    NAME                                       CAPACITY   ACCESS MODES   ...
    pvc-a37b65bd-8384-11e9-b857-42010a800265   2Gi        RWO            ...

.. warning::
    Kubernetes & ``kubectl`` improperly show the capacity of PVCs:
    it remains the same (1G) event after the change.
    The size of the actual PV (Persistent Volume) of each PVC is important!
    This issue is not related to Kopf, so we go around it.



================================================
FILE: examples/README.md
================================================
# Kopf examples

For the examples to work, a sample CRD (Custom Resource Definition) should be created:

```bash
kubectl apply -f crd.yaml
```

Also, some libraries are needed for some operators and handlers:

```bash
pip install -r requirements.txt
```



================================================
FILE: examples/crd.yaml
================================================
# A demo CRD for the Kopf example operators.
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: kopfexamples.kopf.dev
spec:
  scope: Namespaced
  group: kopf.dev
  names:
    kind: KopfExample
    plural: kopfexamples
    singular: kopfexample
    shortNames:
      - kopfexes
      - kopfex
      - kexes
      - kex
  versions:
    - name: v1
      served: true
      storage: true
      subresources: { status: { } }  # comment/uncomment for experiments
      schema:
        openAPIV3Schema:
          type: object
          properties:
            spec:
              type: object
              x-kubernetes-preserve-unknown-fields: true
            status:
              type: object
              x-kubernetes-preserve-unknown-fields: true
      additionalPrinterColumns:
        - name: Duration
          type: string
          priority: 0
          jsonPath: .spec.duration
          description: For how long the pod should sleep.
        - name: Children
          type: string
          priority: 0
          jsonPath: .status.create_fn.children
          description: The children pods created.
        - name: Message
          type: string
          priority: 0
          jsonPath: .status.create_fn.message
          description: As returned from the handler (sometimes).



================================================
FILE: examples/obj.yaml
================================================
# A demo custom resource for the Kopf example operators.
apiVersion: kopf.dev/v1
kind: KopfExample
metadata:
  name: kopf-example-1
  labels:
    somelabel: somevalue
  annotations:
    someannotation: somevalue
spec:
  duration: 1m
  field: value
  items:
  - item1
  - item2



================================================
FILE: examples/requirements.txt
================================================
kopf
pykube-ng
pyyaml



================================================
FILE: examples/.isort.cfg
================================================
[settings]
line_length = 100
multi_line_output = 11
balanced_wrapping = true
combine_as_imports = true
case_sensitive = true

known_third_party = kopf

filter_files = true
skip_glob = kopf/**



================================================
FILE: examples/01-minimal/README.md
================================================
# Kopf minimal example

The minimum codebase needed for to make a runnable Kubernetes operator.

Start the operator:

```bash
kopf run example.py --verbose
```

It does nothing useful, just notices the object creation,
and prints the message to stdout -- can be seen in the operator's output.

In addition, the object's status is updated, as can be seen here:

```bash
$ kubectl apply -f ../obj.yaml
$ kubectl get kopfexamples
NAME             DURATION   CHILDREN   MESSAGE
kopf-example-1   1m                    hello world
```

```bash
$ kubectl describe KopfExample kopf-example-1
Name:         kopf-example-1
Namespace:    default
Labels:       somelabel=somevalue
...
Status:
  Message:  hello world
Events:
  Type    Reason    Age   From  Message
  ----    ------    ----  ----  -------
  Normal  Finished  42s   kopf  All handlers succeeded.
  Normal  Success   43s   kopf  Handler create_fn succeeded.
```

```bash
$ kubectl get KopfExample kopf-example-1 -o yaml
apiVersion: kopf.dev/v1
kind: KopfExample
metadata:
  ...
spec:
  duration: 1m
  field: value
  items:
  - item1
  - item2
status:
  message: hello world
```

Cleanup in the end:

```bash
$ kubectl delete -f ../obj.yaml
```



================================================
FILE: examples/01-minimal/example.py
================================================
import kopf


@kopf.on.create('kopfexamples')
def create_fn(spec, **kwargs):
    print(f"And here we are! Creating: {spec}")
    return {'message': 'hello world'}  # will be the new status



================================================
FILE: examples/02-children/README.md
================================================
# Kopf example with children

This example creates a `Pod` for every created `KopfExample` object,
and attaches it as a child of that example object. The latter means that
when the parent object is deleted, the child pod is also terminated.

Start the operator:

```bash
kopf run example.py --verbose
```

The child pod's id is stored as the parent's status field,
so that it can be seen on the object listing (see also `crd.yaml`):

```bash
$ kubectl apply -f ../obj.yaml
$ kubectl get kopfexamples
NAME             FIELD   CHILDREN
kopf-example-1   value   [aed7f7ac-2971-11e9-b4d3-061441377794]

$ kubectl get pod -l somelabel=somevalue
NAME                   READY   STATUS    RESTARTS   AGE
kopf-example-1-jvlfs   1/1     Running   0          26s
```

```bash
$ kubectl delete -f ../obj.yaml
$ kubectl get pod -l somelabel=somevalue
NAME                   READY   STATUS        RESTARTS   AGE
kopf-example-1-jvlfs   1/1     Terminating   0          52s
```

Cleanup in the end:

```bash
$ kubectl delete -f ../obj.yaml
```



================================================
FILE: examples/02-children/example.py
================================================
import kopf
import pykube
import yaml


@kopf.on.create('kopfexamples')
def create_fn(spec, **kwargs):

    # Render the pod yaml with some spec fields used in the template.
    doc = yaml.safe_load(f"""
        apiVersion: v1
        kind: Pod
        spec:
          containers:
          - name: the-only-one
            image: busybox
            command: ["sh", "-x", "-c"]
            args:
            - |
              echo "FIELD=$FIELD"
              sleep {spec.get('duration', 0)}
            env:
            - name: FIELD
              value: {spec.get('field', 'default-value')}
    """)

    # Make it our child: assign the namespace, name, labels, owner references, etc.
    kopf.adopt(doc)

    # Actually create an object by requesting the Kubernetes API.
    api = pykube.HTTPClient(pykube.KubeConfig.from_env())
    pod = pykube.Pod(api, doc)
    pod.create()
    api.session.close()

    # Update the parent's status.
    return {'children': [pod.metadata['uid']]}



================================================
FILE: examples/03-exceptions/README.md
================================================
# Kopf example with exceptions in the handler

This example raises the exceptions in the handler,
so that it is retried a few times until it succeeds.

Start the operator:

```bash
kopf run example.py --verbose
```

Observe how the exceptions are repored in the operator's log (stderr),
and also briefly reported as the events on the processed object:

```bash
$ kubectl apply -f ../obj.yaml
$ kubectl describe kopfexample kopf-example-1
Name:         kopf-example-1
Namespace:    default
Labels:       somelabel=somevalue
...
Status:
Events:
  Type    Reason       Age   From  Message
  ----    ------       ----  ----  -------
  Error   Exception    9s    kopf  Handler create_fn failed.: First failure.
  Error   MyException  6s    kopf  Handler create_fn failed.: Second failure.
  Normal  Success      4s    kopf  Handler create_fn succeeded.
  Normal  Finished     4s    kopf  All handlers succeeded.
```

Cleanup in the end:

```bash
$ kubectl delete -f ../obj.yaml
```



================================================
FILE: examples/03-exceptions/example.py
================================================
import kopf


class MyException(Exception):
    pass


@kopf.on.create('kopfexamples')
def instant_failure_with_only_a_message(**kwargs):
    raise kopf.PermanentError("Fail once and for all.")


@kopf.on.create('kopfexamples')
def eventual_success_with_a_few_messages(retry, **kwargs):
    if retry < 3:  # 0, 1, 2, 3
        raise kopf.TemporaryError("Expected recoverable error.", delay=1.0)


@kopf.on.create('kopfexamples', retries=3, backoff=1.0)
def eventual_failure_with_tracebacks(**kwargs):
    raise MyException("An error that is supposed to be recoverable.")


@kopf.on.create('kopfexamples', errors=kopf.ErrorsMode.PERMANENT, backoff=1.0)
def instant_failure_with_traceback(**kwargs):
    raise MyException("An error that is supposed to be recoverable.")


# Marks for the e2e tests (see tests/e2e/test_examples.py):
E2E_ALLOW_TRACEBACKS = True
E2E_CREATION_STOP_WORDS = ['Something has changed,']
E2E_SUCCESS_COUNTS = {'eventual_success_with_a_few_messages': 1}
E2E_FAILURE_COUNTS = {'eventual_failure_with_tracebacks': 1,
                      'instant_failure_with_traceback': 1,
                      'instant_failure_with_only_a_message': 1}



================================================
FILE: examples/04-events/README.md
================================================
# Kopf example with the event reporting

The framework reports some basic events on the handling progress.
But the developers can report their own events conveniently.

Start the operator:

```bash
kopf run example.py --verbose
```

The events are shown on the object's description
(and are usually garbage-collected after a few minutes).

```bash
$ kubectl apply -f ../obj.yaml
$ kubectl describe kopfexample kopf-example-1
...
Events:
  Type      Reason      Age   From  Message
  ----      ------      ----  ----  -------
  Normal    SomeReason  5s    kopf  Some message
  Normal    Success     5s    kopf  Handler create_fn succeeded.
  SomeType  SomeReason  6s    kopf  Some message
  Normal    Finished    5s    kopf  All handlers succeeded.
  Error     SomeReason  5s    kopf  Some exception: Exception text.
  Warning   SomeReason  5s    kopf  Some message

```

Note that the events are shown out of any order -- this is a behaviour of the CLI tool or of the API.
It has nothing to do with the framework: the framework reports the timestamps properly.

Cleanup in the end:

```bash
$ kubectl delete -f ../obj.yaml
```



================================================
FILE: examples/04-events/example.py
================================================
"""
Send the custom events for the handled or other objects.
"""
import kopf


@kopf.on.create('kopfexamples')
def create_fn(body, **kwargs):

    # The all-purpose function for the event creation.
    kopf.event(body, type="SomeType", reason="SomeReason", message="Some message")

    # The shortcuts for the conventional events and common cases.
    kopf.info(body, reason="SomeReason", message="Some message")
    kopf.warn(body, reason="SomeReason", message="Some message")

    try:
        raise RuntimeError("Exception text.")
    except Exception:
        kopf.exception(body, reason="SomeReason", message="Some exception:")



================================================
FILE: examples/05-handlers/README.md
================================================
# Kopf example with multiple handlers

Multiple handlers can be registered for the same event.
They are executed in the order of registration.

Besides the standard create-update-delete events, a per-field diff can be registered.
It is called only in case of the specified field changes,
with `old` & `new` set to that field's values.

Start the operator (we skip the verbose mode here, for clarity):

```bash
kopf run example.py
```

Trigger the object creation and monitor the stderr of the operator:

```bash
$ kubectl apply -f ../obj.yaml
```

```
CREATED 1st
[2019-02-05 20:33:50,336] kopf.handling        [INFO    ] [default/kopf-example-1] Handler create_fn_1 succeeded.
CREATED 2nd
[2019-02-05 20:33:50,557] kopf.handling        [INFO    ] [default/kopf-example-1] Handler create_fn_2 succeeded.
[2019-02-05 20:33:50,781] kopf.handling        [INFO    ] [default/kopf-example-1] All handlers succeeded.
```

Now, trigger the object change:

```bash
$ kubectl patch -f ../obj.yaml --type merge -p '{"spec": {"field": "newvalue", "newfield": 100}}'
```

```
UPDATED
[2019-02-05 20:34:06,358] kopf.handling        [INFO    ] [default/kopf-example-1] Handler update_fn succeeded.
FIELD CHANGED: value -> newvalue
[2019-02-05 20:34:06,682] kopf.handling        [INFO    ] [default/kopf-example-1] Handler field_fn/spec.field succeeded.
[2019-02-05 20:34:06,903] kopf.handling        [INFO    ] [default/kopf-example-1] All handlers succeeded.
```

Finally, delete the object:

```bash
$ kubectl delete -f ../obj.yaml
```

```
DELETED 1st
[2019-02-05 20:34:42,496] kopf.handling        [INFO    ] [default/kopf-example-1] Handler delete_fn_1 succeeded.
DELETED 2nd
[2019-02-05 20:34:42,715] kopf.handling        [INFO    ] [default/kopf-example-1] Handler delete_fn_2 succeeded.
[2019-02-05 20:34:42,934] kopf.handling        [INFO    ] [default/kopf-example-1] All handlers succeeded.
```



================================================
FILE: examples/05-handlers/example.py
================================================
import kopf


@kopf.on.resume('kopfexamples')
def resume_fn_1(**kwargs):
    print(f'RESUMED 1st')


@kopf.on.create('kopfexamples')
def create_fn_1(**kwargs):
    print('CREATED 1st')


@kopf.on.resume('kopfexamples')
def resume_fn_2(**kwargs):
    print(f'RESUMED 2nd')


@kopf.on.create('kopfexamples')
def create_fn_2(**kwargs):
    print('CREATED 2nd')


@kopf.on.update('kopfexamples')
def update_fn(old, new, diff, **kwargs):
    print('UPDATED')


@kopf.on.delete('kopfexamples')
def delete_fn_1(**kwargs):
    print('DELETED 1st')


@kopf.on.delete('kopfexamples')
def delete_fn_2(**kwargs):
    print('DELETED 2nd')


@kopf.on.field('kopfexamples', field='spec.field')
def field_fn(old, new, **kwargs):
    print(f'FIELD CHANGED: {old} -> {new}')



================================================
FILE: examples/06-peering/README.md
================================================
# Kopf example with multiple processes and development mode

When multiple operators start for the same cluster (in the cluster or outside),
they become aware about each other, and exchange the basic information about
their liveness and the priorities, and cooperate to avoid the undesired
side-effects (e.g., duplicated children creation, infinite cross-changes).

The main use-case for this is the development mode: when a developer starts
an operator on their workstation, all the deployed operators should pause
and stop processing of the objects, until the developer's operator exits.

In shell A, start an operator:

```bash
kopf run example.py --verbose
```

In shell B, start another operator:

```bash
kopf run example.py --verbose
```

Notice how both A & B complain about the same-priority sibling operator:

```
[2019-02-05 20:42:39,052] kopf.peering         [WARNING ] Possibly conflicting operators with the same priority: [Peer(089e5a18a71d4660b07ae37acc776250, priority=0, lastseen=2019-02-05 19:42:38.932613, lifetime=0:01:00)].
```

```
[2019-02-05 20:42:39,223] kopf.peering         [WARNING ] Possibly conflicting operators with the same priority: [Peer(590581cbceff403e90a3e874379c4daf, priority=0, lastseen=2019-02-05 19:42:23.241150, lifetime=0:01:00)].
```

Now, stop the operator B wtih Ctrl+C (twice), and start it with `--dev` option
(equivalent to `--priority 666`):

```bash
kopf run example.py --verbose --dev
```

Observe how the operator A pauses and lets
operator B to take control over the objects.

```
[2019-02-05 20:43:40,360] kopf.peering         [INFO    ] Pausing operations in favour of [Peer(54e7054f28d948c4985db79410c9ef4a, priority=666, lastseen=2019-02-05 19:43:40.166561, lifetime=0:01:00)].
```

Stop the operator B again with Ctrl+C (twice).
The operator A resumes its operations:

```
[2019-02-05 20:44:54,311] kopf.peering         [INFO    ] Resuming operations after the pause.
```

The same can be achieved with the explicit CLI commands:

```bash
kopf freeze --lifetime 60 --priority 100
kopf resume
```

```
[2019-02-05 20:45:34,354] kopf.peering         [INFO    ] Pausing operations in favour of [Peer(manual, priority=100, lastseen=2019-02-05 19:45:34.226070, lifetime=0:01:00)].
[2019-02-05 20:45:49,427] kopf.peering         [INFO    ] Resuming operations after the pause.
```



================================================
FILE: examples/06-peering/example.py
================================================
import kopf


@kopf.on.create('kopfexamples')
def create_fn(**kwargs):
    pass



================================================
FILE: examples/07-subhandlers/README.md
================================================
# Kopf example with dynamic sub-handlers

It is convenient to re-use the framework's capabilities to track
the handler execution, to skip the finished or failed handlers,
and to retry to recoverable errors -- without the reimplemenation
of the same logic inside of the handlers.

In some cases, however, the required handlers can be identified
only at the handling time, mostly when they are based on the spec,
or on some external environment (databases, remote APIs, other objects).

For this case, the sub-handlers can be useful. The sub-handlers "extend"
the main handler, inside of which they are defined, but delegate
the progress tracking to the framework.

In all aspects, the sub-handler are the same as other handlers:
the same function signatures, the same execution environment,
the same error handling, etc.

Start the operator:

```bash
kopf run example.py --verbose
```

Trigger the object creation and monitor the stderr of the operator:

```bash
$ kubectl apply -f ../obj.yaml
```

Observe how the sub-handlers are nested within the parent handler,
and use the `spec.items` to dynamically decide how many and which
sub-handlers must be executed.

```
[2019-02-19 16:05:56,432] kopf.reactor.handlin [DEBUG   ] [default/kopf-example-1] First appearance: ...
[2019-02-19 16:05:56,432] kopf.reactor.handlin [DEBUG   ] [default/kopf-example-1] Adding the finalizer, thus preventing the actual deletion.

[2019-02-19 16:05:56,645] kopf.reactor.handlin [DEBUG   ] [default/kopf-example-1] Creation is in progress: ...
[2019-02-19 16:05:56,650] kopf.reactor.handlin [DEBUG   ] [default/kopf-example-1] Invoking handler create_fn.
[2019-02-19 16:05:56,654] kopf.reactor.handlin [DEBUG   ] [default/kopf-example-1] Invoking handler create_fn/item1.

=== Handling creation for item1. ===

[2019-02-19 16:05:56,656] kopf.reactor.handlin [INFO    ] [default/kopf-example-1] Handler create_fn/item1 succeeded.
[2019-02-19 16:05:56,982] kopf.reactor.handlin [INFO    ] [default/kopf-example-1] Handler create_fn has unfinished sub-handlers. Will retry soon.

[2019-02-19 16:05:57,200] kopf.reactor.handlin [DEBUG   ] [default/kopf-example-1] Creation is in progress: ...
[2019-02-19 16:05:57,201] kopf.reactor.handlin [DEBUG   ] [default/kopf-example-1] Invoking handler create_fn.
[2019-02-19 16:05:57,203] kopf.reactor.handlin [DEBUG   ] [default/kopf-example-1] Invoking handler create_fn/item2.

=== Handling creation for item2. ===

[2019-02-19 16:05:57,208] kopf.reactor.handlin [INFO    ] [default/kopf-example-1] Handler create_fn/item2 succeeded.
[2019-02-19 16:05:57,419] kopf.reactor.handlin [INFO    ] [default/kopf-example-1] Handler create_fn succeeded.

[2019-02-19 16:05:57,634] kopf.reactor.handlin [INFO    ] [default/kopf-example-1] All handlers succeeded for creation.
```

Try creating the object with more items in it to see more sub-handlers
executed (note: do not change it, but re-create it, as only the creation handler
is implemented in this example; or implement the update handler yourselves).

Cleanup in the end:

```bash
$ kubectl delete -f ../obj.yaml
```



================================================
FILE: examples/07-subhandlers/example.py
================================================
import kopf


@kopf.on.create('kopfexamples')
def create_fn(spec, **kwargs):

    for item in spec.get('items', []):

        @kopf.subhandler(id=item)
        async def create_item_fn(item=item, **kwargs):
            print(f"=== Handling creation for {item}. ===")



================================================
FILE: examples/08-events/README.md
================================================
# Kopf example with spy-handlers for the raw events

Kopf stores its handler status on the objects' status field.
This can be not desired when the objects do not belong to this operator,
but a probably served by some other operator, and are just watched
by the current operator, e.g. for their status fields.

Event-watching handlers can be used as the silent spies on the raw events:
they do not store anything on the object, and do not create the k8s-events.

If the event handler fails, the error is logged to the operator's log,
and then ignored.

Please note that the event handlers are invoked for *every* event received
from the watching stream. This also includes the first-time listing when
the operator starts or restarts. It is the developer's responsibility to make
the handlers idempotent (re-executable with do duplicated side-effects).

Start the operator:

```bash
kopf run example.py --verbose
```

Trigger the object creation and monitor the stderr of the operator:

```bash
$ kubectl apply -f ../obj.yaml
```

Observe how the event-handlers are invoked.

```
[2019-05-28 11:03:29,537] kopf.reactor.handlin [DEBUG   ] [default/kopf-example-1] Invoking handler 'event_fn_with_error'.
[2019-05-28 11:03:29,537] kopf.reactor.handlin [ERROR   ] [default/kopf-example-1] Handler 'event_fn_with_error' failed with an exception. Will ignore.
Traceback (most recent call last):
  File ".../kopf/reactor/handling.py", line 159, in handle_event
  File ".../kopf/reactor/invocation.py", line 64, in invoke
  File "example.py", line 6, in event_fn_with_error
    raise Exception("Oops!")
Exception: Oops!

[2019-05-28 11:03:29,541] kopf.reactor.handlin [DEBUG   ] [default/kopf-example-1] Invoking handler 'normal_event_fn'.
Event received: {'type': 'ADDED', 'object': {'apiVersion': 'kopf.dev/v1', 'kind': 'KopfExample', ...}
[2019-05-28 11:03:29,541] kopf.reactor.handlin [INFO    ] [default/kopf-example-1] Handler 'normal_event_fn' succeeded.
```

Cleanup in the end:

```bash
$ kubectl delete -f ../obj.yaml
```



================================================
FILE: examples/08-events/example.py
================================================
import kopf


@kopf.on.event('kopfexamples')
def event_fn_with_error(**kwargs):
    raise Exception("Oops!")


@kopf.on.event('kopfexamples')
def normal_event_fn(event, **kwargs):
    print(f"Event received: {event!r}")


# Marks for the e2e tests (see tests/e2e/test_examples.py):
E2E_ALLOW_TRACEBACKS = True
E2E_SUCCESS_COUNTS = {'normal_event_fn': 2}



================================================
FILE: examples/09-testing/README.md
================================================
# Kopf example for testing the operator

Kopf provides some basic tools to test the Kopf-based operators.
With these tools, the testing frameworks (pytest in this case)
can run the operator-under-test in the background, while the test
performs the resource manipulation.

To run the tests:

```bash
pytest
```



================================================
FILE: examples/09-testing/example.py
================================================
import kopf


@kopf.on.create('kopfexamples')
def create_fn(logger, **kwargs):
    logger.info("Something was logged here.")



================================================
FILE: examples/09-testing/test_example_09.py
================================================
import os.path
import subprocess
import time

import kopf.testing
import pytest

crd_yaml = os.path.relpath(os.path.join(os.path.dirname(__file__), '..', 'crd.yaml'))
obj_yaml = os.path.relpath(os.path.join(os.path.dirname(__file__), '..', 'obj.yaml'))
example_py = os.path.relpath(os.path.join(os.path.dirname(__file__), 'example.py'))


@pytest.fixture(autouse=True)
def crd_exists():
    subprocess.run(f"kubectl apply -f {crd_yaml}",
                   check=True, timeout=10, capture_output=True, shell=True)


@pytest.fixture(autouse=True)
def obj_absent():
    # Operator is not running in fixtures, so we need a force-delete (or this patch).
    subprocess.run(['kubectl', 'patch', '-f', obj_yaml,
                    '-p', '{"metadata":{"finalizers":[]}}',
                    '--type', 'merge'],
                   check=False, timeout=10, capture_output=True)
    subprocess.run(f"kubectl delete -f {obj_yaml}",
                   check=False, timeout=10, capture_output=True, shell=True)


def test_resource_lifecycle():

    # To prevent lengthy threads in the loop executor when the process exits.
    settings = kopf.OperatorSettings()
    settings.watching.server_timeout = 10

    # Run an operator and simulate some activity with the operated resource.
    with kopf.testing.KopfRunner(
        ['run', '--all-namespaces', '--verbose', '--standalone', example_py],
        timeout=60, settings=settings,
    ) as runner:

        subprocess.run(f"kubectl create -f {obj_yaml}",
                       shell=True, check=True, timeout=10, capture_output=True)
        time.sleep(5)  # give it some time to react
        subprocess.run(f"kubectl delete -f {obj_yaml}",
                       shell=True, check=True, timeout=10, capture_output=True)
        time.sleep(1)  # give it some time to react

    # Ensure that the operator did not die on start, or during the operation.
    assert runner.exception is None
    assert runner.exit_code == 0

    # There are usually more than these messages, but we only check for the certain ones.
    assert '[default/kopf-example-1] Creation is in progress:' in runner.output
    assert '[default/kopf-example-1] Something was logged here.' in runner.output



================================================
FILE: examples/10-builtins/README.md
================================================
# Kopf example for built-in resources

Kopf can also handle the built-in resources, such as Pods, Jobs, etc.

In this example, we take control all over the pods (namespaced/cluster-wide),
and allow the pods to exist for no longer than 30 seconds --
either after creation or after the operator restart.

For no specific reason, just for fun. Maybe, as a way of Chaos Engineering
to force making the resilient applications (tolerant to pod killing).

However, the system namespaces (kube-system, etc) are explicitly protected --
to prevent killing the cluster itself.

Start the operator:

```bash
kopf run example.py --verbose
```

Start a sample pod:

```bash
kubectl run -it --image=ubuntu expr1 -- bash -i
# wait for 30s
```

Since `kubectl run` creates a Deployment, not just a Pod,
a new pod will be created every 30 seconds. Observe with:

```bash
kubectl get pods --watch
```

Cleanup in the end:

```bash
$ kubectl delete deployment expr1
```



================================================
FILE: examples/10-builtins/example.py
================================================
import asyncio

import kopf
import pykube

tasks: dict[str, dict[str, asyncio.Task]] = {}  # dict{namespace: dict{name: asyncio.Task}}


@kopf.on.resume('pods')
@kopf.on.create('pods')
async def pod_in_sight(namespace, name, logger, **kwargs):
    if namespace.startswith('kube-'):
        return
    else:
        task = asyncio.create_task(pod_killer(namespace, name, logger))
        tasks.setdefault(namespace, {})
        tasks[namespace][name] = task


@kopf.on.delete('pods')
async def pod_deleted(namespace, name, **kwargs):
    if namespace in tasks and name in tasks[namespace]:
        task = tasks[namespace][name]
        task.cancel()  # it will also remove from `tasks`


async def pod_killer(namespace, name, logger, timeout=30):
    try:
        logger.info(f"=== Pod killing happens in {timeout}s.")
        await asyncio.sleep(timeout)
        logger.info(f"=== Pod killing happens NOW!")

        api = pykube.HTTPClient(pykube.KubeConfig.from_env())
        pod = pykube.Pod.objects(api, namespace=namespace).get_by_name(name)
        pod.delete()
        api.session.close()

    except asyncio.CancelledError:
        logger.info(f"=== Pod killing is cancelled!")

    finally:
        if namespace in tasks and name in tasks[namespace]:
            del tasks[namespace][name]



================================================
FILE: examples/10-builtins/test_example_10.py
================================================
import os.path
import time

import kopf.testing


def test_pods_reacted():

    example_py = os.path.join(os.path.dirname(__file__), 'example.py')
    with kopf.testing.KopfRunner(
        ['run', '--all-namespaces', '--standalone', '--verbose', example_py],
        timeout=60,
    ) as runner:
        name = _create_pod()
        time.sleep(5)  # give it some time to react
        _delete_pod(name)
        time.sleep(1)  # give it some time to react

    assert runner.exception is None
    assert runner.exit_code == 0

    assert f'[default/{name}] Creation is in progress:' in runner.output
    assert f'[default/{name}] === Pod killing happens in 30s.' in runner.output
    assert f'[default/{name}] Deletion is in progress:' in runner.output
    assert f'[default/{name}] === Pod killing is cancelled!' in runner.output


def _create_pod():
    import pykube
    api = pykube.HTTPClient(pykube.KubeConfig.from_file())
    with api.session:
        pod = pykube.Pod(api, {
            'apiVersion': 'v1',
            'kind': 'Pod',
            'metadata': {'generateName': 'kopf-pod-', 'namespace': 'default'},
            'spec': {
                'containers': [{
                    'name': 'the-only-one',
                    'image': 'busybox',
                    'command': ["sh", "-x", "-c", "sleep 1"],
                }]},
        })
        pod.create()
        return pod.name


def _delete_pod(name):
    import pykube
    api = pykube.HTTPClient(pykube.KubeConfig.from_file())
    with api.session:
        pod = pykube.Pod.objects(api, namespace='default').get_by_name(name)
        pod.delete()



================================================
FILE: examples/11-filtering-handlers/README.md
================================================
# Kopf example for testing the filtering of handlers

Kopf has the ability to execute handlers only if the watched objects
match the filters passed to the handler. This includes matching on:
* labels of a resource
* annotations of a resource

Start the operator:

```bash
kopf run example.py
```

Trigger the object creation and monitor the stderr of the operator:

```bash
$ kubectl apply -f ../obj.yaml
```

```
[2019-07-04 14:19:33,393] kopf.reactor.handlin [INFO    ] [default/kopf-example-1] Label satisfied.
[2019-07-04 14:19:33,395] kopf.reactor.handlin [INFO    ] [default/kopf-example-1] Handler 'create_with_labels_satisfied' succeeded.
[2019-07-04 14:19:33,648] kopf.reactor.handlin [INFO    ] [default/kopf-example-1] Label exists.
[2019-07-04 14:19:33,649] kopf.reactor.handlin [INFO    ] [default/kopf-example-1] Handler 'create_with_labels_exist' succeeded.
[2019-07-04 14:19:33,807] kopf.reactor.handlin [INFO    ] [default/kopf-example-1] Annotation satisfied.
[2019-07-04 14:19:33,809] kopf.reactor.handlin [INFO    ] [default/kopf-example-1] Handler 'create_with_annotations_satisfied' succeeded.
[2019-07-04 14:19:33,966] kopf.reactor.handlin [INFO    ] [default/kopf-example-1] Annotation exists.
[2019-07-04 14:19:33,967] kopf.reactor.handlin [INFO    ] [default/kopf-example-1] Handler 'create_with_annotations_exist' succeeded.
[2019-07-04 14:19:33,967] kopf.reactor.handlin [INFO    ] [default/kopf-example-1] All handlers succeeded for creation.
```

Here, notice that only the handlers that have labels or annotations that match the applied
object are executed, and the ones that don't, aren't.



================================================
FILE: examples/11-filtering-handlers/example.py
================================================
import kopf


def say_yes(value, spec, **_) -> bool:
    return value == 'somevalue' and spec.get('field') is not None


def say_no(value, spec, **_) -> bool:
    return value == 'somevalue' and spec.get('field') == 'not-this-value-for-sure'


@kopf.on.create('kopfexamples', labels={'somelabel': 'somevalue'})
def create_with_labels_matching(logger, **kwargs):
    logger.info("Label is matching.")


@kopf.on.create('kopfexamples', labels={'somelabel': kopf.PRESENT})
def create_with_labels_present(logger, **kwargs):
    logger.info("Label is present.")


@kopf.on.create('kopfexamples', labels={'nonexistent': kopf.ABSENT})
def create_with_labels_absent(logger, **kwargs):
    logger.info("Label is absent.")


@kopf.on.create('kopfexamples', labels={'somelabel': say_yes})
def create_with_labels_callback_matching(logger, **kwargs):
    logger.info("Label callback matching.")


@kopf.on.create('kopfexamples', annotations={'someannotation': 'somevalue'})
def create_with_annotations_matching(logger, **kwargs):
    logger.info("Annotation is matching.")


@kopf.on.create('kopfexamples', annotations={'someannotation': kopf.PRESENT})
def create_with_annotations_present(logger, **kwargs):
    logger.info("Annotation is present.")


@kopf.on.create('kopfexamples', annotations={'nonexistent': kopf.ABSENT})
def create_with_annotations_absent(logger, **kwargs):
    logger.info("Annotation is absent.")


@kopf.on.create('kopfexamples', annotations={'someannotation': say_no})
def create_with_annotations_callback_matching(logger, **kwargs):
    logger.info("Annotation callback mismatch.")


@kopf.on.create('kopfexamples', when=lambda body, **_: True)
def create_with_filter_satisfied(logger, **kwargs):
    logger.info("Filter satisfied.")


@kopf.on.create('kopfexamples', when=lambda body, **_: False)
def create_with_filter_not_satisfied(logger, **kwargs):
    logger.info("Filter not satisfied.")


@kopf.on.create('kopfexamples', field='spec.field', value='value')
def create_with_field_value_satisfied(logger, **kwargs):
    logger.info("Field value is satisfied.")


@kopf.on.create('kopfexamples', field='spec.field', value='something-else')
def create_with_field_value_not_satisfied(logger, **kwargs):
    logger.info("Field value is not satisfied.")


@kopf.on.create('kopfexamples', field='spec.field', value=kopf.PRESENT)
def create_with_field_presence_satisfied(logger, **kwargs):
    logger.info("Field presence is satisfied.")


@kopf.on.create('kopfexamples', field='spec.inexistent', value=kopf.PRESENT)
def create_with_field_presence_not_satisfied(logger, **kwargs):
    logger.info("Field presence is not satisfied.")


@kopf.on.update('kopfexamples',
                field='spec.field', old='value', new='changed')
def update_with_field_change_satisfied(logger, **kwargs):
    logger.info("Field change is satisfied.")


@kopf.daemon('kopfexamples', field='spec.field', value='value')
def daemon_with_field(stopped: kopf.DaemonStopped, logger, **kwargs):
    while not stopped:
        logger.info("Field daemon is satisfied.")
        stopped.wait(1)



================================================
FILE: examples/11-filtering-handlers/test_example_11.py
================================================
import os.path
import subprocess
import time

import kopf.testing
import pytest

crd_yaml = os.path.relpath(os.path.join(os.path.dirname(__file__), '..', 'crd.yaml'))
obj_yaml = os.path.relpath(os.path.join(os.path.dirname(__file__), '..', 'obj.yaml'))
example_py = os.path.relpath(os.path.join(os.path.dirname(__file__), 'example.py'))


@pytest.fixture(autouse=True)
def crd_exists():
    subprocess.run(f"kubectl apply -f {crd_yaml}",
                   check=True, timeout=10, capture_output=True, shell=True)


@pytest.fixture(autouse=True)
def obj_absent():
    # Operator is not running in fixtures, so we need a force-delete (or this patch).
    subprocess.run(['kubectl', 'patch', '-f', obj_yaml,
                    '-p', '{"metadata":{"finalizers":[]}}',
                    '--type', 'merge'],
                   check=False, timeout=10, capture_output=True)
    subprocess.run(f"kubectl delete -f {obj_yaml}",
                   check=False, timeout=10, capture_output=True, shell=True)


def test_handler_filtering():

    # To prevent lengthy threads in the loop executor when the process exits.
    settings = kopf.OperatorSettings()
    settings.watching.server_timeout = 10

    # Run an operator and simulate some activity with the operated resource.
    with kopf.testing.KopfRunner(
        ['run', '--all-namespaces', '--verbose', '--standalone', example_py],
        settings=settings,
    ) as runner:

        subprocess.run(f"kubectl create -f {obj_yaml}",
                       shell=True, check=True, timeout=10, capture_output=True)
        time.sleep(5)  # give it some time to react
        subprocess.run(f"kubectl patch -f {obj_yaml} --type merge -p '" '{"spec":{"field":"changed"}}' "'",
                       shell=True, check=True, timeout=10, capture_output=True)
        time.sleep(2)  # give it some time to react
        subprocess.run(f"kubectl delete -f {obj_yaml}",
                       shell=True, check=True, timeout=10, capture_output=True)
        time.sleep(1)  # give it some time to react

    # Ensure that the operator did not die on start, or during the operation.
    assert runner.exception is None
    assert runner.exit_code == 0

    # Check for correct log lines (to indicate correct handlers were executed).
    assert '[default/kopf-example-1] Label is matching.' in runner.output
    assert '[default/kopf-example-1] Label is present.' in runner.output
    assert '[default/kopf-example-1] Label is absent.' in runner.output
    assert '[default/kopf-example-1] Label callback matching.' in runner.output
    assert '[default/kopf-example-1] Annotation is matching.' in runner.output
    assert '[default/kopf-example-1] Annotation is present.' in runner.output
    assert '[default/kopf-example-1] Annotation is absent.' in runner.output
    assert '[default/kopf-example-1] Annotation callback mismatch.' not in runner.output
    assert '[default/kopf-example-1] Filter satisfied.' in runner.output
    assert '[default/kopf-example-1] Filter not satisfied.' not in runner.output
    assert '[default/kopf-example-1] Field value is satisfied.' in runner.output
    assert '[default/kopf-example-1] Field value is not satisfied.' not in runner.output
    assert '[default/kopf-example-1] Field presence is satisfied.' in runner.output
    assert '[default/kopf-example-1] Field presence is not satisfied.' not in runner.output
    assert '[default/kopf-example-1] Field change is satisfied.' in runner.output
    assert '[default/kopf-example-1] Field daemon is satisfied.' in runner.output



================================================
FILE: examples/12-embedded/README.md
================================================
# Kopf example for embedded operator

Kopf operators can be embedded into arbitrary applications, such as UI;
or they can be orchestrated explicitly by the developers instead of `kopf run`.

In this example, we start the operator in a side thread, while simulating
an application activity in the main thread. In this case, the "application"
just creates and deletes the example objects, but it can be any activity.

Start the operator:

```bash
python example.py
```

Let it run for 6 seconds (mostly due to sleeps: 3 times by 1+1 second).
Here is what it will print (shortened; the actual output is more verbose):

```
Starting the main app.

[DEBUG   ] Pykube is configured via kubeconfig file.
[DEBUG   ] Client is configured via kubeconfig file.
[WARNING ] Default peering object is not found, falling back to the standalone mode.
[WARNING ] OS signals are ignored: running not in the main thread.

Do the main app activity here. Step 1/3.

[DEBUG   ] [default/kopf-example-0] Creation is in progress: ...
[DEBUG   ] [default/kopf-example-0] Deletion is in progress: ...

Do the main app activity here. Step 2/3.

[DEBUG   ] [default/kopf-example-1] Creation is in progress: ...
[DEBUG   ] [default/kopf-example-1] Deletion is in progress: ...

Do the main app activity here. Step 3/3.

[DEBUG   ] [default/kopf-example-2] Creation is in progress: ...
[DEBUG   ] [default/kopf-example-2] Deletion is in progress: ...

Exiting the main app.

[INFO    ] Stop-flag is set to True. Operator is stopping.
[DEBUG   ] Root task 'poster of events' is cancelled.
[DEBUG   ] Root task 'watcher of kopfexamples.kopf.dev' is cancelled.
[DEBUG   ] Root tasks are stopped: finished normally; tasks left: set()
[DEBUG   ] Hung tasks stopping is skipped: no tasks given.
```



================================================
FILE: examples/12-embedded/example.py
================================================
import asyncio
import contextlib
import threading
import time

import kopf
import pykube


@kopf.on.create('kopfexamples')
def create_fn(memo: kopf.Memo, **kwargs):
    print(memo.create_tpl.format(**kwargs))  # type: ignore


@kopf.on.delete('kopfexamples')
def delete_fn(memo: kopf.Memo, **kwargs):
    print(memo.delete_tpl.format(**kwargs))  # type: ignore


def kopf_thread(
        ready_flag: threading.Event,
        stop_flag: threading.Event,
):
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    with contextlib.closing(loop):

        kopf.configure(verbose=True)  # log formatting

        loop.run_until_complete(kopf.operator(
            ready_flag=ready_flag,
            stop_flag=stop_flag,
            memo=kopf.Memo(
                create_tpl="Hello, {name}!",
                delete_tpl="Good bye, {name}!",
            ),
        ))


def main(steps=3):

    # Start the operator and let it initialise.
    print(f"Starting the main app.")
    ready_flag = threading.Event()
    stop_flag = threading.Event()
    thread = threading.Thread(target=kopf_thread, kwargs=dict(
        stop_flag=stop_flag,
        ready_flag=ready_flag,
    ))
    thread.start()
    ready_flag.wait()

    # The operator is active: run the app's activity.
    for step in range(steps):
        print(f"Do the main app activity here. Step {step+1}/{steps}.")
        _create_object(step)
        time.sleep(1.0)
        _delete_object(step)
        time.sleep(1.0)

    # Ask the operator to terminate gracefully (can take a couple of seconds).
    print(f"Exiting the main app.")
    stop_flag.set()
    thread.join()


class KopfExample(pykube.objects.NamespacedAPIObject):
    version = "kopf.dev/v1"
    endpoint = "kopfexamples"
    kind = "KopfExample"


def _create_object(step):
    try:
        api = pykube.HTTPClient(pykube.KubeConfig.from_env())
        kex = KopfExample(api, dict(
            apiVersion='kopf.dev/v1',
            kind='KopfExample',
            metadata=dict(
                namespace='default',
                name=f'kopf-example-{step}',
            ),
        ))
        kex.create()
    except pykube.exceptions.HTTPError as e:
        if e.code in [409]:
            pass
        else:
            raise


def _delete_object(step):
    try:
        api = pykube.HTTPClient(pykube.KubeConfig.from_env())
        kex = KopfExample.objects(api, namespace='default').get_by_name(f'kopf-example-{step}')
        kex.delete()
    except pykube.exceptions.HTTPError as e:
        if e.code in [404]:
            pass
        else:
            raise


if __name__ == '__main__':
    main()



================================================
FILE: examples/12-embedded/test_nothing.py
================================================
"""
Embeddable operators require very customised application-specific testing.
Kopf cannot help here beyond its regular `kopf.testing.KopfRunner` helper,
which is an equivalent of `kopf run` command.

This file exists to disable the implicit e2e tests
(they skip if explicit e2e tests exist in the example directory).
"""



================================================
FILE: examples/13-hooks/README.md
================================================
# Kopf example for startup/cleanup handlers

Kopf operators can have handlers invoked on startup and on cleanup.

The startup handlers are slightly different from the module-level code:
the actual tasks (e.g. API calls for watching) are not started until
all the startup handlers succeed.
If the handlers fail, the operator also fails.

The cleanup handlers are executed when the operator exits either by a signal
(e.g. SIGTERM), or by raising the stop-flag, or by cancelling
the operator's asyncio task.
They are not guaranteed to be fully executed if they take too long.

In this example, we start a background task for every pod we see,
and ask that task to finish when the operator exits. It takes some time
for the tasks to notice the request, so the exiting is not instant.

Start the operator:

```bash
kopf run example.py --verbose
```

Observe the startup logs:

```
[18:51:26,535] kopf.reactor.handlin [DEBUG   ] Invoking handler 'startup_fn_simple'.
[18:51:26,536] kopf.reactor.handlin [INFO    ] Initialising the task-lock...
[18:51:26,536] kopf.reactor.handlin [INFO    ] Handler 'startup_fn_simple' succeeded.
[18:51:26,536] kopf.reactor.handlin [DEBUG   ] Invoking handler 'startup_fn_retried'.
[18:51:26,536] kopf.reactor.handlin [ERROR   ] Handler 'startup_fn_retried' failed temporarily: Going to succeed in 3s
[18:51:27,543] kopf.reactor.handlin [DEBUG   ] Invoking handler 'startup_fn_retried'.
[18:51:27,544] kopf.reactor.handlin [ERROR   ] Handler 'startup_fn_retried' failed temporarily: Going to succeed in 2s
[18:51:28,550] kopf.reactor.handlin [DEBUG   ] Invoking handler 'startup_fn_retried'.
[18:51:28,550] kopf.reactor.handlin [ERROR   ] Handler 'startup_fn_retried' failed temporarily: Going to succeed in 1s
[18:51:29,553] kopf.reactor.handlin [DEBUG   ] Invoking handler 'startup_fn_retried'.
[18:51:29,553] kopf.reactor.handlin [INFO    ] Starting retried...
[18:51:29,554] kopf.reactor.handlin [INFO    ] Handler 'startup_fn_retried' succeeded.
[18:51:29,779] kopf.objects         [DEBUG   ] [kube-system/etcd-minikube] Invoking handler 'pod_task'.
[18:51:29,779] kopf.objects         [INFO    ] [kube-system/etcd-minikube] Handler 'pod_task' succeeded.
[18:51:36,784] kopf.objects         [INFO    ] [kube-system/etcd-minikube] Served by the background task.
```

Terminate the operator (depends on your IDE or CLI environment):

```bash
pkill -TERM kopf
```

Observe the cleanup logs (notice the timing — the final exit is not instant):

```
[18:51:44,122] kopf.reactor.running [INFO    ] Signal SIGINT is received. Operator is stopping.
[18:51:44,123] kopf.reactor.running [DEBUG   ] Root task 'poster of events' is cancelled.
[18:51:44,123] kopf.reactor.running [DEBUG   ] Root task 'watcher of pods.' is cancelled.
[18:51:44,128] kopf.reactor.handlin [DEBUG   ] Invoking handler 'cleanup_fn'.
[18:51:44,129] kopf.reactor.handlin [INFO    ] Cleaning up...
[18:51:44,130] kopf.reactor.handlin [INFO    ] All pod-tasks are requested to stop...
[18:51:44,130] kopf.reactor.handlin [INFO    ] Handler 'cleanup_fn' succeeded.
[18:51:44,130] kopf.reactor.running [DEBUG   ] Root tasks are stopped: finished normally; tasks left: set()
[18:51:46,789] kopf.objects         [INFO    ] [kube-system/etcd-minikube] Served by the background task.
[18:51:46,790] kopf.objects         [INFO    ] [kube-system/etcd-minikube] Serving is finished by request.
[18:51:49,136] kopf.reactor.running [DEBUG   ] Hung tasks are stopped: finished normally; tasks left: set()
```



================================================
FILE: examples/13-hooks/example.py
================================================
import asyncio
import datetime
import random

import kopf
import pykube

LOCK: asyncio.Lock  # requires a loop on creation
STOPPERS: dict[str, dict[str, asyncio.Event]] = {}  # [namespace][name]


@kopf.on.startup()
async def startup_fn_simple(logger, **kwargs):
    logger.info("Initialising the task-lock...")
    global LOCK
    LOCK = asyncio.Lock()  # in the current asyncio loop


@kopf.on.startup()
async def startup_fn_retried(retry, logger, **kwargs):
    if retry < 3:
        raise kopf.TemporaryError(f"Going to succeed in {3-retry}s", delay=1)
    else:
        logger.info("Starting retried...")
        # raise kopf.PermanentError("Unable to start!")


@kopf.on.cleanup()
async def cleanup_fn(logger, **kwargs):
    logger.info("Cleaning up...")
    for namespace in STOPPERS.keys():
        for name, flag in STOPPERS[namespace].items():
            flag.set()
    logger.info("All pod-tasks are requested to stop...")


@kopf.on.login(errors=kopf.ErrorsMode.PERMANENT)
async def login_fn(**kwargs):
    print('Logging in in 2s...')
    await asyncio.sleep(2.0)

    # An equivalent of kopf.login_via_pykube(), but shrinked for demo purposes.
    config = pykube.KubeConfig.from_env()
    ca = config.cluster.get('certificate-authority')
    cert = config.user.get('client-certificate')
    pkey = config.user.get('client-key')
    return kopf.ConnectionInfo(
        server=config.cluster.get('server'),
        ca_path=ca.filename() if ca else None,  # can be a temporary file
        insecure=config.cluster.get('insecure-skip-tls-verify'),
        username=config.user.get('username'),
        password=config.user.get('password'),
        token=config.user.get('token'),
        certificate_path=cert.filename() if cert else None,  # can be a temporary file
        private_key_path=pkey.filename() if pkey else None,  # can be a temporary file
        default_namespace=config.namespace,
        expiration=datetime.datetime.now(datetime.timezone.utc) + datetime.timedelta(seconds=30),
    )


@kopf.on.probe()
async def tasks_count(**kwargs):
    return sum(len(flags) for flags in STOPPERS.values())


@kopf.on.probe()
async def monitored_objects(**kwargs):
    return {namespace: sorted(name for name in STOPPERS[namespace]) for namespace in STOPPERS}


@kopf.on.event('pods')
async def pod_task(namespace, name, logger, **_):
    async with LOCK:
        if namespace not in STOPPERS or name not in STOPPERS[namespace]:
            flag = asyncio.Event()
            STOPPERS.setdefault(namespace, {}).setdefault(name, flag)
            asyncio.create_task(_task_fn(logger, shouldstop=flag))


async def _task_fn(logger, shouldstop: asyncio.Event):
    while not shouldstop.is_set():
        await asyncio.sleep(random.randint(1, 10))
        logger.info("Served by the background task.")
    logger.info("Serving is finished by request.")


# Marks for the e2e tests (see tests/e2e/test_examples.py):
E2E_STARTUP_STOP_WORDS = ['Served by the background task.']
E2E_CLEANUP_STOP_WORDS = ['Hung tasks', 'Root tasks']
E2E_SUCCESS_COUNTS = {'startup_fn_simple': 1, 'startup_fn_retried': 1, 'cleanup_fn': 1}



================================================
FILE: examples/14-daemons/example.py
================================================
import asyncio
import time

import kopf


# Sync daemons in threads are non-interruptable, they must check for the `stopped` flag.
# This daemon exits after 3 attempts and then 30 seconds of running (unless stopped).
@kopf.daemon('kopfexamples', backoff=3)
def background_sync(stopped: kopf.DaemonStopped, spec, logger, retry, patch, **_):
    if retry < 3:
        patch.status['message'] = f"Failed {retry+1} times."
        raise kopf.TemporaryError("Simulated failure.", delay=1)

    started = time.time()
    while not stopped and time.time() - started <= 30:
        logger.info(f"=> Ping from a sync daemon: field={spec['field']!r}, retry={retry}")
        stopped.wait(5.0)

    patch.status['message'] = "Accompanying is finished."


# Async daemons do not need the `stopped` signal, they can rely on `asyncio.CancelledError` raised.
# This daemon runs forever (until stopped, i.e. cancelled). Yet fails to start for 3 first times.
@kopf.daemon('kopfexamples', backoff=3, cancellation_backoff=1.0, cancellation_timeout=0.5,
             annotations={'someannotation': 'somevalue'})
async def background_async(spec, logger, retry, **_):
    if retry < 3:
        raise kopf.TemporaryError("Simulated failure.", delay=1)

    while True:
        logger.info(f"=> Ping from an async daemon: field={spec['field']!r}")
        await asyncio.sleep(5.0)


# Marks for the e2e tests (see tests/e2e/test_examples.py):
E2E_CREATION_STOP_WORDS = ["=> Ping from"]
E2E_DELETION_STOP_WORDS = ["'background_async' is cancelled",
                           "'background_sync' is cancelled",
                           "'background_async' has exited"]



================================================
FILE: examples/15-timers/example.py
================================================
import kopf


@kopf.timer('kopfexamples', idle=5, interval=2)
def every_few_seconds_sync(spec, logger, **_):
    logger.info(f"Ping from a sync timer: field={spec['field']!r}")


@kopf.timer('kopfexamples', idle=10, interval=4)
async def every_few_seconds_async(spec, logger, **_):
    logger.info(f"Ping from an async timer: field={spec['field']!r}")



================================================
FILE: examples/16-indexing/example.py
================================================
import pprint

import kopf


@kopf.index('pods')
def is_running(namespace, name, status, **_):
    return {(namespace, name): status.get('phase') == 'Running'}
    # {('kube-system', 'traefik-...-...'): [True],
    #  ('kube-system', 'helm-install-traefik-...'): [False],
    #    ...}


@kopf.index('pods')
def by_label(labels, name, **_):
    return {(label, value): name for label, value in labels.items()}
    # {('app', 'traefik'): ['traefik-...-...'],
    #  ('job-name', 'helm-install-traefik'): ['helm-install-traefik-...'],
    #  ('helmcharts.helm.cattle.io/chart', 'traefik'): ['helm-install-traefik-...'],
    #    ...}


@kopf.on.probe()  # type: ignore
def pod_count(is_running: kopf.Index, **_):
    return len(is_running)


@kopf.on.probe()  # type: ignore
def pod_names(is_running: kopf.Index, **_):
    return [name for _, name in is_running]


@kopf.timer('kex', interval=5)  # type: ignore
def intervalled(is_running: kopf.Index, by_label: kopf.Index, patch: kopf.Patch, **_):
    pprint.pprint(dict(by_label))
    patch.status['running-pods'] = [
        f"{ns}::{name}"
        for (ns, name), is_running in is_running.items()
        if ns in ['kube-system', 'default']
        if is_running
    ]


# Marks for the e2e tests (see tests/e2e/test_examples.py):
# We do not care: pods can have 6-10 updates here.
E2E_SUCCESS_COUNTS: dict[str, int] = {}



================================================
FILE: examples/17-admission/example.py
================================================
import pathlib

import kopf

ROOT = (pathlib.Path.cwd() / pathlib.Path(__file__)).parent.parent.parent


@kopf.on.startup()
def config(settings: kopf.OperatorSettings, **_):

    # Plain and simple local endpoint with an auto-generated certificate:
    settings.admission.server = kopf.WebhookServer()

    # Plain and simple local endpoint with with provided certificate (e.g. openssl):
    settings.admission.server = kopf.WebhookServer(certfile=ROOT/'cert.pem', pkeyfile=ROOT/'key.pem', port=1234)

    # K3d/K3s-specific server that supports accessing from inside of a VM (a generated certificate):
    settings.admission.server = kopf.WebhookK3dServer(cadump=ROOT/'ca.pem')

    # K3d/K3s-specific server that supports accessing from inside of a VM (a provided certificate):
    settings.admission.server = kopf.WebhookK3dServer(certfile=ROOT/'k3d-cert.pem', pkeyfile=ROOT/'k3d-key.pem', port=1234)

    # Minikube-specific server that supports accessing from inside of a VM (a generated certificate):
    settings.admission.server = kopf.WebhookMinikubeServer(port=1234, cadump=ROOT/'ca.pem')

    # DockerDesktop-specific server that supports accessing from the host:
    settings.admission.server = kopf.WebhookDockerDesktopServer(port=1234)

    # Tunneling Kubernetes->ngrok->local server (anonymous, auto-loaded binary):
    settings.admission.server = kopf.WebhookNgrokTunnel(path='/xyz', port=1234)

    # Tunneling Kubernetes->ngrok->local server (registered users, pre-existing binary):
    settings.admission.server = kopf.WebhookNgrokTunnel(binary="/usr/local/bin/ngrok", token='...', )

    # Tunneling Kubernetes->ngrok->local server (registered users, pre-existing binary, specific region):
    settings.admission.server = kopf.WebhookNgrokTunnel(binary="/usr/local/bin/ngrok", region='eu')

    # Auto-detect the best server (K3d/Minikube/simple) strictly locally:
    settings.admission.server = kopf.WebhookAutoServer()

    # Auto-detect the best server (K3d/Minikube/simple) with external tunneling as a fallback:
    settings.admission.server = kopf.WebhookAutoTunnel()

    # The final configuration for CI/CD (overrides previous values):
    settings.admission.server = kopf.WebhookAutoServer()
    settings.admission.managed = 'auto.kopf.dev'


@kopf.on.validate('kex')
def authhook(headers: kopf.Headers, sslpeer: kopf.SSLPeer, warnings: list[str], **_):
    user_agent = headers.get('User-Agent', '(unidentified)')
    warnings.append(f"Accessing as user-agent: {user_agent}")
    if not sslpeer.get('subject'):
        warnings.append("SSL peer is not identified.")
    else:
        common_names = [val for key, val in sslpeer['subject'][0] if key == 'commonName']
        if common_names:
            warnings.append(f"SSL peer is {common_names[0]}.")
        else:
            warnings.append("SSL peer's common name is absent.")


@kopf.on.validate('kex')
def validate1(spec, dryrun, **_):
    if not dryrun and spec.get('field') == 'wrong':
        raise kopf.AdmissionError("Meh! I don't like it. Change the field.")


@kopf.on.validate('kex', field='spec.field', value='not-allowed')
def validate2(**_):
    raise kopf.AdmissionError("I'm too lazy anyway. Go away!", code=555)


@kopf.on.validate('kex', subresource='*')
def validate_subresources(spec, subresource, status, warnings: list[str], **_):
    if subresource == 'status' and status.get('field') != spec.get('field'):
        raise kopf.AdmissionError("status.field MUST be equal to spec.field!")
    elif subresource is None and status.get('field') != spec.get('field'):
        warnings.append("Also update status.field to match spec.field: "
                        f"{spec.get('field')!r} != {status.get('field')!r}")


@kopf.on.mutate('kex', labels={'somelabel': 'somevalue'})
def mutate1(patch: kopf.Patch, **_):
    patch.spec['injected'] = 123


# Marks for the e2e tests (see tests/e2e/test_examples.py):
# We do not care: pods can have 6-10 updates here.
E2E_SUCCESS_COUNTS: dict[str, int] = {}



================================================
FILE: examples/99-all-at-once/README.md
================================================
# Kopf example with all the features at once

This operator contains all the features of the framework at once.
It is used mostly for the development and debugging.



================================================
FILE: examples/99-all-at-once/example.py
================================================
import asyncio
import pprint
import time

import kopf
import pykube
import yaml


@kopf.on.startup()
async def startup_fn_simple(logger, **kwargs):
    logger.info('Starting in 1s...')
    await asyncio.sleep(1)


@kopf.on.startup()
async def startup_fn_retried(retry, logger, **kwargs):
    if retry < 3:
        raise kopf.TemporaryError(f"Going to succeed in {3-retry}s", delay=1)
    else:
        logger.info('Starting retried...')
        # raise kopf.PermanentError("Unable to start!")


@kopf.on.cleanup()
async def cleanup_fn(logger, **kwargs):
    logger.info('Cleaning up in 3s...')
    await asyncio.sleep(3)


@kopf.on.create('kopfexamples')
def create_1(body, meta, spec, status, **kwargs):
    children = _create_children(owner=body)

    kopf.info(body, reason='AnyReason')
    kopf.event(body, type='Warning', reason='SomeReason', message="Cannot do something")
    kopf.event(children, type='Normal', reason='SomeReason', message="Created as part of the job1step")

    return {'job1-status': 100}


@kopf.on.create('kopfexamples')
def create_2(body, meta, spec, status, retry=None, **kwargs):
    wait_for_something()  # specific for job2, e.g. an external API poller

    if not retry:
        # will be retried by the framework, even if it has been restarted
        raise Exception("Whoops!")

    return {'job2-status': 100}


@kopf.on.update('kopfexamples')
def update(body, meta, spec, status, old, new, diff, **kwargs):
    print('Handling the diff')
    pprint.pprint(list(diff))


@kopf.on.field('kopfexamples', field='spec.lst')
def update_lst(body, meta, spec, status, old, new, **kwargs):
    print(f'Handling the FIELD = {old} -> {new}')


@kopf.on.delete('kopfexamples')
def delete(body, meta, spec, status, **kwargs):
    pass


def _create_children(owner):
    return []


def wait_for_something():
    # Note: intentionally blocking from the asyncio point of view.
    time.sleep(1)


@kopf.on.create('kopfexamples')
def create_pod(**kwargs):

    # Render the pod yaml with some spec fields used in the template.
    pod_data = yaml.safe_load(f"""
        apiVersion: v1
        kind: Pod
        spec:
          containers:
          - name: the-only-one
            image: busybox
            command: ["sh", "-x", "-c", "sleep 1"]
    """)

    # Make it our child: assign the namespace, name, labels, owner references, etc.
    kopf.adopt(pod_data)
    kopf.label(pod_data, {'application': 'kopf-example-10'})

    # Actually create an object by requesting the Kubernetes API.
    api = pykube.HTTPClient(pykube.KubeConfig.from_env())
    pod = pykube.Pod(api, pod_data)
    pod.create()
    api.session.close()


@kopf.on.event('pods', labels={'application': 'kopf-example-10'})
def example_pod_change(logger, **kwargs):
    logger.info("This pod is special for us.")


# Marks for the e2e tests (see tests/e2e/test_examples.py):
E2E_ALLOW_TRACEBACKS = True
E2E_STARTUP_STOP_WORDS = ['Served by the background task.']
E2E_CLEANUP_STOP_WORDS = ['Hung tasks', 'Root tasks']
E2E_CREATION_STOP_WORDS = ['Creation is processed:']
E2E_DELETION_STOP_WORDS = ['Deleted, really deleted']
E2E_SUCCESS_COUNTS = {'create_1': 1, 'create_2': 1, 'create_pod': 1, 'delete': 1,
                      'startup_fn_simple': 1, 'startup_fn_retried': 1, 'cleanup_fn': 1}



================================================
FILE: kopf/__init__.py
================================================
"""
The main Kopf module for all the exported functions & classes.
"""
# isort: skip_file

# Unlike all other places, where we import other modules and refer
# the functions via the modules, this is the framework's top-level interface,
# as it is seen by the users. So, we export the individual functions.

from kopf import (
    on,  # as a separate name on the public namespace
)
from kopf.on import (
    subhandler,
    register,
    daemon,
    timer,
    index,
)
from kopf._cogs.configs.configuration import (
    OperatorSettings,
)
from kopf._cogs.configs.diffbase import (
    DiffBaseStorage,
    AnnotationsDiffBaseStorage,
    StatusDiffBaseStorage,
    MultiDiffBaseStorage,
)
from kopf._cogs.configs.progress import (
    ProgressRecord,
    ProgressStorage,
    AnnotationsProgressStorage,
    StatusProgressStorage,
    MultiProgressStorage,
    SmartProgressStorage,
)
from kopf._cogs.helpers.typedefs import (
    Logger,
)
from kopf._cogs.helpers.versions import (
    version as __version__,
)
from kopf._cogs.structs.bodies import (
    RawEventType,
    RawEvent,
    RawBody,
    Status,
    Spec,
    Meta,
    Body,
    BodyEssence,
    Labels,
    Annotations,
    OwnerReference,
    ObjectReference,
    build_object_reference,
    build_owner_reference,
)
from kopf._cogs.structs.credentials import (
    LoginError,
    ConnectionInfo,
)
from kopf._cogs.structs.dicts import (
    FieldSpec,
    FieldPath,
)
from kopf._cogs.structs.diffs import (
    Diff,
    DiffItem,
    DiffOperation,
)
from kopf._cogs.structs.ephemera import (
    Memo,
    Index,
    Store,
)
from kopf._cogs.structs.ids import (
    HandlerId,
)
from kopf._cogs.structs.patches import (
    Patch,
)
from kopf._cogs.structs.references import (
    Resource,
    EVERYTHING,
)
from kopf._cogs.structs.reviews import (
    WebhookClientConfigService,
    WebhookClientConfig,
    Operation,
    UserInfo,
    Headers,
    SSLPeer,
    WebhookFn,
    WebhookServerProtocol,
)
from kopf._core.actions import (
    lifecycles,  # as a separate name on the public namespace
)
from kopf._core.actions.execution import (
    ErrorsMode,
    TemporaryError,
    PermanentError,
    HandlerTimeoutError,
    HandlerRetriesError,
)
from kopf._core.actions.lifecycles import (
    get_default_lifecycle,
    set_default_lifecycle,
)
from kopf._core.actions.loggers import (
    configure,
    LogFormat,
    ObjectLogger,
    LocalObjectLogger,
)
from kopf._core.engines.admission import (
    AdmissionError,
)
from kopf._core.engines.posting import (
    event,
    info,
    warn,
    exception,
)
from kopf._core.intents.callbacks import (
    not_,
    all_,
    any_,
    none_,
)
from kopf._core.intents.causes import (
    Reason,
)
from kopf._core.intents.filters import (
    ABSENT,
    PRESENT,
)
from kopf._core.intents.registries import (
    OperatorRegistry,
    get_default_registry,
    set_default_registry,
)
from kopf._core.intents.stoppers import (
    DaemonStopped,
    DaemonStoppingReason,
    SyncDaemonStopperChecker,  # deprecated
    AsyncDaemonStopperChecker,  # deprecated
)
from kopf._core.intents.piggybacking import (
    login_via_pykube,
    login_via_client,
    login_with_kubeconfig,
    login_with_service_account,
)
from kopf._core.reactor.running import (
    spawn_tasks,
    run_tasks,
    operator,
    run,
)
from kopf._core.reactor.subhandling import (
    execute,
)
from kopf._kits.hierarchies import (
    adopt,
    label,
    harmonize_naming,
    adjust_namespace,
    append_owner_reference,
    remove_owner_reference,
)
from kopf._kits.webhooks import (
    WebhookServer,
    WebhookK3dServer,
    WebhookMinikubeServer,
    WebhookDockerDesktopServer,
    WebhookNgrokTunnel,
    WebhookAutoServer,
    WebhookAutoTunnel,
)

__all__ = [
    'on', 'lifecycles', 'register', 'execute', 'daemon', 'timer', 'index',
    'configure', 'LogFormat',
    'login_via_pykube',
    'login_via_client',
    'login_with_kubeconfig',
    'login_with_service_account',
    'LoginError',
    'ConnectionInfo',
    'event', 'info', 'warn', 'exception',
    'spawn_tasks', 'run_tasks', 'operator', 'run',
    'adopt', 'label',
    'not_',
    'all_',
    'any_',
    'none_',
    'get_default_lifecycle', 'set_default_lifecycle',
    'build_object_reference', 'build_owner_reference',
    'append_owner_reference', 'remove_owner_reference',
    'ErrorsMode',
    'AdmissionError',
    'WebhookClientConfigService',
    'WebhookClientConfig',
    'Operation',
    'UserInfo',
    'Headers',
    'SSLPeer',
    'WebhookFn',
    'WebhookServerProtocol',
    'WebhookServer',
    'WebhookK3dServer',
    'WebhookMinikubeServer',
    'WebhookNgrokTunnel',
    'WebhookAutoServer',
    'WebhookAutoTunnel',
    'PermanentError',
    'TemporaryError',
    'HandlerTimeoutError',
    'HandlerRetriesError',
    'OperatorRegistry',
    'get_default_registry',
    'set_default_registry',
    'PRESENT', 'ABSENT',
    'OperatorSettings',
    'DiffBaseStorage',
    'AnnotationsDiffBaseStorage',
    'StatusDiffBaseStorage',
    'MultiDiffBaseStorage',
    'ProgressRecord',
    'ProgressStorage',
    'AnnotationsProgressStorage',
    'StatusProgressStorage',
    'MultiProgressStorage',
    'SmartProgressStorage',
    'RawEventType',
    'RawEvent',
    'RawBody',
    'Status',
    'Spec',
    'Meta',
    'Body',
    'BodyEssence',
    'Labels',
    'Annotations',
    'ObjectReference',
    'OwnerReference',
    'Memo', 'Index', 'Store',
    'Logger',
    'ObjectLogger',
    'LocalObjectLogger',
    'FieldSpec',
    'FieldPath',
    'Diff',
    'DiffItem',
    'DiffOperation',
    'HandlerId',
    'Reason',
    'Patch',
    'DaemonStopped',
    'DaemonStoppingReason',
    'SyncDaemonStopperChecker',  # deprecated
    'AsyncDaemonStopperChecker',  # deprecated
    'Resource', 'EVERYTHING',
]



================================================
FILE: kopf/__main__.py
================================================
"""
CLI entry point, when used as a module: `python -m kopf`.

Useful for debugging in the IDEs (use the start-mode "Module", module "kopf").
"""
from kopf import cli

if __name__ == '__main__':
    cli.main()



================================================
FILE: kopf/cli.py
================================================
import asyncio
import dataclasses
import functools
import os
from collections.abc import Collection
from typing import Any, Callable, Optional

import click

from kopf._cogs.aiokits import aioadapters
from kopf._cogs.configs import configuration
from kopf._cogs.helpers import loaders
from kopf._cogs.structs import credentials, references
from kopf._core.actions import loggers
from kopf._core.engines import peering
from kopf._core.intents import registries
from kopf._core.reactor import running
from kopf._kits import loops


@dataclasses.dataclass()
class CLIControls:
    """ `KopfRunner` controls, which are impossible to pass via CLI. """
    ready_flag: Optional[aioadapters.Flag] = None
    stop_flag: Optional[aioadapters.Flag] = None
    vault: Optional[credentials.Vault] = None
    registry: Optional[registries.OperatorRegistry] = None
    settings: Optional[configuration.OperatorSettings] = None
    loop: Optional[asyncio.AbstractEventLoop] = None


# With Click>=8.2.0, that should be `click.Choice[LogFormat]`, but it is good for now, too.
# TODO: when Python 3.9 is dropped, upgrade dependencies to click>=8.2.0, and remake the class here.
#       see: https://github.com/nolar/kopf/pull/1174
class LogFormatParamType(click.Choice):  # type: ignore

    def __init__(self) -> None:
        super().__init__(choices=[v.name.lower() for v in loggers.LogFormat])

    def convert(self, value: Any, param: Any, ctx: Any) -> loggers.LogFormat:
        if isinstance(value, loggers.LogFormat):
            return value
        else:
            name: str = super().convert(value, param, ctx)
            return loggers.LogFormat[name.upper()]


def logging_options(fn: Callable[..., Any]) -> Callable[..., Any]:
    """ A decorator to configure logging in all commands the same way."""
    @click.option('-v', '--verbose', is_flag=True)
    @click.option('-d', '--debug', is_flag=True)
    @click.option('-q', '--quiet', is_flag=True)
    @click.option('--log-format', type=LogFormatParamType(), default='full')
    @click.option('--log-refkey', type=str)
    @click.option('--log-prefix/--no-log-prefix', default=None)
    @functools.wraps(fn)  # to preserve other opts/args
    def wrapper(verbose: bool, quiet: bool, debug: bool,
                log_format: loggers.LogFormat = loggers.LogFormat.FULL,
                log_prefix: Optional[bool] = False,
                log_refkey: Optional[str] = None,
                *args: Any, **kwargs: Any) -> Any:
        loggers.configure(debug=debug, verbose=verbose, quiet=quiet,
                          log_format=log_format, log_refkey=log_refkey, log_prefix=log_prefix)
        return fn(*args, **kwargs)

    return wrapper


@click.group(name='kopf', context_settings=dict(
    auto_envvar_prefix='KOPF',
))
@click.version_option(prog_name='kopf')
@click.make_pass_decorator(CLIControls, ensure=True)
def main(__controls: CLIControls) -> None:
    pass


@main.command()
@logging_options
@click.option('-A', '--all-namespaces', 'clusterwide', is_flag=True)
@click.option('-n', '--namespace', 'namespaces', multiple=True)
@click.option('--standalone', is_flag=True, default=None)
@click.option('--dev', 'priority', type=int, is_flag=True, flag_value=666)
@click.option('-L', '--liveness', 'liveness_endpoint', type=str)
@click.option('-P', '--peering', 'peering_name', type=str, envvar='KOPF_RUN_PEERING')
@click.option('-p', '--priority', type=int)
@click.option('-m', '--module', 'modules', multiple=True)
@click.argument('paths', nargs=-1)
@click.make_pass_decorator(CLIControls, ensure=True)
def run(
        __controls: CLIControls,
        paths: list[str],
        modules: list[str],
        peering_name: Optional[str],
        priority: Optional[int],
        standalone: Optional[bool],
        namespaces: Collection[references.NamespacePattern],
        clusterwide: bool,
        liveness_endpoint: Optional[str],
) -> None:
    """ Start an operator process and handle all the requests. """
    if os.environ.get('KOPF_RUN_NAMESPACE'):  # legacy for single-namespace mode
        namespaces = tuple(namespaces) + (os.environ.get('KOPF_RUN_NAMESPACE', ''),)
    if namespaces and clusterwide:
        raise click.UsageError("Either --namespace or --all-namespaces can be used, not both.")
    if __controls.registry is not None:
        registries.set_default_registry(__controls.registry)
    loaders.preload(
        paths=paths,
        modules=modules,
    )
    with loops.proper_loop(__controls.loop):
        return running.run(
            standalone=standalone,
            namespaces=namespaces,
            clusterwide=clusterwide,
            priority=priority,
            peering_name=peering_name,
            liveness_endpoint=liveness_endpoint,
            registry=__controls.registry,
            settings=__controls.settings,
            stop_flag=__controls.stop_flag,
            ready_flag=__controls.ready_flag,
            vault=__controls.vault,
            loop=__controls.loop,
        )


@main.command()
@logging_options
@click.option('-n', '--namespace', 'namespaces', multiple=True)
@click.option('-A', '--all-namespaces', 'clusterwide', is_flag=True)
@click.option('-i', '--id', type=str, default=None)
@click.option('--dev', 'priority', flag_value=666)
@click.option('-P', '--peering', 'peering_name', required=True, envvar='KOPF_FREEZE_PEERING')
@click.option('-p', '--priority', type=int, default=100, required=True)
@click.option('-t', '--lifetime', type=int, required=True)
@click.option('-m', '--message', type=str)
@click.make_pass_decorator(CLIControls, ensure=True)
def freeze(
        __controls: CLIControls,
        id: Optional[str],
        message: Optional[str],
        lifetime: int,
        namespaces: Collection[references.NamespacePattern],
        clusterwide: bool,
        peering_name: str,
        priority: int,
) -> None:
    """ Pause the resource handling in the operator(s). """
    identity = peering.Identity(id) if id else peering.detect_own_id(manual=True)
    insights = references.Insights()
    settings = configuration.OperatorSettings()
    settings.peering.name = peering_name
    settings.peering.priority = priority
    with loops.proper_loop(__controls.loop):
        return running.run(
            clusterwide=clusterwide,
            namespaces=namespaces,
            insights=insights,
            identity=identity,
            settings=settings,
            _command=peering.touch_command(
                insights=insights,
                identity=identity,
                settings=settings,
                lifetime=lifetime))


@main.command()
@logging_options
@click.option('-n', '--namespace', 'namespaces', multiple=True)
@click.option('-A', '--all-namespaces', 'clusterwide', is_flag=True)
@click.option('-i', '--id', type=str, default=None)
@click.option('-P', '--peering', 'peering_name', required=True, envvar='KOPF_RESUME_PEERING')
@click.make_pass_decorator(CLIControls, ensure=True)
def resume(
        __controls: CLIControls,
        id: Optional[str],
        namespaces: Collection[references.NamespacePattern],
        clusterwide: bool,
        peering_name: str,
) -> None:
    """ Resume the resource handling in the operator(s). """
    identity = peering.Identity(id) if id else peering.detect_own_id(manual=True)
    insights = references.Insights()
    settings = configuration.OperatorSettings()
    settings.peering.name = peering_name
    with loops.proper_loop(__controls.loop):
        return running.run(
            clusterwide=clusterwide,
            namespaces=namespaces,
            insights=insights,
            identity=identity,
            settings=settings,
            _command=peering.touch_command(
                insights=insights,
                identity=identity,
                settings=settings,
                lifetime=0))



================================================
FILE: kopf/on.py
================================================
"""
The decorators for the event handlers. Usually used as::

    import kopf

    @kopf.on.create('kopfexamples')
    def creation_handler(**kwargs):
        pass

This module is a part of the framework's public interface.
"""
import warnings
from collections.abc import Collection
# TODO: add cluster=True support (different API methods)
from typing import Any, Callable, Optional, Union

from kopf._cogs.structs import dicts, references, reviews
from kopf._core.actions import execution
from kopf._core.intents import callbacks, causes, filters, handlers, registries
from kopf._core.reactor import subhandling

ActivityDecorator = Callable[[callbacks.ActivityFn], callbacks.ActivityFn]
IndexingDecorator = Callable[[callbacks.IndexingFn], callbacks.IndexingFn]
WatchingDecorator = Callable[[callbacks.WatchingFn], callbacks.WatchingFn]
ChangingDecorator = Callable[[callbacks.ChangingFn], callbacks.ChangingFn]
WebhookDecorator = Callable[[callbacks.WebhookFn], callbacks.WebhookFn]
DaemonDecorator = Callable[[callbacks.DaemonFn], callbacks.DaemonFn]
TimerDecorator = Callable[[callbacks.TimerFn], callbacks.TimerFn]


def startup(  # lgtm[py/similar-function]
        *,
        # Handler's behaviour specification:
        id: Optional[str] = None,
        param: Optional[Any] = None,
        errors: Optional[execution.ErrorsMode] = None,
        timeout: Optional[float] = None,
        retries: Optional[int] = None,
        backoff: Optional[float] = None,
        # Operator specification:
        registry: Optional[registries.OperatorRegistry] = None,
) -> ActivityDecorator:
    def decorator(  # lgtm[py/similar-function]
            fn: callbacks.ActivityFn,
    ) -> callbacks.ActivityFn:
        real_registry = registry if registry is not None else registries.get_default_registry()
        real_id = registries.generate_id(fn=fn, id=id)
        handler = handlers.ActivityHandler(
            fn=fn, id=real_id, param=param,
            errors=errors, timeout=timeout, retries=retries, backoff=backoff,
            activity=causes.Activity.STARTUP,
        )
        real_registry._activities.append(handler)
        return fn
    return decorator


def cleanup(  # lgtm[py/similar-function]
        *,
        # Handler's behaviour specification:
        id: Optional[str] = None,
        param: Optional[Any] = None,
        errors: Optional[execution.ErrorsMode] = None,
        timeout: Optional[float] = None,
        retries: Optional[int] = None,
        backoff: Optional[float] = None,
        # Operator specification:
        registry: Optional[registries.OperatorRegistry] = None,
) -> ActivityDecorator:
    def decorator(  # lgtm[py/similar-function]
            fn: callbacks.ActivityFn,
    ) -> callbacks.ActivityFn:
        real_registry = registry if registry is not None else registries.get_default_registry()
        real_id = registries.generate_id(fn=fn, id=id)
        handler = handlers.ActivityHandler(
            fn=fn, id=real_id, param=param,
            errors=errors, timeout=timeout, retries=retries, backoff=backoff,
            activity=causes.Activity.CLEANUP,
        )
        real_registry._activities.append(handler)
        return fn
    return decorator


def login(  # lgtm[py/similar-function]
        *,
        # Handler's behaviour specification:
        id: Optional[str] = None,
        param: Optional[Any] = None,
        errors: Optional[execution.ErrorsMode] = None,
        timeout: Optional[float] = None,
        retries: Optional[int] = None,
        backoff: Optional[float] = None,
        # Operator specification:
        registry: Optional[registries.OperatorRegistry] = None,
) -> ActivityDecorator:
    """ ``@kopf.on.login()`` handler for custom (re-)authentication. """
    def decorator(  # lgtm[py/similar-function]
            fn: callbacks.ActivityFn,
    ) -> callbacks.ActivityFn:
        real_registry = registry if registry is not None else registries.get_default_registry()
        real_id = registries.generate_id(fn=fn, id=id)
        handler = handlers.ActivityHandler(
            fn=fn, id=real_id, param=param,
            errors=errors, timeout=timeout, retries=retries, backoff=backoff,
            activity=causes.Activity.AUTHENTICATION,
        )
        real_registry._activities.append(handler)
        return fn
    return decorator


def probe(  # lgtm[py/similar-function]
        *,
        # Handler's behaviour specification:
        id: Optional[str] = None,
        param: Optional[Any] = None,
        errors: Optional[execution.ErrorsMode] = None,
        timeout: Optional[float] = None,
        retries: Optional[int] = None,
        backoff: Optional[float] = None,
        # Operator specification:
        registry: Optional[registries.OperatorRegistry] = None,
) -> ActivityDecorator:
    """ ``@kopf.on.probe()`` handler for arbitrary liveness metrics. """
    def decorator(  # lgtm[py/similar-function]
            fn: callbacks.ActivityFn,
    ) -> callbacks.ActivityFn:
        real_registry = registry if registry is not None else registries.get_default_registry()
        real_id = registries.generate_id(fn=fn, id=id)
        handler = handlers.ActivityHandler(
            fn=fn, id=real_id, param=param,
            errors=errors, timeout=timeout, retries=retries, backoff=backoff,
            activity=causes.Activity.PROBE,
        )
        real_registry._activities.append(handler)
        return fn
    return decorator


def validate(  # lgtm[py/similar-function]
        # Resource type specification:
        __group_or_groupversion_or_name: Optional[Union[str, references.Marker]] = None,
        __version_or_name: Optional[Union[str, references.Marker]] = None,
        __name: Optional[Union[str, references.Marker]] = None,
        *,
        group: Optional[str] = None,
        version: Optional[str] = None,
        kind: Optional[str] = None,
        plural: Optional[str] = None,
        singular: Optional[str] = None,
        shortcut: Optional[str] = None,
        category: Optional[str] = None,
        # Handler's behaviour specification:
        id: Optional[str] = None,
        param: Optional[Any] = None,
        operation: Optional[reviews.Operation] = None,  # deprecated -> .webhooks.*.rules.*.operations[0]
        operations: Optional[Collection[reviews.Operation]] = None,  # -> .webhooks.*.rules.*.operations
        subresource: Optional[str] = None,  # -> .webhooks.*.rules.*.resources[]
        persistent: Optional[bool] = None,
        side_effects: Optional[bool] = None,  # -> .webhooks.*.sideEffects
        ignore_failures: Optional[bool] = None,  # -> .webhooks.*.failurePolicy=Ignore
        # Resource object specification:
        labels: Optional[filters.MetaFilter] = None,
        annotations: Optional[filters.MetaFilter] = None,
        when: Optional[callbacks.WhenFilterFn] = None,
        field: Optional[dicts.FieldSpec] = None,
        value: Optional[filters.ValueFilter] = None,
        # Operator specification:
        registry: Optional[registries.OperatorRegistry] = None,
) -> WebhookDecorator:
    """ ``@kopf.on.validate()`` handler for validating admission webhooks. """
    def decorator(  # lgtm[py/similar-function]
            fn: callbacks.WebhookFn,
    ) -> callbacks.WebhookFn:
        nonlocal operations
        operations = _verify_operations(operation, operations)
        _warn_conflicting_values(field, value)
        _verify_filters(labels, annotations)
        real_registry = registry if registry is not None else registries.get_default_registry()
        real_field = dicts.parse_field(field) or None  # to not store tuple() as a no-field case.
        real_id = registries.generate_id(fn=fn, id=id, suffix=".".join(real_field or []))
        selector = references.Selector(
            __group_or_groupversion_or_name, __version_or_name, __name,
            group=group, version=version,
            kind=kind, plural=plural, singular=singular, shortcut=shortcut, category=category,
        )
        handler = handlers.WebhookHandler(
            fn=fn, id=real_id, param=param,
            errors=None, timeout=None, retries=None, backoff=None,  # TODO: add some meaning later
            selector=selector, labels=labels, annotations=annotations, when=when,
            field=real_field, value=value,
            reason=causes.WebhookType.VALIDATING, operations=operations, subresource=subresource,
            persistent=persistent, side_effects=side_effects, ignore_failures=ignore_failures,
        )
        real_registry._webhooks.append(handler)
        return fn
    return decorator


def mutate(  # lgtm[py/similar-function]
        # Resource type specification:
        __group_or_groupversion_or_name: Optional[Union[str, references.Marker]] = None,
        __version_or_name: Optional[Union[str, references.Marker]] = None,
        __name: Optional[Union[str, references.Marker]] = None,
        *,
        group: Optional[str] = None,
        version: Optional[str] = None,
        kind: Optional[str] = None,
        plural: Optional[str] = None,
        singular: Optional[str] = None,
        shortcut: Optional[str] = None,
        category: Optional[str] = None,
        # Handler's behaviour specification:
        id: Optional[str] = None,
        param: Optional[Any] = None,
        operation: Optional[reviews.Operation] = None,  # deprecated -> .webhooks.*.rules.*.operations[0]
        operations: Optional[Collection[reviews.Operation]] = None,  # -> .webhooks.*.rules.*.operations
        subresource: Optional[str] = None,  # -> .webhooks.*.rules.*.resources[]
        persistent: Optional[bool] = None,
        side_effects: Optional[bool] = None,  # -> .webhooks.*.sideEffects
        ignore_failures: Optional[bool] = None,  # -> .webhooks.*.failurePolicy=Ignore
        # Resource object specification:
        labels: Optional[filters.MetaFilter] = None,
        annotations: Optional[filters.MetaFilter] = None,
        when: Optional[callbacks.WhenFilterFn] = None,
        field: Optional[dicts.FieldSpec] = None,
        value: Optional[filters.ValueFilter] = None,
        # Operator specification:
        registry: Optional[registries.OperatorRegistry] = None,
) -> WebhookDecorator:
    """ ``@kopf.on.mutate()`` handler for mutating admission webhooks. """
    def decorator(  # lgtm[py/similar-function]
            fn: callbacks.WebhookFn,
    ) -> callbacks.WebhookFn:
        nonlocal operations
        operations = _verify_operations(operation, operations)
        _warn_conflicting_values(field, value)
        _verify_filters(labels, annotations)
        real_registry = registry if registry is not None else registries.get_default_registry()
        real_field = dicts.parse_field(field) or None  # to not store tuple() as a no-field case.
        real_id = registries.generate_id(fn=fn, id=id, suffix=".".join(real_field or []))
        selector = references.Selector(
            __group_or_groupversion_or_name, __version_or_name, __name,
            group=group, version=version,
            kind=kind, plural=plural, singular=singular, shortcut=shortcut, category=category,
        )
        handler = handlers.WebhookHandler(
            fn=fn, id=real_id, param=param,
            errors=None, timeout=None, retries=None, backoff=None,  # TODO: add some meaning later
            selector=selector, labels=labels, annotations=annotations, when=when,
            field=real_field, value=value,
            reason=causes.WebhookType.MUTATING, operations=operations, subresource=subresource,
            persistent=persistent, side_effects=side_effects, ignore_failures=ignore_failures,
        )
        real_registry._webhooks.append(handler)
        return fn
    return decorator


def resume(  # lgtm[py/similar-function]
        # Resource type specification:
        __group_or_groupversion_or_name: Optional[Union[str, references.Marker]] = None,
        __version_or_name: Optional[Union[str, references.Marker]] = None,
        __name: Optional[Union[str, references.Marker]] = None,
        *,
        group: Optional[str] = None,
        version: Optional[str] = None,
        kind: Optional[str] = None,
        plural: Optional[str] = None,
        singular: Optional[str] = None,
        shortcut: Optional[str] = None,
        category: Optional[str] = None,
        # Handler's behaviour specification:
        id: Optional[str] = None,
        param: Optional[Any] = None,
        errors: Optional[execution.ErrorsMode] = None,
        timeout: Optional[float] = None,
        retries: Optional[int] = None,
        backoff: Optional[float] = None,
        deleted: Optional[bool] = None,
        # Resource object specification:
        labels: Optional[filters.MetaFilter] = None,
        annotations: Optional[filters.MetaFilter] = None,
        when: Optional[callbacks.WhenFilterFn] = None,
        field: Optional[dicts.FieldSpec] = None,
        value: Optional[filters.ValueFilter] = None,
        # Operator specification:
        registry: Optional[registries.OperatorRegistry] = None,
) -> ChangingDecorator:
    """ ``@kopf.on.resume()`` handler for the object resuming on operator (re)start. """
    def decorator(  # lgtm[py/similar-function]
            fn: callbacks.ChangingFn,
    ) -> callbacks.ChangingFn:
        _warn_conflicting_values(field, value)
        _verify_filters(labels, annotations)
        real_registry = registry if registry is not None else registries.get_default_registry()
        real_field = dicts.parse_field(field) or None  # to not store tuple() as a no-field case.
        real_id = registries.generate_id(fn=fn, id=id, suffix=".".join(real_field or []))
        selector = references.Selector(
            __group_or_groupversion_or_name, __version_or_name, __name,
            group=group, version=version,
            kind=kind, plural=plural, singular=singular, shortcut=shortcut, category=category,
        )
        handler = handlers.ChangingHandler(
            fn=fn, id=real_id, param=param,
            errors=errors, timeout=timeout, retries=retries, backoff=backoff,
            selector=selector, labels=labels, annotations=annotations, when=when,
            field=real_field, value=value, old=None, new=None, field_needs_change=False,
            initial=True, deleted=deleted, requires_finalizer=None,
            reason=None,
        )
        real_registry._changing.append(handler)
        return fn
    return decorator


def create(  # lgtm[py/similar-function]
        # Resource type specification:
        __group_or_groupversion_or_name: Optional[Union[str, references.Marker]] = None,
        __version_or_name: Optional[Union[str, references.Marker]] = None,
        __name: Optional[Union[str, references.Marker]] = None,
        *,
        group: Optional[str] = None,
        version: Optional[str] = None,
        kind: Optional[str] = None,
        plural: Optional[str] = None,
        singular: Optional[str] = None,
        shortcut: Optional[str] = None,
        category: Optional[str] = None,
        # Handler's behaviour specification:
        id: Optional[str] = None,
        param: Optional[Any] = None,
        errors: Optional[execution.ErrorsMode] = None,
        timeout: Optional[float] = None,
        retries: Optional[int] = None,
        backoff: Optional[float] = None,
        # Resource object specification:
        labels: Optional[filters.MetaFilter] = None,
        annotations: Optional[filters.MetaFilter] = None,
        when: Optional[callbacks.WhenFilterFn] = None,
        field: Optional[dicts.FieldSpec] = None,
        value: Optional[filters.ValueFilter] = None,
        # Operator specification:
        registry: Optional[registries.OperatorRegistry] = None,
) -> ChangingDecorator:
    """ ``@kopf.on.create()`` handler for the object creation. """
    def decorator(  # lgtm[py/similar-function]
            fn: callbacks.ChangingFn,
    ) -> callbacks.ChangingFn:
        _warn_conflicting_values(field, value)
        _verify_filters(labels, annotations)
        real_registry = registry if registry is not None else registries.get_default_registry()
        real_field = dicts.parse_field(field) or None  # to not store tuple() as a no-field case.
        real_id = registries.generate_id(fn=fn, id=id, suffix=".".join(real_field or []))
        selector = references.Selector(
            __group_or_groupversion_or_name, __version_or_name, __name,
            group=group, version=version,
            kind=kind, plural=plural, singular=singular, shortcut=shortcut, category=category,
        )
        handler = handlers.ChangingHandler(
            fn=fn, id=real_id, param=param,
            errors=errors, timeout=timeout, retries=retries, backoff=backoff,
            selector=selector, labels=labels, annotations=annotations, when=when,
            field=real_field, value=value, old=None, new=None, field_needs_change=False,
            initial=None, deleted=None, requires_finalizer=None,
            reason=causes.Reason.CREATE,
        )
        real_registry._changing.append(handler)
        return fn
    return decorator


def update(  # lgtm[py/similar-function]
        # Resource type specification:
        __group_or_groupversion_or_name: Optional[Union[str, references.Marker]] = None,
        __version_or_name: Optional[Union[str, references.Marker]] = None,
        __name: Optional[Union[str, references.Marker]] = None,
        *,
        group: Optional[str] = None,
        version: Optional[str] = None,
        kind: Optional[str] = None,
        plural: Optional[str] = None,
        singular: Optional[str] = None,
        shortcut: Optional[str] = None,
        category: Optional[str] = None,
        # Handler's behaviour specification:
        id: Optional[str] = None,
        param: Optional[Any] = None,
        errors: Optional[execution.ErrorsMode] = None,
        timeout: Optional[float] = None,
        retries: Optional[int] = None,
        backoff: Optional[float] = None,
        # Resource object specification:
        labels: Optional[filters.MetaFilter] = None,
        annotations: Optional[filters.MetaFilter] = None,
        when: Optional[callbacks.WhenFilterFn] = None,
        field: Optional[dicts.FieldSpec] = None,
        value: Optional[filters.ValueFilter] = None,
        old: Optional[filters.ValueFilter] = None,
        new: Optional[filters.ValueFilter] = None,
        # Operator specification:
        registry: Optional[registries.OperatorRegistry] = None,
) -> ChangingDecorator:
    """ ``@kopf.on.update()`` handler for the object update or change. """
    def decorator(  # lgtm[py/similar-function]
            fn: callbacks.ChangingFn,
    ) -> callbacks.ChangingFn:
        _warn_conflicting_values(field, value, old, new)
        _verify_filters(labels, annotations)
        real_registry = registry if registry is not None else registries.get_default_registry()
        real_field = dicts.parse_field(field) or None  # to not store tuple() as a no-field case.
        real_id = registries.generate_id(fn=fn, id=id, suffix=".".join(real_field or []))
        selector = references.Selector(
            __group_or_groupversion_or_name, __version_or_name, __name,
            group=group, version=version,
            kind=kind, plural=plural, singular=singular, shortcut=shortcut, category=category,
        )
        handler = handlers.ChangingHandler(
            fn=fn, id=real_id, param=param,
            errors=errors, timeout=timeout, retries=retries, backoff=backoff,
            selector=selector, labels=labels, annotations=annotations, when=when,
            field=real_field, value=value, old=old, new=new, field_needs_change=True,
            initial=None, deleted=None, requires_finalizer=None,
            reason=causes.Reason.UPDATE,
        )
        real_registry._changing.append(handler)
        return fn
    return decorator


def delete(  # lgtm[py/similar-function]
        # Resource type specification:
        __group_or_groupversion_or_name: Optional[Union[str, references.Marker]] = None,
        __version_or_name: Optional[Union[str, references.Marker]] = None,
        __name: Optional[Union[str, references.Marker]] = None,
        *,
        group: Optional[str] = None,
        version: Optional[str] = None,
        kind: Optional[str] = None,
        plural: Optional[str] = None,
        singular: Optional[str] = None,
        shortcut: Optional[str] = None,
        category: Optional[str] = None,
        # Handler's behaviour specification:
        id: Optional[str] = None,
        param: Optional[Any] = None,
        errors: Optional[execution.ErrorsMode] = None,
        timeout: Optional[float] = None,
        retries: Optional[int] = None,
        backoff: Optional[float] = None,
        optional: Optional[bool] = None,
        # Resource object specification:
        labels: Optional[filters.MetaFilter] = None,
        annotations: Optional[filters.MetaFilter] = None,
        when: Optional[callbacks.WhenFilterFn] = None,
        field: Optional[dicts.FieldSpec] = None,
        value: Optional[filters.ValueFilter] = None,
        # Operator specification:
        registry: Optional[registries.OperatorRegistry] = None,
) -> ChangingDecorator:
    """ ``@kopf.on.delete()`` handler for the object deletion. """
    def decorator(  # lgtm[py/similar-function]
            fn: callbacks.ChangingFn,
    ) -> callbacks.ChangingFn:
        _warn_conflicting_values(field, value)
        _verify_filters(labels, annotations)
        real_registry = registry if registry is not None else registries.get_default_registry()
        real_field = dicts.parse_field(field) or None  # to not store tuple() as a no-field case.
        real_id = registries.generate_id(fn=fn, id=id, suffix=".".join(real_field or []))
        selector = references.Selector(
            __group_or_groupversion_or_name, __version_or_name, __name,
            group=group, version=version,
            kind=kind, plural=plural, singular=singular, shortcut=shortcut, category=category,
        )
        handler = handlers.ChangingHandler(
            fn=fn, id=real_id, param=param,
            errors=errors, timeout=timeout, retries=retries, backoff=backoff,
            selector=selector, labels=labels, annotations=annotations, when=when,
            field=real_field, value=value, old=None, new=None, field_needs_change=False,
            initial=None, deleted=None, requires_finalizer=bool(not optional),
            reason=causes.Reason.DELETE,
        )
        real_registry._changing.append(handler)
        return fn
    return decorator


def field(  # lgtm[py/similar-function]
        # Resource type specification:
        __group_or_groupversion_or_name: Optional[Union[str, references.Marker]] = None,
        __version_or_name: Optional[Union[str, references.Marker]] = None,
        __name: Optional[Union[str, references.Marker]] = None,
        *,
        group: Optional[str] = None,
        version: Optional[str] = None,
        kind: Optional[str] = None,
        plural: Optional[str] = None,
        singular: Optional[str] = None,
        shortcut: Optional[str] = None,
        category: Optional[str] = None,
        # Handler's behaviour specification:
        id: Optional[str] = None,
        param: Optional[Any] = None,
        errors: Optional[execution.ErrorsMode] = None,
        timeout: Optional[float] = None,
        retries: Optional[int] = None,
        backoff: Optional[float] = None,
        # Resource object specification:
        labels: Optional[filters.MetaFilter] = None,
        annotations: Optional[filters.MetaFilter] = None,
        when: Optional[callbacks.WhenFilterFn] = None,
        field: dicts.FieldSpec,
        value: Optional[filters.ValueFilter] = None,
        old: Optional[filters.ValueFilter] = None,
        new: Optional[filters.ValueFilter] = None,
        # Operator specification:
        registry: Optional[registries.OperatorRegistry] = None,
) -> ChangingDecorator:
    """ ``@kopf.on.field()`` handler for the individual field changes. """
    def decorator(  # lgtm[py/similar-function]
            fn: callbacks.ChangingFn,
    ) -> callbacks.ChangingFn:
        _warn_conflicting_values(field, value, old, new)
        _verify_filters(labels, annotations)
        real_registry = registry if registry is not None else registries.get_default_registry()
        real_field = dicts.parse_field(field) or None  # to not store tuple() as a no-field case.
        real_id = registries.generate_id(fn=fn, id=id, suffix=".".join(real_field or []))
        selector = references.Selector(
            __group_or_groupversion_or_name, __version_or_name, __name,
            group=group, version=version,
            kind=kind, plural=plural, singular=singular, shortcut=shortcut, category=category,
        )
        handler = handlers.ChangingHandler(
            fn=fn, id=real_id, param=param,
            errors=errors, timeout=timeout, retries=retries, backoff=backoff,
            selector=selector, labels=labels, annotations=annotations, when=when,
            field=real_field, value=value, old=old, new=new, field_needs_change=True,
            initial=None, deleted=None, requires_finalizer=None,
            reason=None,
        )
        real_registry._changing.append(handler)
        return fn
    return decorator


def index(  # lgtm[py/similar-function]
        # Resource type specification:
        __group_or_groupversion_or_name: Optional[Union[str, references.Marker]] = None,
        __version_or_name: Optional[Union[str, references.Marker]] = None,
        __name: Optional[Union[str, references.Marker]] = None,
        *,
        group: Optional[str] = None,
        version: Optional[str] = None,
        kind: Optional[str] = None,
        plural: Optional[str] = None,
        singular: Optional[str] = None,
        shortcut: Optional[str] = None,
        category: Optional[str] = None,
        # Handler's behaviour specification:
        id: Optional[str] = None,
        param: Optional[Any] = None,
        errors: Optional[execution.ErrorsMode] = None,
        timeout: Optional[float] = None,
        retries: Optional[int] = None,
        backoff: Optional[float] = None,
        # Resource object specification:
        labels: Optional[filters.MetaFilter] = None,
        annotations: Optional[filters.MetaFilter] = None,
        when: Optional[callbacks.WhenFilterFn] = None,
        field: Optional[dicts.FieldSpec] = None,
        value: Optional[filters.ValueFilter] = None,
        # Operator specification:
        registry: Optional[registries.OperatorRegistry] = None,
) -> IndexingDecorator:
    """ ``@kopf.index()`` handler for the indexing callbacks. """
    def decorator(  # lgtm[py/similar-function]
            fn: callbacks.IndexingFn,
    ) -> callbacks.IndexingFn:
        _warn_conflicting_values(field, value)
        _verify_filters(labels, annotations)
        real_registry = registry if registry is not None else registries.get_default_registry()
        real_field = dicts.parse_field(field) or None  # to not store tuple() as a no-field case.
        real_id = registries.generate_id(fn=fn, id=id)
        selector = references.Selector(
            __group_or_groupversion_or_name, __version_or_name, __name,
            group=group, version=version,
            kind=kind, plural=plural, singular=singular, shortcut=shortcut, category=category,
        )
        handler = handlers.IndexingHandler(
            fn=fn, id=real_id, param=param,
            errors=errors, timeout=timeout, retries=retries, backoff=backoff,
            selector=selector, labels=labels, annotations=annotations, when=when,
            field=real_field, value=value,
        )
        real_registry._indexing.append(handler)
        return fn
    return decorator


def event(  # lgtm[py/similar-function]
        # Resource type specification:
        __group_or_groupversion_or_name: Optional[Union[str, references.Marker]] = None,
        __version_or_name: Optional[Union[str, references.Marker]] = None,
        __name: Optional[Union[str, references.Marker]] = None,
        *,
        group: Optional[str] = None,
        version: Optional[str] = None,
        kind: Optional[str] = None,
        plural: Optional[str] = None,
        singular: Optional[str] = None,
        shortcut: Optional[str] = None,
        category: Optional[str] = None,
        # Handler's behaviour specification:
        id: Optional[str] = None,
        param: Optional[Any] = None,
        # Resource object specification:
        labels: Optional[filters.MetaFilter] = None,
        annotations: Optional[filters.MetaFilter] = None,
        when: Optional[callbacks.WhenFilterFn] = None,
        field: Optional[dicts.FieldSpec] = None,
        value: Optional[filters.ValueFilter] = None,
        # Operator specification:
        registry: Optional[registries.OperatorRegistry] = None,
) -> WatchingDecorator:
    """ ``@kopf.on.event()`` handler for the silent spies on the events. """
    def decorator(  # lgtm[py/similar-function]
            fn: callbacks.WatchingFn,
    ) -> callbacks.WatchingFn:
        _warn_conflicting_values(field, value)
        _verify_filters(labels, annotations)
        real_registry = registry if registry is not None else registries.get_default_registry()
        real_field = dicts.parse_field(field) or None  # to not store tuple() as a no-field case.
        real_id = registries.generate_id(fn=fn, id=id, suffix=".".join(real_field or []))
        selector = references.Selector(
            __group_or_groupversion_or_name, __version_or_name, __name,
            group=group, version=version,
            kind=kind, plural=plural, singular=singular, shortcut=shortcut, category=category,
        )
        handler = handlers.WatchingHandler(
            fn=fn, id=real_id, param=param,
            errors=None, timeout=None, retries=None, backoff=None,
            selector=selector, labels=labels, annotations=annotations, when=when,
            field=real_field, value=value,
        )
        real_registry._watching.append(handler)
        return fn
    return decorator


def daemon(  # lgtm[py/similar-function]
        # Resource type specification:
        __group_or_groupversion_or_name: Optional[Union[str, references.Marker]] = None,
        __version_or_name: Optional[Union[str, references.Marker]] = None,
        __name: Optional[Union[str, references.Marker]] = None,
        *,
        group: Optional[str] = None,
        version: Optional[str] = None,
        kind: Optional[str] = None,
        plural: Optional[str] = None,
        singular: Optional[str] = None,
        shortcut: Optional[str] = None,
        category: Optional[str] = None,
        # Handler's behaviour specification:
        id: Optional[str] = None,
        param: Optional[Any] = None,
        errors: Optional[execution.ErrorsMode] = None,
        timeout: Optional[float] = None,
        retries: Optional[int] = None,
        backoff: Optional[float] = None,
        initial_delay: Optional[float] = None,
        cancellation_backoff: Optional[float] = None,
        cancellation_timeout: Optional[float] = None,
        cancellation_polling: Optional[float] = None,
        # Resource object specification:
        labels: Optional[filters.MetaFilter] = None,
        annotations: Optional[filters.MetaFilter] = None,
        when: Optional[callbacks.WhenFilterFn] = None,
        field: Optional[dicts.FieldSpec] = None,
        value: Optional[filters.ValueFilter] = None,
        # Operator specification:
        registry: Optional[registries.OperatorRegistry] = None,
) -> DaemonDecorator:
    """ ``@kopf.daemon()`` decorator for the background threads/tasks. """
    def decorator(  # lgtm[py/similar-function]
            fn: callbacks.DaemonFn,
    ) -> callbacks.DaemonFn:
        _warn_conflicting_values(field, value)
        _verify_filters(labels, annotations)
        real_registry = registry if registry is not None else registries.get_default_registry()
        real_field = dicts.parse_field(field) or None  # to not store tuple() as a no-field case.
        real_id = registries.generate_id(fn=fn, id=id, suffix=".".join(real_field or []))
        selector = references.Selector(
            __group_or_groupversion_or_name, __version_or_name, __name,
            group=group, version=version,
            kind=kind, plural=plural, singular=singular, shortcut=shortcut, category=category,
        )
        handler = handlers.DaemonHandler(
            fn=fn, id=real_id, param=param,
            errors=errors, timeout=timeout, retries=retries, backoff=backoff,
            selector=selector, labels=labels, annotations=annotations, when=when,
            field=real_field, value=value,
            initial_delay=initial_delay, requires_finalizer=True,
            cancellation_backoff=cancellation_backoff,
            cancellation_timeout=cancellation_timeout,
            cancellation_polling=cancellation_polling,
        )
        real_registry._spawning.append(handler)
        return fn
    return decorator


def timer(  # lgtm[py/similar-function]
        # Resource type specification:
        __group_or_groupversion_or_name: Optional[Union[str, references.Marker]] = None,
        __version_or_name: Optional[Union[str, references.Marker]] = None,
        __name: Optional[Union[str, references.Marker]] = None,
        *,
        group: Optional[str] = None,
        version: Optional[str] = None,
        kind: Optional[str] = None,
        plural: Optional[str] = None,
        singular: Optional[str] = None,
        shortcut: Optional[str] = None,
        category: Optional[str] = None,
        # Handler's behaviour specification:
        id: Optional[str] = None,
        param: Optional[Any] = None,
        errors: Optional[execution.ErrorsMode] = None,
        timeout: Optional[float] = None,
        retries: Optional[int] = None,
        backoff: Optional[float] = None,
        interval: Optional[float] = None,
        initial_delay: Optional[float] = None,
        sharp: Optional[bool] = None,
        idle: Optional[float] = None,
        # Resource object specification:
        labels: Optional[filters.MetaFilter] = None,
        annotations: Optional[filters.MetaFilter] = None,
        when: Optional[callbacks.WhenFilterFn] = None,
        field: Optional[dicts.FieldSpec] = None,
        value: Optional[filters.ValueFilter] = None,
        # Operator specification:
        registry: Optional[registries.OperatorRegistry] = None,
) -> TimerDecorator:
    """ ``@kopf.timer()`` handler for the regular events. """
    def decorator(  # lgtm[py/similar-function]
            fn: callbacks.TimerFn,
    ) -> callbacks.TimerFn:
        _warn_conflicting_values(field, value)
        _verify_filters(labels, annotations)
        real_registry = registry if registry is not None else registries.get_default_registry()
        real_field = dicts.parse_field(field) or None  # to not store tuple() as a no-field case.
        real_id = registries.generate_id(fn=fn, id=id, suffix=".".join(real_field or []))
        selector = references.Selector(
            __group_or_groupversion_or_name, __version_or_name, __name,
            group=group, version=version,
            kind=kind, plural=plural, singular=singular, shortcut=shortcut, category=category,
        )
        handler = handlers.TimerHandler(
            fn=fn, id=real_id, param=param,
            errors=errors, timeout=timeout, retries=retries, backoff=backoff,
            selector=selector, labels=labels, annotations=annotations, when=when,
            field=real_field, value=value,
            initial_delay=initial_delay, requires_finalizer=True,
            sharp=sharp, idle=idle, interval=interval,
        )
        real_registry._spawning.append(handler)
        return fn
    return decorator


def subhandler(  # lgtm[py/similar-function]
        *,
        # Handler's behaviour specification:
        id: Optional[str] = None,
        param: Optional[Any] = None,
        errors: Optional[execution.ErrorsMode] = None,
        timeout: Optional[float] = None,
        retries: Optional[int] = None,
        backoff: Optional[float] = None,
        # Resource object specification:
        labels: Optional[filters.MetaFilter] = None,
        annotations: Optional[filters.MetaFilter] = None,
        when: Optional[callbacks.WhenFilterFn] = None,
        field: Optional[dicts.FieldSpec] = None,
        value: Optional[filters.ValueFilter] = None,
        old: Optional[filters.ValueFilter] = None,  # only for on.update's subhandlers
        new: Optional[filters.ValueFilter] = None,  # only for on.update's subhandlers
) -> ChangingDecorator:
    """
    ``@kopf.subhandler()`` decorator for the dynamically generated sub-handlers.

    Can be used only inside of the handler function.
    It is efficiently a syntax sugar to look like all other handlers::

        @kopf.on.create('kopfexamples')
        def create(*, spec, **kwargs):

            for task in spec.get('tasks', []):

                @kopf.subhandler(id=f'task_{task}')
                def create_task(*, spec, task=task, **kwargs):
                    pass

    In this example, having spec.tasks set to ``[abc, def]``, this will create
    the following handlers: ``create``, ``create/task_abc``, ``create/task_def``.

    The parent handler is not considered as finished if there are unfinished
    sub-handlers left. Since the sub-handlers will be executed in the regular
    reactor and lifecycle, with multiple low-level events (one per iteration),
    the parent handler will also be executed multiple times, and is expected
    to produce the same (or at least predictable) set of sub-handlers.
    In addition, keep its logic idempotent (not failing on the repeated calls).

    Note: ``task=task`` is needed to freeze the closure variable, so that every
    create function will have its own value, not the latest in the for-cycle.
    """
    def decorator(  # lgtm[py/similar-function]
            fn: callbacks.ChangingFn,
    ) -> callbacks.ChangingFn:
        parent_handler = execution.handler_var.get()
        if not isinstance(parent_handler, handlers.ChangingHandler):
            raise TypeError("Sub-handlers are only supported for resource-changing handlers.")
        _warn_incompatible_parent_with_oldnew(parent_handler, old, new)
        _warn_conflicting_values(field, value, old, new)
        _verify_filters(labels, annotations)
        real_registry = subhandling.subregistry_var.get()
        real_field = dicts.parse_field(field) or None  # to not store tuple() as a no-field case.
        real_id = registries.generate_id(fn=fn, id=id,
                                         prefix=parent_handler.id if parent_handler else None)
        handler = handlers.ChangingHandler(
            fn=fn, id=real_id, param=param,
            errors=errors, timeout=timeout, retries=retries, backoff=backoff,
            selector=None, labels=labels, annotations=annotations, when=when,
            field=real_field, value=value, old=old, new=new,
            field_needs_change=parent_handler.field_needs_change, # inherit dymaically
            initial=None, deleted=None, requires_finalizer=None,
            reason=None,
        )
        real_registry.append(handler)
        return fn
    return decorator


def register(  # lgtm[py/similar-function]
        fn: callbacks.ChangingFn,
        *,
        # Handler's behaviour specification:
        id: Optional[str] = None,
        param: Optional[Any] = None,
        errors: Optional[execution.ErrorsMode] = None,
        timeout: Optional[float] = None,
        retries: Optional[int] = None,
        backoff: Optional[float] = None,
        # Resource object specification:
        labels: Optional[filters.MetaFilter] = None,
        annotations: Optional[filters.MetaFilter] = None,
        when: Optional[callbacks.WhenFilterFn] = None,
) -> callbacks.ChangingFn:
    """
    Register a function as a sub-handler of the currently executed handler.

    Example::

        @kopf.on.create('kopfexamples')
        def create_it(spec, **kwargs):
            for task in spec.get('tasks', []):

                def create_single_task(task=task, **_):
                    pass

                kopf.register(id=task, fn=create_single_task)

    This is efficiently an equivalent for::

        @kopf.on.create('kopfexamples')
        def create_it(spec, **kwargs):
            for task in spec.get('tasks', []):

                @kopf.subhandler(id=task)
                def create_single_task(task=task, **_):
                    pass
    """
    decorator = subhandler(
        id=id, param=param,
        errors=errors, timeout=timeout, retries=retries, backoff=backoff,
        labels=labels, annotations=annotations, when=when,
    )
    return decorator(fn)


def _verify_operations(
        operation: Optional[reviews.Operation] = None,  # deprecated
        operations: Optional[Collection[reviews.Operation]] = None,
) -> Optional[Collection[reviews.Operation]]:
    if operation is not None:
        warnings.warn("operation= is deprecated, use operations={...}.", DeprecationWarning)
        operations = frozenset([] if operations is None else operations) | {operation}
    if operations is not None and not operations:
        raise ValueError(f"Operations should be either None or non-empty. Got empty {operations}.")
    return operations


def _verify_filters(
        labels: Optional[filters.MetaFilter],
        annotations: Optional[filters.MetaFilter],
) -> None:
    if labels is not None:
        for key, val in labels.items():
            if val is None:
                raise ValueError("`None` for label filters is not supported; "
                                 "use kopf.PRESENT or kopf.ABSENT.")
    if annotations is not None:
        for key, val in annotations.items():
            if val is None:
                raise ValueError("`None` for annotation filters is not supported; "
                                 "use kopf.PRESENT or kopf.ABSENT.")


def _warn_conflicting_values(
        field: Optional[dicts.FieldSpec],
        value: Optional[filters.ValueFilter],
        old: Optional[filters.ValueFilter] = None,
        new: Optional[filters.ValueFilter] = None,
) -> None:
    if field is None and (value is not None or old is not None or new is not None):
        raise TypeError("Value/old/new filters are specified without a mandatory field.")
    if value is not None and (old is not None or new is not None):
        raise TypeError("Either value= or old=/new= can be defined, not both.")


def _warn_incompatible_parent_with_oldnew(
        handler: execution.Handler,
        old: Any,
        new: Any,
) -> None:
    if old is not None or new is not None:
        if isinstance(handler, handlers.ChangingHandler):
            is_on_update = handler.reason == causes.Reason.UPDATE
            is_on_field = handler.reason is None and not handler.initial
            if not is_on_update and not is_on_field:
                raise TypeError("Filters old=/new= can only be used in update handlers.")



================================================
FILE: kopf/py.typed
================================================
# Marker file for PEP 561



================================================
FILE: kopf/testing.py
================================================
"""
Helper tools to test the Kopf-based operators.

This module is a part of the framework's public interface.
"""
from kopf._kits.runner import KopfRunner

__all__ = [
    'KopfRunner',
]



================================================
FILE: kopf/_cogs/__init__.py
================================================
[Empty file]


================================================
FILE: kopf/_cogs/aiokits/__init__.py
================================================
"""
General-purpose tools & kits for asyncio primitives.
"""



================================================
FILE: kopf/_cogs/aiokits/aioadapters.py
================================================
import asyncio
import concurrent.futures
import threading
from typing import Any, Optional, Union

from kopf._cogs.aiokits import aiotasks

Flag = Union[aiotasks.Future, asyncio.Event, concurrent.futures.Future[Any], threading.Event]


async def wait_flag(
        flag: Optional[Flag],
) -> Any:
    """
    Wait for a flag to be raised.

    Non-asyncio primitives are generally not our worry,
    but we support them for convenience.
    """
    if flag is None:
        return None
    elif isinstance(flag, asyncio.Future):
        return await flag
    elif isinstance(flag, asyncio.Event):
        return await flag.wait()
    elif isinstance(flag, concurrent.futures.Future):
        loop = asyncio.get_running_loop()
        return await loop.run_in_executor(None, flag.result)
    elif isinstance(flag, threading.Event):
        loop = asyncio.get_running_loop()
        return await loop.run_in_executor(None, flag.wait)
    else:
        raise TypeError(f"Unsupported type of a flag: {flag!r}")


async def raise_flag(
        flag: Optional[Flag],
) -> None:
    """
    Raise a flag.

    Non-asyncio primitives are generally not our worry,
    but we support them for convenience.
    """
    if flag is None:
        return None
    elif isinstance(flag, asyncio.Future):
        flag.set_result(None)
    elif isinstance(flag, asyncio.Event):
        flag.set()
    elif isinstance(flag, concurrent.futures.Future):
        flag.set_result(None)
    elif isinstance(flag, threading.Event):
        flag.set()
    else:
        raise TypeError(f"Unsupported type of a flag: {flag!r}")


def check_flag(
        flag: Optional[Flag],
) -> Optional[bool]:
    """
    Check if a flag is raised.
    """
    if flag is None:
        return None
    elif isinstance(flag, asyncio.Future):
        return flag.done()
    elif isinstance(flag, asyncio.Event):
        return flag.is_set()
    elif isinstance(flag, concurrent.futures.Future):
        return flag.done()
    elif isinstance(flag, threading.Event):
        return flag.is_set()
    else:
        raise TypeError(f"Unsupported type of a flag: {flag!r}")



================================================
FILE: kopf/_cogs/aiokits/aiobindings.py
================================================
import asyncio


async def condition_chain(
        source: asyncio.Condition,
        target: asyncio.Condition,
) -> None:
    """
    A condition chain is a "clean" hack to attach one condition to another.

    It is a "clean" (not "dirty") hack to wake up the webhook configuration
    managers when either the resources are revised (as seen in the insights),
    or a new client config is yielded from the webhook server.
    """
    async with source:
        while True:
            await source.wait()
            async with target:
                target.notify_all()



================================================
FILE: kopf/_cogs/aiokits/aioenums.py
================================================
import asyncio
import enum
import threading
import time
from collections.abc import Awaitable, Generator
from typing import Generic, Optional, TypeVar

FlagReasonT = TypeVar('FlagReasonT', bound=enum.Flag)


class FlagSetter(Generic[FlagReasonT]):
    """
    A boolean flag indicating that the daemon should stop and exit.

    Every daemon gets a ``stopped`` kwarg, which is an event-like object.
    The stopped flag is raised in two cases:

    * The corresponding k8s object is deleted, so the daemon should stop.
    * The whole operator is stopping, so all the daemons should stop too.

    The stopped flag is a graceful way of a daemon termination.
    If the daemons do not react to their stoppers and continue running,
    their tasks are cancelled by raising a `asyncio.CancelledError`.

    .. warning::
        In case of synchronous handlers, which are executed in the threads,
        this can lead to the OS resource leakage:
        there is no way to kill a thread in Python, so it will continue running
        forever or until failed (e.g. on an API call for an absent resource).
        The orphan threads will block the operator's process from exiting,
        thus affecting the speed of restarts.
    """

    def __init__(self) -> None:
        super().__init__()
        self.when: Optional[float] = None
        self.reason: Optional[FlagReasonT] = None
        self.sync_event = threading.Event()
        self.async_event = asyncio.Event()
        self.sync_waiter: SyncFlagWaiter[FlagReasonT] = SyncFlagWaiter(self)
        self.async_waiter: AsyncFlagWaiter[FlagReasonT] = AsyncFlagWaiter(self)

    def __repr__(self) -> str:
        return f'<{self.__class__.__name__}: {self.is_set()}, reason={self.reason}>'

    def is_set(self, reason: Optional[FlagReasonT] = None) -> bool:
        """
        Check if the daemon stopper is set: at all or for a specific reason.
        """
        matching_reason = reason is None or (self.reason is not None and reason in self.reason)
        return matching_reason and self.sync_event.is_set()

    def set(self, reason: Optional[FlagReasonT] = None) -> None:
        reason = reason if reason is not None else self.reason  # to keep existing values
        self.when = self.when if self.when is not None else time.monotonic()
        self.reason = reason if self.reason is None or reason is None else self.reason | reason
        self.sync_event.set()
        self.async_event.set()  # it is thread-safe: always called in operator's event loop.


class FlagWaiter(Generic[FlagReasonT]):
    """
    A minimalistic read-only checker for the daemons from the user side.

    This object is fed into the :kwarg:`stopped` kwarg for the handlers.

    The flag setter is hidden from the users, and is an internal class.
    The users should not be able to trigger the stopping activities.

    Usage::

        @kopf.daemon('kopfexamples')
        def handler(stopped, **kwargs):
            while not stopped:
                ...
                stopped.wait(60)
    """

    def __init__(self, setter: FlagSetter[FlagReasonT]) -> None:
        super().__init__()
        self._setter = setter

    def __repr__(self) -> str:
        return f'<{self.__class__.__name__}: {self.is_set()}, reason={self.reason}>'

    def __bool__(self) -> bool:
        return self._setter.is_set()

    def is_set(self) -> bool:
        return self._setter.is_set()

    @property
    def reason(self) -> Optional[FlagReasonT]:
        return self._setter.reason

    # See the docstring for AsyncFlagPromise for explanation.
    def wait(self, timeout: Optional[float] = None) -> "FlagWaiter[FlagReasonT]":
        # Presumably, `await stopped.wait(n).wait(m)` in async mode.
        raise NotImplementedError("Please report the use-case in the issue tracker if needed.")

    # See the docstring for AsyncFlagPromise for explanation.
    def __await__(self) -> Generator[None, None, "FlagWaiter[FlagReasonT]"]:
        # Presumably, `await stopped` in either sync or async mode.
        raise NotImplementedError("Daemon stoppers should not be awaited directly. "
                                  "Use `await stopped.wait()`.")


class SyncFlagWaiter(FlagWaiter[FlagReasonT], Generic[FlagReasonT]):
    def wait(self, timeout: Optional[float] = None) -> "SyncFlagWaiter[FlagReasonT]":
        self._setter.sync_event.wait(timeout=timeout)
        return self


class AsyncFlagWaiter(FlagWaiter[FlagReasonT], Generic[FlagReasonT]):
    def wait(self, timeout: Optional[float] = None) -> "AsyncFlagPromise[FlagReasonT]":
        # A new checker instance, which is awaitable and returns the original checker in the end.
        return AsyncFlagPromise(self, timeout=timeout)


class AsyncFlagPromise(FlagWaiter[FlagReasonT],
                       Awaitable[AsyncFlagWaiter[FlagReasonT]],
                       Generic[FlagReasonT]):
    """
    An artificial future-like promise for ``await stopped.wait(...)``.

    This is a low-level "clean hack" to simplify the publicly faced types.
    As a trade-off, the complexity moves to the implementation side.

    The only goal is to accept one and only one type for the ``stopped`` kwarg
    in ``@kopf.daemon()`` handlers (protocol :class:`callbacks.DaemonFn`)
    regardless of whether they are sync or async, but still usable for both.

    For this, the kwarg is announced with the base type :class:`DaemonStopped`,
    not as a union of sync or async, or any other double-typed approach.

    The challenge is to support ``stopped.wait()`` in both sync & async modes:

    * sync: ``stopped.wait(float) -> bool``.
    * async: ``await stopped.wait(float) -> bool``.

    The sync case is the primary case as there are no alternatives to it.
    The async case is secondary because such use is discouraged (but supported);
    it is recommended to rely on regular task cancellations in async daemons.

    To solve it, instead of returning a ``bool``, a ``bool``-evaluable object
    is returned --- the checker itself. This solves the primary (sync) use-case:
    just sleep for the requested duration and return bool-evaluable self.

    To follow the established signatures of the primary (sync) use-case,
    the secondary (async) use-case also returns an instance of the checker:
    but not "self"! Instead, it creates a new time-limited & awaitable checker
    for every call to ``stopped.wait(n)``, limiting its life by ``n`` seconds.

    Extra checker instances add some tiny memory overhead, but this is fine
    since the use-case is discouraged and there is a better native alternative.

    Then, going back through the class hierarchy, all classes are made awaitable
    so that this functionality becomes exposed via the declared base class.
    But all checkers except the time-limited one prohibit waiting for them.
    """

    def __init__(self, waiter: AsyncFlagWaiter[FlagReasonT], *, timeout: Optional[float]) -> None:
        super().__init__(waiter._setter)
        self._timeout = timeout
        self._waiter = waiter

    def __await__(self) -> Generator[None, None, AsyncFlagWaiter[FlagReasonT]]:
        name = f"time-limited waiting for the daemon stopper {self._setter!r}"
        coro = asyncio.wait_for(self._setter.async_event.wait(), timeout=self._timeout)
        task = asyncio.create_task(coro, name=name)
        try:
            yield from task
        except asyncio.TimeoutError:
            pass  # the requested time limit is reached, exit regardless of the state
        return self._waiter  # the original checker! not the time-limited one!



================================================
FILE: kopf/_cogs/aiokits/aiotasks.py
================================================
"""
Helpers for orchestrating asyncio tasks.

These utilities only support tasks, not more generic futures, coroutines,
or other awaitables. In most case where we use it, we need specifically tasks,
as we not only wait for them, but also cancel them.

Anyway, ``asyncio`` wraps all awaitables and coroutines into tasks on almost
all function calls with multiple awaiables (e.g. :func:`asyncio.wait`),
so there is no added overhead; instead, the implicit overhead is made explicit.
"""
import asyncio
from collections.abc import Collection, Coroutine
from typing import TYPE_CHECKING, Any, Callable, NamedTuple, Optional, TypeVar

from kopf._cogs.helpers import typedefs

_T = TypeVar('_T')

# A workaround for a difference in tasks at runtime and type-checking time.
# Otherwise, at runtime: TypeError: 'type' object is not subscriptable.
if TYPE_CHECKING:
    Future = asyncio.Future[Any]
    Task = asyncio.Task[Any]
else:
    Future = asyncio.Future
    Task = asyncio.Task


async def cancel_coro(
        coro: Coroutine[Any, Any, Any],
        *,
        name: Optional[str] = None,
) -> None:
    """
    Cancel the coroutine if the wrapped code block is cancelled or fails.

    All coroutines must be awaited to prevent RuntimeWarnings/ResourceWarnings.
    As such, we generally need to create a dummy task and cancel it immediately.
    Despite asyncio tasks are lightweight, they still create an object and thus
    consume memory. This can be undesired when applied at scale --- e.g.
    in the multiplexer: when the watcher exits, it cancels all pending workers.

    To save memory, we first try to close the coroutine with no dummy task.
    As a fallback, the coroutine is cancelled gracefully via a dummy task.

    The context manager should be applied to all async code in the managing
    (i.e. parent/wrapping) coroutine from the beginning of it till the managed
    coro is actually awaited and executed.
    """
    try:
        # A dirty (undocumented) way to close a coro, but it saves memory.
        coro.close()  # OR: coro.throw(asyncio.CancelledError())
    except AttributeError:
        # The official way is to create an extra task object, thus to waste some memory.
        corotask = asyncio.create_task(coro=coro, name=name)
        corotask.cancel()
        try:
            await corotask
        except asyncio.CancelledError:
            pass  # cancellations are expected at this point


async def guard(
        coro: Coroutine[Any, Any, Any],
        name: str,
        *,
        flag: Optional[asyncio.Event] = None,
        finishable: bool = False,
        cancellable: bool = False,
        logger: Optional[typedefs.Logger] = None,
) -> None:
    """
    A guard for a presumably eternal (never-finishing) task.

    An "eternal" task is a task that never exits unless explicitly cancelled.
    If it does, this is a misbehaviour that is logged. Errors are always logged.
    Cancellations are also logged except if the task is said to be cancellable.

    It is used for background tasks that are started but never awaited/checked,
    so that the errors are not escalated properly; or if they are occasionally
    awaited/checked with a significant delay after an error possibly happend,
    but needs to be logged as soon as it happens.
    """
    capname = name.capitalize()

    # Guarded tasks can have prerequisites, which are set in other tasks.
    if flag is not None:
        try:
            await flag.wait()
        except asyncio.CancelledError:
            await cancel_coro(coro=coro, name=name)
            raise

    try:
        await coro
    except asyncio.CancelledError:
        if logger is not None and not cancellable:
            logger.debug(f"{capname} is cancelled.")
        raise
    except Exception as e:
        if logger is not None:
            logger.exception(f"{capname} has failed: {e}")
        raise
    else:
        if logger is not None and not finishable:
            logger.warning(f"{capname} has finished unexpectedly.")


def create_guarded_task(
        coro: Coroutine[Any, Any, Any],
        name: str,
        *,
        flag: Optional[asyncio.Event] = None,
        finishable: bool = False,
        cancellable: bool = False,
        logger: Optional[typedefs.Logger] = None,
) -> Task:
    """
    Create a guarded eternal task. See :func:`guard` for explanation.

    This is only a shortcut for named task creation (name is used in 2 places).
    """
    return asyncio.create_task(
        name=name,
        coro=guard(
            name=name,
            coro=coro,
            flag=flag,
            finishable=finishable,
            cancellable=cancellable,
            logger=logger))


async def wait(
        tasks: Collection[Task],
        *,
        timeout: Optional[float] = None,
        return_when: Any = asyncio.ALL_COMPLETED,
) -> tuple[set[Task], set[Task]]:
    """
    A safer version of :func:`asyncio.wait` -- does not fail on an empty list.
    """
    if not tasks:
        return set(), set()
    done, pending = await asyncio.wait(tasks, timeout=timeout, return_when=return_when)
    return done, pending


async def stop(
        tasks: Collection[Task],
        *,
        title: str,
        quiet: bool = False,
        cancelled: bool = False,
        interval: Optional[float] = None,
        logger: Optional[typedefs.Logger] = None,
) -> tuple[set[Task], set[Task]]:
    """
    Cancel the tasks and wait for them to finish; log if some are stuck.

    By default, all stopping activities are logged. In the quiet mode, logs
    only the tasks that are stuck longer than the designated polling interval,
    and their eventual exit if they were reported as stuck at least once.

    If the interval is not set, no polling is performed, and the stopping
    should happen in one iteration (even if it is going to take an eternity).

    The stopping itself does not have timeouts. It always ends either with
    the tasks stopped/exited, or with the stop-routine itself being cancelled.

    For better logging only, if the stopping routine is marked as performing
    the cancellation already (via ``cancelled=True``), then the cancellation
    of the stopping routine is considered as "double-cancelling".
    This does not affect the behaviour, but only the log messages.
    """
    captitle = title.capitalize()

    if not tasks:
        if logger is not None and not quiet:
            logger.debug(f"{captitle} tasks stopping is skipped: no tasks given.")
        return set(), set()

    for task in tasks:
        task.cancel()

    iterations = 0
    done_ever: set[Task] = set()
    pending: set[Task] = set(tasks)
    while pending:
        iterations += 1

        # If the waiting (current) task is cancelled before the wait is over,
        # propagate the cancellation to all the awaited (sub-) tasks, and let them finish.
        try:
            done_now, pending = await wait(pending, timeout=interval)
        except asyncio.CancelledError:
            # If the waiting (current) task is cancelled while propagating the cancellation
            # (i.e. double-cancelled), let it fail without graceful cleanup. It is urgent, it seems.
            pending = {task for task in tasks if not task.done()}
            if logger is not None and (not quiet or pending or iterations > 1):
                are = 'are' if not pending else 'are not'
                why = 'double-cancelling at stopping' if cancelled else 'cancelling at stopping'
                logger.debug(f"{captitle} tasks {are} stopped: {why}; tasks left: {pending!r}")
            raise  # the repeated cancellation, handled specially.
        else:
            # If the cancellation is propagated normally and the awaited (sub-) tasks exited,
            # consider it as a successful cleanup.
            if logger is not None and (not quiet or pending or iterations > 1):
                are = 'are' if not pending else 'are not'
                why = 'cancelling normally' if cancelled else 'finishing normally'
                logger.debug(f"{captitle} tasks {are} stopped: {why}; tasks left: {pending!r}")
            done_ever |= done_now

    return done_ever, pending


async def reraise(
        tasks: Collection[Task],
) -> None:
    """
    Re-raise errors from tasks, if any. Do nothing if all tasks have succeeded.
    """
    for task in tasks:
        try:
            task.result()  # can raise the regular (non-cancellation) exceptions.
        except asyncio.CancelledError:
            pass  # re-raise anything except regular cancellations/exits


async def all_tasks(
        *,
        ignored: Collection[Task] = frozenset(),
) -> Collection[Task]:
    """
    Return all tasks in the current event loop.

    Equivalent to :func:`asyncio.all_tasks`, but with an exlcusion list.
    The exclusion list is used to exclude the tasks that existed at a point
    in time in the past, to only get the tasks that appeared since then.
    """
    current_task = asyncio.current_task()
    return {task for task in asyncio.all_tasks()
            if task is not current_task and task not in ignored}


class SchedulerJob(NamedTuple):
    coro: Coroutine[Any, Any, Any]
    name: Optional[str]


class Scheduler:
    """
    An scheduler/orchestrator/executor for "fire-and-forget" tasks.

    Coroutines can be spawned via this scheduler and forgotten: no need to wait
    for them or to check their status --- the scheduler will take care of it.

    It is a simplified equivalent of aiojobs, but compatible with Python 3.10.
    Python 3.10 removed the explicit event loops (deprecated since Python 3.7),
    which broke aiojobs. At the same time, aiojobs looks unmaintained
    and contains no essential changes since July 2019 (i.e. for 2+ years).
    Hence the necessity to replicate the functionality.

    The scheduler is needed only for internal use and is not exposed to users.
    It is mainly used in the multiplexer (splitting a single stream of events
    of a resource kind into multiple queues of individual resource objects).
    Therefore, many of the features of aiojobs are removed as unnecessary:
    no individual task/job handling or closing, no timeouts, etc.

    .. note::

        Despite all coros will be wrapped into tasks sooner or later,
        and despite it is convincing to do this earlier and manage the tasks
        rather than our own queue of coros+names, do not do this:
        we want all tasks to refer to their true coros in their reprs,
        not to wrappers which wait until the running capacity is available.
    """

    def __init__(
            self,
            *,
            limit: Optional[int] = None,
            exception_handler: Optional[Callable[[BaseException], None]] = None,
    ) -> None:
        super().__init__()
        self._closed = False
        self._limit = limit
        self._exception_handler = exception_handler
        self._condition = asyncio.Condition()
        self._pending_coros: asyncio.Queue[SchedulerJob] = asyncio.Queue()
        self._running_tasks: set[Task] = set()
        self._cleaning_queue: asyncio.Queue[Task] = asyncio.Queue()
        self._cleaning_task = asyncio.create_task(self._task_cleaner(), name=f"cleaner of {self!r}")
        self._spawning_task = asyncio.create_task(self._task_spawner(), name=f"spawner of {self!r}")

    def empty(self) -> bool:
        """ Check if the scheduler has nothing to do. """
        return self._pending_coros.empty() and not self._running_tasks

    async def wait(self) -> None:
        """
        Wait until the scheduler does nothing, i.e. idling (all tasks are done).
        """
        async with self._condition:
            await self._condition.wait_for(self.empty)

    async def close(self) -> None:
        """
        Stop accepting new tasks and cancel all running/pending ones.
        """

        # Running tasks are cancelled here. Pending tasks are cancelled at actual spawning.
        self._closed = True
        for task in self._running_tasks:
            task.cancel()

        # Wait until all tasks are fully done (it can take some time). This also includes
        # the pending coros, which are spawned and instantly cancelled (to prevent RuntimeWarnings).
        await self.wait()

        # Cleanup the scheduler's own resources.
        await stop({self._spawning_task, self._cleaning_task}, title="scheduler", quiet=True)

    async def spawn(
            self,
            coro: Coroutine[Any, Any, Any],
            *,
            name: Optional[str] = None,
    ) -> None:
        """
        Schedule a coroutine for ownership and eventual execution.

        Coroutine ownership ensures that all "fire-and-forget" coroutines
        that were passed to the scheduler will be awaited (to prevent warnings),
        even if the scheduler is closed before the coroutines are started.
        If a coroutine is added to a closed scheduler, it will be instantly
        cancelled before raising the scheduler's exception.
        """
        if self._closed:
            await cancel_coro(coro=coro, name=name)
            raise RuntimeError("Cannot add new coroutines to a closed and inactive scheduler.")
        async with self._condition:
            await self._pending_coros.put(SchedulerJob(coro=coro, name=name))
            self._condition.notify_all()  # -> task_spawner()

    def _can_spawn(self) -> bool:
        return (not self._pending_coros.empty() and
                (self._limit is None or len(self._running_tasks) < self._limit))

    async def _task_spawner(self) -> None:
        """ An internal meta-task to actually start pending coros as tasks. """
        while True:
            async with self._condition:
                await self._condition.wait_for(self._can_spawn)

                # Spawn as many tasks as allowed and as many coros as available at the moment.
                # Since nothing monitors the tasks "actively", we configure them to report back
                # when they are finished --- to be awaited and released "passively".
                while self._can_spawn():
                    coro, name = self._pending_coros.get_nowait()  # guaranteed by the predicate
                    task = asyncio.create_task(coro=coro, name=name)
                    task.add_done_callback(self._task_done_callback)
                    self._running_tasks.add(task)
                    if self._closed:
                        task.cancel()  # used to await the coros without executing them.

    async def _task_cleaner(self) -> None:
        """ An internal meta-task to cleanup the actually finished tasks. """
        while True:
            task = await self._cleaning_queue.get()

            # Await the task from an outer context to prevent RuntimeWarnings/ResourceWarnings.
            try:
                await task
            except BaseException:
                # The errors are handled in the done-callback. Suppress what has leaked for safety.
                pass

            # Ping other tasks to refill the pool of running tasks (or to close the scheduler).
            async with self._condition:
                self._running_tasks.discard(task)
                self._condition.notify_all()  # -> task_spawner() & close()

    def _task_done_callback(self, task: Task) -> None:
        # When a "fire-and-forget" task is done, release its system resources immediately:
        # nothing else is going to explicitly "await" for it any time soon, so we must do it.
        # But since a callback cannot be async, "awaiting" is done in a background utility task.
        self._running_tasks.discard(task)
        self._cleaning_queue.put_nowait(task)

        # If failed, initiate a callback defined by the owner of the task (if any).
        exc: Optional[BaseException]
        try:
            exc = task.exception()
        except asyncio.CancelledError:
            exc = None
        if exc is not None and self._exception_handler is not None:
            self._exception_handler(exc)



================================================
FILE: kopf/_cogs/aiokits/aiotime.py
================================================
import asyncio
import collections.abc
from collections.abc import Collection
from typing import Optional, Union


async def sleep(
        delays: Union[None, float, Collection[Union[None, float]]],
        wakeup: Optional[asyncio.Event] = None,
) -> Optional[float]:
    """
    Measure the sleep time: either until the timeout, or until the event is set.

    Returns the number of seconds left to sleep, or ``None`` if the sleep was
    not interrupted and reached its specified delay (an equivalent of ``0``).
    In theory, the result can be ``0`` if the sleep was interrupted precisely
    the last moment before timing out; this is unlikely to happen though.
    """
    passed_delays = delays if isinstance(delays, collections.abc.Collection) else [delays]
    actual_delays = [delay for delay in passed_delays if delay is not None]
    minimal_delay = min(actual_delays) if actual_delays else 0

    # Do not go for the real low-level system sleep if there is no need to sleep.
    if minimal_delay <= 0:
        return None

    awakening_event = wakeup if wakeup is not None else asyncio.Event()
    loop = asyncio.get_running_loop()
    try:
        start_time = loop.time()
        await asyncio.wait_for(awakening_event.wait(), timeout=minimal_delay)
    except asyncio.TimeoutError:
        return None  # interruptable sleep is over: uninterrupted.
    else:
        end_time = loop.time()
        duration = end_time - start_time
        return max(0., minimal_delay - duration)



================================================
FILE: kopf/_cogs/aiokits/aiotoggles.py
================================================
import asyncio
from collections.abc import Collection, Iterable, Iterator
from typing import Callable, Optional


class Toggle:
    """
    An synchronisation primitive that can be awaited both until set or cleared.

    For one-directional toggles, `asyncio.Event` is sufficient.
    But these events cannot be awaited until cleared.

    The bi-directional toggles are needed in some places in the code, such as
    in the population/depletion of a `Vault`, or as in the operator's pause.

    The optional name is used only for hinting in reprs. It can be used when
    there are many toggles, and they need to be distinguished somehow.
    """

    def __init__(
            self,
            __state: bool = False,
            *,
            name: Optional[str] = None,
            condition: Optional[asyncio.Condition] = None,
    ) -> None:
        super().__init__()
        self._condition = condition if condition is not None else asyncio.Condition()
        self._state: bool = bool(__state)
        self._name = name

    def __repr__(self) -> str:
        clsname = self.__class__.__name__
        toggled = 'on' if self._state else 'off'
        if self._name is None:
            return f'<{clsname}: {toggled}>'
        else:
            return f'<{clsname}: {self._name}: {toggled}>'

    def __bool__(self) -> bool:
        raise NotImplementedError  # to protect against accidental misuse

    def is_on(self) -> bool:
        return self._state

    def is_off(self) -> bool:
        return not self._state

    async def turn_to(self, __state: bool) -> None:
        """ Turn the toggle on/off, and wake up the tasks waiting for that. """
        async with self._condition:
            self._state = bool(__state)
            self._condition.notify_all()

    async def wait_for(self, __state: bool) -> None:
        """ Wait until the toggle is turned on/off as expected (if not yet). """
        async with self._condition:
            await self._condition.wait_for(lambda: self._state == bool(__state))

    @property
    def name(self) -> Optional[str]:
        return self._name


class ToggleSet(Collection[Toggle]):
    """
    A read-only checker for multiple toggles.

    The toggle-checker does not have its own state to be turned on/off.

    The positional argument is a function, usually :func:`any` or :func:`all`,
    which takes an iterable of all individual toggles' states (on/off),
    and calculates the overall state of the toggle set.

    With :func:`any`, the set is "on" when at least one child toggle is "on"
    (and it has at least one child), and it is "off" when all children toggles
    are "off" (or if it has no children toggles at all).

    With :func:`all`, the set is "on" when all of its children toggles are "on"
    (or it has no children at all), and it is "off" when at least one child
    toggle is "off" (and there is at least one toggle).

    The multi-toggle sets are used mostly for operator pausing,
    e.g. in peering and in index pre-population. For a practical example,
    in peering, every individual peering identified by name and namespace has
    its own individual toggle to manage, but the whole set of toggles of all
    names & namespaces is used for pausing the operator as one single toggle.
    In index pre-population, the toggles are used on the operator's startup
    to temporarily delay the actual resource handling until all index-handlers
    of all involved resources and resource kinds are processed and stored.

    Note: the set can only contain toggles that were produced by the set;
    externally produced toggles cannot be added, since they do not share
    the same condition object, which is used for synchronisation/notifications.
    """

    def __init__(self, fn: Callable[[Iterable[bool]], bool]) -> None:
        super().__init__()
        self._condition = asyncio.Condition()
        self._toggles: set[Toggle] = set()
        self._fn = fn

    def __repr__(self) -> str:
        return repr(self._toggles)

    def __len__(self) -> int:
        return len(self._toggles)

    def __iter__(self) -> Iterator[Toggle]:
        return iter(self._toggles)

    def __contains__(self, toggle: object) -> bool:
        return toggle in self._toggles

    def __bool__(self) -> bool:
        raise NotImplementedError  # to protect against accidental misuse

    def is_on(self) -> bool:
        return self._fn(toggle.is_on() for toggle in self._toggles)

    def is_off(self) -> bool:
        return not self.is_on()

    async def wait_for(self, __state: bool) -> None:
        async with self._condition:
            await self._condition.wait_for(lambda: self.is_on() == bool(__state))

    async def make_toggle(
            self,
            __val: bool = False,
            *,
            name: Optional[str] = None,
    ) -> Toggle:
        toggle = Toggle(__val, name=name, condition=self._condition)
        async with self._condition:
            self._toggles.add(toggle)
            self._condition.notify_all()
        return toggle

    async def drop_toggle(self, toggle: Toggle) -> None:
        async with self._condition:
            self._toggles.discard(toggle)
            self._condition.notify_all()

    async def drop_toggles(self, toggles: Iterable[Toggle]) -> None:
        async with self._condition:
            self._toggles.difference_update(toggles)
            self._condition.notify_all()



================================================
FILE: kopf/_cogs/aiokits/aiovalues.py
================================================
import asyncio
from collections.abc import AsyncIterator, Collection
from typing import Generic, TypeVar

_T = TypeVar('_T')


class Container(Generic[_T]):

    def __init__(self) -> None:
        super().__init__()
        self.changed = asyncio.Condition()
        self._values: Collection[_T] = []  # 0..1 item

    def get_nowait(self) -> _T:  # used mostly in testing
        try:
            return next(iter(self._values))
        except StopIteration:
            raise LookupError("No value is stored in the container.") from None

    async def set(self, value: _T) -> None:
        async with self.changed:
            self._values = [value]
            self.changed.notify_all()

    async def wait(self) -> _T:
        async with self.changed:
            await self.changed.wait_for(lambda: self._values)
        try:
            return next(iter(self._values))
        except StopIteration:  # impossible because of the condition's predicate
            raise LookupError("No value is stored in the container.") from None

    async def reset(self) -> None:
        async with self.changed:
            self._values = []
            self.changed.notify_all()

    async def as_changed(self) -> AsyncIterator[_T]:
        async with self.changed:
            while True:
                try:
                    yield next(iter(self._values))
                except StopIteration:
                    pass
                await self.changed.wait()



================================================
FILE: kopf/_cogs/clients/__init__.py
================================================
"""
All the routines to talk to Kubernetes API and other APIs.

This library is supposed to be mocked when the mocked K8s client is needed,
and only the high-level logic has to be tested, not the API calls themselves.

Beware: this is NOT a Kubernetes client. It is set of dedicated adapters
specially tailored to do the framework-specific tasks, not the generic
Kubernetes object manipulation.

The operators MUST NOT rely on how the framework communicates with the cluster.
Specifically:

Currently, all the routines use the official Kubernetes client.
Eventually, it can be replaced with anything else (e.g. pykube-ng).

Currently, most of the routines are synchronous, i.e. blocking
from the asyncio's point of view. Later, they can be replaced
to async coroutines (if the client supports that),
or put into the asyncio executors (thread pools).
"""



================================================
FILE: kopf/_cogs/clients/api.py
================================================
import asyncio
import collections.abc
import itertools
import json
import ssl
import urllib.parse
from collections.abc import AsyncIterator, Mapping
from typing import Any, Optional

import aiohttp

from kopf._cogs.aiokits import aiotasks
from kopf._cogs.clients import auth, errors
from kopf._cogs.configs import configuration
from kopf._cogs.helpers import typedefs


@auth.authenticated
async def get_default_namespace(
        *,
        context: Optional[auth.APIContext] = None,
) -> Optional[str]:
    if context is None:
        raise RuntimeError("API instance is not injected by the decorator.")
    return context.default_namespace


@auth.authenticated
async def read_sslcert(
        *,
        context: Optional[auth.APIContext] = None,
) -> tuple[str, bytes]:
    if context is None:
        raise RuntimeError("API instance is not injected by the decorator.")

    parsed = urllib.parse.urlparse(context.server)
    host = parsed.hostname or ''  # NB: it cannot be None/empty in our case.
    port = parsed.port or 443
    loop = asyncio.get_running_loop()
    cert = await loop.run_in_executor(None, ssl.get_server_certificate, (host, port))
    return host, cert.encode('ascii')


@auth.authenticated
async def request(
        method: str,
        url: str,  # relative to the server/api root.
        *,
        settings: configuration.OperatorSettings,
        payload: Optional[object] = None,
        headers: Optional[Mapping[str, str]] = None,
        timeout: Optional[aiohttp.ClientTimeout] = None,
        context: Optional[auth.APIContext] = None,  # injected by the decorator
        logger: typedefs.Logger,
) -> aiohttp.ClientResponse:
    if context is None:  # for type-checking!
        raise RuntimeError("API instance is not injected by the decorator.")

    if '://' not in url:
        url = context.server.rstrip('/') + '/' + url.lstrip('/')

    if timeout is None:
        timeout = aiohttp.ClientTimeout(
            total=settings.networking.request_timeout,
            sock_connect=settings.networking.connect_timeout,
        )

    backoffs = settings.networking.error_backoffs
    backoffs = backoffs if isinstance(backoffs, collections.abc.Iterable) else [backoffs]
    count = len(backoffs) + 1 if isinstance(backoffs, collections.abc.Sized) else None
    backoff: Optional[float]
    for retry, backoff in enumerate(itertools.chain(backoffs, [None]), start=1):
        idx = f"#{retry}/{count}" if count is not None else f"#{retry}"
        what = f"{method.upper()} {url}"
        try:
            if retry > 1:
                logger.debug(f"Request attempt {idx}: {what}")

            response = await context.session.request(
                method=method,
                url=url,
                json=payload,
                headers=headers,
                timeout=timeout,
            )
            await errors.check_response(response)  # but do not parse it!

        # aiohttp raises a generic error if the session/transport is closed, so we try to guess.
        # NB: "session closed" will reset the retry counter and do the full cycle with the new creds.
        except RuntimeError as e:
            if context.session.closed:
                # TODO: find a way to gracefully replace the active session in the existing context,
                #       so that all ongoing requests would switch to the new session & credentials.
                logger.error(f"Request attempt {idx} failed; TCP closed; will re-authenticate: {what}")
                raise errors.APISessionClosed("Session is closed.") from e
            raise

        # NOTE(vsaienko): during k8s upgrade API might throw 403 forbidden. Use retries for this exception as well.
        except (aiohttp.ClientConnectionError, errors.APIServerError, asyncio.TimeoutError, errors.APIForbiddenError) as e:
            if '[SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY]' in str(e):  # for ClientOSError
                logger.error(f"Request attempt {idx} failed; SSL closed; will re-authenticate: {what}")
                raise errors.APISessionClosed("SSL data stream is closed.") from e
            elif backoff is None:  # i.e. the last or the only attempt.
                logger.error(f"Request attempt {idx} failed; escalating: {what} -> {e!r}")
                raise
            else:
                logger.error(f"Request attempt {idx} failed; will retry: {what} -> {e!r}")
                await asyncio.sleep(backoff)  # non-awakable! but still cancellable.
        else:
            if retry > 1:
                logger.debug(f"Request attempt {idx} succeeded: {what}")
            return response

    raise RuntimeError("Broken retryable routine.")  # impossible, but needed for type-checking.


async def get(
        url: str,  # relative to the server/api root.
        *,
        settings: configuration.OperatorSettings,
        payload: Optional[object] = None,
        headers: Optional[Mapping[str, str]] = None,
        timeout: Optional[aiohttp.ClientTimeout] = None,
        logger: typedefs.Logger,
) -> Any:
    response = await request(
        method='get',
        url=url,
        payload=payload,
        headers=headers,
        timeout=timeout,
        settings=settings,
        logger=logger,
    )
    async with response:
        return await response.json()


async def post(
        url: str,  # relative to the server/api root.
        *,
        settings: configuration.OperatorSettings,
        payload: Optional[object] = None,
        headers: Optional[Mapping[str, str]] = None,
        timeout: Optional[aiohttp.ClientTimeout] = None,
        logger: typedefs.Logger,
) -> Any:
    response = await request(
        method='post',
        url=url,
        payload=payload,
        headers=headers,
        timeout=timeout,
        settings=settings,
        logger=logger,
    )
    async with response:
        return await response.json()


async def patch(
        url: str,  # relative to the server/api root.
        *,
        settings: configuration.OperatorSettings,
        payload: Optional[object] = None,
        headers: Optional[Mapping[str, str]] = None,
        timeout: Optional[aiohttp.ClientTimeout] = None,
        logger: typedefs.Logger,
) -> Any:
    response = await request(
        method='patch',
        url=url,
        payload=payload,
        headers=headers,
        timeout=timeout,
        settings=settings,
        logger=logger,
    )
    async with response:
        return await response.json()


async def delete(
        url: str,  # relative to the server/api root.
        *,
        settings: configuration.OperatorSettings,
        payload: Optional[object] = None,
        headers: Optional[Mapping[str, str]] = None,
        timeout: Optional[aiohttp.ClientTimeout] = None,
        logger: typedefs.Logger,
) -> Any:
    response = await request(
        method='delete',
        url=url,
        payload=payload,
        headers=headers,
        timeout=timeout,
        settings=settings,
        logger=logger,
    )
    async with response:
        return await response.json()


async def stream(
        url: str,  # relative to the server/api root.
        *,
        settings: configuration.OperatorSettings,
        payload: Optional[object] = None,
        headers: Optional[Mapping[str, str]] = None,
        timeout: Optional[aiohttp.ClientTimeout] = None,
        stopper: Optional[aiotasks.Future] = None,
        logger: typedefs.Logger,
) -> AsyncIterator[Any]:
    response = await request(
        method='get',
        url=url,
        payload=payload,
        headers=headers,
        timeout=timeout,
        settings=settings,
        logger=logger,
    )
    response_close_callback = lambda _: response.close()  # to remove the positional arg.
    if stopper is not None:
        stopper.add_done_callback(response_close_callback)
    try:
        async with response:
            async for line in iter_jsonlines(response.content):
                yield json.loads(line.decode('utf-8'))
    except aiohttp.ClientConnectionError:
        if stopper is not None and stopper.done():
            pass
        else:
            raise
    finally:
        if stopper is not None:
            stopper.remove_done_callback(response_close_callback)


async def iter_jsonlines(
        content: aiohttp.StreamReader,
        chunk_size: int = 1024 * 1024,
) -> AsyncIterator[bytes]:
    """
    Iterate line by line over the response's content.

    Usage::

        async for line in _iter_lines(response.content):
            pass

    This is an equivalent of::

        async for line in response.content:
            pass

    Except that the aiohttp's line iteration fails if the accumulated buffer
    length is above 2**17 bytes, i.e. 128 KB (`aiohttp.streams.DEFAULT_LIMIT`
    for the buffer's low-watermark, multiplied by 2 for the high-watermark).
    Kubernetes secrets and other fields can be much longer, up to MBs in length.

    The chunk size of 1MB is an empirical guess for keeping the memory footprint
    reasonably low on huge amount of small lines (limited to 1 MB in total),
    while ensuring the near-instant reads of the huge lines (can be a problem
    with a small chunk size due to too many iterations).

    .. seealso::
        https://github.com/zalando-incubator/kopf/issues/275
    """

    # Minimize the memory footprint by keeping at most 2 copies of a yielded line in memory
    # (in the buffer and as a yielded value), and at most 1 copy of other lines (in the buffer).
    buffer = b''
    async for data in content.iter_chunked(chunk_size):
        buffer += data
        del data

        start = 0
        index = buffer.find(b'\n', start)
        while index >= 0:
            line = buffer[start:index]
            if line:
                yield line
            del line
            start = index + 1
            index = buffer.find(b'\n', start)

        if start > 0:
            buffer = buffer[start:]

    if buffer:
        yield buffer



================================================
FILE: kopf/_cogs/clients/auth.py
================================================
import base64
import functools
import os
import ssl
import tempfile
from collections.abc import Iterator, Mapping
from contextvars import ContextVar
from typing import Any, Callable, Optional, TypeVar, cast

import aiohttp

from kopf._cogs.clients import errors
from kopf._cogs.helpers import versions
from kopf._cogs.structs import credentials

# Per-operator storage and exchange point for authentication methods.
# Used by the client wrappers to retrieve the credentials and report the failures.
# Set by `spawn_tasks`, so that every operator's task has the same vault.
vault_var: ContextVar[credentials.Vault] = ContextVar('vault_var')

# A typevar to show that we return a function with the same signature as given.
_F = TypeVar('_F', bound=Callable[..., Any])


def authenticated(fn: _F) -> _F:
    """
    A decorator to inject a pre-authenticated session to a requesting routine.

    If a wrapped function fails on the authentication, this will be reported
    back to the credentials vault, which will trigger the re-authentication
    activity. Meanwhile, the request-performing function will be awaiting
    for the new credentials, and re-executed once they are available.
    """
    @functools.wraps(fn)
    async def wrapper(*args: Any, **kwargs: Any) -> Any:

        # If a context is explicitly passed, make it a simple call without re-auth.
        # Exceptions are escalated to a caller, which is probably wrapped itself.
        if 'context' in kwargs:
            context = kwargs['context']
            response = await fn(*args, **kwargs)
            if isinstance(response, aiohttp.ClientResponse):
                # Keep track of responses which are using this context.
                context.add_response(response)
            return response

        # Otherwise, attempt the execution with the vault credentials and re-authenticate on 401s.
        vault: credentials.Vault = vault_var.get()
        async for key, info, context in vault.extended(APIContext, 'contexts'):
            try:
                response = await fn(*args, **kwargs, context=context)
                if isinstance(response, aiohttp.ClientResponse):
                    # Keep track of responses which are using this context.
                    context.add_response(response)
                return response
            except (errors.APIUnauthorizedError, errors.APISessionClosed) as e:
                await vault.invalidate(key, info, exc=e)

        # Normally, either `vault.extended()` or `vault.invalidate()` raise the login errors.
        # The for-cycle can only end if the yielded credentials are not invalidated before trying
        # the next ones -- but this case exits by `return` or by other (non-401) errors.
        raise RuntimeError("Reached an impossible state: the end of the authentication cycle.")

    return cast(_F, wrapper)


class APIContext:
    """
    A container for an aiohttp session and the caches of the environment info.

    The container is constructed only once for every `ConnectionInfo`,
    and then cached for later re-use (see `Vault.extended`).

    We assume that the whole operator runs in the same event loop, so there is
    no need to split the sessions for multiple loops. Synchronous handlers are
    threaded with other event loops per thread, but no operator's requests are
    performed inside of those threads: everything is in the main thread/loop.
    """

    # The main contained object used by the API methods.
    session: aiohttp.ClientSession

    # Contextual information for URL building.
    server: str
    default_namespace: Optional[str]

    # List of open responses.
    responses: list[aiohttp.ClientResponse]

    # Temporary caches of the information retrieved for and from the environment.
    _tempfiles: "_TempFiles"

    def __init__(
            self,
            info: credentials.ConnectionInfo,
    ) -> None:
        super().__init__()

        # Some SSL data are not accepted directly, so we have to use temp files.
        tempfiles = _TempFiles()
        ca_path: Optional[str]
        certificate_path: Optional[str]
        private_key_path: Optional[str]

        if info.ca_path and info.ca_data:
            raise credentials.LoginError("Both CA path & data are set. Need only one.")
        elif info.ca_path:
            ca_path = info.ca_path
        elif info.ca_data:
            ca_path = tempfiles[base64.b64decode(info.ca_data)]
        else:
            ca_path = None

        if info.certificate_path and info.certificate_data:
            raise credentials.LoginError("Both certificate path & data are set. Need only one.")
        elif info.certificate_path:
            certificate_path = info.certificate_path
        elif info.certificate_data:
            certificate_path = tempfiles[base64.b64decode(info.certificate_data)]
        else:
            certificate_path = None

        if info.private_key_path and info.private_key_data:
            raise credentials.LoginError("Both private key path & data are set. Need only one.")
        elif info.private_key_path:
            private_key_path = info.private_key_path
        elif info.private_key_data:
            private_key_path = tempfiles[base64.b64decode(info.private_key_data)]
        else:
            private_key_path = None

        # The SSL part (both client certificate auth and CA verification).
        context: ssl.SSLContext
        if certificate_path and private_key_path:
            context = ssl.create_default_context(
                purpose=ssl.Purpose.SERVER_AUTH,
                cafile=ca_path)
            context.load_cert_chain(
                certfile=certificate_path,
                keyfile=private_key_path)
        else:
            context = ssl.create_default_context(
                cafile=ca_path)

        if info.insecure:
            context.check_hostname = False
            context.verify_mode = ssl.CERT_NONE

        # The token auth part.
        headers: dict[str, str] = {}
        if info.scheme and info.token:
            headers['Authorization'] = f'{info.scheme} {info.token}'
        elif info.scheme:
            headers['Authorization'] = f'{info.scheme}'
        elif info.token:
            headers['Authorization'] = f'Bearer {info.token}'

        # The basic auth part.
        auth: Optional[aiohttp.BasicAuth]
        if info.username and info.password:
            auth = aiohttp.BasicAuth(info.username, info.password)
        else:
            auth = None

        # It is a good practice to self-identify a bit.
        headers['User-Agent'] = f'kopf/{versions.version or "unknown"}'

        # Generic aiohttp session based on the constructed credentials.
        self.session = aiohttp.ClientSession(
            connector=aiohttp.TCPConnector(
                limit=0,
                ssl=context,
            ),
            headers=headers,
            auth=auth,
        )

        # Add the extra payload information. We avoid overriding the constructor.
        self.server = info.server
        self.default_namespace = info.default_namespace

        self.responses = []

        # For purging on garbage collection.
        self._tempfiles = tempfiles

    def flush_closed_responses(self) -> None:
        # There's no point keeping references to already closed responses.
        self.responses[:] = [_response for _response in self.responses if not _response.closed]

    def add_response(self, response: aiohttp.ClientResponse) -> None:
        # Keep track of responses so they can be closed later when the session
        # is closed.
        self.flush_closed_responses()
        if not response.closed:
            self.responses.append(response)

    def close_open_responses(self) -> None:
        # Close all responses that are still open and are using this session.
        for response in self.responses:
            if not response.closed:
                response.close()
        self.responses.clear()

    async def close(self) -> None:
        # Close all open responses that use this session before closing the session itself.
        self.close_open_responses()

        # Closing is triggered by `Vault._flush_caches()` -- forward it to the actual session.
        await self.session.close()

        # Additionally, explicitly remove any temporary files we have created.
        # They will be purged on garbage collection anyway, but it is better to make it sooner.
        self._tempfiles.purge()


class _TempFiles(Mapping[bytes, str]):
    """
    A container for the temporary files, which are purged on garbage collection.

    The files are purged when the container is garbage-collected. The container
    is garbage-collected when its parent `APISession` is garbage-collected or
    explicitly closed (by `Vault` on removal of corresponding credentials).
    """

    def __init__(self) -> None:
        super().__init__()
        self._paths: dict[bytes, str] = {}

    def __del__(self) -> None:
        self.purge()

    def __len__(self) -> int:
        return len(self._paths)

    def __iter__(self) -> Iterator[bytes]:
        return iter(self._paths)

    def __getitem__(self, item: bytes) -> str:
        if item not in self._paths:
            with tempfile.NamedTemporaryFile(delete=False) as f:
                f.write(item)
            self._paths[item] = f.name
        return self._paths[item]

    def purge(self) -> None:
        for _, path in self._paths.items():
            try:
                os.remove(path)
            except OSError:
                pass  # already removed
        self._paths.clear()



================================================
FILE: kopf/_cogs/clients/creating.py
================================================
from typing import Optional, cast

from kopf._cogs.clients import api
from kopf._cogs.configs import configuration
from kopf._cogs.helpers import typedefs
from kopf._cogs.structs import bodies, references


async def create_obj(
        *,
        settings: configuration.OperatorSettings,
        resource: references.Resource,
        namespace: references.Namespace = None,
        name: Optional[str] = None,
        body: Optional[bodies.RawBody] = None,
        logger: typedefs.Logger,
) -> Optional[bodies.RawBody]:
    """
    Create a resource.
    """
    body = body if body is not None else {}
    if namespace is not None:
        body.setdefault('metadata', {}).setdefault('namespace', namespace)
    if name is not None:
        body.setdefault('metadata', {}).setdefault('name', name)

    namespace = cast(references.Namespace, body.get('metadata', {}).get('namespace'))
    created_body: bodies.RawBody = await api.post(
        url=resource.get_url(namespace=namespace),
        payload=body,
        logger=logger,
        settings=settings,
    )
    return created_body



================================================
FILE: kopf/_cogs/clients/errors.py
================================================
"""
K8s API errors.

The underlying client library (now, ``aiohttp``) can be replaced in the future.
We cannot rely on embedding its exceptions all over the code in the framework.
Hence, we have our own hierarchy of exceptions for K8s API errors.

Low-level errors, such as the network connectivity issues, SSL/HTTPS issues,
etc, are escalated from the client library as is, since they are related not
to the domain of K8s API, but rather to the networking and encryption.

The original errors of the client library are chained as the causes of our own
specialised errors -- for better explainability of errors in the stack traces.

Some selected reasons of K8s API errors are made into their own classes,
so that they could be intercepted and handled in other places of the framework.
All other reasons are raised as the base error class and are indistinguishable
from each other (except via the exception's fields).

Unlike the underlying client library's errors, the K8s API errors contain more
information about the reasons -- as provided by K8s API in its response bodies,
not guessed only by HTTP statuses alone.

These errors are not exposed to the users, and the users cannot catch them
with ``except:`` clauses. The users can only see these errors in the logs
as the reasons of failures. However, the errors are exposed to other packages.
"""
import collections.abc
import json
from collections.abc import Collection
from typing import Literal, Optional, TypedDict

import aiohttp


class RawStatusCause(TypedDict):
    field: str
    reason: str
    message: str


class RawStatusDetails(TypedDict):
    name: str
    uid: str
    retryAfterSeconds: int
    kind: str
    group: str
    causes: Collection[RawStatusCause]


# https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#status-v1-meta
class RawStatus(TypedDict):
    apiVersion: str
    kind: Literal["Status"]
    code: int
    status: Literal["Success", "Failure"]
    reason: str
    message: str
    details: RawStatusDetails


class APIError(Exception):

    def __init__(
            self,
            payload: Optional[RawStatus],
            *,
            status: int,
    ) -> None:
        message = payload.get('message') if payload else None
        super().__init__(message, payload)
        self._status = status
        self._payload = payload

    @property
    def status(self) -> int:
        return self._status

    @property
    def code(self) -> Optional[int]:
        return self._payload.get('code') if self._payload else None

    @property
    def message(self) -> Optional[str]:
        return self._payload.get('message') if self._payload else None

    @property
    def details(self) -> Optional[RawStatusDetails]:
        return self._payload.get('details') if self._payload else None


class APIClientError(APIError):  # all 4xx
    pass


class APIServerError(APIError):  # all 5xx
    pass


class APIUnauthorizedError(APIClientError):
    pass


class APIForbiddenError(APIClientError):
    pass


class APINotFoundError(APIClientError):
    pass


class APIConflictError(APIClientError):
    pass


class APISessionClosed(Exception):
    """
    A helper to escalate from inside the requests to cause re-authentication.

    This happens when credentials expire while multiple concurrent requests
    are ongoing (including their retries, mostly their back-off timeouts):
    one random request will raise HTTP 401 and cause the re-authentication,
    while others will retry their requests with the old session (now closed!)
    and get a generic RuntimeError from aiohttp, thus failing their whole task.
    """
    pass


async def check_response(
        response: aiohttp.ClientResponse,
) -> None:
    """
    Check for specialised K8s errors, and raise with extended information.
    """
    if response.status >= 400:

        # Read the response's body before it is closed by raise_for_status().
        payload: Optional[RawStatus]
        try:
            payload = await response.json()
        except (json.JSONDecodeError, aiohttp.ContentTypeError, aiohttp.ClientConnectionError):
            payload = None

        # Better be safe: who knows which sensitive information can be dumped unless kind==Status.
        if not isinstance(payload, collections.abc.Mapping) or payload.get('kind') != 'Status':
            payload = None

        cls = (
            APIUnauthorizedError if response.status == 401 else
            APIForbiddenError if response.status == 403 else
            APINotFoundError if response.status == 404 else
            APIConflictError if response.status == 409 else
            APIClientError if 400 <= response.status < 500 else
            APIServerError if 500 <= response.status < 600 else
            APIError
        )

        # Raise the framework-specific error while keeping the original error in scope.
        # This call also closes the response's body, so it cannot be read afterwards.
        try:
            response.raise_for_status()
        except aiohttp.ClientResponseError as e:
            raise cls(payload, status=response.status) from e



================================================
FILE: kopf/_cogs/clients/events.py
================================================
import copy
import datetime

import aiohttp

from kopf._cogs.clients import api, errors
from kopf._cogs.configs import configuration
from kopf._cogs.helpers import typedefs
from kopf._cogs.structs import bodies, references

MAX_MESSAGE_LENGTH = 1024
CUT_MESSAGE_INFIX = '...'


async def post_event(
        *,
        ref: bodies.ObjectReference,
        type: str,
        reason: str,
        message: str = '',
        resource: references.Resource,
        settings: configuration.OperatorSettings,
        logger: typedefs.Logger,
) -> None:
    """
    Issue an event for the object.

    This is where they can also be accumulated, aggregated, grouped,
    and where the rate-limits should be maintained. It can (and should)
    be done by the client library, as it is done in the Go client.
    """

    # Prevent "event explosion", when core v1 events are handled and create other core v1 events.
    # This can happen with `EVERYTHING` without additional filters, or by explicitly serving them.
    if ref.get('apiVersion') == 'v1' and ref.get('kind') == 'Event':
        return

    # See #164. For cluster-scoped objects, use the current namespace from the current context.
    # It could be "default", but in some systems, we are limited to one specific namespace only.
    namespace_name: str = ref.get('namespace') or (await api.get_default_namespace()) or 'default'
    namespace = references.NamespaceName(namespace_name)
    full_ref: bodies.ObjectReference = copy.copy(ref)
    full_ref['namespace'] = namespace

    # Prevent a common case of event posting errors but shortening the message.
    if len(message) > MAX_MESSAGE_LENGTH:
        infix = CUT_MESSAGE_INFIX
        prefix = message[:MAX_MESSAGE_LENGTH // 2 - (len(infix) // 2)]
        suffix = message[-MAX_MESSAGE_LENGTH // 2 + (len(infix) - len(infix) // 2):]
        message = f'{prefix}{infix}{suffix}'

    now = datetime.datetime.now(datetime.timezone.utc)
    body = {
        'metadata': {
            'namespace': namespace,
            'generateName': settings.posting.event_name_prefix,
        },

        'action': 'Action?',
        'type': type,
        'reason': reason,
        'message': message,

        'reportingComponent': settings.posting.reporting_component,
        'reportingInstance': settings.posting.reporting_instance,
        'source': {'component': settings.posting.reporting_component},  # used in the "From" column in `kubectl describe`.

        'involvedObject': full_ref,

        'firstTimestamp': now.isoformat(),  # seen in `kubectl describe ...`
        'lastTimestamp': now.isoformat(),  # seen in `kubectl get events`
        'eventTime': now.isoformat(),
    }

    try:
        await api.post(
            url=resource.get_url(namespace=namespace),
            headers={'Content-Type': 'application/json'},
            payload=body,
            logger=logger,
            settings=settings,
        )

    # Events are helpful but auxiliary, they should not fail the handling cycle.
    # Yet we want to notice that something went wrong (in logs).
    except errors.APIError as e:
        logger.warning(f"Failed to post an event. Ignoring and continuing. "
                       f"Code: {e.code}. Message: {e.message}. Details: {e.details}"
                       f"Event: type={type!r}, reason={reason!r}, message={message!r}.")
    except aiohttp.ClientResponseError as e:
        logger.warning(f"Failed to post an event. Ignoring and continuing. "
                       f"Status: {e.status}. Message: {e.message}. "
                       f"Event: type={type!r}, reason={reason!r}, message={message!r}.")
    except aiohttp.ServerDisconnectedError as e:
        logger.warning(f"Failed to post an event. Ignoring and continuing. "
                       f"Message: {e.message}. "
                       f"Event: type={type!r}, reason={reason!r}, message={message!r}.")
    except aiohttp.ClientOSError:
        logger.warning(f"Failed to post an event. Ignoring and continuing. "
                       f"Event: type={type!r}, reason={reason!r}, message={message!r}.")



================================================
FILE: kopf/_cogs/clients/fetching.py
================================================
from collections.abc import Collection

from kopf._cogs.clients import api
from kopf._cogs.configs import configuration
from kopf._cogs.helpers import typedefs
from kopf._cogs.structs import bodies, references


async def list_objs(
        *,
        settings: configuration.OperatorSettings,
        resource: references.Resource,
        namespace: references.Namespace,
        logger: typedefs.Logger,
) -> tuple[Collection[bodies.RawBody], str]:
    """
    List the objects of specific resource type.

    The cluster-scoped call is used in two cases:

    * The resource itself is cluster-scoped, and namespacing makes not sense.
    * The operator serves all namespaces for the namespaced custom resource.

    Otherwise, the namespace-scoped call is used:

    * The resource is namespace-scoped AND operator is namespaced-restricted.
    """
    rsp = await api.get(
        url=resource.get_url(namespace=namespace),
        logger=logger,
        settings=settings,
    )

    items: list[bodies.RawBody] = []
    resource_version = rsp.get('metadata', {}).get('resourceVersion', None)
    for item in rsp.get('items', []):
        if 'kind' in rsp:
            item.setdefault('kind', rsp['kind'].removesuffix('List'))
        if 'apiVersion' in rsp:
            item.setdefault('apiVersion', rsp['apiVersion'])
        items.append(item)

    return items, resource_version



================================================
FILE: kopf/_cogs/clients/patching.py
================================================
from typing import Optional

from kopf._cogs.clients import api, errors
from kopf._cogs.configs import configuration
from kopf._cogs.helpers import typedefs
from kopf._cogs.structs import bodies, patches, references


async def patch_obj(
        *,
        settings: configuration.OperatorSettings,
        resource: references.Resource,
        namespace: references.Namespace,
        name: Optional[str],
        patch: patches.Patch,
        logger: typedefs.Logger,
) -> Optional[bodies.RawBody]:
    """
    Patch a resource of specific kind.

    Unlike the object listing, the namespaced call is always
    used for the namespaced resources, even if the operator serves
    the whole cluster (i.e. is not namespace-restricted).

    Returns the patched body. The patched body can be partial (status-only,
    no-status, or empty) -- depending on whether there were fields in the body
    or in the status to patch; if neither had fields for patching, the result
    is an empty body. The result should only be used to check against the patch:
    if there was nothing to patch, it does not matter if the fields are absent.

    Returns ``None`` if the underlying object is absent, as detected by trying
    to patch it and failing with HTTP 404. This can happen if the object was
    deleted in the operator's handlers or externally during the processing,
    so that the framework was unaware of these changes until the last moment.
    """
    as_subresource = 'status' in resource.subresources
    body_patch = dict(patch)  # shallow: for mutation of the top-level keys below.
    status_patch = body_patch.pop('status', None) if as_subresource else None

    # Patch & reconstruct the actual body as reported by the server. The reconstructed body can be
    # partial or empty -- if the body/status patches are empty. This is fine: it is only used
    # to verify that the patched fields are matching the patch. No patch? No mismatch!
    try:
        patched_body = bodies.RawBody()

        if body_patch:
            patched_body = await api.patch(
                url=resource.get_url(namespace=namespace, name=name),
                headers={'Content-Type': 'application/merge-patch+json'},
                payload=body_patch,
                settings=settings,
                logger=logger,
            )

        if status_patch:
            response = await api.patch(
                url=resource.get_url(namespace=namespace, name=name,
                                     subresource='status' if as_subresource else None),
                headers={'Content-Type': 'application/merge-patch+json'},
                payload={'status': status_patch},
                settings=settings,
                logger=logger,
            )
            patched_body['status'] = response.get('status')

        return patched_body

    except errors.APINotFoundError:
        return None



================================================
FILE: kopf/_cogs/clients/scanning.py
================================================
import asyncio
from collections.abc import Collection, Mapping
from typing import Optional

from kopf._cogs.clients import api, errors
from kopf._cogs.configs import configuration
from kopf._cogs.helpers import typedefs
from kopf._cogs.structs import references


async def read_version(
        *,
        settings: configuration.OperatorSettings,
        logger: typedefs.Logger,
) -> Mapping[str, str]:
    rsp: Mapping[str, str] = await api.get('/version', settings=settings, logger=logger)
    return rsp


async def scan_resources(
        *,
        settings: configuration.OperatorSettings,
        logger: typedefs.Logger,
        groups: Optional[Collection[str]] = None,
) -> Collection[references.Resource]:
    coros = {
        _read_old_api(groups=groups, settings=settings, logger=logger),
        _read_new_apis(groups=groups, settings=settings, logger=logger),
    }
    resources: set[references.Resource] = set()
    for coro in asyncio.as_completed(coros):
        resources.update(await coro)
    return resources


async def _read_old_api(
        *,
        settings: configuration.OperatorSettings,
        logger: typedefs.Logger,
        groups: Optional[Collection[str]],
) -> Collection[references.Resource]:
    resources: set[references.Resource] = set()
    if groups is None or '' in groups:
        rsp = await api.get('/api', settings=settings, logger=logger)
        coros = {
            _read_version(
                url=f'/api/{version_name}',
                group='',
                version=version_name,
                preferred=True,
                settings=settings,
                logger=logger,
            )
            for version_name in rsp['versions']
        }
        for coro in asyncio.as_completed(coros):
            resources.update(await coro)
    return resources


async def _read_new_apis(
        *,
        settings: configuration.OperatorSettings,
        logger: typedefs.Logger,
        groups: Optional[Collection[str]],
) -> Collection[references.Resource]:
    resources: set[references.Resource] = set()
    if groups is None or set(groups or {}) - {''}:
        rsp = await api.get('/apis', settings=settings, logger=logger)
        items = [d for d in rsp['groups'] if groups is None or d['name'] in groups]
        coros = {
            _read_version(
                url=f'/apis/{group_dat["name"]}/{version["version"]}',
                group=group_dat['name'],
                version=version['version'],
                preferred=version['version'] == group_dat['preferredVersion']['version'],
                settings=settings,
                logger=logger,
            )
            for group_dat in items
            for version in group_dat['versions']
        }
        for coro in asyncio.as_completed(coros):
            resources.update(await coro)
    return resources


async def _read_version(
        *,
        url: str,
        group: str,
        version: str,
        preferred: bool,
        settings: configuration.OperatorSettings,
        logger: typedefs.Logger,
) -> Collection[references.Resource]:
    try:
        rsp = await api.get(url, settings=settings, logger=logger)
    except errors.APINotFoundError:
        # This happens when the last and the only resource of a group/version
        # has been deleted, the whole group/version is gone, and we rescan it.
        return set()
    else:
        # Note: builtins' singulars are empty strings in K3s (reasons unknown):
        # fall back to the lowercased kind so that the selectors could match.
        return {
            references.Resource(
                group=group,
                version=version,
                kind=resource['kind'],
                plural=resource['name'],
                singular=resource['singularName'] or resource['kind'].lower(),
                shortcuts=frozenset(resource.get('shortNames', [])),
                categories=frozenset(resource.get('categories', [])),
                subresources=frozenset(
                    subresource['name'].split('/', 1)[-1]
                    for subresource in rsp.get('resources', [])
                    if subresource['name'].startswith(f'{resource["name"]}/')
                ),
                namespaced=resource['namespaced'],
                preferred=preferred,
                verbs=frozenset(resource.get('verbs') or []),
            )
            for resource in rsp.get('resources', [])
            if '/' not in resource['name']
        }



================================================
FILE: kopf/_cogs/clients/watching.py
================================================
"""
Watching and streaming watch-events.

Kubernetes client's watching streams are synchronous. To make them asynchronous,
we put them into a `concurrent.futures.ThreadPoolExecutor`,
and yield from there asynchronously.

However, async/await coroutines misbehave with `StopIteration` exceptions
raised by the `next` method: see `PEP-479`_.

As a workaround, we replace `StopIteration` with our custom `StopStreaming`
inherited from `RuntimeError` (as suggested by `PEP-479`_),
and re-implement the generators to make them async.

All of this is a workaround for the standard Kubernetes client's limitations.
They would not be needed if the client library were natively asynchronous.

.. _PEP-479: https://www.python.org/dev/peps/pep-0479/
"""
import asyncio
import contextlib
import enum
import logging
from collections.abc import AsyncIterator
from typing import Optional, Union, cast

import aiohttp

from kopf._cogs.aiokits import aiotasks, aiotoggles
from kopf._cogs.clients import api, errors, fetching
from kopf._cogs.configs import configuration
from kopf._cogs.structs import bodies, references

logger = logging.getLogger(__name__)

HTTP_TOO_MANY_REQUESTS_CODE = 429
DEFAULT_RETRY_DELAY_SECONDS = 1


class WatchingError(Exception):
    """
    Raised when an unexpected error happens in the watch-stream API.
    """


class Bookmark(enum.Enum):
    """ Special marks sent in the stream among raw events. """
    LISTED = enum.auto()  # the listing is over, now streaming.


async def infinite_watch(
        *,
        settings: configuration.OperatorSettings,
        resource: references.Resource,
        namespace: references.Namespace,
        operator_paused: Optional[aiotoggles.ToggleSet] = None,  # None for tests & observation
        _iterations: Optional[int] = None,  # used in tests/mocks/fixtures
) -> AsyncIterator[Union[Bookmark, bodies.RawEvent]]:
    """
    Stream the watch-events infinitely.

    This routine is extracted because it is difficult to test infinite loops.
    It is made as simple as possible, and is assumed to work without testing.

    This routine never ends gracefully. If a watcher's stream fails,
    a new one is recreated, and the stream continues.
    It only exits with unrecoverable exceptions.
    """
    how = ' (paused)' if operator_paused is not None and operator_paused.is_on() else ''
    where = f'in {namespace!r}' if namespace is not None else 'cluster-wide'
    logger.debug(f"Starting the watch-stream for {resource} {where}{how}.")
    try:
        while _iterations is None or _iterations > 0:  # equivalent to `while True` in non-test mode
            _iterations = None if _iterations is None else _iterations - 1
            async with streaming_block(
                namespace=namespace,
                resource=resource,
                operator_paused=operator_paused,
            ) as operator_pause_waiter:
                stream = continuous_watch(
                    settings=settings,
                    resource=resource,
                    namespace=namespace,
                    operator_pause_waiter=operator_pause_waiter,
                )
                try:
                    async for raw_event in stream:
                        yield raw_event
                except errors.APIClientError as ex:
                    if ex.code != HTTP_TOO_MANY_REQUESTS_CODE:
                        raise

                    retry_after = ex.details.get("retryAfterSeconds") if ex.details else None
                    retry_wait = retry_after or DEFAULT_RETRY_DELAY_SECONDS
                    logger.warning(
                        f"Receiving `too many requests` error from server, will retry after "
                        f"{retry_wait} seconds. Error details: {ex}"
                    )
                    await asyncio.sleep(retry_wait)
            await asyncio.sleep(settings.watching.reconnect_backoff)
    finally:
        logger.debug(f"Stopping the watch-stream for {resource} {where}.")


@contextlib.asynccontextmanager
async def streaming_block(
        *,
        resource: references.Resource,
        namespace: references.Namespace,
        operator_paused: Optional[aiotoggles.ToggleSet] = None,  # None for tests & observation
) -> AsyncIterator[aiotasks.Future]:
    """
    Block the execution until un-paused; signal when it is active again.

    This prevents both watching and listing while the operator is paused,
    until it is off. Specifically, the watch-stream closes its connection
    once paused, so the while-true & for-event-in-stream cycles exit,
    and the streaming coroutine is started again by `infinite_stream()`
    (the watcher timeout is swallowed by the pause time).

    Returns a future (or a task) that is set (or finished) when paused again.

    A stop-future is a client-specific way of terminating the streaming HTTPS
    connections when paused again. The low-level streaming API call attaches
    its `response.close()` to the future's "done" callback,
    so that the stream is closed once the operator is paused.

    Note: this routine belongs to watching and does not belong to peering.
    The pause can be managed in any other ways: as an imaginary edge case,
    imagine a operator with UI with a "pause" button that pauses the operator.
    """
    where = f'in {namespace!r}' if namespace is not None else 'cluster-wide'

    # Block until unpaused before even starting the API communication.
    if operator_paused is not None and operator_paused.is_on():
        names = {toggle.name for toggle in operator_paused if toggle.is_on() and toggle.name}
        pause_reason = f" (blockers: {', '.join(names)})" if names else ""
        logger.debug(f"Pausing the watch-stream for {resource} {where}{pause_reason}.")

        await operator_paused.wait_for(False)

        names = {toggle.name for toggle in operator_paused if toggle.is_on() and toggle.name}
        resuming_reason = f" (resolved: {', '.join(names)})" if names else ""
        logger.debug(f"Resuming the watch-stream for {resource} {where}{resuming_reason}.")

    # Create the signalling future for when paused again.
    operator_pause_waiter: aiotasks.Future
    if operator_paused is not None:
        operator_pause_waiter = asyncio.create_task(
            operator_paused.wait_for(True),
            name=f"pause-waiter for {resource}")
    else:
        operator_pause_waiter = asyncio.Future()  # a dummy just to have it

    # Go for the streaming with the prepared pauseing/unpausing setup.
    try:
        yield operator_pause_waiter
    finally:
        with contextlib.suppress(asyncio.CancelledError):
            operator_pause_waiter.cancel()
            await operator_pause_waiter


async def continuous_watch(
        *,
        settings: configuration.OperatorSettings,
        resource: references.Resource,
        namespace: references.Namespace,
        operator_pause_waiter: aiotasks.Future,
) -> AsyncIterator[Union[Bookmark, bodies.RawEvent]]:

    # First, list the resources regularly, and get the list's resource version.
    # Simulate the events with type "None" event - used in detection of causes.
    try:
        objs, resource_version = await fetching.list_objs(
            logger=logger,
            settings=settings,
            resource=resource,
            namespace=namespace,
        )
        for obj in objs:
            yield {'type': None, 'object': obj}

    except (aiohttp.ClientConnectionError, aiohttp.ClientPayloadError, asyncio.TimeoutError):
        return

    # Notify the watcher that the initial listing is over, even if there was nothing yielded.
    yield Bookmark.LISTED

    # Repeat through disconnects of the watch as long as the resource version is valid (no errors).
    # The individual watching API calls are disconnected by timeout even if the stream is fine.
    while not operator_pause_waiter.done():

        # Then, watch the resources starting from the list's resource version.
        stream = watch_objs(
            settings=settings,
            resource=resource,
            namespace=namespace,
            since=resource_version,
            operator_pause_waiter=operator_pause_waiter,
        )
        async for raw_input in stream:
            raw_type = raw_input['type']
            raw_object = raw_input['object']

            # "410 Gone" is for the "resource version too old" error, we must restart watching.
            # The resource versions are lost by k8s after a few minutes (5 as per the official doc).
            # The error occurs when there is nothing happening for a few minutes. This is normal.
            if raw_type == 'ERROR' and cast(bodies.RawError, raw_object)['code'] == 410:
                where = f'in {namespace!r}' if namespace is not None else 'cluster-wide'
                logger.debug(f"Restarting the watch-stream for {resource} {where}.")
                return  # out of the regular stream, to the infinite stream.

            # Other watch errors should be fatal for the operator.
            if raw_type == 'ERROR':
                raise WatchingError(f"Error in the watch-stream: {raw_object}")

            # Ensure that the event is something we understand and can handle.
            if raw_type not in ['ADDED', 'MODIFIED', 'DELETED']:
                logger.warning(f"Ignoring an unsupported event type: {raw_input!r}")
                continue

            # Keep the latest seen resource version for continuation of the stream on disconnects.
            body = cast(bodies.RawBody, raw_object)
            resource_version = body.get('metadata', {}).get('resourceVersion', resource_version)

            # Yield normal events to the consumer. Errors are already filtered out.
            yield cast(bodies.RawEvent, raw_input)


async def watch_objs(
        *,
        settings: configuration.OperatorSettings,
        resource: references.Resource,
        namespace: references.Namespace,
        since: Optional[str] = None,
        operator_pause_waiter: aiotasks.Future,
) -> AsyncIterator[bodies.RawInput]:
    """
    Watch objects of a specific resource type.

    The cluster-scoped call is used in two cases:

    * The resource itself is cluster-scoped, and namespacing makes not sense.
    * The operator serves all namespaces for the namespaced custom resource.

    Otherwise, the namespace-scoped call is used:

    * The resource is namespace-scoped AND operator is namespaced-restricted.
    """
    params: dict[str, str] = {}
    params['watch'] = 'true'
    if since is not None:
        params['resourceVersion'] = since
    if settings.watching.server_timeout is not None:
        params['timeoutSeconds'] = str(settings.watching.server_timeout)

    connect_timeout = (
        settings.watching.connect_timeout if settings.watching.connect_timeout is not None else
        settings.networking.connect_timeout if settings.networking.connect_timeout is not None else
        settings.networking.request_timeout
    )

    # Stream the parsed events from the response until it is closed server-side,
    # or until it is closed client-side by the pause-waiting future's callbacks.
    try:
        async for raw_input in api.stream(
            url=resource.get_url(namespace=namespace, params=params),
            logger=logger,
            settings=settings,
            stopper=operator_pause_waiter,
            timeout=aiohttp.ClientTimeout(
                total=settings.watching.client_timeout,
                sock_connect=connect_timeout,
            ),
        ):
            yield raw_input

    except (aiohttp.ClientConnectionError, aiohttp.ClientPayloadError, asyncio.TimeoutError):
        pass



================================================
FILE: kopf/_cogs/configs/__init__.py
================================================
[Empty file]


================================================
FILE: kopf/_cogs/configs/configuration.py
================================================
"""
All configuration flags, options, settings to fine-tune an operator.

All settings are grouped semantically just for convenience
(instead of a flat mega-object with all the values in it).

The individual groups or settings can eventually be moved or regrouped within
the root object, while keeping the legacy names for backward compatibility.

.. note::

    There is a discussion on usage of such words as "configuration",
    "preferences", "settings", "options", "properties" in the internet:

    * https://stackoverflow.com/q/2074384/857383
    * https://qr.ae/pNvt40
    * etc.

    In this framework, they are called *"settings"* (plural).
    Combined, they form a *"configuration"* (singular).

    Some of the settings are flags, some are scalars, some are optional,
    some are not (but all of them have reasonable defaults).

    Regardless of the exact class and module names, all of these terms can be
    used interchangeably -- but so that it is understandable what is meant.
"""
import concurrent.futures
import dataclasses
import logging
from collections.abc import Iterable
from typing import Optional, Union

from kopf._cogs.configs import diffbase, progress
from kopf._cogs.structs import reviews


@dataclasses.dataclass
class ProcessSettings:
    """
    Settings for Kopf's OS processes: e.g. when started via CLI as `kopf run`.
    """

    ultimate_exiting_timeout: Optional[float] = 10 * 60
    """
    How long to wait for the graceful exit before SIGKILL'ing the operator.

    This is the last resort to make the operator exit instead of getting stuck
    at exiting due to the framework's bugs, operator's bugs, threads left,
    daemons not exiting, etc.

    The countdown goes from when a graceful signal arrives (SIGTERM/SIGINT),
    regardless of what is happening in the graceful exiting routine.

    Measured in seconds. Set to `None` to disable (on your own risk).

    The default is 10 minutes -- high enough for all common sense cases,
    and higher than K8s pods' ``terminationGracePeriodSeconds`` --
    to let K8s kill the operator's pod instead, if it can.
    """


@dataclasses.dataclass
class PostingSettings:

    enabled: bool = True
    """
    Should the log messages be sent as Kubernetes Events for an object.
    The events can be seen in ``kubectl describe`` output for the object.

    This also affects ``kopf.event()`` and similar functions
    (``kopf.info()``, ``kopf.warn()``, ``kopf.exception()``).
    """

    level: int = logging.INFO
    """
    A minimal level of logging events that will be posted as K8s Events.
    The default is ``logging.INFO`` (i.e. all info, warning, errors are posted).

    This also affects ``kopf.event()`` and similar functions
    (``kopf.info()``, ``kopf.warn()``, ``kopf.exception()``).
    """

    reporting_component: str = 'kopf'
    reporting_instance: str = 'dev'
    event_name_prefix: str = 'kopf-event-'


@dataclasses.dataclass
class PeeringSettings:

    name: str = 'default'
    """
    The name of the peering object to use.
    Distinct peering objects are isolated peering neighbourhoods,
    i.e. operators in one of them are not visible to operators in others.
    """

    stealth: bool = False
    """
    Should this operator log its keep-alives?

    In some cases, it might be undesired to log regular keep-alives while
    they actually happen (to keep the logs clean and readable).

    Note that some occasions are logged unconditionally:

    * those affecting the operator's behaviour, such as pauses/resumes;
    * those requiring human intervention, such as absence of a peering object
      in the auto-detection mode (to make the peering mandatory or standalone).
    """

    priority: int = 0
    """
    The operator's priority to use. The operators with lower priority pause
    when they see operators with higher or the same priority --
    to avoid double-processing and double-handling of the resources.
    """

    lifetime: int = 60
    """
    For how long (in seconds) the operator's record is considered actual
    by other operators before assuming that the corresponding operator
    is not functioning and the paused state should be re-evaluated.

    The peered operators will update their records as long as they are running,
    slightly faster than their records expires (5-10 seconds earlier).

    Note that it is the lifetime of the current operator. For operators that
    do not communicate their lifetime (broken?), it is always assumed to be
    60 seconds regardless of this operator's configuration (a hard-coded value).
    """

    mandatory: bool = False
    """
    Is peering mandatory for this operator, or optional? If it is mandatory,
    the operator will fail to run in the absence of the peering CRD or object.
    If optional, it will continue in the standalone mode (i.e. without peering).
    """

    standalone: bool = False
    """
    Should the operator be forced to run without peering even if it exists?
    Normally, operator automatically detect the peering objects named "default",
    and run with peering enabled if they exist, or standalone if absent.
    But they can be forced to either mode (standalone or mandatory peering).
    """

    clusterwide: bool = False
    """
    Should the peering be clusterwide or namespaced?

    Usually, the peering has the same mode as the operator, using the peering
    objects either in the served namespaces or cluster-wide accordingly.

    In exceptional cases, the peering can mismatch the operator's mode:
    e.g. using the cluster-wide peering while the operator is namespaced.
    """

    @property
    def namespaced(self) -> bool:
        """ An inverse of ``clusterwide``, for code readability. """
        return not self.clusterwide

    @namespaced.setter
    def namespaced(self, value: bool) -> None:
        self.clusterwide = not value


@dataclasses.dataclass
class WatchingSettings:

    server_timeout: Optional[float] = None
    """
    The maximum duration of one streaming request. Patched in some tests.
    If ``None``, then obey the server-side timeouts (they seem to be random).
    """

    client_timeout: Optional[float] = None
    """
    An HTTP/HTTPS session timeout to use in watch requests.
    """

    connect_timeout: Optional[float] = None
    """
    An HTTP/HTTPS connection timeout to use in watch requests.
    """

    reconnect_backoff: float = 0.1
    """
    How long should a pause be between watch requests (to prevent API flooding).
    """


@dataclasses.dataclass
class BatchingSettings:
    """
    Settings for how raw events are batched and processed.
    """

    worker_limit: Optional[int] = None
    """
    How many workers can be running simultaneously on per-object event queue.
    If ``None``, there is no limit to the number of workers (as many as needed).
    """

    idle_timeout: float = 5.0
    """
    How soon an idle worker is exited and garbage-collected if no events arrive.
    """

    batch_window: float = 0.1
    """
    How fast/slow does a worker deplete the queue when an event is received.
    All events arriving within this window will be ignored except the last one.
    """

    exit_timeout: float = 2.0
    """
    How soon a worker is cancelled when the parent watcher is going to exit.
    This is the time given to the worker to deplete and process the queue.
    """

    error_delays: Iterable[float] = (1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610)
    """
    Backoff intervals in case of unexpected errors in the framework.

    For more information on error throttling, see :ref:`error-throttling`.
    """


@dataclasses.dataclass
class ScanningSettings:
    """
    Settings for dynamic runtime observation of the cluster's setup.
    """

    disabled: bool = False
    """
    Should the cluster's dynamic monitoring for resources/namespaces be off?

    If enabled (the default), then the operator will try to observe
    the namespaces and custom resources, and will gracefully start/stop
    the watch streams for them (also the peering activities, if applicable).
    This requires RBAC permissions to list/watch the V1 namespaces and CRDs.

    If disabled or if enabled but the permission is not granted, then only
    the specific namespaces will be served, with namespace patterns ignored;
    and only the resources detected at startup will be served, with added CRDs
    or CRD versions being ignored, and the deleted CRDs causing failures.

    The default mode is good enough for most cases, unless the strict
    (non-dynamic) mode is intended -- to prevent the warnings in the logs.
    """


@dataclasses.dataclass
class AdmissionSettings:

    server: Optional[reviews.WebhookServerProtocol] = None
    """
    A way of accepting admission requests from Kubernetes.

    In production, only a `kopf.WebhookServer` is sufficient.
    If development, a tunnel from the cluster to the operator might be needed.

    If no server is configured (the default), then no server is started.
    If admission handlers are detected with no server configured,
    an error is raised and the operator fails to start (with a hint).

    Kopf provides several webhook configs, servers, and tunnels out of the box
    (they also serve as examples for implementing custom tunnels).
    `kopf.WebhookServer`,
    `kopf.WebhookK3dServer`, `kopf.WebhookMinikubeServer`, `kopf.WebhookDockerDesktopServer`,
    `kopf.WebhookNgrokTunnel`, `kopf.WebhookInletsTunnel`.

    .. seealso::
        :doc:`/admission`.
    """

    managed: Optional[str] = None
    """
    The names of managed ``[Validating/Mutating]WebhookConfiguration`` objects.

    If not set (the default), Kopf does not manage the configuration and
    expects that the requests come from a manually pre-created configuration.

    If set, Kopf creates the validating/mutating configurations objects with
    this name and continuously keeps them up to date with the currently served
    resources and client configs as they change at runtime.
    All existing webhooks in these configuration objects are overwritten.

    This feature requires the ``patch`` and ``create`` RBAC verbs
    for ``admissionregistration.k8s.io``'s resources (:doc:`/admission`).
    """


@dataclasses.dataclass
class ExecutionSettings:
    """
    Settings for synchronous handlers execution (e.g. thread-/process-pools).
    """

    executor: concurrent.futures.Executor = dataclasses.field(
        default_factory=concurrent.futures.ThreadPoolExecutor)
    """
    The executor to be used for synchronous handler invocation.

    It can be changed at runtime (e.g. to reset the pool size). Already running
    handlers (specific invocations) will continue with their original executors.
    """

    _max_workers: Optional[int] = None

    @property
    def max_workers(self) -> Optional[int]:
        """
        How many threads/processes is dedicated to handler execution.

        It can be changed at runtime (the threads/processes are not terminated).
        """
        return self._max_workers

    @max_workers.setter
    def max_workers(self, value: int) -> None:
        if value < 1:
            raise ValueError("Can't set thread pool limit lower than 1.")
        self._max_workers = value

        if hasattr(self.executor, '_max_workers'):
            self.executor._max_workers = value
        else:
            raise TypeError("Current executor does not support `max_workers`.")


@dataclasses.dataclass
class NetworkingSettings:

    request_timeout: Optional[float] = 5 * 60  # == aiohttp.client.DEFAULT_TIMEOUT
    """
    A timeout for the entire duration of an API request (in seconds).

    The timeout is only applied to all short atomic requests.
    For watch-streams, use one of ``settings.watching.client_timeout``
    or ``settings.watching.server_timeout``.
    """

    connect_timeout: Optional[float] = None
    """
    A timeout for the connection & handshake of an API request (in seconds).
    """

    error_backoffs: Union[float, Iterable[float]] = (1, 1, 2, 3, 5, 8, 13, 21)
    """
    How many times and with which delays (seconds) to retry the API errors.

    For more information on the API errors retrying, see :doc:`api-retrying`.
    """


@dataclasses.dataclass
class PersistenceSettings:

    finalizer: str = 'kopf.zalando.org/KopfFinalizerMarker'
    """
    A string marker to be put on a list of finalizers to block the object
    from being deleted without framework's/operator's permission.
    """

    progress_storage: progress.ProgressStorage = dataclasses.field(
        default_factory=progress.SmartProgressStorage)
    """
    How to persist the handlers' state between multiple handling cycles.
    """

    diffbase_storage: diffbase.DiffBaseStorage = dataclasses.field(
        default_factory=diffbase.AnnotationsDiffBaseStorage)
    """
    How the resource's essence (non-technical, contentful fields) are stored.
    """


@dataclasses.dataclass
class BackgroundSettings:
    """
    Settings for background routines in general, daemons & timers specifically.
    """

    cancellation_polling: float = 60
    """
    How often (in seconds) to poll the status of an exiting daemon/timer
    when it has no cancellation timeout set (i.e. when it is assumed to
    exit gracefully by its own, but it does not).
    """

    instant_exit_timeout: Optional[float] = None
    """
    For how long (in seconds) to wait for a daemon/timer to exit instantly.

    If they continue running after the stopper is set for longer than this time,
    then external polling is initiated via the resource's persistence storages,
    as for regular handlers.

    The "instant exit" timeout is neither combined with any other timeouts, nor
    deducted from any other timeouts, such as the daemon cancellation timeout.

    So, keep the timeout low: 0.001, 0.01, or 0.1 are good enough; 1.0 is risky.
    Big delays can cause slower operator reaction to the resource deletion
    or operator exiting, but can reduce the amount of unnecessary patches.

    If the timeout is not set (the default), then a limited amount of zero-time
    asyncio event loop cycles is used instead.
    """

    instant_exit_zero_time_cycles: Optional[int] = 10
    """
    How many asyncio cycles to give to a daemon/timer to exit instantly.

    There is a speed-up hack to let the daemons/timers to exit instantly,
    without external patching & polling. For this, ``asyncio.sleep(0)`` is used
    to give control back to the event loop and their coroutines. However,
    the daemons/timers can do extra `await` calls (even zero-time) before
    actually exiting, which prematurely returns the control flow back
    to the daemon-stopper coroutine.

    This configuration value is a maximum amount of zero-time `await` statements
    that can happen before exiting: both in the daemon and in the framework.

    It the daemons/timers coroutines exit earlier, extra cycles are not used.
    If they continue running after that, then external polling is initiated
    via the resource's persistence storages, as for regular handlers.

    All of this happens with zero delays, so no slowdown is expected
    (but a bit of CPU will be consumed).

    If an "instant exit" timeout is set, the zero-time cycles are not used.

    PS: The default value is a rough guess on a typical code complexity.
    """


@dataclasses.dataclass
class OperatorSettings:
    process: ProcessSettings = dataclasses.field(default_factory=ProcessSettings)
    posting: PostingSettings = dataclasses.field(default_factory=PostingSettings)
    peering: PeeringSettings = dataclasses.field(default_factory=PeeringSettings)
    watching: WatchingSettings = dataclasses.field(default_factory=WatchingSettings)
    batching: BatchingSettings = dataclasses.field(default_factory=BatchingSettings)
    scanning: ScanningSettings = dataclasses.field(default_factory=ScanningSettings)
    admission: AdmissionSettings =dataclasses.field(default_factory=AdmissionSettings)
    execution: ExecutionSettings = dataclasses.field(default_factory=ExecutionSettings)
    background: BackgroundSettings = dataclasses.field(default_factory=BackgroundSettings)
    networking: NetworkingSettings = dataclasses.field(default_factory=NetworkingSettings)
    persistence: PersistenceSettings = dataclasses.field(default_factory=PersistenceSettings)



================================================
FILE: kopf/_cogs/configs/conventions.py
================================================
"""
Some reusable implementation details regarding naming in K8s.

This module implements conventions for annotations & labels with restrictions.
They are used to identify operator's own keys consistently during the run,
keep backward- & forward-compatibility of naming schemas across the versions,
and to detect cross-operator keys to prevent ping-pong effects (if Kopf-based).

This is mostly important for storages in the shared spaces, such as annotations
or labels of a resource being handled: to distinguish operator-related and
unrelated keys (e.g. manually added annotations/labels).

For some fields, such as annotations and labels, K8s puts extra restrictions
on the alphabet used and on lengths of the names, name parts, and values.
All such restrictions are implemented here in combination with the conventions.

Terminology (to be on the same page; `aligned with K8s's documentation`__):

__ https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/#syntax-and-character-set

* **prefix** is a fqdn-like optional part (e.g. `kopf.zalando.org/`);
* **name** is the main part of an annotation/label (e.g. `kopf-managed`);
* **key** (a dictionary key, a full key) is the **prefix** plus the **name**,
  possibly suffixed, infixed (named-prefixed), fully or partially hashed
  (e.g. `example.com/kopf-managed` or `kopf.zalando.org/handler1.subhandlerA`),
  but used as a dictionary key for the annotations (hence the name).

Note: there are also progress storages' **record keys**, which are not related
to the annotations/labels keys of this convention: they correspond to the names:
in most cases, they will be used as the annotation names with special symbols
replaced; in some cases, they will be cut and hash-suffixed.
"""
import base64
import hashlib
import warnings
from collections.abc import Collection, Iterable
from typing import Any, Optional

from kopf._cogs.structs import bodies, patches


class CollisionEvadingConvention:
    """
    A helper mixin to evade collisions in annotations propagated down by K8s.

    For some resources, such as ReplicaSets owned by Deployments,
    the annotations are implicitly propagated by Kubernetes
    from the owning resources down to the owned resources.

    As a result, if both resources are served by Kopf-based operators with
    the same or default identity, the owner's annotations overwrite those
    of the resource, which causes all kinds of chaos when e.g. the diff-base
    mismatches the resource's schema or the handlers' progress is miscalculated.

    To evade this, Kopf adds special marks to all annotations of all resources
    known to be overwritten by Kubernetes -- in order to preserve the state
    regardless of whether the parent's annotations are already propagated:
    this can happen much later when the owning resource is started to be served
    hours, days, months after the owned resource has stored its state.

    The only known case at the moment is caused by this behaviour in Kubernetes:

    * https://github.com/kubernetes/kubernetes/blob/v1.20.2/pkg/controller/deployment/util/deployment_util.go#L230-L234
    * https://github.com/kubernetes/kubernetes/blob/v1.20.2/pkg/controller/deployment/util/deployment_util.go#L310-L341

    We assume this does not happen to other resources unless proven otherwise.
    """

    def mark_key(self, key: str, *, body: bodies.Body) -> str:
        owners = body.meta.get('ownerReferences', [])
        kind = body.get('kind')
        if kind == 'ReplicaSet' and any(owner['kind'] == 'Deployment' for owner in owners):
            return f"{key}-ofDRS"  # no need to generalise for a single known case
        else:
            return key


class StorageKeyFormingConvention(CollisionEvadingConvention):
    """
    A helper mixin to manage annotations/labels naming as per K8s restrictions.

    Used both in the diff-base storages and the progress storages where
    applicable. It provides a few optional methods to manage annotation
    prefixes, keys, and names (in this context, a name is a prefix + a key).
    Specifically, the annotations keys are split to V1 & V2 (would be V3, etc).

    **V1** keys were implemented overly restrictive: the length of 63 chars
    was applied to the whole annotation key, including the prefix.

    This caused unnecessary and avoidable loss of useful information: e.g.
    ``lengthy-operator-name-to-hit-63-chars.example.com/update-OJOYLA``
    instead of
    ``lengthy-operator-name-to-hit-63-chars.example.com/update.sub1``.

    `K8s says`__ that only the name is max 63 chars long, while the whole key
    (i.e. including the prefix, if present) can be max 253 chars.

    __ https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/#syntax-and-character-set

    **V2** keys implement this new hashing approach, trying to keep as much
    information in the keys as possible. Only the really lengthy keys
    will be cut the same way as V1 keys.

    If the prefix is longer than 189 chars (253-63-1), the full key could
    be longer than the limit of 253 chars -- e.g. with lengthy handler ids,
    more often for field-handlers or sub-handlers. In that case,
    the annotation keys are not shortened, and the patching would fail.

    It is the developer's responsibility to choose the prefix short enough
    to fit into K8s's restrictions. However, a warning is issued on storage
    creation, so that the strict-mode operators could convert the warning
    into an exception, thus failing the operator startup.

    For smoother upgrades of operators from V1 to V2, and for safer rollbacks
    from V2 to V1, both versions of keys are stored in annotations.
    At some point in time, the V1 keys will be read, purged, but not stored,
    thus cutting the rollback possibilities to Kopf versions with V1 keys.

    Since the annotations are purged in case of a successful handling cycle,
    this multi-versioned behaviour will most likely be unnoticed by the users,
    except when investigating the issues with persistence.

    This mode can be controlled via the storage's constructor parameter
    ``v1=True/False`` (the default is ``True`` for the time of transition).
    """

    def __init__(
            self,
            *args: Any,
            prefix: str,
            v1: bool,
            **kwargs: Any,
    ) -> None:
        super().__init__(*args, **kwargs)
        self.prefix = prefix
        self.v1 = v1

        if not self.prefix:
            raise ValueError("Annotations storages must be prefixed.")

        # 253 is the max length, 63 is the most lengthy name part, 1 is for the "/" separator.
        if len(self.prefix or '') > 253 - 63 - 1:
            warnings.warn("The annotations prefix is too long. It can cause errors when PATCHing.")

    def make_keys(self, key: str, *, body: Optional[bodies.Body] = None) -> Iterable[str]:
        key = key if body is None else self.mark_key(key, body=body)
        v2_keys = [self.make_v2_key(key)]
        v1_keys = [self.make_v1_key(key)] if self.v1 else []
        return v2_keys + list(set(v1_keys) - set(v2_keys))

    @staticmethod
    def make_safe_key(key: str) -> str:
        replacements = {'/': '.', '<': '_', '>': '_'}
        for k, v in replacements.items():
            key = key.replace(k, v)
        return key

    def make_v1_key(self, key: str, max_length: int = 63) -> str:

        # K8s has a limitation on the allowed charsets in annotation/label keys.
        # https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/#syntax-and-character-set
        safe_key = self.make_safe_key(key)

        # K8s has a limitation of 63 chars per annotation/label key.
        # Force it to 63 chars by replacing the tail with a consistent hash (with full alphabet).
        # Force it to end with alnums instead of altchars or trailing chars (K8s requirement).
        prefix = f'{self.prefix}/' if self.prefix else ''
        if len(safe_key) <= max_length - len(prefix):
            suffix = ''
        else:
            suffix = self.make_suffix(safe_key)

        full_key = f'{prefix}{safe_key[:max_length - len(prefix) - len(suffix)]}{suffix}'
        return full_key

    def make_v2_key(self, key: str, max_length: int = 63) -> str:
        prefix = f'{self.prefix}/' if self.prefix else ''
        suffix = self.make_suffix(key) if len(key) > max_length else ''
        key_limit = max(0, max_length - len(suffix))
        safe_key =  self.make_safe_key(key)
        final_key = f'{prefix}{safe_key[:key_limit]}{suffix}'
        return final_key

    def make_suffix(self, key: str) -> str:
        digest = hashlib.blake2b(key.encode('utf-8'), digest_size=4).digest()
        alnums = base64.b64encode(digest, altchars=b'-.').decode('ascii')
        return f'-{alnums}'.rstrip('=-.')


class StorageKeyMarkingConvention:
    """
    A mixin to detect annotations of other Kopf-based operators.

    The detection of other Kopf-based operators' annotations should prevent
    "ping-pong" effects of multiple operators handling the same resources:

    (1) operator A persists its state into an object;
    (2) operator B believes it is a valid essential change and reacts;
    (3) operator B persists its updated state (diff-base), which contains
        the state of the operator A as the essential payload;
    (4) operator A believes the new change is a valid essential change;
    (∞) and so it continues forever (the annotations sizes can explode fast).

    To detect the annotations as belonging to Kopf-based operators, the storages
    inject a marker into the annotation names, and later detect these markers.

    As an extra safety measure, all names of the whole `domain.tld/` prefix,
    both V1 & V2, are detected as marked if there is at least one marked V2 name
    under that prefix -- assuming that the prefix is for a Kopf-based operator.
    For non-prefixed storages, the V1 names are detected by their V2
    counterparts with some additional treatment (marker & hashes removed).

    The marker and the marking are not configurable to prevent turning them off,
    except as by writing self-made storages. In that case, all ping-pong issues
    are considered as intended and handled by the storage/operator developers.

    This logic is already included into all Kopf-provided storages, both with
    and without annotations, so there is no need to explicitly configure it.
    The only case where this class can be of direct use, is when custom storages
    are implemented, but other operators' annotations still have to be cleaned.
    """

    __KNOWN_MARKERS = frozenset([
        'kopf-managed',
    ])

    __KNOWN_PREFIXES = frozenset([
        'kopf.zalando.org',
    ])

    def _detect_marked_prefixes(self, keys: Collection[str]) -> Collection[str]:
        """
        Detect annotation prefixes managed by any other Kopf-based operators.
        """
        prefixes: set[str] = set()
        for prefix, name in (key.split('/', 1) for key in keys if '/' in key):
            if name in self.__KNOWN_MARKERS:
                prefixes.add(prefix)
            elif prefix in self.__KNOWN_PREFIXES:
                prefixes.add(prefix)
            elif any(prefix.endswith(f'.{p}') for p in self.__KNOWN_PREFIXES):
                prefixes.add(prefix)
        return frozenset(prefixes)

    def _store_marker(
            self,
            prefix: str,
            patch: patches.Patch,
            body: bodies.Body,
    ) -> None:
        """
        Store a Kopf-branding marker to make this operator's prefix detectable.
        """
        value = 'yes'
        if prefix and not prefix.startswith('kopf.'):
            marker = f'{prefix}/kopf-managed'
            if marker not in body.metadata.annotations and marker not in patch.metadata.annotations:
                patch.metadata.annotations[marker] = value


class StorageStanzaCleaner:
    """
    A mixin used internally to remove unwanted annotations and empty stanzas.
    """

    @staticmethod
    def remove_annotations(essence: bodies.BodyEssence, keys_to_remove: Collection[str]) -> None:
        """ Remove annotations (in-place). """
        current_keys = essence.get('metadata', {}).get('annotations', {})
        if frozenset(keys_to_remove) & frozenset(current_keys):
            essence['metadata']['annotations'] = {
                key: val
                for key, val in essence.get('metadata', {}).get('annotations', {}).items()
                if key not in keys_to_remove
            }

    @staticmethod
    def remove_empty_stanzas(essence: bodies.BodyEssence) -> None:
        """ Remove (in-place) the parent structs/stanzas if they are empty. """
        if 'annotations' in essence.get('metadata', {}) and not essence['metadata']['annotations']:
            del essence['metadata']['annotations']
        if 'labels' in essence.get('metadata', {}) and not essence['metadata']['labels']:
            del essence['metadata']['labels']
        if 'metadata' in essence and not essence['metadata']:
            del essence['metadata']
        if 'status' in essence and not essence['status']:
            del essence['status']



================================================
FILE: kopf/_cogs/configs/diffbase.py
================================================
import abc
import copy
import json
from collections.abc import Collection, Iterable
from typing import Any, Optional, cast

from kopf._cogs.configs import conventions
from kopf._cogs.structs import bodies, dicts, patches


class DiffBaseStorage(conventions.StorageKeyMarkingConvention,
                      conventions.StorageStanzaCleaner,
                      metaclass=abc.ABCMeta):
    """
    Store the base essence for diff calculations, i.e. last handled state.

    The "essence" is a snapshot of meaningful fields, which must be tracked
    to identify the actual changes on the object (or absence of such).

    Used in the handling routines to check if there were significant changes
    (i.e. not the internal and system changes, like the uids, links, etc),
    and to get the exact per-field diffs for the specific handler functions.

    Conceptually similar to how ``kubectl apply`` stores the applied state
    on any object, and then uses that for the patch calculation:
    https://kubernetes.io/docs/concepts/overview/object-management-kubectl/declarative-config/
    """

    def build(
            self,
            *,
            body: bodies.Body,
            extra_fields: Optional[Iterable[dicts.FieldSpec]] = None,
    ) -> bodies.BodyEssence:
        """
        Extract only the relevant fields for the state comparisons.

        The framework ignores all the system fields (mostly from metadata)
        and the status senza completely. Except for some well-known and useful
        metadata, such as labels and annotations (except for sure garbage).

        A special set of fields can be provided even if they are supposed
        to be removed. This is used, for example, for handlers which react
        to changes in the specific fields in the status stanza,
        while the rest of the status stanza is removed.

        It is generally not a good idea to override this method in custom
        stores, unless a different definition of an object's essence is needed.
        """

        # Always use a copy, so that future changes do not affect the extracted essence.
        essence = cast(dict[Any, Any], copy.deepcopy(dict(body)))

        # The top-level identifying fields never change, so there is not need to track them.
        if 'apiVersion' in essence:
            del essence['apiVersion']
        if 'kind' in essence:
            del essence['kind']

        # Purge the whole stenzas with system info (extra-fields are restored below).
        if 'metadata' in essence:
            del essence['metadata']
        if 'status' in essence:
            del essence['status']

        # We want some selected metadata to be tracked implicitly.
        dicts.cherrypick(src=body, dst=essence, fields=[
            'metadata.labels',
            'metadata.annotations',  # but not all of them! deleted below.
        ], picker=copy.deepcopy)

        # But we do not want all the annotations, only the potentially useful ones.
        # Also exclude the annotations of other Kopf-based operators' storages.
        annotations = essence.get('metadata', {}).get('annotations', {})
        ignored_prefixes = self._detect_marked_prefixes(annotations)
        for annotation in list(annotations):
            if any(annotation.startswith(f'{prefix}/') for prefix in ignored_prefixes):
                del annotations[annotation]
            elif annotation == 'kubectl.kubernetes.io/last-applied-configuration':
                del annotations[annotation]

        # Restore all explicitly whitelisted extra-fields from the original body.
        dicts.cherrypick(src=body, dst=essence, fields=extra_fields, picker=copy.deepcopy)

        self.remove_empty_stanzas(cast(bodies.BodyEssence, essence))
        return cast(bodies.BodyEssence, essence)

    @abc.abstractmethod
    def fetch(
            self,
            *,
            body: bodies.Body,
    ) -> Optional[bodies.BodyEssence]:
        raise NotImplementedError

    @abc.abstractmethod
    def store(
            self,
            *,
            body: bodies.Body,
            patch: patches.Patch,
            essence: bodies.BodyEssence,
    ) -> None:
        raise NotImplementedError


class AnnotationsDiffBaseStorage(conventions.StorageKeyFormingConvention, DiffBaseStorage):

    def __init__(
            self,
            *,
            prefix: str = 'kopf.zalando.org',
            key: str = 'last-handled-configuration',
            v1: bool = True,  # will be switched to False a few releases later
    ) -> None:
        super().__init__(prefix=prefix, v1=v1)
        self.key = key

    def build(
            self,
            *,
            body: bodies.Body,
            extra_fields: Optional[Iterable[dicts.FieldSpec]] = None,
    ) -> bodies.BodyEssence:
        essence = super().build(body=body, extra_fields=extra_fields)
        self.remove_annotations(essence, set(self.make_keys(self.key, body=body)))
        self.remove_empty_stanzas(essence)
        return essence

    def fetch(
            self,
            *,
            body: bodies.Body,
    ) -> Optional[bodies.BodyEssence]:
        for full_key in self.make_keys(self.key, body=body):
            encoded = body.metadata.annotations.get(full_key, None)
            decoded = json.loads(encoded) if encoded is not None else None
            if decoded is not None:
                return cast(bodies.BodyEssence, decoded)
        return None

    def store(
            self,
            *,
            body: bodies.Body,
            patch: patches.Patch,
            essence: bodies.BodyEssence,
    ) -> None:
        encoded: str = json.dumps(essence, separators=(',', ':'))  # NB: no spaces
        encoded += '\n'  # for better kubectl presentation without wrapping (same as kubectl's one)
        for full_key in self.make_keys(self.key, body=body):
            patch.metadata.annotations[full_key] = encoded
        self._store_marker(prefix=self.prefix, patch=patch, body=body)


class StatusDiffBaseStorage(DiffBaseStorage):

    def __init__(
            self,
            *,
            name: str = 'kopf',
            field: dicts.FieldSpec = 'status.{name}.last-handled-configuration',
    ) -> None:
        super().__init__()
        self._name = name
        real_field = field.format(name=self._name) if isinstance(field, str) else field
        self._field = dicts.parse_field(real_field)

    @property
    def field(self) -> dicts.FieldPath:
        return self._field

    @field.setter
    def field(self, field: dicts.FieldSpec) -> None:
        real_field = field.format(name=self._name) if isinstance(field, str) else field
        self._field = dicts.parse_field(real_field)

    def build(
            self,
            *,
            body: bodies.Body,
            extra_fields: Optional[Iterable[dicts.FieldSpec]] = None,
    ) -> bodies.BodyEssence:
        essence = super().build(body=body, extra_fields=extra_fields)

        # Work around an issue with mypy not treating TypedDicts as MutableMappings.
        essence_dict = cast(dict[Any, Any], essence)
        dicts.remove(essence_dict, self.field)

        return essence

    def fetch(
            self,
            *,
            body: bodies.Body,
    ) -> Optional[bodies.BodyEssence]:
        encoded: Optional[str] = dicts.resolve(body, self.field, None)
        essence: Optional[bodies.BodyEssence] = json.loads(encoded) if encoded is not None else None
        return essence

    def store(
            self,
            *,
            body: bodies.Body,
            patch: patches.Patch,
            essence: bodies.BodyEssence,
    ) -> None:
        # Store as a single string instead of full dict -- to avoid merges and unexpected data.
        encoded: str = json.dumps(essence, separators=(',', ':'))  # NB: no spaces
        dicts.ensure(patch, self.field, encoded)


class MultiDiffBaseStorage(DiffBaseStorage):

    def __init__(
            self,
            storages: Collection[DiffBaseStorage],
    ) -> None:
        super().__init__()
        self.storages = storages

    def build(
            self,
            *,
            body: bodies.Body,
            extra_fields: Optional[Iterable[dicts.FieldSpec]] = None,
    ) -> bodies.BodyEssence:
        essence = super().build(body=body, extra_fields=extra_fields)
        for storage in self.storages:
            # Let the individual stores to also clean the essence from their own fields.
            # For this, assume the the previous essence _is_ the body (what's left of it).
            essence = storage.build(body=bodies.Body(essence), extra_fields=extra_fields)
        return essence

    def fetch(
            self,
            *,
            body: bodies.Body,
    ) -> Optional[bodies.BodyEssence]:
        for storage in self.storages:
            content = storage.fetch(body=body)
            if content is not None:
                return content
        return None

    def store(
            self,
            *,
            body: bodies.Body,
            patch: patches.Patch,
            essence: bodies.BodyEssence,
    ) -> None:
        for storage in self.storages:
            storage.store(body=body, patch=patch, essence=essence)



================================================
FILE: kopf/_cogs/configs/progress.py
================================================
"""
State stores are used to track the handlers' states across handling cycles.

Specifically, they track which handlers are finished, which are not yet,
and how many retries were there, and some other information.

There could be more than one low-level k8s watch-events per one actual
high-level kopf-event (a cause). The handlers are called at different times,
and the overall handling routine should persist the handler status somewhere.

When the full event cycle is executed (possibly including multiple re-runs),
the state of all involved handlers is purged. The life-long persistence of state
is not intended: otherwise, multiple distinct causes will clutter the status
and collide with each other (especially critical for multiple updates).

Other unrelated handlers (e.g. from other operators) can co-exist with
the involved handlers (if stored in the resource itself), as the handler states
are independent of each other, and are purged individually, not all at once.

---

Originally, the handlers' state was persisted in ``.status.kopf.progress``.
But due to stricter Kubernetes schemas for built-in resources, they had to
move to annotations. As part of such move, any state persistence engines
are made possible by inheriting and overriding the base classes, though it is
considered an advanced use-case and is only briefly mentioned in the docs.

In all cases, the persisted state for each handler is a fixed-structure dict
with the following keys:

* ``started`` is a timestamp when the handler was first called.
* ``stopped`` is a timestamp when the handler either finished or failed.
* ``delayed`` is a timestamp when the handler should be invoked again (retried).
* ``retries`` is a number of retries so far or in total (if succeeded/failed).
* ``success`` is a boolean flag for a final success (no re-executions).
* ``failure`` is a boolean flag for a final failure (no retries).
* ``message`` is a descriptive message of the last error (an exception).

All timestamps are strings in ISO8601 format in UTC (no explicit ``Z`` suffix).
"""
import abc
import copy
import json
from collections.abc import Collection, Mapping
from typing import Any, Optional, TypedDict, cast

from kopf._cogs.configs import conventions
from kopf._cogs.structs import bodies, dicts, ids, patches


class ProgressRecord(TypedDict, total=True):
    """ A single record stored for persistence of a single handler. """
    started: Optional[str]
    stopped: Optional[str]
    delayed: Optional[str]
    purpose: Optional[str]
    retries: Optional[int]
    success: Optional[bool]
    failure: Optional[bool]
    message: Optional[str]
    subrefs: Optional[Collection[ids.HandlerId]]


class ProgressStorage(conventions.StorageStanzaCleaner, metaclass=abc.ABCMeta):
    """
    Base class and an interface for all persistent states.

    The state is persisted strict per-handler, not for all handlers at once:
    to support overlapping operators (assuming different handler ids) storing
    their state on the same fields of the resource (e.g. ``state.kopf``).

    This also ensures that no extra logic for state merges will be needed:
    the handler states are atomic (i.e. state fields are not used separately)
    but independent: i.e. handlers should be persisted on their own, unrelated
    to other handlers; i.e. never combined to other atomic structures.

    If combining is still needed with performance optimization in mind (e.g.
    for relational/transactional databases), the keys can be cached in memory
    for short time, and ``flush()`` can be overridden to actually store them.
    """

    @abc.abstractmethod
    def fetch(
            self,
            *,
            key: ids.HandlerId,
            body: bodies.Body,
    ) -> Optional[ProgressRecord]:
        raise NotImplementedError

    @abc.abstractmethod
    def store(
            self,
            *,
            key: ids.HandlerId,
            record: ProgressRecord,
            body: bodies.Body,
            patch: patches.Patch,
    ) -> None:
        raise NotImplementedError

    @abc.abstractmethod
    def purge(
            self,
            *,
            key: ids.HandlerId,
            body: bodies.Body,
            patch: patches.Patch,
    ) -> None:
        raise NotImplementedError

    @abc.abstractmethod
    def touch(
            self,
            *,
            body: bodies.Body,
            patch: patches.Patch,
            value: Optional[str],
    ) -> None:
        raise NotImplementedError

    @abc.abstractmethod
    def clear(self, *, essence: bodies.BodyEssence) -> bodies.BodyEssence:
        return copy.deepcopy(essence)

    def flush(self) -> None:
        pass


class AnnotationsProgressStorage(conventions.StorageKeyFormingConvention,
                                 conventions.StorageKeyMarkingConvention,
                                 ProgressStorage):
    """
    State storage in ``.metadata.annotations`` with JSON-serialised content.

    An example without a prefix:

    .. code-block: yaml

        metadata:
          annotations:
            create_fn_1: '{"started": "2020-02-14T16:58:25.396364", "stopped":
                           "2020-02-14T16:58:25.401844", "retries": 1, "success": true}'
            create_fn_2: '{"started": "2020-02-14T16:58:25.396421", "retries": 0}'
        spec: ...
        status: ...

    An example with a prefix:

    .. code-block: yaml

        metadata:
          annotations:
            kopf.zalando.org/create_fn_1: '{"started": "2020-02-14T16:58:25.396364", "stopped":
                                    "2020-02-14T16:58:25.401844", "retries": 1, "success": true}'
            kopf.zalando.org/create_fn_2: '{"started": "2020-02-14T16:58:25.396421", "retries": 0}'
        spec: ...
        status: ...

    For the annotations' naming conventions, hashing, and V1 & V2 differences,
    see :class:`AnnotationsNamingMixin`.
    """

    def __init__(
            self,
            *,
            prefix: str = 'kopf.zalando.org',
            verbose: bool = False,
            touch_key: str = 'touch-dummy',  # NB: not dotted, but dashed
            v1: bool = True,  # will be switched to False a few releases later
    ) -> None:
        super().__init__(prefix=prefix, v1=v1)
        self.verbose = verbose
        self.touch_key = touch_key

    def fetch(
            self,
            *,
            key: ids.HandlerId,
            body: bodies.Body,
    ) -> Optional[ProgressRecord]:
        for full_key in self.make_keys(key, body=body):
            key_field = ['metadata', 'annotations', full_key]
            encoded = dicts.resolve(body, key_field, None)
            decoded = json.loads(encoded) if encoded is not None else None
            if decoded is not None:
                return cast(ProgressRecord, decoded)
        return None

    def store(
            self,
            *,
            key: ids.HandlerId,
            record: ProgressRecord,
            body: bodies.Body,
            patch: patches.Patch,
    ) -> None:
        decoded = {key: val for key, val in record.items() if self.verbose or val is not None}
        encoded = json.dumps(decoded, separators=(',', ':'))  # NB: no spaces
        for full_key in self.make_keys(key, body=body):
            key_field = ['metadata', 'annotations', full_key]
            dicts.ensure(patch, key_field, encoded)
        self._store_marker(prefix=self.prefix, patch=patch, body=body)

    def purge(
            self,
            *,
            key: ids.HandlerId,
            body: bodies.Body,
            patch: patches.Patch,
    ) -> None:
        absent = object()
        for full_key in self.make_keys(key, body=body):
            key_field = ['metadata', 'annotations', full_key]
            body_value = dicts.resolve(body, key_field, absent)
            patch_value = dicts.resolve(patch, key_field, absent)
            if body_value is not absent:
                dicts.ensure(patch, key_field, None)
            elif patch_value is not absent:
                dicts.remove(patch, key_field)

    def touch(
            self,
            *,
            body: bodies.Body,
            patch: patches.Patch,
            value: Optional[str],
    ) -> None:
        for full_key in self.make_keys(self.touch_key, body=body):
            key_field = ['metadata', 'annotations', full_key]
            body_value = dicts.resolve(body, key_field, None)
            if body_value != value:  # also covers absent-vs-None cases.
                dicts.ensure(patch, key_field, value)
                self._store_marker(prefix=self.prefix, patch=patch, body=body)

    def clear(self, *, essence: bodies.BodyEssence) -> bodies.BodyEssence:
        essence = super().clear(essence=essence)
        annotations = essence.get('metadata', {}).get('annotations', {})
        keys = {key for key in annotations if self.prefix and key.startswith(f'{self.prefix}/')}
        self.remove_annotations(essence, keys)
        self.remove_empty_stanzas(essence)
        return essence


class StatusProgressStorage(ProgressStorage):
    """
    State storage in ``.status`` stanza with deep structure.

    The structure is this:

    .. code-block: yaml

        metadata: ...
        spec: ...
        status: ...
            kopf:
                progress:
                    handler1:
                        started: 2018-12-31T23:59:59,999999
                        stopped: 2018-01-01T12:34:56,789000
                        success: true
                    handler2:
                        started: 2018-12-31T23:59:59,999999
                        stopped: 2018-01-01T12:34:56,789000
                        failure: true
                        message: "Error message."
                    handler3:
                        started: 2018-12-31T23:59:59,999999
                        retries: 30
                    handler3/sub1:
                        started: 2018-12-31T23:59:59,999999
                        delayed: 2018-01-01T12:34:56,789000
                        retries: 10
                        message: "Not ready yet."
                    handler3/sub2:
                        started: 2018-12-31T23:59:59,999999
    """

    def __init__(
            self,
            *,
            name: str = 'kopf',
            field: dicts.FieldSpec = 'status.{name}.progress',
            touch_field: dicts.FieldSpec = 'status.{name}.dummy',
    ) -> None:
        super().__init__()
        self._name = name

        real_field = field.format(name=name) if isinstance(field, str) else field
        self._field = dicts.parse_field(real_field)

        real_field = touch_field.format(name=name) if isinstance(touch_field, str) else touch_field
        self._touch_field = dicts.parse_field(real_field)

    @property
    def field(self) -> dicts.FieldPath:
        return self._field

    @field.setter
    def field(self, field: dicts.FieldSpec) -> None:
        real_field = field.format(name=self._name) if isinstance(field, str) else field
        self._field = dicts.parse_field(real_field)

    @property
    def touch_field(self) -> dicts.FieldPath:
        return self._touch_field

    @touch_field.setter
    def touch_field(self, field: dicts.FieldSpec) -> None:
        real_field = field.format(name=self._name) if isinstance(field, str) else field
        self._touch_field = dicts.parse_field(real_field)

    def fetch(
            self,
            *,
            key: ids.HandlerId,
            body: bodies.Body,
    ) -> Optional[ProgressRecord]:
        container: Mapping[ids.HandlerId, ProgressRecord]
        container = dicts.resolve(body, self.field, {})
        return container.get(key, None)

    def store(
            self,
            *,
            key: ids.HandlerId,
            record: ProgressRecord,
            body: bodies.Body,
            patch: patches.Patch,
    ) -> None:
        # Nones are cleaned by K8s API itself.
        dicts.ensure(patch, self.field + (key,), record)

    def purge(
            self,
            *,
            key: ids.HandlerId,
            body: bodies.Body,
            patch: patches.Patch,
    ) -> None:
        absent = object()
        key_field = self.field + (key,)
        body_value = dicts.resolve(body, key_field, absent)
        patch_value = dicts.resolve(patch, key_field, absent)
        if body_value is not absent:
            dicts.ensure(patch, key_field, None)
        elif patch_value is not absent:
            dicts.remove(patch, key_field)

    def touch(
            self,
            *,
            body: bodies.Body,
            patch: patches.Patch,
            value: Optional[str],
    ) -> None:
        key_field = self.touch_field
        body_value = dicts.resolve(body, key_field, None)
        if body_value != value:  # also covers absent-vs-None cases.
            dicts.ensure(patch, key_field, value)

    def clear(self, *, essence: bodies.BodyEssence) -> bodies.BodyEssence:
        essence = super().clear(essence=essence)

        # Work around an issue with mypy not treating TypedDicts as MutableMappings.
        essence_dict = cast(dict[Any, Any], essence)
        dicts.remove(essence_dict, self.field)

        self.remove_empty_stanzas(essence)
        return essence


class MultiProgressStorage(ProgressStorage):

    def __init__(
            self,
            storages: Collection[ProgressStorage],
    ) -> None:
        super().__init__()
        self.storages = storages

    def fetch(
            self,
            *,
            key: ids.HandlerId,
            body: bodies.Body,
    ) -> Optional[ProgressRecord]:
        for storage in self.storages:
            content = storage.fetch(key=key, body=body)
            if content is not None:
                return content
        return None

    def store(
            self,
            *,
            key: ids.HandlerId,
            record: ProgressRecord,
            body: bodies.Body,
            patch: patches.Patch,
    ) -> None:
        for storage in self.storages:
            storage.store(key=key, record=record, body=body, patch=patch)

    def purge(
            self,
            *,
            key: ids.HandlerId,
            body: bodies.Body,
            patch: patches.Patch,
    ) -> None:
        for storage in self.storages:
            storage.purge(key=key, body=body, patch=patch)

    def touch(
            self,
            *,
            body: bodies.Body,
            patch: patches.Patch,
            value: Optional[str],
    ) -> None:
        for storage in self.storages:
            storage.touch(body=body, patch=patch, value=value)

    def clear(self, *, essence: bodies.BodyEssence) -> bodies.BodyEssence:
        for storage in self.storages:
            essence = storage.clear(essence=essence)
        return essence


class SmartProgressStorage(MultiProgressStorage):

    def __init__(
            self,
            *,
            name: str = 'kopf',
            field: dicts.FieldSpec = 'status.{name}.progress',
            touch_key: str = 'touch-dummy',  # NB: not dotted, but dashed
            touch_field: dicts.FieldSpec = 'status.{name}.dummy',
            prefix: str = 'kopf.zalando.org',
            v1: bool = True,  # will be switched to False a few releases later
            verbose: bool = False,
    ) -> None:
        super().__init__([
            AnnotationsProgressStorage(v1=v1, prefix=prefix, verbose=verbose, touch_key=touch_key),
            StatusProgressStorage(name=name, field=field, touch_field=touch_field),
        ])



================================================
FILE: kopf/_cogs/helpers/__init__.py
================================================
"""
General-purpose helpers not related to the framework itself
(neither to the reactor nor to the engines nor to the structs),
which are used to prepare and control the runtime environment.

These are things that should better be in the standard library
or in the dependencies.

Utilities do not depend on anything in the framework. For most cases,
they do not even implement any entities or behaviours of the domain
of K8s Operators, but rather some unrelated low-level patterns.

As a rule of thumb, helpers MUST be abstracted from the framework
to such an extent that they could be extracted as reusable libraries.
If they implement concepts of the framework, they are not "helpers"
(consider making them _kits, structs, engines, or the reactor parts).
"""



================================================
FILE: kopf/_cogs/helpers/hostnames.py
================================================
import ipaddress
import socket
from typing import Optional


def get_descriptive_hostname() -> str:
    """
    Look for non-numeric hostnames of the machine where the operator runs.

    The purpose is the host identification, not the actual host accessability.

    Similar to :func:`socket.getfqdn`, but IPv6 pseudo-hostnames are excluded --
    they are not helpful in identifying the actual host running the operator:
    e.g. "1.0.0...0.ip6.arpa".
    """
    try:
        hostname, aliases, ipaddrs = socket.gethostbyaddr(socket.gethostname())
    except OSError:
        pass
    else:
        ipv4: Optional[ipaddress.IPv4Address]
        ipv6: Optional[ipaddress.IPv6Address]
        parsed: list[tuple[str, Optional[ipaddress.IPv4Address], Optional[ipaddress.IPv6Address]]]
        parsed = []
        for name in [hostname] + list(aliases) + list(ipaddrs):
            try:
                ipv4 = ipaddress.IPv4Address(name)
            except ipaddress.AddressValueError:
                ipv4 = None
            try:
                ipv6 = ipaddress.IPv6Address(name)
            except ipaddress.AddressValueError:
                ipv6 = None
            parsed.append((name, ipv4, ipv6))

        # Dotted hostname (fqdn) is always better, unless it is an ARPA-name or an IP-address.
        for name, ipv4, ipv6 in parsed:
            if '.' in name and not name.endswith('.arpa') and not ipv4 and not ipv6:
                return remove_useless_suffixes(name)

        # Non-dotted hostname is fine too, unless it is ARPA-name/IP-address or a localhost.
        for name, ipv4, ipv6 in parsed:
            if name != 'localhost' and not name.endswith('.arpa') and not ipv4 and not ipv6:
                return remove_useless_suffixes(name)

    return remove_useless_suffixes(socket.gethostname())


def remove_useless_suffixes(hostname: str) -> str:
    suffixes = ['.local', '.localdomain']
    while any(hostname.endswith(suffix) for suffix in suffixes):
        for suffix in suffixes:
            hostname = hostname.removesuffix(suffix)
    return hostname



================================================
FILE: kopf/_cogs/helpers/loaders.py
================================================
"""
Module- and file-loading to trigger the handlers to be registered.

Since the framework is based on the decorators to register the handlers,
the files/modules with these handlers should be loaded first,
thus executing the decorators.

The files/modules to be loaded are usually specified on the command-line.
Currently, two loading modes are supported, both are equivalent to Python CLI:

* Plain files files (`kopf run file.py`).
* Importable modules (`kopf run -m pkg.mod`).

Multiple files/modules can be specified. They will be loaded in the order.
"""

import importlib
import importlib.abc
import importlib.util
import os.path
import sys
from collections.abc import Iterable
from typing import cast


def preload(
        paths: Iterable[str],
        modules: Iterable[str],
) -> None:
    """
    Ensure the handlers are registered by loading/importing the files/modules.
    """

    for idx, path in enumerate(paths):
        sys.path.insert(0, os.path.abspath(os.path.dirname(path)))
        name = f'__kopf_script_{idx}__{path}'  # same pseudo-name as '__main__'
        spec = importlib.util.spec_from_file_location(name, path)
        module = importlib.util.module_from_spec(spec) if spec is not None else None
        loader = cast(importlib.abc.Loader, spec.loader) if spec is not None else None
        if module is not None and loader is not None:
            sys.modules[name] = module
            loader.exec_module(module)
        else:
            raise ImportError(f"Failed loading {path}: no module or loader.")

    for name in modules:
        importlib.import_module(name)



================================================
FILE: kopf/_cogs/helpers/thirdparty.py
================================================
"""
Type definitions from optional 3rd-party libraries, e.g. pykube-ng & kubernetes.

This utility does all the trickery needed to import the libraries if possible,
or to skip them and make typing/runtime dummies for the rest of the codebase.
"""
import abc
from typing import Any, Optional


# Since client libraries are optional, support their objects only if they are installed.
# If not installed, use a dummy class to miss all isinstance() checks for that library.
class _dummy: pass


# Do these imports look excessive? ==> https://github.com/python/mypy/issues/10063
# TL;DR: Strictly `from...import...as...`, AND strictly same-named (`X as X`).
try:
    from pykube.objects import APIObject as APIObject
    PykubeObject = APIObject
except ImportError:
    PykubeObject = _dummy

try:
    from kubernetes.client import V1ObjectMeta as V1ObjectMeta, V1OwnerReference as V1OwnerReference
except ImportError:
    V1ObjectMeta = V1OwnerReference = None


# Kubernetes client does not have any common base classes, its code is fully generated.
# Only recognise classes from a specific module. Ignore all API/HTTP/auth-related tools.
class KubernetesModel(abc.ABC):
    @classmethod
    def __subclasshook__(cls, subcls: Any) -> Any:  # suppress types in this hack
        if cls is KubernetesModel:
            if any(C.__module__.startswith('kubernetes.client.models.') for C in subcls.__mro__):
                return True
        return NotImplemented

    @property
    def metadata(self) -> Optional[V1ObjectMeta]:
        raise NotImplementedError

    @metadata.setter
    def metadata(self, _: Optional[V1ObjectMeta]) -> None:
        raise NotImplementedError



================================================
FILE: kopf/_cogs/helpers/typedefs.py
================================================
"""
Rudimentary type [re-]definitions for cross-versioned Python & mypy.

The problem is that new mypy versions often bring type-sheds with StdLib types
defined as generics, while the old Python runtime (down to 3.9 & 3.10)
does not support the usual syntax.
Examples: asyncio.Task, asyncio.Future, logging.LoggerAdapter, and others.

This modules defines them in a most suitable and reusable way. Plus it adds
some common plain type definitions used across the codebase (for convenience).
"""
import logging
from typing import TYPE_CHECKING, Any, Union

if TYPE_CHECKING:
    LoggerAdapter = logging.LoggerAdapter[Any]
else:
    LoggerAdapter = logging.LoggerAdapter

# As publicly exposed: we only promise that it is based on one of the built-in loggable classes.
# Mind that these classes have multi-versioned stubs, so we avoid redefining the protocol ourselves.
Logger = Union[logging.Logger, LoggerAdapter]



================================================
FILE: kopf/_cogs/helpers/versions.py
================================================
"""
Detecting the framework's own version.

The codebase does not contain the version directly, as it would require
code changes on every release. Kopf's releases depend on tagging rather
than in-code version bumps (Kopf's authour believes that versions belong
to the versioning system, not to the codebase).

The version is determined only once at startup when the code is loaded.
"""
from typing import Optional

version: Optional[str] = None

try:
    import importlib.metadata
except ImportError:
    pass
else:
    try:
        name, *_ = __name__.split('.')  # usually "kopf", unless renamed/forked.
        version = importlib.metadata.version(name)
    except Exception:
        pass  # installed as an egg, from git, etc.



================================================
FILE: kopf/_cogs/structs/__init__.py
================================================
"""
All the functions to manipulate the resource fields, state changes, etc.

Grouped by the type of the fields and the purpose of the manipulation.

Used in the handling routines to check if there were significant changes at all
(i.e. not our own internal and system changes, like the uids, links, etc),
and to get the exact per-field diffs for the specific handler functions.

All the functions are purely data-manipulative and computational.
No external calls or any i/o activities are done here.
"""



================================================
FILE: kopf/_cogs/structs/bodies.py
================================================
"""
All the structures coming from/to the Kubernetes API.

The usage of these classes is spread over the codebase, so they are extracted
into a separate module of such type definitions.

For strict type-checking, they are detailed to the per-field level
(e.g. `TypedDict` instead of just ``Mapping[Any, Any]``) --
as used by the framework. The operators can use arbitrary fields at runtime,
which are not declared in the type definitions at type-checking time.

In case the operators are also type-checked, type casting can be used
(without `cast`, this code fails at type-checking, though works at runtime)::

    from typing import cast
    import kopf

    class MyMeta(kopf.Meta):
        unknownField: str

    @kopf.on.create('kopfexamples')
    def create_fn(*args, meta: kopf.Meta, **kwargs):
        meta = cast(MyMeta, meta)
        print(meta['unknownField'])

.. note::

    There is a strict separation of objects coming from/to the Kubernetes API
    and from (but not to) the users:

    The Kubernetes-originated objects are dicts or dict-like custom classes.
    The framework internally expect them to be such. Arbitrary 3rd-party
    classes are not supported and are not delivered to the handlers.

    The user-originated objects can be either one of the Kubernetes-originated
    framework-supported types (dicts/dict-like), or a 3rd-party class,
    such as from ``pykube-ng``, ``kubernetes`` client, etc -- as long as it is
    supported by the framework's object-processing functions.

    In the future, extra classes can be added for the user-originated objects
    and object-processing functions. The internal dicts will remain the same.
"""

from collections.abc import Mapping
from typing import Any, Literal, Optional, TypedDict, Union, cast

from kopf._cogs.structs import dicts, references

# Make sure every kwarg has a corresponding same-named type in the root package.
Labels = Mapping[str, str]
Annotations = Mapping[str, str]

#
# Everything marked "raw" is a plain unwrapped unprocessed data as JSON-decoded
# from Kubernetes API, usually as retrieved in watching or fetching API calls.
# "Input" is a parsed JSON as is, while "event" is an "input" without "errors".
# All non-used payload falls into `Any`, and is not type-checked.
#

# ``None`` is used for the listing, when the pseudo-watch-stream is simulated.
RawInputType = Literal[None, 'ADDED', 'MODIFIED', 'DELETED', 'ERROR']
RawEventType = Literal[None, 'ADDED', 'MODIFIED', 'DELETED']


class RawMeta(TypedDict, total=False):
    uid: str
    name: str
    namespace: str
    labels: Labels
    annotations: Annotations
    finalizers: list[str]
    resourceVersion: str
    deletionTimestamp: str
    creationTimestamp: str
    selfLink: str


class RawBody(TypedDict, total=False):
    apiVersion: str
    kind: str
    metadata: RawMeta
    spec: Mapping[str, Any]
    status: Mapping[str, Any]


# A special payload for type==ERROR (this is not a connection or client error).
class RawError(TypedDict, total=False):
    apiVersion: str     # usually: Literal['v1']
    kind: str           # usually: Literal['Status']
    metadata: Mapping[Any, Any]
    code: int
    reason: str
    status: str
    message: str


# As received from the stream before processing the errors and special cases.
class RawInput(TypedDict, total=True):
    type: RawInputType
    object: Union[RawBody, RawError]


# As passed to the framework after processing the errors and special cases.
class RawEvent(TypedDict, total=True):
    type: RawEventType
    object: RawBody


#
# Body/Meta essences only contain the fields relevant for object diff tracking.
# They are presented to the user as part of the diff's `old`/`new` fields & kwargs.
# Added for stricter type checking, to differentiate from the actual Body/Meta.
#


class MetaEssence(TypedDict, total=False):
    labels: Labels
    annotations: Annotations


class BodyEssence(TypedDict, total=False):
    metadata: MetaEssence
    spec: Mapping[str, Any]
    status: Mapping[str, Any]


#
# Enhanced dict-wrappers for easier typed access to well-known typed fields,
# with live view of updates and changes in the root body (for daemon's)
# Despite they are just MappingViews with no extensions, they are separated
# for stricter typing of arguments.
#


class Meta(dicts.MappingView[str, Any]):

    def __init__(self, __src: "Body") -> None:
        super().__init__(__src, 'metadata')
        self._labels: dicts.MappingView[str, str] = dicts.MappingView(self, 'labels')
        self._annotations: dicts.MappingView[str, str] = dicts.MappingView(self, 'annotations')

    @property
    def labels(self) -> Labels:
        return self._labels

    @property
    def annotations(self) -> Annotations:
        return self._annotations

    @property
    def uid(self) -> Optional[str]:
        return cast(Optional[str], self.get('uid'))

    @property
    def name(self) -> Optional[str]:
        return cast(Optional[str], self.get('name'))

    @property
    def namespace(self) -> references.Namespace:
        return cast(references.Namespace, self.get('namespace'))

    @property
    def creation_timestamp(self) -> Optional[str]:
        return cast(Optional[str], self.get('creationTimestamp'))

    @property
    def deletion_timestamp(self) -> Optional[str]:
        return cast(Optional[str], self.get('deletionTimestamp'))


class Spec(dicts.MappingView[str, Any]):
    def __init__(self, __src: "Body") -> None:
        super().__init__(__src, 'spec')


class Status(dicts.MappingView[str, Any]):
    def __init__(self, __src: "Body") -> None:
        super().__init__(__src, 'status')


class Body(dicts.ReplaceableMappingView[str, Any]):

    def __init__(self, __src: Mapping[str, Any]) -> None:
        super().__init__(__src)
        self._meta = Meta(self)
        self._spec = Spec(self)
        self._status = Status(self)

    @property
    def metadata(self) -> Meta:
        return self._meta

    @property
    def meta(self) -> Meta:
        return self._meta

    @property
    def spec(self) -> Spec:
        return self._spec

    @property
    def status(self) -> Status:
        return self._status


#
# Other API types, which are not body parts.
#

class ObjectReference(TypedDict, total=False):
    apiVersion: str
    kind: str
    namespace: Optional[str]
    name: str
    uid: str


class OwnerReference(TypedDict, total=False):
    controller: bool
    blockOwnerDeletion: bool
    apiVersion: str
    kind: str
    name: str
    uid: str


def build_object_reference(
        body: Body,
) -> ObjectReference:
    """
    Construct an object reference for the events.

    Keep in mind that some fields can be absent: e.g. ``namespace``
    for cluster resources, or e.g. ``apiVersion`` for ``kind: Node``, etc.
    """
    ref = dict(
        apiVersion=body.get('apiVersion'),
        kind=body.get('kind'),
        name=body.get('metadata', {}).get('name'),
        uid=body.get('metadata', {}).get('uid'),
        namespace=body.get('metadata', {}).get('namespace'),
    )
    return cast(ObjectReference, {key: val for key, val in ref.items() if val})


def build_owner_reference(
        body: Body,
        *,
        controller: Optional[bool] = True,
        block_owner_deletion: Optional[bool] = True,
) -> OwnerReference:
    """
    Construct an owner reference object for the parent-children relationships.

    The structure needed to link the children objects to the current object as a parent.
    See https://kubernetes.io/docs/concepts/workloads/controllers/garbage-collection/

    Keep in mind that some fields can be absent: e.g. ``namespace``
    for cluster resources, or e.g. ``apiVersion`` for ``kind: Node``, etc.
    """
    ref = dict(
        controller=controller,
        blockOwnerDeletion=block_owner_deletion,
        apiVersion=body.get('apiVersion'),
        kind=body.get('kind'),
        name=body.get('metadata', {}).get('name'),
        uid=body.get('metadata', {}).get('uid'),
    )
    return cast(OwnerReference, {key: val for key, val in ref.items() if val is not None})



================================================
FILE: kopf/_cogs/structs/credentials.py
================================================
"""
Authentication-related structures.

Kopf handles some rudimentary authentication directly, and exposes the ways
to implement custom authentication methods (via `on.login` handlers).

For that, a minimally sufficient data structure is introduced -- both
to bring all the credentials together in a structured and type-annotated way,
and to receive them from the operators' login-handlers with custom auth methods.

The "rudimentary" is defined as the information passed to the HTTP protocol
and TCP/SSL connection only, i.e. everything usable in a generic HTTP client,
and nothing more than that:

* TCP server host & port.
* SSL verification/ignorance flag.
* SSL certificate authority.
* SSL client certificate and its private key.
* HTTP ``Authorization: Basic username:password``.
* HTTP ``Authorization: Bearer token`` (or other schemes: Bearer, Digest, etc).
* URL's default namespace for the cases when this is implied.

.. seealso::
    :func:`authentication` and :mod:`piggybacking`.
"""
import asyncio
import collections
import dataclasses
import datetime
import random
from collections.abc import AsyncIterable, AsyncIterator, Mapping
from typing import Callable, NewType, Optional, TypeVar, cast

from kopf._cogs.aiokits import aiotoggles


class LoginError(Exception):
    """ Raised when the operator cannot login to the API. """


class AccessError(Exception):
    """ Raised when the operator cannot access the cluster API. """


@dataclasses.dataclass(frozen=True)
class ConnectionInfo:
    """
    A single endpoint with specific credentials and connection flags to use.
    """
    server: str  # e.g. "https://localhost:443"
    ca_path: Optional[str] = None
    ca_data: Optional[bytes] = None
    insecure: Optional[bool] = None
    username: Optional[str] = None
    password: Optional[str] = None
    scheme: Optional[str] = None  # RFC-7235/5.1: e.g. Bearer, Basic, Digest, etc.
    token: Optional[str] = None
    certificate_path: Optional[str] = None
    certificate_data: Optional[bytes] = None
    private_key_path: Optional[str] = None
    private_key_data: Optional[bytes] = None
    default_namespace: Optional[str] = None  # used for cluster objects' k8s-events.
    priority: int = 0
    expiration: Optional[datetime.datetime] = None  # TZ-aware or TZ-naive (implies UTC)


_T = TypeVar('_T', bound=object)

# Usually taken from the HandlerId (also a string), but semantically it is on its own.
VaultKey = NewType('VaultKey', str)


@dataclasses.dataclass
class VaultItem:
    """
    The actual item stored in the vault. It is never exposed externally.

    Used for proper garbage collection when the key is removed from the vault
    (to avoid orchestrating extra cache structures and keeping them in sync).

    The caches are populated by `Vault.extended` on-demand.
    """
    info: ConnectionInfo
    caches: Optional[dict[str, object]] = None


class Vault(AsyncIterable[tuple[VaultKey, ConnectionInfo]]):
    """
    A store for currently valid authentication methods.

    *Through we call it a vault to add a sense of security.*

    Normally, only one authentication method is used at a time in multiple
    methods and tasks (e.g. resource watching/patching, peering, etc.).

    Multiple methods to represent the same principal is an unusual case,
    but it is also possible as a side effect. Same for multiple distinct
    identities of a single operator.

    The credentials store is created once for an operator (a task),
    and is then used by multiple tasks running in parallel:

    * Consumed by the API client wrappers to authenticate in the API.
    * Reported by the API client wrappers if some of the credentials fail.
    * Populated by the authenticator background task when and if needed.

    .. seealso::
        :func:`auth.authenticated` and :func:`authentication`.
    """
    _current: dict[VaultKey, VaultItem]
    _invalid: dict[VaultKey, list[VaultItem]]

    def __init__(
            self,
            __src: Optional[Mapping[str, object]] = None,
    ) -> None:
        super().__init__()
        self._current = {}
        self._invalid = collections.defaultdict(list)
        self._next_expiration: Optional[datetime.datetime] = None

        if __src is not None:
            self._update_converted(__src)

        # Mark a pre-populated vault to be usable instantly,
        # or trigger the initial authentication for an empty vault.
        self._guard = asyncio.Condition()
        self._ready: bool = not self.is_empty()

    def __repr__(self) -> str:
        return f'<{self.__class__.__name__}: {self._current!r}>'

    def __bool__(self) -> bool:
        raise NotImplementedError("The vault should not be evaluated as bool.")

    async def __aiter__(
            self,
    ) -> AsyncIterator[tuple[VaultKey, ConnectionInfo]]:
        async for key, item in self._items():
            yield key, item.info

    async def extended(
            self,
            factory: Callable[[ConnectionInfo], _T],
            purpose: Optional[str] = None,
    ) -> AsyncIterator[tuple[VaultKey, ConnectionInfo, _T]]:
        """
        Iterate the connection info items with their cached object.

        The cached objects are identified by the purpose (an arbitrary string).
        Multiple types of objects can be cached under different names.

        The factory is a one-argument function of a `ConnectionInfo`,
        that returns the object to be cached for this connection info.
        It is called only once per item and purpose.
        """
        purpose = purpose if purpose is not None else repr(factory)
        async for key, item in self._items():
            if item.caches is None:  # quick-check with no locking overhead.
                async with self._guard:
                    if item.caches is None:  # securely synchronised check.
                        item.caches = {}
            if purpose not in item.caches:  # quick-check with no locking overhead.
                async with self._guard:
                    if purpose not in item.caches:  # securely synchronised check.
                        item.caches[purpose] = factory(item.info)
            yield key, item.info, cast(_T, item.caches[purpose])

    async def _items(
            self,
    ) -> AsyncIterator[tuple[VaultKey, VaultItem]]:
        """
        Yield the raw items as stored in the vault in random order.

        The items are yielded until either all of them are depleted,
        or until the yielded one does not fail (no `.invalidate` call made).
        Restart on every re-authentication (if new items are added).
        """

        # Yield the connection infos until either all of them are depleted,
        # or until the yielded one does not fail (no `.invalidate` call made).
        # Restart on every re-authentication (if new items are added).
        while True:

            async with self._guard:

                # Whether on the 1st run, or during the active re-authentication,
                # ensure that the items are ready before yielding them.
                await self._guard.wait_for(lambda: self._ready)

                # Check for expiration strictly after a possible re-authentication.
                # This might cause another re-authentication if the credentials are pre-expired.
                await self._expire()

                # Select the items to yield and let it (i.e. a consumer task) work.
                yielded_key, yielded_item = self.select()

            # Yield strictly outside of locks/conditions. The vault must be free for invalidations.
            yield yielded_key, yielded_item

            # If the yielded item has been invalidated, assume that this item has failed.
            # Otherwise (the item is in the list), it has succeeded -- we are done iterating.
            # Note: checked by identity, in case a similar item is re-added as a different object.
            async with self._guard:
                if yielded_key in self._current and self._current[yielded_key] is yielded_item:
                    break

    def select(self) -> tuple[VaultKey, VaultItem]:
        """
        Select the next item (not the info!) to try (and do so infinitely).

        .. warning::
            This method is not async/await-safe: if the data change on the go,
            it can lead to improper items returned.
        """
        if not self._current:
            raise LoginError("Ran out of valid credentials. Consider installing "
                             "an API client library or adding a login handler. See more: "
                             "https://kopf.readthedocs.io/en/stable/authentication/")
        prioritised: dict[int, list[tuple[VaultKey, VaultItem]]]
        prioritised = collections.defaultdict(list)
        for key, item in self._current.items():
            prioritised[item.info.priority].append((key, item))
        top_priority = max(list(prioritised.keys()))
        key, item = random.choice(prioritised[top_priority])
        return key, item

    async def expire(self) -> None:  # unused, but declared public (deprecate)
        """
        Discard the expired credentials, and re-authenticate as needed.

        Unlike invalidation, the expired credentials are not remembered
        and not blocked from reappearing.
        """
        # Quick & lockless for speed: it is done on every API call, we have no time for locks.
        now = datetime.datetime.now(datetime.timezone.utc)
        if self._next_expiration is not None and now >= self._next_expiration:
            async with self._guard:
                await self._expire()

    async def _expire(self) -> None:
        """
        Discard the expired credentials, and re-authenticate as needed.

        Unlike invalidation, the expired credentials are not remembered
        and not blocked from reappearing.
        """
        now = datetime.datetime.now(datetime.timezone.utc)

        # Avoid waiting for re-auth afterwards if there is nothing to expire or change.
        expired = False
        if self._next_expiration is not None and now >= self._next_expiration:
            for key, item in list(self._current.items()):
                expiration = item.info.expiration
                if expiration is not None:
                    if expiration.tzinfo is None:
                        expiration = expiration.replace(tzinfo=datetime.timezone.utc)
                    if now >= expiration:
                        await self._flush_caches(item)
                        del self._current[key]
                        expired = True
            self._update_expiration()

        # Initiate a re-authentication activity, and block until it is finished.
        if expired and not self._current:  # i.e. nothing is left at all
            self._ready = False
            self._guard.notify_all()
            await self._guard.wait_for(lambda: self._ready)

    async def invalidate(
            self,
            key: VaultKey,
            info: ConnectionInfo,
            *,
            exc: Optional[Exception] = None,
    ) -> None:
        """
        Discard the specified credentials, and re-authenticate as needed.

        Multiple calls can be made for a single authenticator and credentials,
        if used for multiple requests at the same time (a common case).
        All of them will be blocked the same way, until one and only one
        re-authentication happens in a background task. They will be
        unblocked at the same instant once the new credentials are ready.

        If the re-authentication fails in the background task, this method
        re-raises the original exception (most likely a HTTP 401 error),
        and lets the client tasks to fail in their own stack.
        The background task continues to run and tries to re-authenticate
        on the next API calls until cancelled due to the operator exit.
        """
        # Exclude the failed connection items from the list of available ones.
        # But keep a short history of invalid items, so that they are not re-added.
        # The history size is estimated by the number of parallel streams trying to re-auth at once.
        async with self._guard:
            # Note: not "==", but "is". If not the same, then it was invalidated by other consumers,
            # the new current credentials is something new to use (maybe equal to the old one).
            if key in self._current and self._current[key].info is info:
                await self._flush_caches(self._current[key])
                self._invalid[key] = self._invalid[key][-2:] + [self._current[key]]
                del self._current[key]
                self._update_expiration()

            # Initiate a re-authentication activity, and block until it is finished.
            if not self._current:  # i.e. nothing is left at all
                self._ready = False
                self._guard.notify_all()
                await self._guard.wait_for(lambda: self._ready)

            # If the re-auth has failed, re-raise the original exception in the current stack.
            # If the original exception is unknown, raise normally on the next iteration's yield.
            # The error here is optional -- for better stack traces of the original exception `exc`.
            # Keep in mind, this routine is called in parallel from many tasks for the same keys.
            if not self._current:
                if exc is not None:
                    raise LoginError("Ran out of valid credentials. Consider installing "
                                     "an API client library or adding a login handler. See more: "
                                     "https://kopf.readthedocs.io/en/stable/authentication/") from exc

    async def populate(
            self,
            __src: Mapping[str, object],
    ) -> None:
        """
        Add newly retrieved credentials.

        Used by :func:`authentication` to add newly retrieved credentials
        from the authentication activity handlers. Some of the credentials
        can be duplicates of the existing ones -- only one of them is used then.
        """
        async with self._guard:

            # Remember the new credentials or replace the old ones. If we already see that the item
            # is invalid (as seen in our short per-key history), we keep it as such -- this prevents
            # repeatedly invalid credentials from causing infinite re-authentication again & again.
            self._update_converted(__src)

            # Notify the consuming tasks (API clients) that new credentials are ready to be used.
            # Those tasks can be blocked in `vault.invalidate()` if there are no credentials left.
            self._ready = True
            self._guard.notify_all()

    def is_empty(self) -> bool:
        now = datetime.datetime.now(datetime.timezone.utc)
        expirations = [
            dt if dt is None or dt.tzinfo is not None else dt.replace(tzinfo=datetime.timezone.utc)
            for dt in (item.info.expiration for item in self._current.values())
        ]
        return all(dt is not None and now >= dt for dt in expirations)  # i.e. expired

    async def wait_for_readiness(self) -> None:
        async with self._guard:
            await self._guard.wait_for(lambda: self._ready)

    async def wait_for_emptiness(self) -> None:
        async with self._guard:
            await self._guard.wait_for(lambda: not self._ready)

    async def close(self) -> None:
        """
        Finalize all the cached objects when the operator is ending.
        """
        async with self._guard:
            for key in self._current:
                await self._flush_caches(self._current[key])

    async def _flush_caches(
            self,
            item: VaultItem,
    ) -> None:
        """
        Call the finalizers and garbage-collect the cached objects.

        Mainly used to garbage-collect aiohttp sessions and its derivatives
        when the connection info items are removed from the vault -- so that
        the sessions/connectors would not complain that they were not close.

        Built-in garbage-collection is not sufficient, as it is synchronous,
        and cannot call the async coroutines like `aiohttp.ClientSession.close`.

        .. note::
            Currently, we assume the ``close()`` method only (both sync/async).
            There is no need to generalise to customizable finalizer callbacks.
            This can change in the future.
        """

        # Close the closable objects.
        if item.caches:
            for obj in item.caches.values():
                if hasattr(obj, 'close'):
                    if asyncio.iscoroutinefunction(getattr(obj, 'close')):
                        await getattr(obj, 'close')()
                    else:
                        getattr(obj, 'close')()

        # Garbage-collect other resources (e.g. files, memory, etc).
        item.caches = None

    def _update_converted(
            self,
            __src: Mapping[str, object],
    ) -> None:
        for key, info in __src.items():
            key = VaultKey(str(key))
            if not isinstance(info, ConnectionInfo):
                raise ValueError("Only ConnectionInfo instances are currently accepted.")
            if info not in [data.info for data in self._invalid[key]]:
                self._current[key] = VaultItem(info=info)
        self._update_expiration()

    def _update_expiration(self) -> None:
        expirations = [
            dt if dt.tzinfo is not None else dt.replace(tzinfo=datetime.timezone.utc)
            for dt in (item.info.expiration for item in self._current.values())
            if dt is not None
        ]
        self._next_expiration = min(expirations) if expirations else None



================================================
FILE: kopf/_cogs/structs/dicts.py
================================================
"""
Some basic dicts and field-in-a-dict manipulation helpers.
"""
import collections.abc
import enum
from collections.abc import Iterable, Iterator, Mapping, MutableMapping
from typing import Any, Callable, Generic, Optional, TypeVar, Union

from kopf._cogs.helpers import thirdparty

FieldPath = tuple[str, ...]
FieldSpec = Union[None, str, FieldPath, list[str]]

_T = TypeVar('_T')
_K = TypeVar('_K', bound=str)  # int & bool keys are possible but discouraged
_V = TypeVar('_V')


class _UNSET(enum.Enum):
    token = enum.auto()


def parse_field(
        field: FieldSpec,
) -> FieldPath:
    """
    Convert any field into a tuple of nested sub-fields.

    Supported notations:

    * ``None`` (for root of a dict).
    * ``"field.subfield"``
    * ``("field", "subfield")``
    * ``["field", "subfield"]``
    """
    if field is None:
        return ()
    elif isinstance(field, str):
        return tuple(field.split('.'))
    elif isinstance(field, (list, tuple)):
        return tuple(field)
    else:
        raise ValueError(f"Field must be either a str, or a list/tuple. Got {field!r}")


def resolve_obj(
        d: Union[None, thirdparty.KubernetesModel, Mapping[Any, Any]],
        field: FieldSpec,
        default: Union[_T, _UNSET] = _UNSET.token,
) -> Union[Any, _T]:
    """
    Mirrors `resolve`, but for a nested mix of dict keys & object attributes.

    While `resolve` is used mostly in certain dictionaries (e.g. diffs),
    this function is used for walking over 3rd-party API objects & models
    with nested structures. The algorithm is essentially the same.
    """
    path = parse_field(field)
    try:
        result = d
        for key in path:
            if isinstance(result, collections.abc.Mapping):
                result = result[key]
            elif isinstance(result, thirdparty.KubernetesModel):
                attrmap: Mapping[str, str] = getattr(result, 'attribute_map', {})
                attrs = [attr for attr, schema_key in attrmap.items() if schema_key == key]
                key = attrs[0] if attrs else key
                result = getattr(result, key)
            elif not isinstance(result, (tuple, list, set, frozenset, str, bytes)):
                result = getattr(result, key)
            elif not isinstance(default, _UNSET):
                return default
            else:
                raise TypeError(f"The structure has no field {key!r}: {result!r}")
        return result
    except (AttributeError, KeyError):
        if not isinstance(default, _UNSET):
            return default
        raise


def resolve(
        d: Optional[Mapping[Any, Any]],
        field: FieldSpec,
        default: Union[_T, _UNSET] = _UNSET.token,
) -> Union[Any, _T]:
    """
    Retrieve a nested sub-field from a dict.

    If ``default`` is provided, then all non-existent and non-mapping values
    are assumed to be empty dictionaries, and ``default`` is returned.

    Otherwise (with no default), attempts to get the inexistent keys will
    raise either a ``TypeError`` or ``KeyError``:

    * ``KeyError`` for actual absence of keys while the structures are correct.
    * ``TypeError`` for attempting to get a key for a non-dictionary:
      e.g. ``None['key']``, ``"string"['key']``, ``123['key']``, etc.

    This essentially means the "safe" mode of resolution, where the obvious
    errors are silenced, as we cannot dive deep into non-dictionary values.

    Silencing errors goes against The Zen of Python, but we need this for K8s:
    if the resources are corrupted externally (e.g. by editing manually),
    we ignore that corrupted data as if there is no data at all,
    and continue running instead of unrecoverably failing the processing.

    Examples of data that can be corrupted:

    * diff-base and progress in the status stanza (``status.kopf.progress``);
      this does not apply to annotations, as their structure is ensured by K8s.
    * field-specific handers for fields (e.g. ``spec.struct.field``)
      when the parent structure (``spec.struct`` or even ``spec``)
      is not a mapping or absent.
    """
    path = parse_field(field)
    try:
        result = d
        for key in path:
            if isinstance(result, collections.abc.Mapping):
                result = result[key]
            elif not isinstance(default, _UNSET):
                return default
            else:
                raise TypeError(f"The structure is not a dict with field {key!r}: {result!r}")
        return result
    except KeyError:
        if not isinstance(default, _UNSET):
            return default
        raise


def ensure(
        d: MutableMapping[Any, Any],
        field: FieldSpec,
        value: Any,
) -> None:
    """
    Force-set a nested sub-field in a dict.

    If some levels of parents are missing, they are created as empty dicts
    (this what makes it "ensuring", not just "setting").
    """
    result = d
    path = parse_field(field)
    if not path:
        raise ValueError("Setting a root of a dict is impossible. Provide the specific fields.")
    for key in path[:-1]:
        try:
            result = result[key]
        except KeyError:
            result = result.setdefault(key, {})
    result[path[-1]] = value


def remove(
        d: MutableMapping[Any, Any],
        field: FieldSpec,
) -> None:
    """
    Remove a nested sub-field from a dict, and all empty parents (optionally).

    All intermediate parents that become empty after the removal are also
    removed, making the whole original dict cleaner. For single-field removal,
    use a built-in ``del d[key]`` operation.

    If the target key is absent already, or any of the intermediate parents
    is absent (which implies that the target key is also absent), no error
    is raised, since the goal of deletion is achieved. The empty parents are
    anyway removed.
    """
    path = parse_field(field)
    if not path:
        raise ValueError("Removing a root of a dict is impossible. Provide a specific field.")

    elif len(path) == 1:
        try:
            del d[path[0]]
        except KeyError:
            pass  # already absent

    else:
        try:
            # Recursion is the easiest way to implement it, assuming the bodies/patches are shallow.
            remove(d[path[0]], path[1:])
        except KeyError:
            pass  # already absent
        else:
            # Clean the parent dict if it has become empty due to deletion of the only sub-key.
            # Upper parents will be handled by upper recursion functions.
            if d[path[0]] == {}:  # but not None, and not False, etc.
                del d[path[0]]


def cherrypick(
        src: Mapping[Any, Any],
        dst: MutableMapping[Any, Any],
        fields: Optional[Iterable[FieldSpec]],
        picker: Optional[Callable[[_T], _T]] = None,
) -> None:
    """
    Copy all specified fields between dicts (from src to dst).
    """
    picker = picker if picker is not None else lambda x: x
    fields = fields if fields is not None else []
    for field in fields:
        try:
            ensure(dst, field, picker(resolve(src, field)))
        except KeyError:
            pass  # absent in the source, nothing to merge


def walk(
        objs: Union[_T,
                    Iterable[_T],
                    Iterable[Union[_T,
                                   Iterable[_T]]]],
        *,
        nested: Optional[Iterable[FieldSpec]] = None,
) -> Iterator[_T]:
    """
    Iterate over objects, flattening the lists/tuples/iterables recursively.

    In plain English, the source is either an object, or a list/tuple/iterable
    of objects with any level of nesting. The dicts/mappings are excluded,
    despite they are iterables too, as they are treated as objects themselves.

    For the output, it yields all the objects in a flat iterable suitable for::

        for obj in walk(objs):
            pass

    The type declares only 2-level nesting, but this is done only
    for type-checker's limitations. The actual nesting can be infinite.
    It is highly unlikely that there will be anything deeper than one level.
    """
    if objs is None:
        pass
    elif isinstance(objs, thirdparty.PykubeObject):
        # Pykube is yielded as an underlying dict, never as its own class.
        yield from walk(objs.obj, nested=nested)
    elif isinstance(objs, thirdparty.KubernetesModel):
        yield objs  # type: ignore
        for subfield in (nested if nested is not None else []):
            try:
                yield resolve_obj(objs, parse_field(subfield))
            except (AttributeError, KeyError):
                pass  # do not dive deep into non-existent fields or non-dicts
    elif isinstance(objs, collections.abc.Mapping):
        yield objs  # type: ignore
        for subfield in (nested if nested is not None else []):
            try:
                yield resolve(objs, parse_field(subfield))
            except KeyError:
                pass  # avoid diving into non-dicts, ignore them
    elif isinstance(objs, collections.abc.Iterable):
        for obj in objs:
            yield from walk(obj, nested=nested)
    else:
        yield objs  # NB: not a mapping or a known type => no nested sub-fields.


class MappingView(Mapping[_K, _V], Generic[_K, _V]):
    """
    A lazy resolver for the "on-demand" dict keys.

    This is needed to have :kwarg:`spec`, :kwarg:`status`, and other fields
    to be *assumed* as dicts, even if they are actually not present.
    And to prevent their implicit creation with ``.setdefault('spec', {})``,
    which produces unwanted side-effects (actually adds this field).

    >>> body = {}
    >>> spec = MappingView(body, 'spec')
    >>> spec.get('field', 'default')
    ... 'default'
    >>> body['spec'] = {'field': 'value'}
    >>> spec.get('field', 'default')
    ... 'value'
    """
    _src: Mapping[_K, _V]

    def __init__(self, __src: Mapping[Any, Any], __path: FieldSpec = None) -> None:
        super().__init__()
        self._src = __src
        self._path = parse_field(__path)

    def __repr__(self) -> str:
        return repr(dict(self))

    def __len__(self) -> int:
        return len(resolve(self._src, self._path, {}))

    def __iter__(self) -> Iterator[Any]:
        return iter(resolve(self._src, self._path, {}))

    def __getitem__(self, item: _K) -> _V:
        return resolve(self._src, self._path + (item,))


class MutableMappingView(MappingView[_K, _V], MutableMapping[_K, _V], Generic[_K, _V]):
    """
    A mapping view with values stored and sub-dicts auto-created.

    >>> patch = {}
    >>> status = MutableMappingView(patch, 'status')
    >>> status.get('field', 'default')
    ... 'default'
    >>> patch
    ... {}
    >>> status['field'] = 'value'
    >>> patch
    ... {'status': {'field': 'value'}}
    >>> status.get('field', 'default')
    ... 'value'
    """
    _src: MutableMapping[_K, _V]  # typing clarification

    def __delitem__(self, item: _K) -> None:
        d = resolve(self._src, self._path)
        del d[item]

    def __setitem__(self, item: _K, value: _V) -> None:
        ensure(self._src, self._path + (item,), value)


class ReplaceableMappingView(MappingView[_K, _V], Generic[_K, _V]):
    """
    A mapping view where the whole source can be replaced atomically.

    All derived mapping views that use this mapping view as their source will
    immediately notice the change.

    The method names are intentionally long and multi-word -- to not have
    potential collisions with regular expected attributes/properties.

    >>> body = ReplaceableMappingView()
    >>> spec = MappingView(body, 'spec')
    >>> spec.get('field', 'default')
    ... 'default'
    >>> body._replace_with({'spec': {'field': 'value'}})
    >>> spec.get('field', 'default')
    ... 'value'
    """

    def _replace_from(self, __src: MappingView[_K, _V]) -> None:
        self._src = __src._src

    def _replace_with(self, __src: Mapping[_K, _V]) -> None:
        self._src = __src



================================================
FILE: kopf/_cogs/structs/diffs.py
================================================
"""
All the functions to calculate the diffs of the dicts.
"""
import collections.abc
import enum
from collections.abc import Iterable, Iterator, Sequence
from typing import Any, NamedTuple, Union, overload

from kopf._cogs.structs import dicts


class DiffScope(enum.Flag):
    """
    Scope limitation for the diffs' fields to be noticed or ignored.

    In the full-scoped diff (the default), both objects (diff source & target)
    are treated equally important, and the diff is calculated from the left
    to the right one for all fields.

    In the left-scoped diff, only the left object (diff source) is considered
    important, and only the differences for the fields found in the left object
    (source) are checked. Extra fields in the right object (target) are ignored.

    In the right-scoped diff, only the fields in the right object (diff target)
    are scanned. Extra fields in the left object (diff source) are ignored.
    """
    RIGHT = enum.auto()
    LEFT = enum.auto()
    FULL = LEFT | RIGHT


class DiffOperation(str, enum.Enum):
    ADD = 'add'
    CHANGE = 'change'
    REMOVE = 'remove'

    def __str__(self) -> str:
        return str(self.value)

    def __repr__(self) -> str:
        return repr(self.value)


class DiffItem(NamedTuple):
    operation: DiffOperation
    field: dicts.FieldPath
    old: Any
    new: Any

    def __repr__(self) -> str:
        return repr(tuple(self))

    def __eq__(self, other: object) -> bool:
        if isinstance(other, collections.abc.Sequence):
            return tuple(self) == tuple(other)
        else:
            return NotImplemented

    def __ne__(self, other: object) -> bool:
        if isinstance(other, collections.abc.Sequence):
            return tuple(self) != tuple(other)
        else:
            return NotImplemented

    @property
    def op(self) -> DiffOperation:
        return self.operation


class Diff(Sequence[DiffItem]):

    def __init__(self, __items: Iterable[DiffItem]):
        super().__init__()
        self._items = tuple(DiffItem(*item) for item in __items)

    def __hash__(self) -> int:
        # Hashes mark diffs as immutable to be usable as dataclasses' defaults in Python 3.11.
        return hash(self._items)

    def __repr__(self) -> str:
        return repr(self._items)

    def __len__(self) -> int:
        return len(self._items)

    def __iter__(self) -> Iterator[DiffItem]:
        return iter(self._items)

    @overload
    def __getitem__(self, i: int) -> DiffItem: ...

    @overload
    def __getitem__(self, s: slice) -> Sequence[DiffItem]: ...

    def __getitem__(self, item: Union[int, slice]) -> Union[DiffItem, Sequence[DiffItem]]:
        return self._items[item]

    def __eq__(self, other: object) -> bool:
        if isinstance(other, collections.abc.Sequence):
            return tuple(self) == tuple(other)
        else:
            return NotImplemented

    def __ne__(self, other: object) -> bool:
        if isinstance(other, collections.abc.Sequence):
            return tuple(self) != tuple(other)
        else:
            return NotImplemented


def reduce_iter(
        d: Diff,
        path: dicts.FieldPath,
) -> Iterator[DiffItem]:
    for op, field, old, new in d:

        # As-is diff (i.e. a root field).
        if not path:
            yield DiffItem(op, tuple(field), old, new)

        # The diff-field is longer than the path: get "spec.struct" when "spec.struct.field" is set.
        # Retranslate the diff with the field prefix shrinked.
        elif tuple(field[:len(path)]) == tuple(path):
            yield DiffItem(op, tuple(field[len(path):]), old, new)

        # The diff-field is shorter than the path: get "spec.struct" when "spec={...}" is added.
        # Generate a new diff, with new ops, for the resolved sub-field.
        elif tuple(field) == tuple(path[:len(field)]):
            tail = path[len(field):]
            old_tail = dicts.resolve(old, tail, default=None)
            new_tail = dicts.resolve(new, tail, default=None)
            yield from diff_iter(old_tail, new_tail)


def reduce(
        d: Diff,
        path: dicts.FieldPath,
) -> Diff:
    return Diff(reduce_iter(d, path))


def diff_iter(
        a: Any,
        b: Any,
        path: dicts.FieldPath = (),
        *,
        scope: DiffScope = DiffScope.FULL,
) -> Iterator[DiffItem]:
    """
    Calculate the diff between two dicts.

    Yields the tuple of form ``(op, path, old, new)``,
    where ``op`` is either ``"add"``/``"change"``/``"remove"``,
    ``path`` is a tuple with the field names (empty tuple means root),
    and the ``old`` & ``new`` values (`None` for addition/removal).

    List values are treated as a whole, and not recursed into.
    Therefore, an addition/removal of a list item is considered
    as a change of the whole value.

    If the deep diff for lists/sets is needed, see the libraries:

    * https://dictdiffer.readthedocs.io/en/latest/
    * https://github.com/seperman/deepdiff
    * https://python-json-patch.readthedocs.io/en/latest/tutorial.html
    """
    if a == b:  # incl. cases when both are None
        pass
    elif a is None:
        yield DiffItem(DiffOperation.ADD, path, a, b)
    elif b is None:
        yield DiffItem(DiffOperation.REMOVE, path, a, b)
    elif isinstance(a, collections.abc.Mapping) and isinstance(b, collections.abc.Mapping):
        a_keys = frozenset(a.keys())
        b_keys = frozenset(b.keys())
        for key in (b_keys - a_keys if DiffScope.RIGHT in scope else ()):
            yield from diff_iter(None, b[key], path=path+(key,), scope=scope)
        for key in (a_keys - b_keys if DiffScope.LEFT in scope else ()):
            yield from diff_iter(a[key], None, path=path+(key,), scope=scope)
        for key in (a_keys & b_keys):
            yield from diff_iter(a[key], b[key], path=path+(key,), scope=scope)
    else:
        yield DiffItem(DiffOperation.CHANGE, path, a, b)


def diff(
        a: Any,
        b: Any,
        path: dicts.FieldPath = (),
        *,
        scope: DiffScope = DiffScope.FULL,
) -> Diff:
    """
    Same as `diff`, but returns the whole tuple instead of iterator.
    """
    return Diff(diff_iter(a, b, path=path, scope=scope))


EMPTY = diff(None, None)



================================================
FILE: kopf/_cogs/structs/ephemera.py
================================================
from collections.abc import Collection, Mapping
from typing import Any, Generic, NewType, TypeVar

# For users, memos are exposed as `Any`, though usually used with `kopf.Memo`.
# However, the framework cannot rely on any methods/properties of it, so it is
# declared as a nearly empty `object` for stricter internal type-checking.
AnyMemo = NewType('AnyMemo', object)


class Memo(dict[Any, Any]):
    """
    A container to hold arbitrary keys-values assigned by operator developers.

    It is used in the :kwarg:`memo` kwarg to all resource handlers, isolated
    per individual resource object (not the resource kind).

    The values can be accessed either as dictionary keys (the memo is a ``dict``
    under the hood) or as object attributes (except for methods of ``dict``).

    See more in :doc:`/memos`.

    >>> memo = Memo()

    >>> memo.f1 = 100
    >>> memo['f1']
    ... 100

    >>> memo['f2'] = 200
    >>> memo.f2
    ... 200

    >>> set(memo.keys())
    ... {'f1', 'f2'}
    """

    def __setattr__(self, key: str, value: Any) -> None:
        self[key] = value

    def __delattr__(self, key: str) -> None:
        try:
            del self[key]
        except KeyError as e:
            raise AttributeError(str(e))

    def __getattr__(self, key: str) -> Any:
        try:
            return self[key]
        except KeyError as e:
            raise AttributeError(str(e))


_K = TypeVar('_K')
_V = TypeVar('_V')


class Store(Collection[_V], Generic[_V]):
    """
    A collection of all values under a single unique index key.

    Multiple objects can yield the same keys, so all their values
    are accumulated into collections. When an object is deleted
    or stops matching the filters, all associated values are discarded.

    The order of values is not guaranteed.

    The values are not deduplicated, so duplicates are possible if multiple
    objects return the same values from their indexing functions.

    .. note::
        This class is only an abstract interface of an indexed store.
        The actual implementation is in `.indexing.Store`.

    .. seealso:
        :doc:`/indexing`.
    """


class Index(Mapping[_K, Store[_V]], Generic[_K, _V]):
    """
    A mapping of index keys to collections of values indexed under those keys.

    A single index is identified by a handler id and is populated by values
    usually from a single indexing function (the ``@kopf.index()`` decorator).

    .. note::
        This class is only an abstract interface of an index.
        The actual implementation is in `.indexing.Index`.

    .. seealso:
        :doc:`/indexing`.
    """


# Only an abstract interface. Implementated in `~indexing.Indices`.
Indices = Mapping[str, Index[Any, Any]]



================================================
FILE: kopf/_cogs/structs/finalizers.py
================================================
"""
All the functions to manipulate the object finalization and deletion.

Finalizers are used to block the actual deletion until the finalizers
are removed, meaning that the operator has done all its duties
to "release" the object (e.g. cleanups; delete-handlers in our case).
"""
from kopf._cogs.structs import bodies, patches


def is_deletion_ongoing(
        body: bodies.Body,
) -> bool:
    return body.get('metadata', {}).get('deletionTimestamp', None) is not None


def is_deletion_blocked(
        body: bodies.Body,
        finalizer: str,
) -> bool:
    finalizers = body.get('metadata', {}).get('finalizers', [])
    return finalizer in finalizers


def block_deletion(
        *,
        body: bodies.Body,
        patch: patches.Patch,
        finalizer: str,
) -> None:
    if not is_deletion_blocked(body=body, finalizer=finalizer):
        finalizers = body.get('metadata', {}).get('finalizers', [])
        patch.setdefault('metadata', {}).setdefault('finalizers', list(finalizers))
        patch['metadata']['finalizers'].append(finalizer)


def allow_deletion(
        *,
        body: bodies.Body,
        patch: patches.Patch,
        finalizer: str,
) -> None:
    if is_deletion_blocked(body=body, finalizer=finalizer):
        finalizers = body.get('metadata', {}).get('finalizers', [])
        patch.setdefault('metadata', {}).setdefault('finalizers', list(finalizers))
        if finalizer in patch['metadata']['finalizers']:
            patch['metadata']['finalizers'].remove(finalizer)



================================================
FILE: kopf/_cogs/structs/ids.py
================================================
from typing import NewType

# Strings are taken from the users, but then tainted as this type for stricter type-checking:
# to prevent usage of some other strings (e.g. operator id) as the handlers ids.
# It is so much ubiquitous that it deserves its own module to avoid circular dependencies.
HandlerId = NewType('HandlerId', str)



================================================
FILE: kopf/_cogs/structs/patches.py
================================================
"""
All the structures needed for Kubernetes patching.

Currently, it is implemented via a JSON merge-patch (RFC 7386),
i.e. a simple dictionary with field overrides, and ``None`` for field deletions.

In the future, it can be extended to a standalone object, which exposes
a dict-like behaviour, and remembers the changes in order of their execution,
and then generates the JSON patch (RFC 6902).
"""
import collections.abc
from typing import Any, Literal, Optional, TypedDict

from kopf._cogs.structs import bodies, dicts

JSONPatchOp = Literal["add", "replace", "remove"]


def _escaped_path(keys: list[str]) -> str:
    """Provides an appropriately escaped path for JSON Patches.

    See https://datatracker.ietf.org/doc/html/rfc6901#section-3 for more details.
    """
    return '/'.join(map(lambda key: key.replace('~', '~0').replace('/', '~1'), keys))


class JSONPatchItem(TypedDict, total=False):
    op: JSONPatchOp
    path: str
    value: Optional[Any]


JSONPatch = list[JSONPatchItem]


class MetaPatch(dicts.MutableMappingView[str, Any]):
    _labels: dicts.MutableMappingView[str, Optional[str]]
    _annotations: dicts.MutableMappingView[str, Optional[str]]

    def __init__(self, __src: "Patch") -> None:
        super().__init__(__src, 'metadata')
        self._labels = dicts.MutableMappingView(self, 'labels')
        self._annotations = dicts.MutableMappingView(self, 'annotations')

    @property
    def labels(self) -> dicts.MutableMappingView[str, Optional[str]]:
        return self._labels

    @property
    def annotations(self) -> dicts.MutableMappingView[str, Optional[str]]:
        return self._annotations


class SpecPatch(dicts.MutableMappingView[str, Any]):
    def __init__(self, __src: "Patch") -> None:
        super().__init__(__src, 'spec')


class StatusPatch(dicts.MutableMappingView[str, Any]):
    def __init__(self, __src: "Patch") -> None:
        super().__init__(__src, 'status')


# Event-handling structures, used internally in the framework and handlers only.
class Patch(dict[str, Any]):

    def __init__(
        self,
        __src: Optional[collections.abc.MutableMapping[str, Any]] = None,
        body: Optional[bodies.RawBody] = None
    ) -> None:
        super().__init__(__src or {})
        self._meta = MetaPatch(self)
        self._spec = SpecPatch(self)
        self._status = StatusPatch(self)
        self._original = body

    @property
    def metadata(self) -> MetaPatch:
        return self._meta

    @property
    def meta(self) -> MetaPatch:
        return self._meta

    @property
    def spec(self) -> SpecPatch:
        return self._spec

    @property
    def status(self) -> StatusPatch:
        return self._status

    def as_json_patch(self) -> JSONPatch:
        return [] if not self else self._as_json_patch(self, keys=[''])

    def _as_json_patch(self, value: object, keys: list[str]) -> JSONPatch:
        result: JSONPatch = []
        if value is None:
            result.append(JSONPatchItem(op='remove', path=_escaped_path(keys)))
        elif len(keys) > 1 and self._original and not self._is_in_original_path(keys):
            result.append(JSONPatchItem(op='add', path=_escaped_path(keys), value=value))
        elif isinstance(value, collections.abc.Mapping) and value:
            for key, val in value.items():
                result.extend(self._as_json_patch(val, keys + [key]))
        else:
            result.append(JSONPatchItem(op='replace', path=_escaped_path(keys), value=value))
        return result

    def _is_in_original_path(self, keys: list[str]) -> bool:
        _search = self._original
        for key in keys:
            if key == '':
                continue
            try:
                _search = _search[key]  # type: ignore
            except (KeyError, TypeError):
                return False
        return True



================================================
FILE: kopf/_cogs/structs/references.py
================================================
import asyncio
import dataclasses
import enum
import fnmatch
import re
import urllib.parse
from collections.abc import Collection, Iterable, Iterator, Mapping, MutableMapping
from typing import NewType, Optional, Union

# A namespace specification with globs, negations, and some minimal syntax; see `match_namespace()`.
# Regexps are also supported if pre-compiled from the code, not from the CLI options as raw strings.
NamespacePattern = Union[str, re.Pattern[str]]

# A specific really existing addressable namespace (at least, the one assumed to be so).
# Made as a NewType for stricter type-checking to avoid collisions with patterns and other strings.
NamespaceName = NewType('NamespaceName', str)

# A namespace reference usable in the API calls. `None` means cluster-wide API calls.
Namespace = Optional[NamespaceName]


def select_specific_namespaces(patterns: Iterable[NamespacePattern]) -> Collection[NamespaceName]:
    """
    Select the namespace specifications that can be used as direct namespaces.

    It is used in a fallback scenario when the namespace observation is either
    disabled or not possible due to restricted permission, while the normal
    operation is still possible in the very specific configured namespaces.
    """
    return {
        NamespaceName(pattern)
        for pattern in patterns
        if isinstance(pattern, str)  # excl. regexps & etc.
        if not('!' in pattern or '*' in pattern or '?' in pattern or ',' in pattern)
    }


def match_namespace(name: NamespaceName, pattern: NamespacePattern) -> bool:
    """
    Check if the specific namespace matches a namespace specification.

    Each individual namespace pattern is a string that follows some syntax:

    * the pattern consists of comma-separated parts (spaces are ignored);
    * each part is either an inclusive or an exclusive (negating) glob;
    * each glob can have ``*`` and ``?`` placeholders for any or one symbols;
    * the exclusive globs start with ``!``;
    * if the the first glob is exclusive, then a preceding catch-all is implied.

    A check of whether a namespace matches the individual pattern, is done by
    iterating the pattern's globs left-to-right: the exclusive patterns exclude
    it from the match; the first inclusive pattern does the initial match, while
    the following inclusive patterns only re-match it if it was excluded before;
    i.e., they do not do the full initial match.

    For example, the pattern ``"myapp-*, !*-pr-*, *pr-123"``
    will match ``myapp-test``, ``myapp-live``, even ``myapp-pr-123``,
    but not ``myapp-pr-456`` and certainly not ``otherapp-pr-123``.
    The latter one, despite it matches the last glob, is not included
    because it was not matched by the initial pattern.

    On the other hand, the pattern ``"!*-pr-*, *pr-123"``
    (equivalent to ``"*, !*-pr-*, *pr-123"``) will match ``myapp-test``,
    ``myapp-live``, ``myapp-pr-123``, ``anyapp-anything``,
    and even ``otherapp-pr-123`` -- though not ``myapp-pr-456``.
    Unlike in the first example, the otherapp's namespace was included initially
    by the first glob (the implied ``*``), and therefore could be re-matched
    by the last glob ``*pr-123`` after being excluded by ``!*-pr-*``.

    While these are theoretical capabilities of this pattern-matching algorithm,
    it is not expected that they will be abused too much. The main intention is
    to have simple one-glob patterns (either inclusive or exclusive),
    only rarely followed by a single negation.
    """

    # Regexps are powerful enough on their own -- we do not parse or interpret them.
    if isinstance(pattern, re.Pattern):
        return bool(pattern.fullmatch(name))

    # The first pattern should be an inclusive one. Unless it is, prepend a catch-all pattern.
    globs = [glob.strip() for glob in pattern.split(',')]
    if not globs or globs[0].startswith('!'):
        globs.insert(0, '*')

    # Iterate and calculate: every inclusive pattern makes the namespace to match regardless,
    # of the previous result; every exclusive pattern un-matches it if it was matched before.
    matches = first_match = fnmatch.fnmatch(name, globs[0])
    for glob in globs[1:]:
        if glob.startswith('!'):
            matches = matches and not fnmatch.fnmatch(name, glob.lstrip('!'))
        else:
            matches = matches or (first_match and fnmatch.fnmatch(name, glob))

    return matches


# Detect conventional API versions for some cases: e.g. in "myresources.v1alpha1.example.com".
# Non-conventional versions are indistinguishable from API groups ("myresources.foo1.example.com").
# See also: https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/
K8S_VERSION_PATTERN = re.compile(r'^v\d+(?:(?:alpha|beta)\d+)?$')


@dataclasses.dataclass(frozen=True, eq=False, repr=False)
class Resource:
    """
    A reference to a very specific custom or built-in resource kind.

    It is used to form the K8s API URLs. Generally, K8s API only needs
    an API group, an API version, and a plural name of the resource.
    All other names are remembered to match against resource selectors,
    for logging, and for informational purposes.
    """

    group: str
    """
    The resource's API group; e.g. ``"kopf.dev"``, ``"apps"``, ``"batch"``.
    For Core v1 API resources, an empty string: ``""``.
    """

    version: str
    """
    The resource's API version; e.g. ``"v1"``, ``"v1beta1"``, etc.
    """

    plural: str
    """
    The resource's plural name; e.g. ``"pods"``, ``"kopfexamples"``.
    It is used as an API endpoint, together with API group & version.
    """

    kind: Optional[str] = None
    """
    The resource's kind (as in YAML files); e.g. ``"Pod"``, ``"KopfExample"``.
    """

    singular: Optional[str] = None
    """
    The resource's singular name; e.g. ``"pod"``, ``"kopfexample"``.
    """

    shortcuts: frozenset[str] = frozenset()
    """
    The resource's short names; e.g. ``{"po"}``, ``{"kex", "kexes"}``.
    """

    categories: frozenset[str] = frozenset()
    """
    The resource's categories, to which the resource belongs; e.g. ``{"all"}``.
    """

    subresources: frozenset[str] = frozenset()
    """
    The resource's subresources, if defined; e.g. ``{"status", "scale"}``.
    """

    namespaced: Optional[bool] = None
    """
    Whether the resource is namespaced (``True``) or cluster-scoped (``False``).
    """

    preferred: bool = True  # against conventions, but makes versionless selectors match by default.
    """
    Whether the resource belong to a "preferred" API version.
    Only "preferred" resources are served when the version is not specified.
    """

    verbs: frozenset[str] = frozenset()
    """
    All available verbs for the resource, as supported by K8s API;
    e.g., ``{"list", "watch", "create", "update", "delete", "patch"}``.
    Note that it is not the same as all verbs permitted by RBAC.
    """

    def __hash__(self) -> int:
        return hash((self.group, self.version, self.plural))

    def __eq__(self, other: object) -> bool:
        if isinstance(other, Resource):
            self_tuple = (self.group, self.version, self.plural)
            other_tuple = (other.group, other.version, other.plural)
            return self_tuple == other_tuple
        else:
            return NotImplemented

    def __repr__(self) -> str:
        plural_main, *subs = self.plural.split('/')
        name_text = f'{plural_main}.{self.version}.{self.group}'.strip('.')
        subs_text = f'/{"/".join(subs)}' if subs else ''
        return f'{name_text}{subs_text}'

    # Mostly for tests, to be used as `@kopf.on.event(*resource, ...)`
    def __iter__(self) -> Iterator[str]:
        return iter((self.group, self.version, self.plural))

    def get_url(
            self,
            *,
            server: Optional[str] = None,
            namespace: Namespace = None,
            name: Optional[str] = None,
            subresource: Optional[str] = None,
            params: Optional[Mapping[str, str]] = None,
    ) -> str:
        """
        Build a URL to be used with K8s API.

        If the namespace is not set, a cluster-wide URL is returned.
        For cluster-scoped resources, the namespace is ignored.

        If the name is not set, the URL for the resource list is returned.
        Otherwise (if set), the URL for the individual resource is returned.

        If subresource is set, that subresource's URL is returned,
        regardless of whether such a subresource is known or not.

        Params go to the query parameters (``?param1=value1&param2=value2...``).
        """
        if subresource is not None and name is None:
            raise ValueError("Subresources can be used only with specific resources by their name.")
        if not self.namespaced and namespace is not None:
            raise ValueError(f"Specific namespaces are not supported for cluster-scoped resources.")
        if self.namespaced and namespace is None and name is not None:
            raise ValueError("Specific namespaces are required for specific namespaced resources.")

        parts: list[Optional[str]] = [
            '/api' if self.group == '' and self.version == 'v1' else '/apis',
            self.group,
            self.version,
            'namespaces' if self.namespaced and namespace is not None else None,
            namespace if self.namespaced and namespace is not None else None,
            self.plural,
            name,
            subresource,
        ]

        query = urllib.parse.urlencode(params, encoding='utf-8') if params else ''
        path = '/'.join([part for part in parts if part])
        url = path + ('?' if query else '') + query
        return url if server is None else server.rstrip('/') + '/' + url.lstrip('/')


class Marker(enum.Enum):
    """
    A special marker to handle all resources possible, built-in and custom.
    """
    EVERYTHING = enum.auto()


# An explicit catch-all marker for positional arguments of resource selectors.
EVERYTHING = Marker.EVERYTHING


@dataclasses.dataclass(frozen=True)
class Selector:
    """
    A resource specification that can match several resource kinds.

    The resource specifications are not usable in K8s API calls, as the API
    has no endpoints with masks or placeholders for unknown or catch-all
    resource identifying parts (e.g. any API group, any API version, any name).

    They are used only locally in the operator to match against the actual
    resources with specific names (:class:`Resource`). The handlers are
    defined with resource specifications, but are invoked with specific
    resource kinds. Even if those specifications look very concrete and allow
    no variations, they still remain specifications.
    """

    arg1: dataclasses.InitVar[Union[None, str, Marker]] = None
    arg2: dataclasses.InitVar[Union[None, str, Marker]] = None
    arg3: dataclasses.InitVar[Union[None, str, Marker]] = None
    argN: dataclasses.InitVar[None] = None  # a runtime guard against too many positional arguments

    group: Optional[str] = None
    version: Optional[str] = None

    kind: Optional[str] = None
    plural: Optional[str] = None
    singular: Optional[str] = None
    shortcut: Optional[str] = None
    category: Optional[str] = None
    any_name: Optional[Union[str, Marker]] = None

    def __post_init__(
            self,
            arg1: Union[None, str, Marker],
            arg2: Union[None, str, Marker],
            arg3: Union[None, str, Marker],
            argN: None,  # a runtime guard against too many positional arguments
    ) -> None:

        # Since the class is frozen & read-only, post-creation field adjustment is done via a hack.
        # This is the same hack as used in the frozen dataclasses to initialise their fields.
        if argN is not None:
            raise TypeError("Too many positional arguments. Max 3 positional args are accepted.")
        elif arg3 is not None:
            object.__setattr__(self, 'group', arg1)
            object.__setattr__(self, 'version', arg2)
            object.__setattr__(self, 'any_name', arg3)
        elif arg2 is not None and isinstance(arg1, str) and '/' in arg1:
            object.__setattr__(self, 'group', arg1.rsplit('/', 1)[0])
            object.__setattr__(self, 'version', arg1.rsplit('/')[-1])
            object.__setattr__(self, 'any_name', arg2)
        elif arg2 is not None and arg1 == 'v1':
            object.__setattr__(self, 'group', '')
            object.__setattr__(self, 'version', arg1)
            object.__setattr__(self, 'any_name', arg2)
        elif arg2 is not None:
            object.__setattr__(self, 'group', arg1)
            object.__setattr__(self, 'any_name', arg2)
        elif arg1 is not None and isinstance(arg1, Marker):
            object.__setattr__(self, 'any_name', arg1)
        elif arg1 is not None and '.' in arg1 and K8S_VERSION_PATTERN.match(arg1.split('.')[1]):
            if len(arg1.split('.')) >= 3:
                object.__setattr__(self, 'group', arg1.split('.', 2)[2])
            object.__setattr__(self, 'version', arg1.split('.')[1])
            object.__setattr__(self, 'any_name', arg1.split('.')[0])
        elif arg1 is not None and '.' in arg1:
            object.__setattr__(self, 'group', arg1.split('.', 1)[1])
            object.__setattr__(self, 'any_name', arg1.split('.')[0])
        elif arg1 is not None:
            object.__setattr__(self, 'any_name', arg1)

        # Verify that explicit & interpreted arguments have produced an unambiguous specification.
        names = [self.kind, self.plural, self.singular, self.shortcut, self.category, self.any_name]
        clean = [name for name in names if name is not None]
        if len(clean) > 1:
            raise TypeError(f"Ambiguous resource specification with names {clean}")
        if len(clean) < 1:
            raise TypeError(f"Unspecific resource with no names.")

        # For reasons unknown, the singular is empty for ALL builtin resources. This does not affect
        # the checks unless defined as e.g. ``singular=""``, which would match ALL builtins at once.
        # Thus we prohibit it until clarified why is it so, what does it mean, how to deal with it.
        if any([name == '' for name in names]):
            raise TypeError("Names must not be empty strings; either None or specific strings.")

    def __repr__(self) -> str:
        kwargs = {f.name: getattr(self, f.name) for f in dataclasses.fields(self)}
        kwtext = ', '.join([f'{key!s}={val!r}' for key, val in kwargs.items() if val is not None])
        clsname = self.__class__.__name__
        return f'{clsname}({kwtext})'

    @property
    def is_specific(self) -> bool:
        return (self.kind is not None or
                self.shortcut is not None or
                self.plural is not None or
                self.singular is not None or
                (self.any_name is not None and not isinstance(self.any_name, Marker)))

    def check(self, resource: Resource) -> bool:
        """
        Check if a specific resources matches this resource specification.
        """
        # Core v1 events are excluded from EVERYTHING: they are implicitly produced during handling,
        # and thus trigger unnecessary handling cycles (even for other resources, not for events).
        return (
            (self.group is None or self.group == resource.group) and
            ((self.version is None and resource.preferred) or self.version == resource.version) and
            (self.kind is None or self.kind == resource.kind) and
            (self.plural is None or self.plural == resource.plural) and
            (self.singular is None or self.singular == resource.singular) and
            (self.category is None or self.category in resource.categories) and
            (self.shortcut is None or self.shortcut in resource.shortcuts) and
            (self.any_name is None or
             self.any_name == resource.kind or
             self.any_name == resource.plural or
             self.any_name == resource.singular or
             self.any_name in resource.shortcuts or
             (self.any_name is Marker.EVERYTHING and
              not EVENTS.check(resource) and
              not EVENTS_K8S.check(resource))))

    def select(self, resources: Collection[Resource]) -> Collection[Resource]:
        result = {resource for resource in resources if self.check(resource)}

        # Core v1 API group's priority is hard-coded in K8s and kubectl. Do the same. For example:
        # whenever "pods" is specified, and "pods.v1" & "pods.v1beta1.metrics.k8s.io" are found,
        # implicitly give priority to "v1" and hide the existence of non-"v1" groups.
        # But not if they are specified by categories! -- In that case, keep all resources as is.
        if self.is_specific:
            v1only = {resource for resource in result if resource.group == ''}
            result = v1only or result

        return result


# Some predefined API endpoints that we use in the framework itself (not exposed to the operators).
# Note: the CRDs are versionless: we do not look into its ``spec`` stanza, we only watch for
# the fact of changes, so the schema does not matter, any cluster-preferred API version would work.
# Note: the peering resources are usually either zalando.org/v1 or kopf.dev/v1; if both co-exist,
# then both will be served (for keepalives and pausing). It is done for domain name transitioning.
CRDS = Selector('apiextensions.k8s.io', 'customresourcedefinitions')
EVENTS = Selector('v1', 'events')
EVENTS_K8S = Selector('events.k8s.io', 'events')  # only for exclusion from EVERYTHING
NAMESPACES = Selector('v1', 'namespaces')
CLUSTER_PEERINGS_K = Selector('kopf.dev/v1', 'clusterkopfpeerings')
CLUSTER_PEERINGS_Z = Selector('zalando.org/v1', 'clusterkopfpeerings')
NAMESPACED_PEERINGS_K = Selector('kopf.dev/v1', 'kopfpeerings')
NAMESPACED_PEERINGS_Z = Selector('zalando.org/v1', 'kopfpeerings')
MUTATING_WEBHOOK = Selector('admissionregistration.k8s.io', 'mutatingwebhookconfigurations')
VALIDATING_WEBHOOK = Selector('admissionregistration.k8s.io', 'validatingwebhookconfigurations')


class Backbone(Mapping[Selector, Resource]):
    """
    Actual resources used in the core (reactor & engines) of the framework.

    Why? The codebase only refers to the resources by API group/version & names.
    The actual resources can be different in different clusters, usually due
    to different versions: e.g. "v1" vs. "v1beta1" for CRDs.

    The actual backbone resources are detected in the initial cluster scanning
    during the operator startup in :func:`resource_scanner`.

    The backbone resources cannot be changed at runtime after they are found
    for the first time -- since the core tasks are already started with those
    resource definitions, and cannot be easily restarted.

    This does not apply to the resources of the operator (not the framework!),
    where the resources can be created, changed, and deleted at runtime easily.
    """

    def __init__(self) -> None:
        super().__init__()
        self._items: MutableMapping[Selector, Resource] = {}
        self._revised = asyncio.Condition()
        self.selectors = [
            NAMESPACES, EVENTS, CRDS,
            MUTATING_WEBHOOK, VALIDATING_WEBHOOK,
            CLUSTER_PEERINGS_K, NAMESPACED_PEERINGS_K,
            CLUSTER_PEERINGS_Z, NAMESPACED_PEERINGS_Z,
        ]

    def __len__(self) -> int:
        return len(self._items)

    def __iter__(self) -> Iterator[Selector]:
        return iter(self._items)

    def __getitem__(self, item: Selector) -> Resource:
        return self._items[item]

    async def fill(
            self,
            *,
            resources: Iterable[Resource],
    ) -> None:
        async with self._revised:
            for resource in resources:
                for spec in self.selectors:
                    if spec not in self._items:
                        if spec.check(resource):
                            self._items[spec] = resource
            self._revised.notify_all()

    async def wait_for(
            self,
            selector: Selector,
    ) -> Resource:
        """
        Wait for the actual resource to be found in the cluster scanning.

        The resources can be cached in-memory. Once the resource is retrieved,
        it never changes in memory even if it changes in the cluster. This is
        intentional -- to match with the nature of the cluster scanning,
        which waits for the resources and then starts background jobs,
        which are not easy to terminate without terminating the whole operator.
        """
        async with self._revised:
            await self._revised.wait_for(lambda: selector in self)
        return self[selector]


@dataclasses.dataclass(frozen=True)
class Insights:
    """
    Actual resources & namespaces served by the operator.
    """

    # The precomputed specialised sets of resources as used in independent parts of the framework.
    # - **Indexed** resources block the operator startup until all objects are initially indexed.
    # - **Watched** resources spawn the watch-streams; the set excludes all webhook-only resources.
    # - **Webhook** resources are served via webhooks; the set excludes all watch-only resources.
    webhook_resources: set[Resource] = dataclasses.field(default_factory=set)
    indexed_resources: set[Resource] = dataclasses.field(default_factory=set)
    watched_resources: set[Resource] = dataclasses.field(default_factory=set)
    namespaces: set[Namespace] = dataclasses.field(default_factory=set)
    backbone: Backbone = dataclasses.field(default_factory=Backbone)

    # Signalled when anything changes in the insights.
    revised: asyncio.Condition = dataclasses.field(default_factory=asyncio.Condition)

    # The flags that are set after the initial listing is finished. Not cleared afterwards.
    ready_namespaces: asyncio.Event = dataclasses.field(default_factory=asyncio.Event)
    ready_resources: asyncio.Event = dataclasses.field(default_factory=asyncio.Event)



================================================
FILE: kopf/_cogs/structs/reviews.py
================================================
"""
Admission reviews: requests & responses, also the webhook server protocols.
"""
from collections.abc import AsyncIterator, Awaitable, Mapping
from typing import Any, Callable, Literal, Optional, Protocol, TypedDict, Union

from kopf._cogs.structs import bodies

Headers = Mapping[str, str]
SSLPeer = Mapping[str, Any]

Operation = Literal['CREATE', 'UPDATE', 'DELETE', 'CONNECT']


class RequestKind(TypedDict):
    group: str
    version: str
    kind: str


class RequestResource(TypedDict):
    group: str
    version: str
    resource: str


class UserInfo(TypedDict):
    username: str
    uid: str
    groups: list[str]


class CreateOptions(TypedDict, total=False):
    apiVersion: Literal["meta.k8s.io/v1"]
    kind: Literal["CreateOptions"]


class UpdateOptions(TypedDict, total=False):
    apiVersion: Literal["meta.k8s.io/v1"]
    kind: Literal["UpdateOptions"]


class DeleteOptions(TypedDict, total=False):
    apiVersion: Literal["meta.k8s.io/v1"]
    kind: Literal["DeleteOptions"]


class RequestPayload(TypedDict):
    uid: str
    kind: RequestKind
    resource: RequestResource
    subResource: Optional[str]
    requestKind: RequestKind
    requestResource: RequestResource
    requestSubResource: Optional[str]
    userInfo: UserInfo
    name: str
    namespace: Optional[str]
    operation: Operation
    options: Union[None, CreateOptions, UpdateOptions, DeleteOptions]
    dryRun: bool
    object: bodies.RawBody
    oldObject: Optional[bodies.RawBody]


class Request(TypedDict):
    apiVersion: Literal["admission.k8s.io/v1", "admission.k8s.io/v1beta1"]
    kind: Literal["AdmissionReview"]
    request: RequestPayload


class ResponseStatus(TypedDict, total=False):
    code: int
    message: str


class ResponsePayload(TypedDict, total=False):
    uid: str
    allowed: bool
    warnings: Optional[list[str]]
    status: Optional[ResponseStatus]
    patch: Optional[str]
    patchType: Optional[Literal["JSONPatch"]]


class Response(TypedDict):
    apiVersion: Literal["admission.k8s.io/v1", "admission.k8s.io/v1beta1"]
    kind: Literal["AdmissionReview"]
    response: ResponsePayload


class WebhookClientConfigService(TypedDict, total=False):
    namespace: Optional[str]
    name: Optional[str]
    path: Optional[str]
    port: Optional[int]


class WebhookClientConfig(TypedDict, total=False):
    """
    A config of clients (apiservers) to access the webhooks' server (operators).

    This dictionary is put into managed webhook configurations "as is".
    The fields & type annotations are only for hinting.

    Kopf additionally modifies the url and the service's path to inject
    handler ids as the last path component. This must be taken into account
    by custom webhook servers.
    """
    caBundle: Optional[str]  # if absent, the default apiservers' trust chain is used.
    url: Optional[str]
    service: Optional[WebhookClientConfigService]


class WebhookFn(Protocol):
    """
    A framework-provided function to call when an admission request is received.

    The framework provides the actual function. Custom webhook servers must
    accept the function, invoke it accordingly on admission requests, wait
    for the admission response, serialise it and send it back. They do not
    implement this function. This protocol only declares the exact signature.
    """
    def __call__(
            self,
            request: Request,
            *,
            webhook: Optional[str] = None,
            headers: Optional[Mapping[str, str]] = None,
            sslpeer: Optional[Mapping[str, Any]] = None,
    ) -> Awaitable[Response]: ...


# A server (either a coroutine or a callable object).
WebhookServerProtocol = Callable[[WebhookFn], AsyncIterator[WebhookClientConfig]]



================================================
FILE: kopf/_core/__init__.py
================================================
[Empty file]


================================================
FILE: kopf/_core/actions/__init__.py
================================================
[Empty file]


================================================
FILE: kopf/_core/actions/application.py
================================================
"""
Routines to apply effects accumulated during the reaction cycle.

Applying the effects is the last step in the reacting cycle:

* :mod:`queueing` streams the events to per-object processing tasks,
* :mod:`causation` detects what has happened based on the received events,
* :mod:`handling` decides how to react, and invokes the handlers,
* :mod:`effects` applies the effects accumulated during the handling.

The effects are accumulated both from the framework and from the operator's
handlers, and are generally of these three intermixed kinds:

* Patching the object with all the results & states persisted.
* Sleeping for the duration of known absence of activity (or until interrupted).
* Touching the object to trigger the next reaction cycle.

It is used from in :mod:`processing`, :mod:`actitivies`, and :mod:`daemons` --
all the modules, of which the reactor's core consists.
"""
import asyncio
import datetime
from collections.abc import Collection
from typing import Optional

from kopf._cogs.aiokits import aiotime
from kopf._cogs.clients import patching
from kopf._cogs.configs import configuration
from kopf._cogs.helpers import typedefs
from kopf._cogs.structs import bodies, dicts, diffs, patches, references
from kopf._core.actions import loggers

# How often to wake up from the long sleep, to show liveness in the logs.
WAITING_KEEPALIVE_INTERVAL = 10 * 60

# K8s-managed fields that are removed completely when patched to an empty list/dict.
KNOWN_INCONSISTENCIES = (
    dicts.parse_field('metadata.annotations'),
    dicts.parse_field('metadata.finalizers'),
    dicts.parse_field('metadata.labels'),
)


async def apply(
        *,
        settings: configuration.OperatorSettings,
        resource: references.Resource,
        body: bodies.Body,
        patch: patches.Patch,
        delays: Collection[float],
        logger: loggers.ObjectLogger,
        stream_pressure: Optional[asyncio.Event] = None,  # None for tests
) -> bool:
    delay = min(delays) if delays else None

    # Delete dummies on occasion, but don't trigger special patching for them [discussable].
    if patch:  # TODO: LATER: and the dummies are there (without additional methods?)
        settings.persistence.progress_storage.touch(body=body, patch=patch, value=None)

    # Actually patch if it was not empty originally or after the dummies removal.
    await patch_and_check(
        settings=settings,
        resource=resource,
        logger=logger,
        patch=patch,
        body=body,
    )

    # Sleep strictly after patching, never before -- to keep the status proper.
    # The patching above, if done, interrupts the sleep instantly, so we skip it at all.
    # Note: a zero-second or negative sleep is still a sleep, it will trigger a dummy patch.
    applied = False
    if delay and patch:
        logger.debug(f"Sleeping was skipped because of the patch, {delay} seconds left.")
    elif delay is not None:
        if delay > WAITING_KEEPALIVE_INTERVAL:
            limit = WAITING_KEEPALIVE_INTERVAL
            logger.debug(f"Sleeping for {delay} (capped {limit}) seconds for the delayed handlers.")
            unslept_delay = await aiotime.sleep(limit, wakeup=stream_pressure)
        elif delay > 0:
            logger.debug(f"Sleeping for {delay} seconds for the delayed handlers.")
            unslept_delay = await aiotime.sleep(delay, wakeup=stream_pressure)
        else:
            unslept_delay = None  # no need to sleep? means: slept in full.

        # Exclude cases when touching immediately after patching (including: ``delay == 0``).
        if patch and not delay:
            pass
        elif unslept_delay is not None:
            logger.debug(f"Sleeping was interrupted by new changes, {unslept_delay} seconds left.")
        else:
            # Any unique always-changing value will work; not necessary a timestamp.
            value = datetime.datetime.now(datetime.timezone.utc).isoformat()
            touch = patches.Patch()
            settings.persistence.progress_storage.touch(body=body, patch=touch, value=value)
            await patch_and_check(
                settings=settings,
                resource=resource,
                logger=logger,
                patch=touch,
                body=body,
            )
    elif not patch:  # no patch/touch and no delay
        applied = True
    return applied


async def patch_and_check(
        *,
        settings: configuration.OperatorSettings,
        resource: references.Resource,
        body: bodies.Body,
        patch: patches.Patch,
        logger: typedefs.Logger,
) -> None:
    """
    Apply a patch and verify that it is applied correctly.

    The inconsistencies are checked only against what was in the patch.
    Other unexpected changes in the body are ignored, including the system
    fields, such as generations, resource versions, and other unrelated fields,
    such as other statuses, spec, labels, annotations, etc.

    Selected false-positive inconsistencies are explicitly ignored
    for K8s-managed fields, such as finalizers, labels or annotations:
    whenever an empty list/dict is stored, such fields are completely removed.
    For normal fields (e.g. in spec/status), an empty list/dict is still
    a value and is persisted in the object and matches with the patch.
    """
    if patch:
        logger.debug(f"Patching with: {patch!r}")
        resulting_body = await patching.patch_obj(
            settings=settings,
            resource=resource,
            namespace=body.metadata.namespace,
            name=body.metadata.name,
            patch=patch,
            logger=logger,
        )
        inconsistencies = diffs.diff(patch, resulting_body, scope=diffs.DiffScope.LEFT)
        inconsistencies = diffs.Diff(
            diffs.DiffItem(op, field, old, new)
            for op, field, old, new in inconsistencies
            if old or new or field not in KNOWN_INCONSISTENCIES
        )
        if resulting_body is None:
            logger.debug(f"Patching was skipped: the object does not exist anymore.")
        elif inconsistencies:
            logger.warning(f"Patching failed with inconsistencies: {inconsistencies}")



================================================
FILE: kopf/_core/actions/execution.py
================================================
"""
Execution of pre-selected handlers, in batches or individually.

These functions are invoked from `kopf._core.reactor.processing`,
where the raw watch-events are interpreted and wrapped into extended *causes*.

The handler execution can also be used in other places, such as in-memory
activities, when there is no underlying Kubernetes object to patch'n'watch.
"""
import asyncio
import contextlib
import dataclasses
import datetime
import enum
from collections.abc import AsyncIterator, Collection, Iterable, Mapping, MutableMapping, Sequence
from contextvars import ContextVar
from typing import Any, AsyncContextManager, Callable, NewType, Optional, Protocol, TypeVar

from kopf._cogs.configs import configuration
from kopf._cogs.helpers import typedefs
from kopf._cogs.structs import ids
from kopf._core.actions import invocation

# The default delay duration for the regular exception in retry-mode.
DEFAULT_RETRY_DELAY = 1 * 60


class PermanentError(Exception):
    """ A fatal handler error, the retries are useless. """


class TemporaryError(Exception):
    """ A potentially recoverable error, should be retried. """
    def __init__(
            self,
            __msg: Optional[str] = None,
            delay: Optional[float] = DEFAULT_RETRY_DELAY,
    ) -> None:
        super().__init__(__msg)
        self.delay = delay


class HandlerTimeoutError(PermanentError):
    """ An error for the handler's timeout (if set). """


class HandlerRetriesError(PermanentError):
    """ An error for the handler's retries exceeded (if set). """


class HandlerChildrenRetry(TemporaryError):
    """ An internal pseudo-error to retry for the next sub-handlers attempt. """


class ErrorsMode(enum.Enum):
    """ How arbitrary (non-temporary/non-permanent) exceptions are treated. """
    IGNORED = enum.auto()
    TEMPORARY = enum.auto()
    PERMANENT = enum.auto()


# A specialised type to highlight the purpose or origin of the data of type Any,
# to not be mixed with other arbitrary Any values, where it is indeed "any".
Result = NewType('Result', object)


@dataclasses.dataclass(frozen=True)
class Outcome:
    """
    An in-memory outcome of one single invocation of one single handler.

    Conceptually, an outcome is similar to the async futures, but some cases
    are handled specially: e.g., the temporary errors have exceptions,
    but the handler should be retried later, unlike with the permanent errors.

    Note the difference: `HandlerState` is a persistent state of the handler,
    possibly after a few executions, and consisting of simple data types
    (for YAML/JSON serialisation) rather than the actual in-memory objects.
    """
    final: bool
    delay: Optional[float] = None
    result: Optional[Result] = None
    exception: Optional[Exception] = None
    subrefs: Collection[ids.HandlerId] = ()


@dataclasses.dataclass(frozen=True)
class HandlerState:
    """
    A persisted state of a single handler, as stored on the resource's status.

    Note the difference: `Outcome` is for in-memory results of handlers,
    which is then additionally converted before being storing as a state.

    Active handler states are those used in .done/.delays for the current
    handling cycle & the current cause. Passive handler states are those
    carried over for logging of counts/extras, and for final state purging,
    but not participating in the current handling cycle.
    """
    started: Optional[datetime.datetime] = None  # None means this information was lost.
    stopped: Optional[datetime.datetime] = None  # None means it is still running (e.g. delayed).
    delayed: Optional[datetime.datetime] = None  # None means it is finished (succeeded/failed).
    retries: int = 0
    success: bool = False
    failure: bool = False

    @property
    def finished(self) -> bool:
        return bool(self.success or self.failure)

    @property
    def sleeping(self) -> bool:
        ts = self.delayed
        now = datetime.datetime.now(datetime.timezone.utc)
        return not self.finished and ts is not None and ts > now

    @property
    def awakened(self) -> bool:
        return bool(not self.finished and not self.sleeping)

    @property
    def runtime(self) -> datetime.timedelta:
        now = datetime.datetime.now(datetime.timezone.utc)
        return now - (self.started if self.started else now)


class State(Mapping[ids.HandlerId, HandlerState]):
    pass


@dataclasses.dataclass
class Cause(invocation.Kwargable):
    """ Base non-specific cause as used in the framework's reactor. """
    logger: typedefs.Logger

    @property
    def _kwargs(self) -> Mapping[str, Any]:
        # Similar to `dataclasses.asdict()`, but not recursive for other dataclasses.
        return {field.name: getattr(self, field.name) for field in dataclasses.fields(self)}


CauseT = TypeVar('CauseT', bound=Cause)


@dataclasses.dataclass(frozen=True)
class Handler:
    """ A handler is a function bound with its behavioral constraints. """
    id: ids.HandlerId
    fn: invocation.Invokable
    param: Optional[Any]
    errors: Optional[ErrorsMode]
    timeout: Optional[float]
    retries: Optional[int]
    backoff: Optional[float]

    # Used in the logs. Overridden in some (but not all) handler types for better log messages.
    def __str__(self) -> str:
        return f"Handler {self.id!r}"

    # Overridden in handlers with fields for causes with field-specific old/new/diff.
    def adjust_cause(self, cause: CauseT) -> CauseT:
        return cause


class LifeCycleFn(Protocol):
    """ A callback type for handlers selection based on the event/cause. """
    def __call__(
            self,
            handlers: Sequence[Handler],
            *,
            state: State,
            **kwargs: Any,
    ) -> Sequence[Handler]: ...


# The task-local context; propagated down the stack instead of multiple kwargs.
# Used in `@kopf.subhandler` and `kopf.execute()` to add/get the sub-handlers.
sublifecycle_var: ContextVar[Optional[LifeCycleFn]] = ContextVar('sublifecycle_var')
subsettings_var: ContextVar[configuration.OperatorSettings] = ContextVar('subsettings_var')
subrefs_var: ContextVar[Iterable[set[ids.HandlerId]]] = ContextVar('subrefs_var')
handler_var: ContextVar[Handler] = ContextVar('handler_var')
cause_var: ContextVar[Cause] = ContextVar('cause_var')


ExtraContext = Callable[[], AsyncContextManager[None]]


@contextlib.asynccontextmanager
async def no_extra_context() -> AsyncIterator[None]:
    yield


async def execute_handlers_once(
        lifecycle: LifeCycleFn,
        settings: configuration.OperatorSettings,
        handlers: Collection[Handler],
        cause: Cause,
        state: State,
        extra_context: ExtraContext = no_extra_context,
        default_errors: ErrorsMode = ErrorsMode.TEMPORARY,
) -> Mapping[ids.HandlerId, Outcome]:
    """
    Call the next handler(s) from the chain of the handlers.

    Keep the record on the progression of the handlers in the object's state,
    and use it on the next invocation to determined which handler(s) to call.

    This routine is used both for the global handlers (via global registry),
    and for the sub-handlers (via a simple registry of the current handler).
    """

    # Filter and select the handlers to be executed right now, on this event reaction cycle.
    handlers_todo = [h for h in handlers if state[h.id].awakened]
    handlers_plan = lifecycle(handlers_todo, state=state, **cause.kwargs)

    # Execute all planned (selected) handlers in one event reaction cycle, even if there are a few.
    outcomes: MutableMapping[ids.HandlerId, Outcome] = {}
    for handler in handlers_plan:
        outcome = await execute_handler_once(
            settings=settings,
            handler=handler,
            state=state[handler.id],
            cause=cause,
            lifecycle=lifecycle,  # just a default for the sub-handlers, not used directly.
            extra_context=extra_context,
            default_errors=default_errors,
        )
        outcomes[handler.id] = outcome

    return outcomes


async def execute_handler_once(
        settings: configuration.OperatorSettings,
        handler: Handler,
        cause: Cause,
        state: HandlerState,
        lifecycle: Optional[LifeCycleFn] = None,
        extra_context: ExtraContext = no_extra_context,
        default_errors: ErrorsMode = ErrorsMode.TEMPORARY,
) -> Outcome:
    """
    Execute one and only one handler for one and only one time.

    *Execution* means not just *calling* the handler in properly set context
    (see `_call_handler`), but also interpreting its result and errors, and
    wrapping them into am `Outcome` object -- to be stored in the state.

    The *execution* can be long -- depending on how the handler is implemented.
    For daemons, it is normal to run for hours and days if needed.
    This is different from the regular handlers, which are supposed
    to be finished as soon as possible.

    This method is not supposed to raise any exceptions from the handlers:
    exceptions mean the failure of execution itself.
    """
    errors_mode = handler.errors if handler.errors is not None else default_errors
    backoff = handler.backoff if handler.backoff is not None else DEFAULT_RETRY_DELAY
    logger = cause.logger

    # Mutable accumulator for all the sub-handlers of any level deep; populated in `kopf.execute`.
    subrefs: set[ids.HandlerId] = set()

    # The exceptions are handled locally and are not re-raised, to keep the operator running.
    try:
        logger.debug(f"{handler} is invoked.")

        if handler.timeout is not None and state.runtime.total_seconds() >= handler.timeout:
            raise HandlerTimeoutError(f"{handler} has timed out after {state.runtime}.")

        if handler.retries is not None and state.retries >= handler.retries:
            raise HandlerRetriesError(f"{handler} has exceeded {state.retries} retries.")

        result = await invoke_handler(
            handler=handler,
            cause=cause,
            retry=state.retries,
            started=state.started or datetime.datetime.now(datetime.timezone.utc),  # "or" is for type-checking.
            runtime=state.runtime,
            settings=settings,
            lifecycle=lifecycle,  # just a default for the sub-handlers, not used directly.
            extra_context=extra_context,
            subrefs=subrefs,
        )

    # The cancellations are an excepted way of stopping the handler. Especially for daemons.
    except asyncio.CancelledError:
        logger.warning(f"{handler} is cancelled. Will escalate.")
        raise

    # Unfinished children cause the regular retry, but with less logging and event reporting.
    except HandlerChildrenRetry as e:
        logger.debug(f"{handler} has unfinished sub-handlers. Will retry soon.")
        return Outcome(final=False, exception=e, delay=e.delay, subrefs=subrefs)

    # Definitely a temporary error, regardless of the error strictness.
    except TemporaryError as e:
        logger.error(f"{handler} failed temporarily: {str(e) or repr(e)}")
        return Outcome(final=False, exception=e, delay=e.delay, subrefs=subrefs)

    # Same as permanent errors below, but with better logging for our internal cases.
    except HandlerTimeoutError as e:
        logger.error(f"{str(e) or repr(e)}")  # already formatted
        return Outcome(final=True, exception=e, subrefs=subrefs)
        # TODO: report the handling failure somehow (beside logs/events). persistent status?

    # Definitely a permanent error, regardless of the error strictness.
    except PermanentError as e:
        logger.error(f"{handler} failed permanently: {str(e) or repr(e)}")
        return Outcome(final=True, exception=e, subrefs=subrefs)
        # TODO: report the handling failure somehow (beside logs/events). persistent status?

    # Regular errors behave as either temporary or permanent depending on the error strictness.
    except Exception as e:
        if errors_mode == ErrorsMode.IGNORED:
            logger.exception(f"{handler} failed with an exception. Will ignore.")
            return Outcome(final=True, subrefs=subrefs)
        elif errors_mode == ErrorsMode.TEMPORARY:
            logger.exception(f"{handler} failed with an exception. Will retry.")
            return Outcome(final=False, exception=e, delay=backoff, subrefs=subrefs)
        elif errors_mode == ErrorsMode.PERMANENT:
            logger.exception(f"{handler} failed with an exception. Will stop.")
            return Outcome(final=True, exception=e, subrefs=subrefs)
            # TODO: report the handling failure somehow (beside logs/events). persistent status?
        else:
            raise RuntimeError(f"Unknown mode for errors: {errors_mode!r}")

    # No errors means the handler should be excluded from future runs in this reaction cycle.
    else:
        logger.info(f"{handler} succeeded.")
        return Outcome(final=True, result=result, subrefs=subrefs)


async def invoke_handler(
        *,
        handler: Handler,
        cause: Cause,
        retry: int,
        started: datetime.datetime,
        runtime: datetime.timedelta,
        settings: configuration.OperatorSettings,
        lifecycle: Optional[LifeCycleFn],
        subrefs: set[ids.HandlerId],
        extra_context: ExtraContext,
) -> Optional[Result]:
    """
    Invoke one handler only, according to the calling conventions.

    Specifically, calculate the handler-specific fields (e.g. field diffs).

    Ensure the global context for this asyncio task is set to the handler and
    its cause -- for proper population of the sub-handlers via the decorators
    (see `@kopf.subhandler`).
    """

    # For the field-handlers, the old/new/diff values must match the field, not the whole object.
    cause = handler.adjust_cause(cause)

    # The context makes it possible and easy to pass the kwargs _through_ the user-space handlers:
    # from the framework to the framework's helper functions (e.g. sub-handling, hierarchies, etc).
    with invocation.context([
        (sublifecycle_var, lifecycle),
        (subsettings_var, settings),
        (subrefs_var, list(subrefs_var.get([])) + [subrefs]),
        (handler_var, handler),
        (cause_var, cause),
    ]):
        async with extra_context():
            result = await invocation.invoke(
                handler.fn,
                settings=settings,
                kwargsrc=cause,
                kwargs=dict(
                    param=handler.param,
                    retry=retry,
                    started=started,
                    runtime=runtime,
                ),
            )

            # Since we know that we invoked the handler, we cast "any" result to a handler result.
            return Result(result)



================================================
FILE: kopf/_core/actions/invocation.py
================================================
"""
Invoking the callbacks, including the args/kwargs preparation.

Both sync & async functions are supported, so as their partials.
Also, decorated wrappers and lambdas are recognized.
All of this goes via the same invocation logic and protocol.
"""
import asyncio
import contextlib
import contextvars
import functools
from collections.abc import Coroutine, Iterable, Iterator, Mapping
from typing import Any, Callable, Optional, TypeVar, Union, final

from kopf._cogs.configs import configuration

# An internal typing hack shows that the handler can be sync fn with the result,
# or an async fn which returns a coroutine which, in turn, returns the result.
# Used in some protocols only and is never exposed to other modules.
_R = TypeVar('_R')
SyncOrAsync = Union[_R, Coroutine[None, None, _R]]

# A generic sync-or-async callable with no args/kwargs checks (unlike in protocols).
# Used for the Handler and generic invocation methods (which do not care about protocols).
Invokable = Callable[..., SyncOrAsync[Optional[object]]]


class Kwargable:
    """
    Something that can provide kwargs to the function invocation rotuine.

    Technically, there is only one source of kwargs in the framework --
    `Cause` and descendants across the source code (e.g. ``causes.py``).
    However, we do not want to introduce a new dependency of a low-level
    function invocation module on the specialised causation logic & structures.
    For this reason, the `Cause` & `Kwargable` classes are split.
    """

    @property
    def _kwargs(self) -> Mapping[str, Any]:
        return {}

    @property
    def _sync_kwargs(self) -> Mapping[str, Any]:
        return self._kwargs

    @property
    def _async_kwargs(self) -> Mapping[str, Any]:
        return self._kwargs

    @property
    def _super_kwargs(self) -> Mapping[str, Any]:
        return {}

    @final
    @property
    def kwargs(self) -> Mapping[str, Any]:
        return dict(self._kwargs) | dict(self._super_kwargs)

    @final
    @property
    def sync_kwargs(self) -> Mapping[str, Any]:
        return dict(self._sync_kwargs) | dict(self._super_kwargs)

    @final
    @property
    def async_kwargs(self) -> Mapping[str, Any]:
        return dict(self._async_kwargs) | dict(self._super_kwargs)


@contextlib.contextmanager
def context(
        values: Iterable[tuple[contextvars.ContextVar[Any], Any]],
) -> Iterator[None]:
    """
    A context manager to set the context variables temporarily.
    """
    tokens: list[tuple[contextvars.ContextVar[Any], contextvars.Token[Any]]] = []
    try:
        for var, val in values:
            token = var.set(val)
            tokens.append((var, token))
        yield
    finally:
        for var, token in reversed(tokens):
            var.reset(token)


async def invoke(
        fn: Invokable,
        *,
        settings: Optional[configuration.OperatorSettings] = None,
        kwargsrc: Optional[Kwargable] = None,
        kwargs: Optional[Mapping[str, Any]] = None,  # includes param, retry, started, runtime, etc.
) -> Any:
    """
    Invoke a single function, but safely for the main asyncio process.

    Used mostly for handler functions, and potentially slow & blocking code.
    Other callbacks are called directly, and are expected to be synchronous
    (such as handler-selecting (lifecycles) and resource-filtering (``when=``)).

    A full set of the arguments is provided, expanding the cause to some easily
    usable aliases. The function is expected to accept ``**kwargs`` for the args
    that it does not use -- for forward compatibility with the new features.

    The synchronous methods are executed in the executor (threads or processes),
    thus making it non-blocking for the main event loop of the operator.
    See: https://pymotw.com/3/asyncio/executors.html
    """
    kwargs = {} if kwargs is None else kwargs
    if is_async_fn(fn):
        kwargs = dict(kwargs) | ({} if kwargsrc is None else dict(kwargsrc.async_kwargs))
        result = await fn(**kwargs)  # type: ignore
    else:
        kwargs = dict(kwargs) | ({} if kwargsrc is None else dict(kwargsrc.sync_kwargs))

        # Not that we want to use functools, but for executors kwargs, it is officially recommended:
        # https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.loop.run_in_executor
        real_fn = functools.partial(fn, **kwargs)

        # Copy the asyncio context from current thread to the handlr's thread.
        # It can be copied 2+ times if there are sub-sub-handlers (rare case).
        context = contextvars.copy_context()
        real_fn = functools.partial(context.run, real_fn)

        # Prevent orphaned threads during daemon/handler cancellation. It is better to be stuck
        # in the task than to have orphan threads which deplete the executor's pool capacity.
        # Cancellation is postponed until the thread exits, but it happens anyway (for consistency).
        # Note: the docs say the result is a future, but typesheds say it is a coroutine => cast()!
        loop = asyncio.get_running_loop()
        executor = settings.execution.executor if settings is not None else None
        future = loop.run_in_executor(executor, real_fn)
        cancellation: Optional[asyncio.CancelledError] = None
        while not future.done():
            try:
                await asyncio.shield(future)  # slightly expensive: creates tasks
            except asyncio.CancelledError as e:
                cancellation = e
        if cancellation is not None:
            raise cancellation
        result = future.result()

    return result


def is_async_fn(
        fn: Optional[Invokable],
) -> bool:
    if fn is None:
        return False
    elif isinstance(fn, functools.partial):
        return is_async_fn(fn.func)
    elif hasattr(fn, '__wrapped__'):  # @functools.wraps()
        return is_async_fn(fn.__wrapped__)
    else:
        return asyncio.iscoroutinefunction(fn)



================================================
FILE: kopf/_core/actions/lifecycles.py
================================================
"""
A few simple lifecycles for the handlers.

New lifecycles can be implemented the same way: accept ``handlers``
in the order they are registered (except those already succeeded),
and return the list of handlers in the order and amount to be executed.

The default behaviour of the framework is the most simplistic:
execute in the order they are registered, one by one.
"""
import logging
import random
from collections.abc import Sequence
from typing import Any, Optional

from kopf._core.actions import execution

logger = logging.getLogger(__name__)

Handlers = Sequence[execution.Handler]


def all_at_once(handlers: Handlers, **_: Any) -> Handlers:
    """ Execute all handlers at once, in one event reaction cycle, if possible. """
    return handlers


def one_by_one(handlers: Handlers, **_: Any) -> Handlers:
    """ Execute handlers one at a time, in the order they were registered. """
    return handlers[:1]


def randomized(handlers: Handlers, **_: Any) -> Handlers:
    """ Execute one handler at a time, in the random order. """
    return [random.choice(handlers)] if handlers else []


def shuffled(handlers: Handlers, **_: Any) -> Handlers:
    """ Execute all handlers at once, but in the random order. """
    return random.sample(handlers, k=len(handlers)) if handlers else []


def asap(handlers: Handlers, *, state: execution.State, **_: Any) -> Handlers:
    """ Execute one handler at a time, skip on failure, try the next one, retry after the full cycle. """

    def keyfn(handler: execution.Handler) -> int:
        return state[handler.id].retries or 0

    return sorted(handlers, key=keyfn)[:1]


_default_lifecycle: execution.LifeCycleFn = asap


def get_default_lifecycle() -> execution.LifeCycleFn:
    return _default_lifecycle


def set_default_lifecycle(lifecycle: Optional[execution.LifeCycleFn]) -> None:
    global _default_lifecycle
    if _default_lifecycle is not None:
        logger.warning(f"The default lifecycle is already set to {_default_lifecycle}, overriding it to {lifecycle}.")
    _default_lifecycle = lifecycle if lifecycle is not None else asap



================================================
FILE: kopf/_core/actions/loggers.py
================================================
"""
A connection between object logger and background k8s-event poster.

Everything logged to the object logger (except for debug information) is also
posted as a k8s-event -- in the background by `kopf._core.engines.posting`.

This eliminates the need to log & post the same messages, which complicates
the operators' code, and can lead to information loss or mismatch
(e.g. when logging call is added, but posting is forgotten).
"""
import copy
import enum
import logging
from collections.abc import MutableMapping
from typing import Any, Optional

# Luckily, we do not mock these ones in tests, so we can import them into our namespace.
try:
    # python-json-logger>=3.1.0
    from pythonjsonlogger.core import RESERVED_ATTRS as _pjl_RESERVED_ATTRS
    from pythonjsonlogger.json import JsonFormatter as _pjl_JsonFormatter
except ImportError:
    # python-json-logger<3.1.0
    from pythonjsonlogger.jsonlogger import JsonFormatter as _pjl_JsonFormatter  # type: ignore
    from pythonjsonlogger.jsonlogger import RESERVED_ATTRS as _pjl_RESERVED_ATTRS  # type: ignore

from kopf._cogs.configs import configuration
from kopf._cogs.helpers import typedefs
from kopf._cogs.structs import bodies

logger = logging.getLogger('kopf.objects')

# A key for object references in JSON logs, as seen by the log parsers.
DEFAULT_JSON_REFKEY = 'object'


class LogFormat(enum.Enum):
    """ Log formats, as specified on CLI. """
    PLAIN = '%(message)s'
    FULL = '[%(asctime)s] %(name)-20.20s [%(levelname)-8.8s] %(message)s'
    JSON = '-json-'  # not used for formatting, only for detection


class ObjectFormatter(logging.Formatter):
    pass


class ObjectTextFormatter(ObjectFormatter, logging.Formatter):
    pass


class ObjectJsonFormatter(ObjectFormatter, _pjl_JsonFormatter):
    def __init__(
            self,
            *args: Any,
            refkey: Optional[str] = None,
            **kwargs: Any,
    ) -> None:
        # Avoid type checking, as the args are not in the parent consructor.
        reserved_attrs = kwargs.pop('reserved_attrs', _pjl_RESERVED_ATTRS)
        reserved_attrs = set(reserved_attrs)
        reserved_attrs |= {'k8s_skip', 'k8s_ref', 'settings'}
        kwargs |= dict(reserved_attrs=reserved_attrs)
        kwargs.setdefault('timestamp', True)
        super().__init__(*args, **kwargs)
        self._refkey: str = refkey or DEFAULT_JSON_REFKEY

    def add_fields(
            self,
            log_record: dict[str, object],
            record: logging.LogRecord,
            message_dict: dict[str, object],
    ) -> None:
        super().add_fields(log_record, record, message_dict)

        if self._refkey and hasattr(record, 'k8s_ref'):
            ref = getattr(record, 'k8s_ref')
            log_record[self._refkey] = ref

        if 'severity' not in log_record:
            log_record['severity'] = (
                "debug" if record.levelno <= logging.DEBUG else
                "info" if record.levelno <= logging.INFO else
                "warn" if record.levelno <= logging.WARNING else
                "error" if record.levelno <= logging.ERROR else
                "fatal")


class ObjectPrefixingMixin(ObjectFormatter):
    def format(self, record: logging.LogRecord) -> str:
        if hasattr(record, 'k8s_ref'):
            ref = getattr(record, 'k8s_ref')
            namespace = ref.get('namespace', '')
            name = ref.get('name', '')
            prefix = f"[{namespace}/{name}]" if namespace else f"[{name}]"
            record = copy.copy(record)  # shallow
            record.msg = f"{prefix} {record.msg}"
        return super().format(record)


class ObjectPrefixingTextFormatter(ObjectPrefixingMixin, ObjectTextFormatter):
    pass


class ObjectPrefixingJsonFormatter(ObjectPrefixingMixin, ObjectJsonFormatter):
    pass


class ObjectLogger(typedefs.LoggerAdapter):
    """
    A logger/adapter to carry the object identifiers for formatting.

    The identifiers are then used both for formatting the per-object messages
    in `ObjectPrefixingFormatter`, and when posting the per-object k8s-events.

    Constructed in event handling of each individual object.

    The internal structure is made the same as an object reference in K8s API,
    but can change over time to anything needed for our internal purposes.
    However, as little information should be carried as possible,
    and the information should be protected against the object modification
    (e.g. in case of background posting via the queue; see `K8sPoster`).
    """

    def __init__(self, *, body: bodies.Body, settings: configuration.OperatorSettings) -> None:
        super().__init__(logger, dict(
            settings=settings,
            k8s_skip=False,
            k8s_ref=dict(
                apiVersion=body.get('apiVersion'),
                kind=body.get('kind'),
                name=body.get('metadata', {}).get('name'),
                uid=body.get('metadata', {}).get('uid'),
                namespace=body.get('metadata', {}).get('namespace'),
            ),
        ))

    def process(
            self,
            msg: str,
            kwargs: MutableMapping[str, Any],
    ) -> tuple[str, MutableMapping[str, Any]]:
        # Native logging overwrites the message's extra with the adapter's extra.
        # We merge them, so that both message's & adapter's extras are available.
        kwargs["extra"] = (self.extra or {}) | kwargs.get('extra', {})
        return msg, kwargs


class LocalObjectLogger(ObjectLogger):
    """
    The same as `ObjectLogger`, but does not post the messages as k8s-events.

    Used in the resource-watching handlers to log the handler's invocation
    successes/failures without overloading K8s with excessively many k8s-events.

    This class is used internally only and is not exposed publicly in any way.
    """

    def log(self, *args: Any, **kwargs: Any) -> None:
        kwargs['extra'] = dict(kwargs.pop('extra', {}), k8s_skip=True)
        return super().log(*args, **kwargs)


class TerseObjectLogger(LocalObjectLogger):
    """
    The same as 'LocalObjectLogger`, but more terse (less wordy).

    In the normal mode, only logs warnings & errors (but not infos).
    In the verbose mode, only logs warnings & errors & infos (but not debugs).

    Used for resource indexers: there can be hundreds or thousands of them,
    they are typically verbose, they are called often due to cluster changes
    (e.g. for pods). On the other hand, they are lightweight, so there is
    no much need to know what is happening until warnings/errors happen.
    """
    def isEnabledFor(self, level: int) -> bool:
        return super().isEnabledFor(level if level >= logging.WARNING else level - 10)


def configure(
        debug: Optional[bool] = None,
        verbose: Optional[bool] = None,
        quiet: Optional[bool] = None,
        log_format: LogFormat = LogFormat.FULL,
        log_prefix: Optional[bool] = False,
        log_refkey: Optional[str] = None,
) -> None:
    log_level = 'DEBUG' if debug or verbose else 'WARNING' if quiet else 'INFO'
    formatter = make_formatter(log_format=log_format, log_prefix=log_prefix, log_refkey=log_refkey)
    handler = logging.StreamHandler()
    handler.setFormatter(formatter)
    logger = logging.getLogger()
    logger.addHandler(handler)
    logger.setLevel(log_level)

    # Prevent the low-level logging unless in the debug mode. Keep only the operator's messages.
    # For no-propagation loggers, add a dummy null handler to prevent printing the messages.
    for name in ['asyncio']:
        logger = logging.getLogger(name)
        logger.propagate = bool(debug)
        if not debug:
            logger.handlers[:] = [logging.NullHandler()]


def make_formatter(
        log_format: LogFormat = LogFormat.FULL,
        log_prefix: Optional[bool] = False,
        log_refkey: Optional[str] = None,
) -> ObjectFormatter:
    log_prefix = log_prefix if log_prefix is not None else bool(log_format is not LogFormat.JSON)
    if log_format is LogFormat.JSON:
        if log_prefix:
            return ObjectPrefixingJsonFormatter(refkey=log_refkey)
        else:
            return ObjectJsonFormatter(refkey=log_refkey)
    elif isinstance(log_format, LogFormat):
        if log_prefix:
            return ObjectPrefixingTextFormatter(log_format.value)
        else:
            return ObjectTextFormatter(log_format.value)
    elif isinstance(log_format, str):
        if log_prefix:
            return ObjectPrefixingTextFormatter(log_format)
        else:
            return ObjectTextFormatter(log_format)
    else:
        raise ValueError(f"Unsupported log format: {log_format!r}")



================================================
FILE: kopf/_core/actions/progression.py
================================================
"""
The routines to manipulate the handler progression over the event cycle.

Used to track which handlers are finished, which are not yet,
and how many retries were there.

There could be more than one low-level k8s watch-events per one actual
high-level kopf-event (a cause). The handlers are called at different times,
and the overall handling routine should persist the handler status somewhere.

The states are persisted in a state storage: see `kopf._cogs.configs.progress`.
"""

import collections.abc
import copy
import dataclasses
import datetime
from collections.abc import Collection, Iterable, Iterator, Mapping
from typing import Any, NamedTuple, Optional, overload

import iso8601

from kopf._cogs.configs import progress
from kopf._cogs.structs import bodies, ids, patches
from kopf._core.actions import execution


@dataclasses.dataclass(frozen=True)
class HandlerState(execution.HandlerState):
    """
    A persisted state of a single handler, as stored on the resource's status.

    Note the difference: `Outcome` is for in-memory results of handlers,
    which is then additionally converted before being storing as a state.

    Active handler states are those used in .done/.delays for the current
    handling cycle & the current cause. Passive handler states are those
    carried over for logging of counts/extras, and for final state purging,
    but not participating in the current handling cycle.
    """

    # Some fields may overlap the base class's fields, but this is fine (the types are the same).
    active: Optional[bool] = None  # is it used in done/delays [T]? or only in counters/purges [F]?
    started: Optional[datetime.datetime] = None  # None means this information was lost.
    stopped: Optional[datetime.datetime] = None  # None means it is still running (e.g. delayed).
    delayed: Optional[datetime.datetime] = None  # None means it is finished (succeeded/failed).
    purpose: Optional[str] = None  # None is a catch-all marker for upgrades/rollbacks.
    retries: int = 0
    success: bool = False
    failure: bool = False
    message: Optional[str] = None
    subrefs: Collection[ids.HandlerId] = ()  # ids of actual sub-handlers of all levels deep.
    _origin: Optional[progress.ProgressRecord] = None  # to check later if it has actually changed.

    @classmethod
    def from_scratch(cls, *, purpose: Optional[str] = None) -> "HandlerState":
        return cls(
            active=True,
            started=datetime.datetime.now(datetime.timezone.utc),
            purpose=purpose,
        )

    @classmethod
    def from_storage(cls, __d: progress.ProgressRecord) -> "HandlerState":
        return cls(
            active=False,
            started=_parse_iso8601(__d.get('started')) or datetime.datetime.now(datetime.timezone.utc),
            stopped=_parse_iso8601(__d.get('stopped')),
            delayed=_parse_iso8601(__d.get('delayed')),
            purpose=__d.get('purpose') if __d.get('purpose') else None,
            retries=__d.get('retries') or 0,
            success=__d.get('success') or False,
            failure=__d.get('failure') or False,
            message=__d.get('message'),
            subrefs=__d.get('subrefs') or (),
            _origin=__d,
        )

    def for_storage(self) -> progress.ProgressRecord:
        return progress.ProgressRecord(
            started=None if self.started is None else _format_iso8601(self.started),
            stopped=None if self.stopped is None else _format_iso8601(self.stopped),
            delayed=None if self.delayed is None else _format_iso8601(self.delayed),
            purpose=None if self.purpose is None else str(self.purpose),
            retries=None if self.retries is None else int(self.retries),
            success=None if self.success is None else bool(self.success),
            failure=None if self.failure is None else bool(self.failure),
            message=None if self.message is None else str(self.message),
            subrefs=None if not self.subrefs else list(sorted(self.subrefs)),
        )

    def as_in_storage(self) -> Mapping[str, Any]:
        # Nones are not stored by Kubernetes, so we filter them out for comparison.
        return {key: val for key, val in self.for_storage().items() if val is not None}

    def as_active(self) -> "HandlerState":
        return dataclasses.replace(self, active=True)

    def with_purpose(
            self,
            purpose: Optional[str],
    ) -> "HandlerState":
        return dataclasses.replace(self, purpose=purpose)

    def with_outcome(
            self,
            outcome: execution.Outcome,
    ) -> "HandlerState":
        now = datetime.datetime.now(datetime.timezone.utc)
        cls = type(self)
        return cls(
            active=self.active,
            purpose=self.purpose,
            started=self.started if self.started else now,
            stopped=self.stopped if self.stopped else now if outcome.final else None,
            delayed=now + datetime.timedelta(seconds=outcome.delay) if outcome.delay is not None else None,
            success=bool(outcome.final and outcome.exception is None),
            failure=bool(outcome.final and outcome.exception is not None),
            retries=(self.retries if self.retries is not None else 0) + 1,
            message=None if outcome.exception is None else str(outcome.exception),
            subrefs=list(sorted(set(self.subrefs) | set(outcome.subrefs))),  # deduplicate
            _origin=self._origin,
        )


class StateCounters(NamedTuple):
    success: int
    failure: int
    running: int


class State(execution.State):
    """
    A state of selected handlers, as persisted in the object's status.

    The state consists of simple YAML-/JSON-serializable values only:
    string handler ids as the keys; strings, booleans, integers as the values.

    The state is immutable: once created, it cannot be changed, and does not
    reflect the changes in the object's status. A new state is created every
    time some changes/outcomes are merged into the current state.
    """
    _states: Mapping[ids.HandlerId, HandlerState]

    def __init__(
            self,
            __src: Mapping[ids.HandlerId, HandlerState],
            *,
            purpose: Optional[str] = None,
    ):
        super().__init__()
        self._states = dict(__src)
        self.purpose = purpose

    @classmethod
    def from_scratch(cls) -> "State":
        return cls({})

    @classmethod
    def from_storage(
            cls,
            *,
            body: bodies.Body,
            storage: progress.ProgressStorage,
            handlers: Iterable[execution.Handler],
    ) -> "State":
        handler_ids = {handler.id for handler in handlers}
        handler_states: dict[ids.HandlerId, HandlerState] = {}
        for handler_id in handler_ids:
            content = storage.fetch(key=handler_id, body=body)
            if content is not None:
                handler_states[handler_id] = HandlerState.from_storage(content)
        return cls(handler_states)

    def with_purpose(
            self,
            purpose: Optional[str],
            handlers: Iterable[execution.Handler] = (),  # to be re-purposed
    ) -> "State":
        handler_states: dict[ids.HandlerId, HandlerState] = dict(self)
        for handler in handlers:
            handler_states[handler.id] = handler_states[handler.id].with_purpose(purpose)
        cls = type(self)
        return cls(handler_states, purpose=purpose)

    def with_handlers(
            self,
            handlers: Iterable[execution.Handler],
    ) -> "State":
        handler_states: dict[ids.HandlerId, HandlerState] = dict(self)
        for handler in handlers:
            if handler.id not in handler_states:
                handler_states[handler.id] = HandlerState.from_scratch(purpose=self.purpose)
            else:
                handler_states[handler.id] = handler_states[handler.id].as_active()
        cls = type(self)
        return cls(handler_states, purpose=self.purpose)

    def with_outcomes(
            self,
            outcomes: Mapping[ids.HandlerId, execution.Outcome],
    ) -> "State":
        unknown_ids = [handler_id for handler_id in outcomes if handler_id not in self]
        if unknown_ids:
            raise RuntimeError(f"Unexpected outcomes for unknown handlers: {unknown_ids!r}")

        cls = type(self)
        return cls({
            handler_id: (handler_state if handler_id not in outcomes else
                         handler_state.with_outcome(outcomes[handler_id]))
            for handler_id, handler_state in self._states.items()
        }, purpose=self.purpose)

    def without_successes(self) -> "State":
        cls = type(self)
        return cls({
            handler_id: handler_state
            for handler_id, handler_state in self._states.items()
            if not handler_state.success # i.e. failures & in-progress/retrying
        })

    def store(
            self,
            body: bodies.Body,
            patch: patches.Patch,
            storage: progress.ProgressStorage,
    ) -> None:
        for handler_id, handler_state in self._states.items():
            full_record = handler_state.for_storage()
            pure_record = handler_state.as_in_storage()
            if pure_record != handler_state._origin:
                storage.store(key=handler_id, record=full_record, body=body, patch=patch)
        storage.flush()

    def purge(
            self,
            *,
            body: bodies.Body,
            patch: patches.Patch,
            storage: progress.ProgressStorage,
            handlers: Iterable[execution.Handler],
    ) -> None:
        # Purge only our own handlers and their direct & indirect sub-handlers of all levels deep.
        # Ignore other handlers (e.g. handlers of other operators).
        handler_ids = {handler.id for handler in handlers}
        for handler_id in handler_ids:
            storage.purge(key=handler_id, body=body, patch=patch)
        for handler_id, handler_state in self._states.items():
            if handler_id not in handler_ids:
                storage.purge(key=handler_id, body=body, patch=patch)
            for subref in handler_state.subrefs:
                storage.purge(key=subref, body=body, patch=patch)
        storage.flush()

    def __len__(self) -> int:
        return len(self._states)

    def __iter__(self) -> Iterator[ids.HandlerId]:
        return iter(self._states)

    def __getitem__(self, item: ids.HandlerId) -> HandlerState:
        return self._states[item]

    @property
    def done(self) -> bool:
        # In particular, no handlers means that it is "done" even before doing.
        return all(
            handler_state.finished for handler_state in self._states.values()
            if handler_state.active
        )

    @property
    def extras(self) -> Mapping[str, StateCounters]:
        purposes = {
            handler_state.purpose for handler_state in self._states.values()
            if handler_state.purpose is not None and handler_state.purpose != self.purpose
        }
        counters = {
            purpose: StateCounters(
                success=len([1 for handler_state in self._states.values()
                            if handler_state.purpose == purpose and handler_state.success]),
                failure=len([1 for handler_state in self._states.values()
                            if handler_state.purpose == purpose and handler_state.failure]),
                running=len([1 for handler_state in self._states.values()
                            if handler_state.purpose == purpose and not handler_state.finished]),
            )
            for purpose in purposes
        }
        return counters

    @property
    def counts(self) -> StateCounters:
        purposeful_states = [
            handler_state for handler_state in self._states.values()
            if self.purpose is None or handler_state.purpose is None
               or handler_state.purpose == self.purpose
        ]
        return StateCounters(
            success=len([1 for handler_state in purposeful_states if handler_state.success]),
            failure=len([1 for handler_state in purposeful_states if handler_state.failure]),
            running=len([1 for handler_state in purposeful_states if not handler_state.finished]),
        )

    @property
    def delay(self) -> Optional[float]:
        delays = self.delays  # calculate only once, to save bit of CPU
        return min(delays) if delays else None

    @property
    def delays(self) -> Collection[float]:
        """
        Resulting delays for the handlers (only the postponed ones).

        The delays are then reduced to one single sleep in the top-level
        processing routine, based on all delays of different origin:
        e.g. postponed daemons, stopping daemons, temporarily failed handlers.
        """
        now = datetime.datetime.now(datetime.timezone.utc)
        return [
            max(0, (handler_state.delayed - now).total_seconds()) if handler_state.delayed else 0
            for handler_state in self._states.values()
            if handler_state.active and not handler_state.finished
        ]


def deliver_results(
        *,
        outcomes: Mapping[ids.HandlerId, execution.Outcome],
        patch: patches.Patch,
) -> None:
    """
    Store the results (as returned from the handlers) to the resource.

    This is not the handlers' state persistence, but the results' persistence.

    First, the state persistence is stored under ``.status.kopf.progress``,
    and can (later) be configured to be stored in different fields for different
    operators operating the same objects: ``.status.kopf.{somename}.progress``.
    The handlers' result are stored in the top-level ``.status``.

    Second, the handler results can (also later) be delivered to other objects,
    e.g. to their owners or label-selected related objects. For this, another
    class/module will be added.

    For now, we keep state- and result persistence in one module, but separated.
    """
    for handler_id, outcome in outcomes.items():
        if outcome.exception is not None:
            pass
        elif outcome.result is None:
            pass
        elif isinstance(outcome.result, collections.abc.Mapping):
            # TODO: merge recursively (patch-merge), do not overwrite the keys if they are present.
            patch.setdefault('status', {}).setdefault(handler_id, {}).update(outcome.result)
        else:
            patch.setdefault('status', {})[handler_id] = copy.deepcopy(outcome.result)


@overload
def _format_iso8601(val: None) -> None: ...


@overload
def _format_iso8601(val: datetime.datetime) -> str: ...


def _format_iso8601(val: Optional[datetime.datetime]) -> Optional[str]:
    return None if val is None else val.isoformat(timespec='microseconds')


@overload
def _parse_iso8601(val: None) -> None: ...


@overload
def _parse_iso8601(val: str) -> datetime.datetime: ...


def _parse_iso8601(val: Optional[str]) -> Optional[datetime.datetime]:
    return None if val is None else iso8601.parse_date(val)  # always TZ-aware



================================================
FILE: kopf/_core/actions/throttlers.py
================================================
import asyncio
import contextlib
import dataclasses
import time
from collections.abc import AsyncGenerator, Iterable, Iterator
from typing import Optional, Union

from kopf._cogs.aiokits import aiotime
from kopf._cogs.helpers import typedefs


@dataclasses.dataclass(frozen=False)
class Throttler:
    """ A state of throttling for one specific purpose (there can be a few). """
    source_of_delays: Optional[Iterator[float]] = None
    last_used_delay: Optional[float] = None
    active_until: Optional[float] = None  # internal clock


@contextlib.asynccontextmanager
async def throttled(
        *,
        throttler: Throttler,
        delays: Iterable[float],
        wakeup: Optional[asyncio.Event] = None,
        logger: typedefs.Logger,
        errors: Union[type[BaseException], tuple[type[BaseException], ...]] = Exception,
) -> AsyncGenerator[bool, None]:
    """
    A helper to throttle any arbitrary operation.
    """

    # The 1st sleep: if throttling is already active, but was interrupted by a queue replenishment.
    # It is needed to properly process the latest known event after the successful sleep.
    if throttler.active_until is not None:
        remaining_time = throttler.active_until - time.monotonic()
        unslept_time = await aiotime.sleep(remaining_time, wakeup=wakeup)
        if unslept_time is None:
            logger.info("Throttling is over. Switching back to normal operations.")
            throttler.active_until = None

    # Run only if throttling either is not active initially, or has just finished sleeping.
    should_run = throttler.active_until is None
    try:
        yield should_run

    except Exception as e:

        # If it is not an error-of-interest, escalate normally. BaseExceptions are escalated always.
        if not isinstance(e, errors):
            raise

        # If the code does not follow the recommendation to not run, escalate.
        if not should_run:
            raise

        # Activate throttling if not yet active, or reuse the active sequence of delays.
        if throttler.source_of_delays is None:
            throttler.source_of_delays = iter(delays)

        # Choose a delay. If there are none, avoid throttling at all.
        delay = next(throttler.source_of_delays, throttler.last_used_delay)
        if delay is not None:
            throttler.last_used_delay = delay
            throttler.active_until = time.monotonic() + delay
            logger.exception(f"Throttling for {delay} seconds due to an unexpected error: {e!r}")

    else:
        # Reset the throttling. Release the iterator to keep the memory free during normal run.
        if should_run:
            throttler.source_of_delays = throttler.last_used_delay = None

    # The 2nd sleep: if throttling has been just activated (i.e. there was a fresh error).
    # It is needed to have better logging/sleeping without workers exiting for "no events".
    if throttler.active_until is not None and should_run:
        remaining_time = throttler.active_until - time.monotonic()
        unslept_time = await aiotime.sleep(remaining_time, wakeup=wakeup)
        if unslept_time is None:
            throttler.active_until = None
            logger.info("Throttling is over. Switching back to normal operations.")



================================================
FILE: kopf/_core/engines/__init__.py
================================================
"""
Engines are things that run around the reactor (see `kopf._core.reactor`)
to help it to function at full strength, but are not part of it.
For example, all never-ending side-tasks for peering and k8s-event-posting.

The reactor and engines exchange the state with each other (bi-directionally)
via the provided synchronization objects, usually asyncio events & queues.
"""



================================================
FILE: kopf/_core/engines/activities.py
================================================
"""
Supporting tasks for startup/cleanup and to keep the operator functional.

Consumes a credentials vault, and monitors that it has enough credentials.
When the credentials are invalidated (i.e. excluded), run the re-authentication
activity and populates with the new credentials (fully or partially).

The process is intentionally split into multiple packages:

* Authenticating background task (this module) is a part of the reactor,
  as it will not be able to run without up-to-date credentials,
  and since it initiates the reactor's activities and invokes the handlers.
* Vault are the data structures used mostly in the API clients wrappers
  (which are the low-level modules, so they cannot import the credentials
  from the high-level modules such as the reactor/engines).
* Specific authentication methods, such as the authentication piggybacking,
  belong to neither the reactor, nor the engines, nor the client wrappers.
"""
import logging
from collections.abc import Mapping, MutableMapping
from typing import NoReturn

from kopf._cogs.aiokits import aiotime
from kopf._cogs.configs import configuration
from kopf._cogs.structs import credentials, ephemera, ids
from kopf._core.actions import execution, lifecycles, progression
from kopf._core.intents import causes, registries

logger = logging.getLogger(__name__)


class ActivityError(Exception):
    """ An error in the activity, as caused by mandatory handlers' failures. """

    def __init__(
            self,
            msg: str,
            *,
            outcomes: Mapping[ids.HandlerId, execution.Outcome],
    ) -> None:
        super().__init__(msg)
        self.outcomes = outcomes


async def authenticator(
        *,
        registry: registries.OperatorRegistry,
        settings: configuration.OperatorSettings,
        indices: ephemera.Indices,
        vault: credentials.Vault,
        memo: ephemera.AnyMemo,
) -> NoReturn:
    """ Keep the credentials forever up to date. """
    counter: int = 0 if vault.is_empty() else 1
    while True:
        await authenticate(
            registry=registry,
            settings=settings,
            indices=indices,
            vault=vault,
            memo=memo,
            _activity_title="Re-authentication" if counter else "Initial authentication",
        )
        counter += 1


async def authenticate(
        *,
        registry: registries.OperatorRegistry,
        settings: configuration.OperatorSettings,
        indices: ephemera.Indices,
        vault: credentials.Vault,
        memo: ephemera.AnyMemo,
        _activity_title: str = "Authentication",
) -> None:
    """ Retrieve the credentials once, successfully or not, and exit. """
    # We do not need the locks protection here. There is only one activity for vault population.
    # Even with 2+ activities, if the vault is empty, all consumers will be blocked by waiting.
    # The API clients wake up only on the final population with the internal lock protection.

    # Sleep most of the time waiting for a signal to re-auth.
    await vault.wait_for_emptiness()

    # Log initial and re-authentications differently, for readability.
    logger.info(f"{_activity_title} has been initiated.")

    activity_results = await run_activity(
        lifecycle=lifecycles.all_at_once,
        registry=registry,
        settings=settings,
        activity=causes.Activity.AUTHENTICATION,
        indices=indices,
        memo=memo,
    )

    if activity_results:
        logger.info(f"{_activity_title} has finished.")
    else:
        logger.warning(f"{_activity_title} has failed: "
                       f"no credentials were retrieved from the login handlers.")

    # Feed the credentials into the vault, and unfreeze the re-authenticating clients.
    await vault.populate({str(handler_id): info for handler_id, info in activity_results.items()})


async def run_activity(
        *,
        lifecycle: execution.LifeCycleFn,
        registry: registries.OperatorRegistry,
        settings: configuration.OperatorSettings,
        activity: causes.Activity,
        indices: ephemera.Indices,
        memo: ephemera.AnyMemo,
) -> Mapping[ids.HandlerId, execution.Result]:
    logger = logging.getLogger(f'kopf.activities.{activity.value}')

    # For the activity handlers, we have neither bodies, nor patches, just the state.
    cause = causes.ActivityCause(
        logger=logger,
        activity=activity,
        settings=settings,
        indices=indices,
        memo=memo,
    )
    handlers = registry._activities.get_handlers(activity=activity)
    state = progression.State.from_scratch().with_handlers(handlers)
    outcomes: MutableMapping[ids.HandlerId, execution.Outcome] = {}
    while not state.done:
        current_outcomes = await execution.execute_handlers_once(
            lifecycle=lifecycle,
            settings=settings,
            handlers=handlers,
            cause=cause,
            state=state,
        )
        outcomes.update(current_outcomes)
        state = state.with_outcomes(current_outcomes)
        await aiotime.sleep(state.delay)

    # Activities assume that all handlers must eventually succeed.
    # We raise from the 1st exception only: just to have something real in the tracebacks.
    # For multiple handlers' errors, the logs should be investigated instead.
    exceptions = [outcome.exception
                  for outcome in outcomes.values()
                  if outcome.exception is not None]
    if exceptions:
        raise ActivityError("One or more handlers failed.", outcomes=outcomes) from exceptions[0]

    # If nothing has failed, we return identifiable results. The outcomes/states are internal.
    # The order of results is not guaranteed (the handlers can succeed on one of the retries).
    results = {handler_id: outcome.result
               for handler_id, outcome in outcomes.items()
               if outcome.result is not None}
    return results



================================================
FILE: kopf/_core/engines/admission.py
================================================
import abc
import asyncio
import base64
import copy
import json
import logging
import re
import urllib.parse
from collections.abc import Collection, Iterable, Mapping
from typing import Any, AsyncContextManager, Literal, Optional, TypedDict

from kopf._cogs.aiokits import aiovalues
from kopf._cogs.clients import creating, errors, patching
from kopf._cogs.configs import configuration
from kopf._cogs.structs import bodies, diffs, ephemera, ids, patches, references, reviews
from kopf._core.actions import execution, lifecycles, loggers, progression
from kopf._core.intents import causes, filters, handlers, registries

logger = logging.getLogger(__name__)


class AdmissionError(execution.PermanentError):
    """
    Raised by admission handlers when an API operation under check is bad.

    An admission error behaves the same as `kopf.PermanentError`, but provides
    admission-specific payload for the response: a message & a numeric code.

    This error type is preferred when selecting only one error to report back
    to apiservers as the admission review result -- in case multiple handlers
    are called in one admission request, i.e. when the webhook endpoints
    are not mapped to the handler ids (e.g. when configured manually).
    """
    def __init__(
            self,
            message: Optional[str] = '',
            code: Optional[int] = 500,
    ) -> None:
        super().__init__(message)
        self.code = code


class WebhookError(Exception):
    """
    Raised when a webhook request is bad, not an API operation under check.
    """


class MissingDataError(WebhookError):
    """ An admission is requested but some expected data are missing. """


class UnknownResourceError(WebhookError):
    """ An admission is made for a resource that the operator does not have. """


class AmbiguousResourceError(WebhookError):
    """ An admission is made for one resource, but we (somehow) found a few. """


class MemoGetter(metaclass=abc.ABCMeta):
    """
    An interface as a way to break the reversed dependency of modules:

    * The lower-level admission engine needs `Memories` for memos.
    * The memories are implemented in the higher-level `reactor.inventory`.
    * The inventory must be there in the high-level reactor because
      it requires specialised memory classes from `daemons`, `indexing`, etc.
    * And the inventory cannot be shifted down from the reactor to engines
      because it is not an engine semantically.

    Implemented by `inventory.Memories` or by any of its views.
    """
    @abc.abstractmethod
    async def recall_memo(
            self,
            raw_body: bodies.RawBody,
            *,
            memobase: Optional[ephemera.AnyMemo] = None,
            ephemeral: bool = False,
    ) -> ephemera.AnyMemo:
        raise NotImplementedError


async def serve_admission_request(
        # Required for all webhook servers, meaningless without it:
        request: reviews.Request,
        *,
        # Optional for webhook servers that can recognise this information:
        headers: Optional[Mapping[str, str]] = None,
        sslpeer: Optional[Mapping[str, Any]] = None,
        webhook: Optional[ids.HandlerId] = None,
        reason: Optional[causes.WebhookType] = None,  # TODO: undocumented: requires typing clarity!
        # Injected by partial() from spawn_tasks():
        settings: configuration.OperatorSettings,
        memories: MemoGetter,
        memobase: ephemera.AnyMemo,
        registry: registries.OperatorRegistry,
        insights: references.Insights,
        indices: ephemera.Indices,
) -> reviews.Response:
    """
    The actual and the only implementation of the `WebhookFn` protocol.

    This function is passed to all webhook servers/tunnels to be called
    whenever a new admission request is received.

    Some parameters are provided by the framework itself via partial binding,
    so that the resulting function matches the `WebhookFn` protocol. Other
    parameters are passed by the webhook servers when they call the function.
    """

    # Reconstruct the cause specially for web handlers.
    resource = find_resource(request=request, insights=insights)
    subresource = request.get('request', {}).get('subResource')
    operation = request.get('request', {}).get('operation')
    userinfo = request.get('request', {}).get('userInfo')
    new_body = request.get('request', {}).get('object')
    old_body = request.get('request', {}).get('oldObject')
    raw_body = new_body if new_body is not None else old_body
    if userinfo is None:
        raise MissingDataError("User info is missing from the admission request.")
    if raw_body is None:
        raise MissingDataError("Either old or new object is missing from the admission request.")

    memo = await memories.recall_memo(raw_body, memobase=memobase, ephemeral=operation=='CREATE')
    body = bodies.Body(raw_body)
    old = bodies.Body(old_body) if old_body is not None else None
    new = bodies.Body(new_body) if new_body is not None else None
    diff = diffs.diff(old, new)
    patch = patches.Patch(body=raw_body)
    warnings: list[str] = []
    cause = causes.WebhookCause(
        resource=resource,
        indices=indices,
        logger=loggers.LocalObjectLogger(body=body, settings=settings),
        patch=patch,
        memo=memo,
        body=body,
        userinfo=userinfo,
        warnings=warnings,
        operation=operation,
        subresource=subresource,
        dryrun=bool(request.get('request', {}).get('dryRun')),
        sslpeer=sslpeer if sslpeer is not None else {},  # ensure a mapping even if not provided.
        headers=headers if headers is not None else {},  # ensure a mapping even if not provided.
        webhook=webhook,
        reason=reason,
        old=old,
        new=new,
        diff=diff,
    )

    # Retrieve the handlers to be executed; maybe only one if the webhook server provides a hint.
    handlers_ = registry._webhooks.get_handlers(cause)
    state = progression.State.from_scratch().with_handlers(handlers_)
    outcomes = await execution.execute_handlers_once(
        lifecycle=lifecycles.all_at_once,
        settings=settings,
        handlers=handlers_,
        cause=cause,
        state=state,
        default_errors=execution.ErrorsMode.PERMANENT,
    )

    # Construct the response as per Kubernetes's conventions and expectations.
    response = build_response(
        request=request,
        outcomes=outcomes,
        warnings=warnings,
        jsonpatch=patch.as_json_patch(),
    )
    return response


def find_resource(
        *,
        request: reviews.Request,
        insights: references.Insights,
) -> references.Resource:
    """
    Identify the requested resource by its meta-information (as discovered).
    """
    # NB: Absent keys in the request are not acceptable, they must be provided.
    request_payload: reviews.RequestPayload = request['request']
    request_resource: reviews.RequestResource = request_payload['resource']
    group = request_resource['group']
    version = request_resource['version']
    plural = request_resource['resource']
    selector = references.Selector(group=group, version=version, plural=plural)
    resources = selector.select(insights.webhook_resources)
    if not resources:
        raise UnknownResourceError(f"The specified resource has no handlers: {request_resource}")
    elif len(resources) > 1:
        raise AmbiguousResourceError(f"The specified resource is ambiguous: {request_resource}")
    else:
        return list(resources)[0]


def build_response(
        *,
        request: reviews.Request,
        outcomes: Mapping[ids.HandlerId, execution.Outcome],
        warnings: Collection[str],
        jsonpatch: patches.JSONPatch,
) -> reviews.Response:
    """
    Construct the admission review response to a review request.
    """
    allowed = all(outcome.exception is None for id, outcome in outcomes.items())
    response = reviews.Response(
        apiVersion=request.get('apiVersion', 'admission.k8s.io/v1'),
        kind=request.get('kind', 'AdmissionReview'),
        response=reviews.ResponsePayload(
            uid=request.get('request', {}).get('uid', ''),
            allowed=allowed))
    if warnings:
        response['response']['warnings'] = [str(warning) for warning in warnings]
    if jsonpatch:
        encoded_patch: str = base64.b64encode(json.dumps(jsonpatch).encode('utf-8')).decode('ascii')
        response['response']['patch'] = encoded_patch
        response['response']['patchType'] = 'JSONPatch'

    # Prefer specialised admission errors to all other errors, Kopf's own errors to arbitrary ones.
    errors = [outcome.exception for outcome in outcomes.values() if outcome.exception is not None]
    errors.sort(key=lambda error: (
        0 if isinstance(error, AdmissionError) else
        1 if isinstance(error, execution.PermanentError) else
        2 if isinstance(error, execution.TemporaryError) else
        9
    ))
    if errors:
        response['response']['status'] = reviews.ResponseStatus(
            message=str(errors[0]) or repr(errors[0]),
            code=(errors[0].code if isinstance(errors[0], AdmissionError) else None) or 500,
        )
    return response


async def admission_webhook_server(
        *,
        settings: configuration.OperatorSettings,
        registry: registries.OperatorRegistry,
        insights: references.Insights,
        webhookfn: reviews.WebhookFn,
        container: aiovalues.Container[reviews.WebhookClientConfig],
) -> None:

    # Verify that the operator is configured properly (after the startup activities are done).
    has_admission = bool(registry._webhooks.get_all_handlers())
    if settings.admission.server is None and has_admission:
        raise Exception(
            "Admission handlers exist, but no admission server/tunnel is configured "
            "in `settings.admission.server`. "
            "More: https://kopf.readthedocs.io/en/stable/admission/")

    # Do not start the endpoints until resources are scanned.
    # Otherwise, we generate 404 "Not Found" for requests that arrive too early.
    await insights.ready_resources.wait()

    # Communicate all the client configs the server yields: both the initial one and the updates.
    # On each such change, the configuration manager will wake up and reconfigure the webhooks.
    if settings.admission.server is None:
        await asyncio.Event().wait()
    elif isinstance(settings.admission.server, AsyncContextManager):
        async with settings.admission.server as server:
            async for client_config in server(webhookfn):
                await container.set(client_config)
    else:
        async for client_config in settings.admission.server(webhookfn):
            await container.set(client_config)


async def validating_configuration_manager(
        *,
        registry: registries.OperatorRegistry,
        settings: configuration.OperatorSettings,
        insights: references.Insights,
        container: aiovalues.Container[reviews.WebhookClientConfig],
) -> None:
    await configuration_manager(
        reason=causes.WebhookType.VALIDATING,
        selector=references.VALIDATING_WEBHOOK,
        registry=registry, settings=settings,
        insights=insights, container=container,
    )


async def mutating_configuration_manager(
        *,
        registry: registries.OperatorRegistry,
        settings: configuration.OperatorSettings,
        insights: references.Insights,
        container: aiovalues.Container[reviews.WebhookClientConfig],
) -> None:
    await configuration_manager(
        reason=causes.WebhookType.MUTATING,
        selector=references.MUTATING_WEBHOOK,
        registry=registry, settings=settings,
        insights=insights, container=container,
    )


async def configuration_manager(
        *,
        reason: causes.WebhookType,
        selector: references.Selector,
        registry: registries.OperatorRegistry,
        settings: configuration.OperatorSettings,
        insights: references.Insights,
        container: aiovalues.Container[reviews.WebhookClientConfig],
) -> None:
    """
    Manage the webhook configurations dynamically.

    This is one of an operator's root tasks that run forever.
    If exited, the whole operator exits as by an error.

    The manager waits for changes in one of these:

    * Observed resources in the cluster (via insights).
    * A new webhook client config yielded by the webhook server.

    On either of these occasion, the manager rebuilds the webhook configuration
    and applies it to the specified configuration resources in the cluster
    (for which it needs some RBAC permissions).
    Besides, it also creates an webhook configuration resource if it is absent.
    """

    # Do nothing if not managed. The root task cannot be skipped from creation,
    # since the managed mode is only set at the startup activities.
    if settings.admission.managed is None:
        await asyncio.Event().wait()
        return

    # Wait until the prerequisites for managing are available (scanned from the cluster).
    await insights.ready_resources.wait()
    resource = await insights.backbone.wait_for(selector)
    all_handlers = registry._webhooks.get_all_handlers()
    all_handlers = [h for h in all_handlers if h.reason == reason]

    # Optionally (if configured), pre-create the configuration objects if they are absent.
    # Use the try-or-fail strategy instead of check-and-do -- to reduce the RBAC requirements.
    try:
        await creating.create_obj(
            settings=settings,
            resource=resource,
            logger=logger,
            name=settings.admission.managed,
        )
    except errors.APIConflictError:
        pass  # exists already
    except errors.APIForbiddenError:
        logger.error(f"Not enough RBAC permissions to create a {resource}.")
        raise

    # Execute either when actually changed (yielded from the webhook server),
    # or the condition is chain-notified (from the insights: on resources/namespaces revision).
    # Ignore inconsistencies: they are expected -- the server fills the defaults.
    client_config: Optional[reviews.WebhookClientConfig] = None
    try:
        async for client_config in container.as_changed():
            logger.info(f"Reconfiguring the {reason.value} webhook {settings.admission.managed}.")
            webhooks = build_webhooks(
                all_handlers,
                resources=insights.webhook_resources,
                name_suffix=settings.admission.managed,
                client_config=client_config)
            await patching.patch_obj(
                settings=settings,
                resource=resource,
                namespace=None,
                name=settings.admission.managed,
                patch=patches.Patch({'webhooks': webhooks}),
                logger=logger,
            )
    finally:
        # Attempt to remove all managed webhooks, except for the strict ones.
        if client_config is not None:
            logger.info(f"Cleaning up the admission webhook {settings.admission.managed}.")
            webhooks = build_webhooks(
                all_handlers,
                resources=insights.webhook_resources,
                name_suffix=settings.admission.managed,
                client_config=client_config,
                persistent_only=True)
            await patching.patch_obj(
                settings=settings,
                resource=resource,
                namespace=None,
                name=settings.admission.managed,
                patch=patches.Patch({'webhooks': webhooks}),
                logger=logger,
            )


def build_webhooks(
        handlers_: Iterable[handlers.WebhookHandler],
        *,
        resources: Iterable[references.Resource],
        name_suffix: str,
        client_config: reviews.WebhookClientConfig,
        persistent_only: bool = False,
) -> list[dict[str, Any]]:
    """
    Construct the content for ``[Validating|Mutating]WebhookConfiguration``.

    This function concentrates all conventions how Kopf manages the webhook.
    """
    return [
        {
            'name': _normalize_name(handler.id, suffix=name_suffix),
            'sideEffects': 'NoneOnDryRun' if handler.side_effects else 'None',
            'failurePolicy': 'Ignore' if handler.ignore_failures else 'Fail',
            'matchPolicy': 'Equivalent',
            'rules': [
                {
                    'apiGroups': [resource.group],
                    'apiVersions': [resource.version],
                    'resources': (
                        [resource.plural] if handler.subresource is None else
                        [f'{resource.plural}/{handler.subresource}']
                    ),
                    'operations': list(handler.operations or ['*']),
                    'scope': '*',  # doesn't matter since a specific resource is used.
                }
                for resource in resources
                if handler.selector is not None  # None is used only in sub-handlers, ignore here.
                if handler.selector.check(resource)
            ],
            'objectSelector': _build_labels_selector(handler.labels),
            'clientConfig': _inject_handler_id(client_config, handler.id),
            'timeoutSeconds': 30,  # a permitted maximum is 30.
            'admissionReviewVersions': ['v1', 'v1beta1'],  # only those understood by Kopf itself.
        }
        for handler in handlers_
        if not persistent_only or handler.persistent
    ]


class MatchExpression(TypedDict, total=False):
    key: str
    operator: Literal['Exists', 'DoesNotExist', 'In', 'NotIn']
    values: Optional[Collection[str]]


def _build_labels_selector(labels: Optional[filters.MetaFilter]) -> Optional[Mapping[str, Any]]:
    # https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#resources-that-support-set-based-requirements
    exprs: Collection[MatchExpression] = [
        {'key': key, 'operator': 'Exists'} if val is filters.MetaFilterToken.PRESENT else
        {'key': key, 'operator': 'DoesNotExist'} if val is filters.MetaFilterToken.ABSENT else
        {'key': key, 'operator': 'In', 'values': [str(val)]}
        for key, val in (labels or {}).items()
        if not callable(val)
    ]
    return {'matchExpressions': exprs} if exprs else None


BAD_WEBHOOK_NAME = re.compile(r'[^\w\d\.-]')


def _normalize_name(id: ids.HandlerId, suffix: str) -> str:
    """
    Normalize the webhook name to what Kubernetes accepts as normal.

    The restriction is: *a lowercase RFC 1123 subdomain must consist
    of lower case alphanumeric characters, \'-\' or \'.\',
    and must start and end with an alphanumeric character.*

    The actual name is not that important, it is for informational purposes
    only. In the managed configurations, it will be rewritten every time.
    """
    name = f'{id}'.replace('/', '.').replace('_', '-')  # common cases, for beauty
    name = BAD_WEBHOOK_NAME.sub(lambda s: s.group(0).encode('utf-8').hex(), name)  # uncommon cases
    return f'{name}.{suffix}' if suffix else name


def _inject_handler_id(config: reviews.WebhookClientConfig, id: ids.HandlerId) -> reviews.WebhookClientConfig:
    config = copy.deepcopy(config)

    url_id = urllib.parse.quote(id)
    url = config.get('url')
    if url is not None:
        config['url'] = f'{url.rstrip("/")}/{url_id}'

    service = config.get('service')
    if service is not None:
        path = service.get('path', '')
        service['path'] = f"{path}/{url_id}"

    return config



================================================
FILE: kopf/_core/engines/daemons.py
================================================
"""
Daemons are background tasks accompanying the individual resource objects.

Every ``@kopf.daemon`` and ``@kopf.timer`` handler produces a separate
asyncio task to either directly execute the daemon, or to trigger one-shot
handlers by schedule. The wrapping tasks are always async; the sync functions
are called in thread executors as part of a regular handler invocation.

These tasks are remembered in the per-resources *memories* (arbitrary data
containers) through the life-cycle of the operator.

Since the operators are event-driven conceptually, there are no background tasks
running for every individual resources normally (i.e. without the daemons),
so there are no connectors between the operator's root tasks and the daemons,
so there is no way to stop/kill/cancel the daemons when the operator exits.

For this, there is an artificial root task spawned to kill all the daemons
when the operator exits, and all root tasks are gracefully/forcedly terminated.
Otherwise, all the daemons would be considered as "hung" tasks and will be
force-killed after some timeout -- which can be avoided, since we are aware
of the daemons, and they are not actually "hung".
"""
import abc
import asyncio
import dataclasses
import time
import warnings
from collections.abc import Collection, Iterable, Mapping, MutableMapping, Sequence
from typing import Optional

from kopf._cogs.aiokits import aiotasks, aiotime, aiotoggles
from kopf._cogs.configs import configuration
from kopf._cogs.helpers import typedefs
from kopf._cogs.structs import bodies, ids, patches
from kopf._core.actions import application, execution, lifecycles, loggers, progression
from kopf._core.intents import causes, handlers as handlers_, stoppers


@dataclasses.dataclass(frozen=True)
class Daemon:
    task: aiotasks.Task  # a guarding task of the daemon.
    logger: typedefs.Logger
    handler: handlers_.SpawningHandler
    stopper: stoppers.DaemonStopper  # a signaller for the termination and its reason.


@dataclasses.dataclass(frozen=False)
class DaemonsMemory:
    # For background and timed threads/tasks (invoked with the kwargs of the last-seen body).
    live_fresh_body: Optional[bodies.Body] = None
    idle_reset_time: float = dataclasses.field(default_factory=time.monotonic)
    forever_stopped: set[ids.HandlerId] = dataclasses.field(default_factory=set)
    running_daemons: dict[ids.HandlerId, Daemon] = dataclasses.field(default_factory=dict)


class DaemonsMemoriesIterator(metaclass=abc.ABCMeta):
    """
    Re-iterable view of all the running daemons for the `daemon_killer`.

    Implemented in `Memories`. This is a clean hack to resolve circular imports
    (the daemon killer needs memories, but the memories contain Daemon records)
    by splitting the specialised interface (this class) from the implementation.
    """
    @abc.abstractmethod
    def iter_all_daemon_memories(self) -> Iterable[DaemonsMemory]:
        raise NotImplementedError


async def spawn_daemons(
        *,
        settings: configuration.OperatorSettings,
        handlers: Sequence[handlers_.SpawningHandler],
        daemons: MutableMapping[ids.HandlerId, Daemon],
        cause: causes.SpawningCause,
        memory: DaemonsMemory,
) -> Collection[float]:
    """
    Ensure that all daemons are spawned for this individual resource.

    This function can be called multiple times on multiple handling cycles
    (though usually should be called on the first-seen occasion), so it must
    be idempotent: not having duplicating side-effects on multiple calls.
    """
    if memory.live_fresh_body is None:  # for type-checking; "not None" is ensured in processing.
        raise RuntimeError("A daemon is spawned with None as body. This is a bug. Please report.")
    for handler in handlers:
        if handler.id not in daemons:
            stopper = stoppers.DaemonStopper()
            daemon_cause = causes.DaemonCause(
                resource=cause.resource,
                indices=cause.indices,
                logger=cause.logger,
                memo=cause.memo,
                body=memory.live_fresh_body,
                patch=patches.Patch(),  # not the same as the one-shot spawning patch!
                stopper=stopper,  # for checking (passed to kwargs)
            )
            daemon = Daemon(
                stopper=stopper,  # for stopping (outside of causes)
                handler=handler,
                logger=loggers.LocalObjectLogger(body=cause.body, settings=settings),
                task=asyncio.create_task(_runner(
                    settings=settings,
                    daemons=daemons,  # for self-garbage-collection
                    handler=handler,
                    cause=daemon_cause,
                    memory=memory,
                ), name=f'runner of {handler.id}'),  # sometimes, daemons; sometimes, timers.
            )
            daemons[handler.id] = daemon
    return []


async def match_daemons(
        *,
        settings: configuration.OperatorSettings,
        handlers: Sequence[handlers_.SpawningHandler],
        daemons: MutableMapping[ids.HandlerId, Daemon],
) -> Collection[float]:
    """
    Re-match the running daemons with the filters, and stop those mismatching.

    Stopping can take a few iterations, same as `stop_daemons` would do.
    """
    matching_daemon_ids = {handler.id for handler in handlers}
    mismatching_daemons = {
        daemon.handler.id: daemon
        for daemon in daemons.values()
        if daemon.handler.id not in matching_daemon_ids
    }
    delays = await stop_daemons(
        settings=settings,
        daemons=mismatching_daemons,
        reason=stoppers.DaemonStoppingReason.FILTERS_MISMATCH,
    )
    return delays


async def stop_daemons(
        *,
        settings: configuration.OperatorSettings,
        daemons: Mapping[ids.HandlerId, Daemon],
        reason: stoppers.DaemonStoppingReason = stoppers.DaemonStoppingReason.RESOURCE_DELETED,
) -> Collection[float]:
    """
    Terminate all daemons of an individual resource (gracefully and by force).

    All daemons are terminated in parallel to speed up the termination
    (especially taking into account that some daemons can take time to finish).

    The daemons are asked to terminate as soon as the object is marked
    for deletion. It can take some time until the deletion handlers also
    finish their work. The object is not physically deleted until all
    the daemons are terminated (by putting a finalizer on it).

    **Notes on this non-trivial implementation:**

    There is a same-purpose function `stop_daemon`, which works fully in-memory.
    That method is used when killing the daemons on operator exit.
    This method is used when the resource is deleted.

    The difference is that in this method (termination with delays and patches),
    other on-deletion handlers can be happening at the same time as the daemons
    are being terminated (it can take time due to backoffs and timeouts).
    In the end, the finalizer should be removed only once all deletion handlers
    have succeeded and all daemons are terminated -- not earlier than that.
    None of this (handlers and finalizers) is needed for the operator exiting.

    To know "when" the next check of daemons should be performed:

    * EITHER the operator should block this resource's processing and wait until
      the daemons are terminated -- thus leaking daemon's abstractions and logic
      and tools (e.g. a task scheduler) to the upper level of processing;

    * OR the daemons termination should mimic the change-detection handlers
      and simulate the delays with multiple handling cycles -- in order to
      re-check the daemon's status regularly until they are done.

    Both of this approaches have the same complexity. But the latter one
    keep the logic isolated into the daemons module/routines (a bit cleaner).

    Hence, these duplicating methods of termination for different cases
    (as by their surrounding circumstances: deletion handlers and finalizers).
    """
    delays: list[float] = []
    now = time.monotonic()
    for daemon in list(daemons.values()):
        logger = daemon.logger
        stopper = daemon.stopper
        age = (now - (stopper.when or now))

        handler = daemon.handler
        if isinstance(handler, handlers_.DaemonHandler):
            backoff = handler.cancellation_backoff
            timeout = handler.cancellation_timeout
            polling = handler.cancellation_polling or settings.background.cancellation_polling
        elif isinstance(handler, handlers_.TimerHandler):
            backoff = None
            timeout = None
            polling = settings.background.cancellation_polling
        else:
            raise RuntimeError(f"Unsupported daemon handler: {handler!r}")

        # Whatever happens with other flags & logs & timings, this flag must be surely set.
        if not stopper.is_set(reason=reason):
            stopper.set(reason=reason)
            await _wait_for_instant_exit(settings=settings, daemon=daemon)

        # Try different approaches to exiting the daemon based on timings.
        if daemon.task.done():
            pass  # same as if the daemon is not in the structure anymore (self-deleted on exit).

        elif backoff is not None and age < backoff:
            if not stopper.is_set(reason=stoppers.DaemonStoppingReason.DAEMON_SIGNALLED):
                stopper.set(reason=stoppers.DaemonStoppingReason.DAEMON_SIGNALLED)
                logger.debug(f"{handler} is signalled to exit gracefully.")
                await _wait_for_instant_exit(settings=settings, daemon=daemon)
            if not daemon.task.done():  # due to "instant exit"
                delays.append(backoff - age)

        elif timeout is not None and age < timeout + (backoff or 0):
            if not stopper.is_set(reason=stoppers.DaemonStoppingReason.DAEMON_CANCELLED):
                stopper.set(reason=stoppers.DaemonStoppingReason.DAEMON_CANCELLED)
                logger.debug(f"{handler} is signalled to exit by force.")
                daemon.task.cancel()
                await _wait_for_instant_exit(settings=settings, daemon=daemon)
            if not daemon.task.done():  # due to "instant exit"
                delays.append(timeout + (backoff or 0) - age)

        elif timeout is not None:
            if not stopper.is_set(reason=stoppers.DaemonStoppingReason.DAEMON_ABANDONED):
                stopper.set(reason=stoppers.DaemonStoppingReason.DAEMON_ABANDONED)
                logger.warning(f"{handler} did not exit in time. Leaving it orphaned.")
                warnings.warn(f"{handler} did not exit in time.", ResourceWarning)

        else:
            logger.debug(f"{handler} is still exiting. The next check is in {polling} seconds.")
            delays.append(polling)

    return delays


async def daemon_killer(
        *,
        settings: configuration.OperatorSettings,
        memories: DaemonsMemoriesIterator,
        operator_paused: aiotoggles.ToggleSet,
) -> None:
    """
    An operator's root task to kill the daemons on the operator's demand.

    The "demand" comes in two cases: when the operator is exiting (gracefully
    or not), and when the operator is pausing because of peering. In that case,
    all watch-streams are disconnected, and all daemons/timers should stop.

    When pausing, the daemons/timers are stopped via their regular stopping
    procedure: with graceful or forced termination, backoffs, timeouts.

    .. warning::

        Each daemon will be respawned on the next K8s watch-event strictly
        after the previous daemon is fully stopped.
        There are never 2 instances of the same daemon running in parallel.

        In normal cases (enough time is given to stop), this is usually done
        by the post-pause re-listing event. In rare cases when the re-pausing
        happens faster than the daemon is stopped (highly unlikely to happen),
        that event can be missed because the daemon is being stopped yet,
        so the respawn can happen with a significant delay.

        This issue is considered low-priority & auxiliary, so as the peering
        itself. It can be fixed later. Workaround: make daemons to exit fast.
    """
    # Unlimited job pool size —- the same as if we would be managing the tasks directly.
    # Unlimited timeout in `close()` -- since we have our own per-daemon timeout management.
    scheduler = aiotasks.Scheduler()
    try:
        while True:

            # Stay here while the operator is running normally, until it is paused.
            await operator_paused.wait_for(True)

            # The stopping tasks are "fire-and-forget" -- we do not get (or care of) the result.
            # The daemons remain resumable, since they exit not on their own accord.
            for memory in memories.iter_all_daemon_memories():
                for daemon in memory.running_daemons.values():
                    await scheduler.spawn(
                        name=f"pausing stopper of {daemon}",
                        coro=stop_daemon(
                            settings=settings,
                            daemon=daemon,
                            reason=stoppers.DaemonStoppingReason.OPERATOR_PAUSING))

            # Stay here while the operator is paused, until it is resumed.
            # The fresh stream of watch-events will spawn new daemons naturally.
            await operator_paused.wait_for(False)

    # Terminate all running daemons when the operator exits (and this task is cancelled).
    finally:
        for memory in memories.iter_all_daemon_memories():
            for daemon in memory.running_daemons.values():
                await scheduler.spawn(
                    name=f"exiting stopper of {daemon}",
                    coro=stop_daemon(
                        settings=settings,
                        daemon=daemon,
                        reason=stoppers.DaemonStoppingReason.OPERATOR_EXITING))
        await scheduler.wait()  # prevent insta-cancelling our own coros (daemon stoppers).
        await scheduler.close()


async def stop_daemon(
        *,
        settings: configuration.OperatorSettings,
        daemon: Daemon,
        reason: stoppers.DaemonStoppingReason,
) -> None:
    """
    Stop a single daemon.

    The purpose is the same as in `stop_daemons`, but this function
    is called on operator exiting, so there is no multi-step handling,
    everything happens in memory and linearly (while respecting the timing).

    For explanation on different implementations, see `stop_daemons`.
    """
    handler = daemon.handler
    if isinstance(handler, handlers_.DaemonHandler):
        backoff = handler.cancellation_backoff
        timeout = handler.cancellation_timeout
    elif isinstance(handler, handlers_.TimerHandler):
        backoff = None
        timeout = None
    else:
        raise RuntimeError(f"Unsupported daemon handler: {handler!r}")

    # Whatever happens with other flags & logs & timings, this flag must be surely set.
    daemon.stopper.set(reason=reason)
    await _wait_for_instant_exit(settings=settings, daemon=daemon)

    if daemon.task.done():
        daemon.logger.debug(f"{handler} has exited gracefully.")

    # Try different approaches to exiting the daemon based on timings.
    if not daemon.task.done() and backoff is not None:
        daemon.stopper.set(reason=stoppers.DaemonStoppingReason.DAEMON_SIGNALLED)
        daemon.logger.debug(f"{handler} is signalled to exit gracefully.")
        await aiotasks.wait([daemon.task], timeout=backoff)

    if not daemon.task.done() and timeout is not None:
        daemon.stopper.set(reason=stoppers.DaemonStoppingReason.DAEMON_CANCELLED)
        daemon.logger.debug(f"{handler} is signalled to exit by force.")
        daemon.task.cancel()
        await aiotasks.wait([daemon.task], timeout=timeout)

    if not daemon.task.done():
        daemon.stopper.set(reason=stoppers.DaemonStoppingReason.DAEMON_ABANDONED)
        daemon.logger.warning(f"{handler} did not exit in time. Leaving it orphaned.")
        warnings.warn(f"{handler} did not exit in time.", ResourceWarning)


async def _wait_for_instant_exit(
        *,
        settings: configuration.OperatorSettings,
        daemon: Daemon,
) -> None:
    """
    Wait for a kind-of-instant exit of a daemon/timer.

    It might be so, that the daemon exits instantly (if written properly).
    Avoid resource patching and unnecessary handling cycles in this case:
    just give the asyncio event loop an extra time & cycles to finish it.

    There is nothing "instant", of course. Any code takes some time to execute.
    We just assume that the "instant" is something defined by a small timeout
    and a few zero-time asyncio cycles (read as: zero-time `await` statements).
    """

    if daemon.task.done():
        pass

    elif settings.background.instant_exit_timeout is not None:
        await aiotasks.wait([daemon.task], timeout=settings.background.instant_exit_timeout)

    elif settings.background.instant_exit_zero_time_cycles is not None:
        for _ in range(settings.background.instant_exit_zero_time_cycles):
            await asyncio.sleep(0)
            if daemon.task.done():
                break


async def _runner(
        *,
        settings: configuration.OperatorSettings,
        daemons: MutableMapping[ids.HandlerId, Daemon],
        handler: handlers_.SpawningHandler,
        memory: DaemonsMemory,
        cause: causes.DaemonCause,
) -> None:
    """
    Guard a running daemon during its life cycle.

    Note: synchronous daemons are awaited to the exit and postpone cancellation.
    The runner will not exit until the thread exits. See `invoke` for details.
    """
    stopper = cause.stopper

    try:
        if isinstance(handler, handlers_.DaemonHandler):
            await _daemon(settings=settings, handler=handler, cause=cause)
        elif isinstance(handler, handlers_.TimerHandler):
            await _timer(settings=settings, handler=handler, cause=cause, memory=memory)
        else:
            raise RuntimeError("Cannot determine which task wrapper to use. This is a bug.")

    finally:

        # Prevent future re-spawns for those exited on their own, for no reason.
        # Only the filter-mismatching or peering-pausing daemons can be re-spawned.
        if stopper.reason is None:
            memory.forever_stopped.add(handler.id)

        # If this daemon is never going to be called again, we can release the
        # live_fresh_body to save some memory.
        if handler.id in memory.forever_stopped:
            # If any other running daemon is referencing this Kubernetes
            # resource, we can't free it
            can_free = True
            this_daemon = daemons[handler.id]
            for running_daemon in memory.running_daemons.values():
                if running_daemon is not this_daemon:
                    can_free = False
                    break
            if can_free:
                memory.live_fresh_body = None

        # Save the memory by not remembering the exited daemons (they may be never re-spawned).
        del daemons[handler.id]

        # Whatever happened, make sure the sync threads of asyncio threaded executor are notified:
        # in a hope that they will exit maybe some time later to free the OS/asyncio resources.
        # A possible case: operator is exiting and cancelling all "hung" non-root tasks, etc.
        stopper.set(reason=stoppers.DaemonStoppingReason.DONE)


async def _daemon(
        *,
        settings: configuration.OperatorSettings,
        handler: handlers_.DaemonHandler,
        cause: causes.DaemonCause,
) -> None:
    """
    A long-running guarding task for a resource daemon handler.

    The handler is executed either once or repeatedly, based on the handler
    declaration.

    A few kinds of errors are suppressed, those expected from the daemons when
    they are cancelled due to the resource deletion.
    """
    resource = cause.resource
    stopper = cause.stopper
    logger = cause.logger
    patch = cause.patch
    body = cause.body

    if handler.initial_delay is not None:
        await aiotime.sleep(handler.initial_delay, wakeup=cause.stopper.async_event)

    # Similar to activities (in-memory execution), but applies patches on every attempt.
    state = progression.State.from_scratch().with_handlers([handler])
    while not stopper.is_set() and not state.done:

        outcomes = await execution.execute_handlers_once(
            lifecycle=lifecycles.all_at_once,  # there is only one anyway
            settings=settings,
            handlers=[handler],
            cause=cause,
            state=state,
        )
        state = state.with_outcomes(outcomes)
        progression.deliver_results(outcomes=outcomes, patch=patch)
        await application.patch_and_check(
            settings=settings,
            resource=resource,
            logger=logger,
            patch=patch,
            body=body,
        )
        patch.clear()

        # The in-memory sleep does not react to resource changes, but only to stopping.
        if state.delay:
            await aiotime.sleep(state.delay, wakeup=cause.stopper.async_event)

    if stopper.is_set():
        logger.debug(f"{handler} has exited on request and will not be retried or restarted.")
    else:
        logger.debug(f"{handler} has exited on its own and will not be retried or restarted.")


async def _timer(
        *,
        settings: configuration.OperatorSettings,
        handler: handlers_.TimerHandler,
        memory: DaemonsMemory,
        cause: causes.DaemonCause,
) -> None:
    """
    A long-running guarding task for resource timer handlers.

    Each individual handler for each individual k8s-object gets its own task.
    Despite asyncio can schedule the delayed execution of the callbacks
    with ``loop.call_later()`` and ``loop.call_at()``, we do not use them:

    * First, the callbacks are synchronous, making it impossible to patch
      the k8s-objects with the returned results of the handlers.

    * Second, our timers are more sophisticated: they track the last-seen time,
      obey the idle delays, and are instantly terminated/cancelled on the object
      deletion or on the operator exit.

    * Third, sharp timing would require an external timestamp storage anyway,
      which is easier to keep as a local variable inside of a function.

    It is hard to implement all of this with native asyncio timers.
    It is much easier to have an extra task which mostly sleeps,
    but calls the handling functions from time to time.
    """
    resource = cause.resource
    stopper = cause.stopper
    logger = cause.logger
    patch = cause.patch
    body = cause.body

    if handler.initial_delay is not None:
        await aiotime.sleep(handler.initial_delay, wakeup=stopper.async_event)

    # Similar to activities (in-memory execution), but applies patches on every attempt.
    state = progression.State.from_scratch().with_handlers([handler])
    while not stopper.is_set():  # NB: ignore state.done! it is checked below explicitly.

        # Reset success/failure retry counters & timers if it has succeeded. Keep it if failed.
        # Every next invocation of a successful handler starts the retries from scratch (from zero).
        if state.done:
            state = progression.State.from_scratch().with_handlers([handler])

        # Both `now` and `last_seen_time` are moving targets: the last seen time is updated
        # on every watch-event received, and prolongs the sleep. The sleep is never shortened.
        if handler.idle is not None:
            while not stopper.is_set() and time.monotonic() - memory.idle_reset_time < handler.idle:
                delay = memory.idle_reset_time + handler.idle - time.monotonic()
                await aiotime.sleep(delay, wakeup=stopper.async_event)
            if stopper.is_set():
                continue

        # Remember the start time for the sharp timing and idle-time-waster below.
        started = time.monotonic()

        # Execute the handler as usually, in-memory, but handle its outcome on every attempt.
        outcomes = await execution.execute_handlers_once(
            lifecycle=lifecycles.all_at_once,  # there is only one anyway
            settings=settings,
            handlers=[handler],
            cause=cause,
            state=state,
        )
        state = state.with_outcomes(outcomes)
        progression.deliver_results(outcomes=outcomes, patch=patch)
        await application.patch_and_check(
            settings=settings,
            resource=resource,
            logger=logger,
            patch=patch,
            body=body,
        )
        patch.clear()

        # For temporary errors, override the schedule by the one provided by errors themselves.
        # It can be either a delay from TemporaryError, or a backoff for an arbitrary exception.
        if not state.done:
            await aiotime.sleep(state.delays, wakeup=stopper.async_event)

        # For sharp timers, calculate how much time is left to fit the interval grid:
        #       |-----|-----|-----|-----|-----|-----|---> (interval=5, sharp=True)
        #       [slow_handler]....[slow_handler]....[slow...
        elif handler.interval is not None and handler.sharp:
            passed_duration = time.monotonic() - started
            remaining_delay = handler.interval - (passed_duration % handler.interval)
            await aiotime.sleep(remaining_delay, wakeup=stopper.async_event)

        # For regular (non-sharp) timers, simply sleep from last exit to the next call:
        #       |-----|-----|-----|-----|-----|-----|---> (interval=5, sharp=False)
        #       [slow_handler].....[slow_handler].....[slow...
        elif handler.interval is not None:
            await aiotime.sleep(handler.interval, wakeup=stopper.async_event)

        # For idle-only no-interval timers, wait till the next change (i.e. idling reset).
        # NB: This will skip the handler in the same tact (1/64th of a second) even if changed.
        elif handler.idle is not None:
            while memory.idle_reset_time <= started:
                await aiotime.sleep(handler.idle, wakeup=stopper.async_event)

        # Only in case there are no intervals and idling, treat it as a one-shot handler.
        # This makes the handler practically meaningless, but technically possible.
        else:
            break



================================================
FILE: kopf/_core/engines/indexing.py
================================================
import collections.abc
import dataclasses
from collections.abc import Iterable, Iterator, Mapping
from typing import Any, Generic, Optional, TypeVar

from kopf._cogs.configs import configuration
from kopf._cogs.helpers import typedefs
from kopf._cogs.structs import bodies, ephemera, ids, patches, references
from kopf._core.actions import execution, lifecycles, progression
from kopf._core.intents import causes, handlers, registries

Key = tuple[references.Namespace, Optional[str], Optional[str]]
_K = TypeVar('_K')
_V = TypeVar('_V')


class Store(ephemera.Store[_V], Generic[_V]):
    """
    A specific implementation of `.ephemera.Store` usable by inxeders.

    The resources-to-values association is internal and is not exposed
    to handlers or operators. Currently, it is a dictionary
    with the keys of form ``(namespace, name, uid)`` of type `Key`,
    but the implementation can later change without notice.

    The store is O(1) for updates/deletions due to ``dict`` used internally.
    """
    __items: dict[Key, _V]

    def __init__(self) -> None:
        super().__init__()
        self.__items = {}

    def __repr__(self) -> str:
        return repr(list(self.__items.values()))

    def __bool__(self) -> bool:
        return bool(self.__items)

    def __len__(self) -> int:
        return len(self.__items)

    def __iter__(self) -> Iterator[_V]:
        return iter(self.__items.values())

    def __contains__(self, obj: object) -> bool:
        return any(val == obj for val in self.__items.values())

    # Indexers' internal protocol. Must not be used by handlers & operators.
    def _discard(self, acckey: Key) -> None:
        try:
            del self.__items[acckey]
        except KeyError:
            pass  # already absent

    # Indexers' internal protocol. Must not be used by handlers & operators.
    def _replace(self, acckey: Key, obj: _V) -> None:
        # Minimise the dict updates and rehashes for no need: only update if really changed.
        if acckey not in self.__items or self.__items[acckey] != obj:
            self.__items[acckey] = obj


class Index(ephemera.Index[_K, _V], Generic[_K, _V]):
    """
    A specific implementation of `.ephemera.Index` usable by indexers.

    The indexers and all writing interfaces for indices are not exposed
    to handlers or operators or developers, they remain strictly internal.
    Only the read-only indices and stores are exposed.

    The forward index points to the indexed values of one or more objects.
    The lookups are O(1), as Python's dict description promises.

    The reverse index points to the main index's keys where a specific object
    is stored, thus reducing the updates/deletions from O(K) to O(k), where
    "K" is the number of all keys, "k" is the number of keys per object.
    Assuming the amount of keys per object is usually fixed, it is O(1).
    """
    __items: dict[_K, Store[_V]]
    __reverse: dict[Key, set[_K]]

    def __init__(self) -> None:
        super().__init__()
        self.__items = {}
        self.__reverse = {}

    def __repr__(self) -> str:
        return repr(self.__items)

    def __bool__(self) -> bool:
        return bool(self.__items)

    def __len__(self) -> int:
        return len(self.__items)

    def __iter__(self) -> Iterator[_K]:
        return iter(self.__items)

    def __getitem__(self, item: _K) -> Store[_V]:
        return self.__items[item]

    def __contains__(self, item: object) -> bool:  # for performant lookups!
        return item in self.__items

    # Indexers' internal protocol. Must not be used by handlers & operators.
    def _discard(self, acckey: Key, obj_keys: Optional[Iterable[_K]] = None) -> None:
        # We know all the keys where that object is indexed, so we delete only from there.
        # Assume that the reverse/forward indices are consistent. If not, fix it, not "fall back".
        if acckey in self.__reverse:
            obj_keys = obj_keys if obj_keys is not None else self.__reverse[acckey].copy()
            for obj_key in obj_keys:

                # Discard from that store and remove all freshly emptied stores.
                store = self.__items[obj_key]
                store._discard(acckey)
                if not store:
                    del self.__items[obj_key]

                # One by one -- so that the reverse index is consistent even in case of errors.
                self.__reverse[acckey].discard(obj_key)

            if not self.__reverse[acckey]:
                del self.__reverse[acckey]

    # Indexers' internal protocol. Must not be used by handlers & operators.
    def _replace(self, acckey: Key, obj: Mapping[_K, _V]) -> None:
        # Remember where the object is stored, so that the updates/deletions are O(1) later.
        try:
            reverse = self.__reverse[acckey]
        except KeyError:
            reverse = self.__reverse[acckey] = set()

        # Update (append or replace) all stores that are still related to `obj`.
        for obj_key, obj_val in obj.items():
            try:
                store = self.__items[obj_key]
            except KeyError:
                store = self.__items[obj_key] = Store()
            store._replace(acckey, obj_val)
            reverse.add(obj_key)

        # Discard from all stores that surely do not contain `obj` anymore.
        self._discard(acckey, reverse - set(obj.keys()))


class OperatorIndexer:
    """
    Indexers are read-write managers of read-only and minimalistic indices.

    .. note::
        Indexers are internal to the framework, they are not exposed
        to the operator developers (except for embedded operators).
    """
    index: Index[Any, Any]

    def __init__(self) -> None:
        super().__init__()
        self.index = Index()

    def __repr__(self) -> str:
        return repr(self.index)

    def discard(self, key: Key) -> None:
        """ Remove all values of the object, and keep ready for re-indexing. """
        self.index._discard(key)

    def replace(self, key: Key, obj: object) -> None:
        """ Store/merge the object's indexing results. """
        obj = obj if isinstance(obj, collections.abc.Mapping) else {None: obj}
        self.index._replace(key, obj)


class OperatorIndexers(dict[ids.HandlerId, OperatorIndexer]):

    def __init__(self) -> None:
        super().__init__()
        self.indices = OperatorIndices(self)

    def ensure(self, __handlers: Iterable[handlers.IndexingHandler]) -> None:
        """
        Pre-create indices/indexers to match the existing handlers.

        Any other indices will cause a KeyError at runtime.
        This is done to control the consistency of in-memory structures.
        """
        for handler in __handlers:
            self[handler.id] = OperatorIndexer()

    def discard(
            self,
            body: bodies.Body,
    ) -> None:
        """ Remove all values of this object from all indexers. Forget it! """
        key = self.make_key(body)
        for id, indexer in self.items():
            indexer.discard(key)

    def replace(
            self,
            body: bodies.Body,
            outcomes: Mapping[ids.HandlerId, execution.Outcome],
    ) -> None:
        """ Interpret the indexing results and apply them to the indices. """
        key = self.make_key(body)

        # Store the values: either for new objects or those re-matching the filters.
        for id, outcome in outcomes.items():
            if outcome.exception is not None:
                self[id].discard(key)
            elif outcome.result is not None:
                self[id].replace(key, outcome.result)

        # Purge the values: for those stopped matching the filters.
        for id, indexer in self.items():
            if id not in outcomes:
                indexer.discard(key)

    def make_key(self, body: bodies.Body) -> Key:
        """
        Make a key to address an object in internal containers.

        The key is not exposed to the users via indices,
        so its structure and type can be safely changed any time.

        However, the key must be as lightweight as possible:
        no dataclasses or namedtuples, only builtins.

        The name and namespace do not add value on top of the uid's uniqueness.
        They are here for debugging and for those rare objects
        that have no uid but are still exposed via the K8s API
        (highly unlikely to be indexed though).
        """
        meta = body.get('metadata', {})
        return (meta.get('namespace'), meta.get('name'), meta.get('uid'))


class OperatorIndices(ephemera.Indices):
    """
    A read-only view of indices of the operator.

    This view is carried through the whole call stack of the operator
    in a cause object, and later unfolded into the kwargs of the handlers.

    Why? First, carrying the indexers creates a circular import chain:

    * "causation" requires "OperatorIndexers" from "indexing".
    * "indexing" requires "IndexingCause" from "causation".

    The chain is broken by having a separate interface: `~ephemera.Indices`,
    while the implementation remains here.

    Second, read-write indexers create a temptation to modify them
    in modules and components that should not do this.
    Only "indexing" (this module) should modify the indices via indexers.
    """

    def __init__(self, indexers: "OperatorIndexers") -> None:
        super().__init__()
        self.__indexers = indexers

    def __len__(self) -> int:
        return len(self.__indexers)

    def __iter__(self) -> Iterator[str]:
        return iter(self.__indexers)

    def __getitem__(self, id: str) -> Index[Any, Any]:
        return self.__indexers[ids.HandlerId(id)].index

    def __contains__(self, id: object) -> bool:
        return id in self.__indexers


@dataclasses.dataclass(frozen=False)
class IndexingMemory:
    # For indexing errors backoffs/retries/timeouts. It is None when successfully indexed.
    indexing_state: Optional[progression.State] = None


async def index_resource(
        *,
        indexers: OperatorIndexers,
        registry: registries.OperatorRegistry,
        settings: configuration.OperatorSettings,
        resource: references.Resource,
        raw_event: bodies.RawEvent,
        memory: IndexingMemory,
        logger: typedefs.Logger,
        memo: ephemera.AnyMemo,
        body: bodies.Body,
) -> None:
    """
    Populate the indices from the received event. Log but ignore all errors.

    This is a lightweight and standalone process, which is executed before
    any real handlers are invoked. Multi-step calls are also not supported.
    If the handler fails, it fails and is never retried.

    Note: K8s-event posting is skipped for `kopf.on.event` handlers,
    as they should be silent. Still, the messages are logged normally.
    """
    if not registry._indexing.has_handlers(resource=resource):
        pass
    elif raw_event['type'] == 'DELETED':
        # Do not index it if it is deleted. Just discard quickly (ASAP!).
        indexers.discard(body=body)
    else:
        # Otherwise, go for full indexing with handlers invocation with all kwargs.
        cause = causes.IndexingCause(
            resource=resource,
            indices=indexers.indices,
            logger=logger,
            patch=patches.Patch(),  # NB: not applied. TODO: get rid of it!
            memo=memo,
            body=body,
        )

        # Note: the indexing state contains only failures & retries. Successes will be re-executed.
        indexing_handlers = registry._indexing.get_handlers(cause=cause)
        state = memory.indexing_state
        state = state if state is not None else progression.State.from_scratch()
        state = state.with_handlers(indexing_handlers)
        outcomes = await execution.execute_handlers_once(
            lifecycle=lifecycles.all_at_once,
            settings=settings,
            handlers=indexing_handlers,
            cause=cause,
            state=state,
            default_errors=execution.ErrorsMode.IGNORED,
        )
        indexers.replace(body=body, outcomes=outcomes)

        # Remember only failures & retries. Omit successes -- let them be re-executed every time.
        state = state.with_outcomes(outcomes).without_successes()
        memory.indexing_state = state if state else None



================================================
FILE: kopf/_core/engines/peering.py
================================================
"""
Peer monitoring: knowing which other operators do run, and exchanging the basic signals with them.

The main use-case is to suppress all deployed operators when a developer starts a dev-/debug-mode
operator for the same cluster on their workstation -- to avoid any double-processing.

See also: `kopf freeze` & `kopf resume` CLI commands for the same purpose.

WARNING: There are **NO** per-object locks between the operators, so only one operator
should be functional for the cluster, i.e. only one with the highest priority running.
If the operator sees the violations of this constraint, it will print the warnings
pointing to another same-priority operator, but will continue to function.

The "signals" exchanged are only the keep-alive notifications from the operator being alive,
and detection of other operators hard termination (by timeout rather than by clear exit).

The peers monitoring covers both the in-cluster operators running,
and the dev-mode operators running in the dev workstations.

For this, special CRDs ``kind: ClusterKopfPeering`` & ``kind: KopfPeering``
should be registered in the cluster, and their ``status`` field is used
by all the operators to sync their keep-alive info.

The namespace-bound operators (e.g. `--namespace=`) report their individual
namespaces are part of the payload, can see all other cluster and namespaced
operators (even from the different namespaces), and behave accordingly.

The CRD is not applied automatically, so you have to deploy it yourself explicitly.
To disable the peers monitoring, use the `--standalone` CLI option.
"""

import asyncio
import datetime
import getpass
import logging
import os
import random
from collections.abc import Iterable, Mapping
from typing import Any, NewType, NoReturn, Optional, cast

import iso8601

from kopf._cogs.aiokits import aiotasks, aiotime, aiotoggles
from kopf._cogs.clients import patching
from kopf._cogs.configs import configuration
from kopf._cogs.helpers import hostnames
from kopf._cogs.structs import bodies, patches, references

logger = logging.getLogger(__name__)

Identity = NewType('Identity', str)


# The class used to represent a peer in the parsed peers list (for convenience).
# The extra fields are for easier calculation when and if the peer is dead to the moment.
class Peer:

    def __init__(
            self,
            *,
            identity: Identity,
            priority: int = 0,
            lifetime: int = 60,
            lastseen: Optional[str] = None,
            **_: Any,  # for the forward-compatibility with the new fields
    ):
        super().__init__()
        self.identity = identity
        self.priority = priority
        self.lifetime = datetime.timedelta(seconds=int(lifetime))
        self.lastseen = (iso8601.parse_date(lastseen) if lastseen is not None else
                         datetime.datetime.now(datetime.timezone.utc))
        self.deadline = self.lastseen + self.lifetime
        self.is_dead = self.deadline <= datetime.datetime.now(datetime.timezone.utc)

    def __repr__(self) -> str:
        clsname = self.__class__.__name__
        options = ", ".join(f"{key!s}={val!r}" for key, val in self.as_dict().items())
        return f"<{clsname} {self.identity}: {options}>"

    def as_dict(self) -> dict[str, Any]:
        # Only the non-calculated and non-identifying fields.
        return {
            'priority': int(self.priority),
            'lifetime': int(self.lifetime.total_seconds()),
            'lastseen': str(self.lastseen.isoformat()),
        }


async def process_peering_event(
        *,
        raw_event: bodies.RawEvent,
        namespace: references.Namespace,
        resource: references.Resource,
        identity: Identity,
        settings: configuration.OperatorSettings,
        autoclean: bool = True,
        stream_pressure: Optional[asyncio.Event] = None,  # None for tests
        conflicts_found: Optional[aiotoggles.Toggle] = None,  # None for tests & observation
        # Must be accepted whether used or not -- as passed by watcher()/worker().
        resource_indexed: Optional[aiotoggles.Toggle] = None,  # None for tests & observation
        operator_indexed: Optional[aiotoggles.ToggleSet] = None,  # None for tests & observation
) -> None:
    """
    Handle a single update of the peers by us or by other operators.

    When an operator with a higher priority appears, pause this operator.
    When conflicting operators disappear or become presumably dead,
    resume the event handling in the current operator (un-pause it).
    """
    body: bodies.RawBody = raw_event['object']
    meta: bodies.RawMeta = raw_event['object']['metadata']

    # Silently ignore the peering objects which are not ours to worry.
    if meta.get('name') != settings.peering.name:
        return

    # Find if we are still the highest priority operator.
    pairs = cast(Mapping[str, Mapping[str, Any]], body.get('status', {}))
    peers = [Peer(identity=Identity(opid), **opinfo) for opid, opinfo in pairs.items()]
    dead_peers = [peer for peer in peers if peer.is_dead]
    live_peers = [peer for peer in peers if not peer.is_dead and peer.identity != identity]
    prio_peers = [peer for peer in live_peers if peer.priority > settings.peering.priority]
    same_peers = [peer for peer in live_peers if peer.priority == settings.peering.priority]

    if autoclean and dead_peers:
        await clean(peers=dead_peers, settings=settings, resource=resource, namespace=namespace)

    if conflicts_found is None:
        pass

    elif prio_peers:
        if conflicts_found.is_off():
            logger.info(f"Pausing operations in favour of {prio_peers}.")
            await conflicts_found.turn_to(True)

    elif same_peers:
        logger.warning(f"Possibly conflicting operators with the same priority: {same_peers}.")
        if conflicts_found.is_off():
            logger.warning(f"Pausing all operators, including self: {peers}")
            await conflicts_found.turn_to(True)

    else:
        if conflicts_found.is_on():
            logger.info(f"Resuming operations after the pause. Conflicting operators with the same priority are gone.")
            await conflicts_found.turn_to(False)

    # Either wait for external updates (and exit when they arrive), or until the blocking peers
    # are expected to expire, and force the immediate re-evaluation by a certain change of self.
    # This incurs an extra PATCH request besides usual keepalives, but in the complete silence
    # from other peers that existed a moment earlier, this should not be a problem.
    now = datetime.datetime.now(datetime.timezone.utc)
    delays = [(peer.deadline - now).total_seconds() for peer in same_peers + prio_peers]
    unslept = await aiotime.sleep(delays, wakeup=stream_pressure)
    if unslept is None and delays:
        await touch(
            identity=identity,
            settings=settings,
            resource=resource,
            namespace=namespace,
        )


async def keepalive(
        *,
        namespace: references.Namespace,
        resource: references.Resource,
        identity: Identity,
        settings: configuration.OperatorSettings,
) -> NoReturn:
    """
    An ever-running coroutine to regularly send our own keep-alive status for the peers.
    """
    try:
        while True:
            await touch(
                identity=identity,
                settings=settings,
                resource=resource,
                namespace=namespace,
            )

            # How often do we update. Keep limited to avoid k8s api flooding.
            # Should be slightly less than the lifetime, enough for a patch request to finish.
            # A little jitter is added to evenly distribute the keep-alives over time.
            lifetime = settings.peering.lifetime
            duration = min(lifetime, max(1, lifetime - random.randint(5, 10)))
            await asyncio.sleep(max(1, duration))
    finally:
        try:
            await asyncio.shield(touch(
                identity=identity,
                settings=settings,
                resource=resource,
                namespace=namespace,
                lifetime=0,
            ))
        except asyncio.CancelledError:
            pass  # cancellations are treated as normal exiting
        except Exception:
            logger.exception(f"Couldn't remove self from the peering. Ignoring.")


async def touch(
        *,
        identity: Identity,
        settings: configuration.OperatorSettings,
        resource: references.Resource,
        namespace: references.Namespace,
        lifetime: Optional[int] = None,
) -> None:
    name = settings.peering.name
    peer = Peer(
        identity=identity,
        priority=settings.peering.priority,
        lifetime=settings.peering.lifetime if lifetime is None else lifetime,
    )

    patch = patches.Patch()
    patch |= {'status': {identity: None if peer.is_dead else peer.as_dict()}}
    rsp = await patching.patch_obj(
        settings=settings,
        resource=resource,
        namespace=namespace,
        name=name,
        patch=patch,
        logger=logger,
    )

    if not settings.peering.stealth or rsp is None:
        where = f"in {namespace!r}" if namespace else "cluster-wide"
        result = "not found" if rsp is None else "ok"
        logger.debug(f"Keep-alive in {name!r} {where}: {result}.")


async def clean(
        *,
        peers: Iterable[Peer],
        settings: configuration.OperatorSettings,
        resource: references.Resource,
        namespace: references.Namespace,
) -> None:
    name = settings.peering.name
    patch = patches.Patch()
    patch |= {'status': {peer.identity: None for peer in peers}}
    await patching.patch_obj(
        settings=settings,
        resource=resource,
        namespace=namespace,
        name=name,
        patch=patch,
        logger=logger,
    )


def detect_own_id(*, manual: bool) -> Identity:
    """
    Detect or generate the id for ourselves, i.e. the execute operator.

    It is constructed easy to detect in which pod it is running
    (if in the cluster), or who runs the operator (if not in the cluster,
    i.e. in the dev-mode), and how long ago was it started.

    The pod id can be specified by::

        env:
        - name: POD_ID
          valueFrom:
            fieldRef:
              fieldPath: metadata.name

    Used in the `kopf._core.reactor.queueing` when the reactor starts,
    but is kept here, close to the rest of the peering logic.
    """

    pod = os.environ.get('POD_ID', None)
    if pod is not None:
        return Identity(pod)

    user = getpass.getuser()
    host = hostnames.get_descriptive_hostname()
    now = datetime.datetime.now(datetime.timezone.utc).strftime("%Y%m%d%H%M%S")
    rnd = ''.join(random.choices('abcdefhijklmnopqrstuvwxyz0123456789', k=3))
    return Identity(f'{user}@{host}' if manual else f'{user}@{host}/{now}/{rnd}')


def guess_selectors(settings: configuration.OperatorSettings) -> Iterable[references.Selector]:
    if settings.peering.standalone:
        return []
    elif settings.peering.clusterwide:
        return [references.CLUSTER_PEERINGS_K, references.CLUSTER_PEERINGS_Z]
    elif settings.peering.namespaced:
        return [references.NAMESPACED_PEERINGS_K, references.NAMESPACED_PEERINGS_Z]
    else:
        raise TypeError("Unidentified peering mode (none of standalone/cluster/namespaced).")


async def touch_command(
        *,
        lifetime: Optional[int],
        insights: references.Insights,
        identity: Identity,
        settings: configuration.OperatorSettings,
) -> None:

    await asyncio.wait({
        asyncio.create_task(insights.ready_namespaces.wait()),
        asyncio.create_task(insights.ready_resources.wait()),
    })

    selectors = guess_selectors(settings=settings)
    resources = [insights.backbone[s] for s in selectors if s in insights.backbone]
    if not resources:
        raise RuntimeError(f"Cannot find the peering resource for {selectors}.")

    await aiotasks.wait({
        aiotasks.create_guarded_task(
            name="peering command", finishable=True, logger=logger,
            coro=touch(
                namespace=namespace,
                resource=resource,
                identity=identity,
                settings=settings,
                lifetime=lifetime),
        )
        for namespace in insights.namespaces
        for resource in resources
    })



================================================
FILE: kopf/_core/engines/posting.py
================================================
"""
All the functions to write the Kubernetes events for the Kubernetes objects.

They are used internally in the handling routines to show the progress,
and can be used directly from the handlers to add arbitrary custom events.

The actual k8s-event posting runs in the background,
and posts the k8s-events as soon as they are queued.

The k8s-events are queued in two ways:

* Explicit calls to `kopf.event`, `kopf.info`, `kopf.warn`, `kopf.exception`.
* Logging messages made on the object logger (above INFO level by default).

This also includes all logging messages posted by the framework itself.
"""
import asyncio
import logging
import sys
from collections.abc import Iterable, Iterator
from contextvars import ContextVar
from typing import TYPE_CHECKING, NamedTuple, NoReturn, Optional, Union, cast

from kopf._cogs.clients import events
from kopf._cogs.configs import configuration
from kopf._cogs.structs import bodies, dicts, references
from kopf._core.actions import loggers

logger = logging.getLogger(__name__)

if TYPE_CHECKING:
    K8sEventQueue = asyncio.Queue["K8sEvent"]
else:
    K8sEventQueue = asyncio.Queue

# Logging and event-posting can happen cross-thread: e.g. in sync-executors.
# We have to remember our main event-loop with the queue consumer, to make
# thread-safe coro calls both from inside that event-loop and from outside.
event_queue_loop_var: ContextVar[asyncio.AbstractEventLoop] = ContextVar('event_queue_loop_var')
event_queue_var: ContextVar[K8sEventQueue] = ContextVar('event_queue_var')

# Per-operator container for settings. We only need a log level from there.
# This variable is dedicated to a posting engine, as the call chain is interrupted
# by user-side handlers (no pass-through `settings` arg).
settings_var: ContextVar[configuration.OperatorSettings] = ContextVar('settings_var')


class K8sEvent(NamedTuple):
    """
    A single k8s-event to be posted, with all ref-information preserved.
    It can exist and be posted even after the object is garbage-collected.
    """
    ref: bodies.ObjectReference
    type: str
    reason: str
    message: str


def enqueue(
        ref: bodies.ObjectReference,
        type: str,
        reason: str,
        message: str,
) -> None:
    loop = event_queue_loop_var.get()
    queue = event_queue_var.get()
    event = K8sEvent(ref=ref, type=type, reason=reason, message=message)

    # Events can be posted from another thread than the event-loop's thread
    # (e.g. from sync-handlers, or from explicitly started per-object threads),
    # or from the same thread (async-handlers and the framework itself).
    running_loop: Optional[asyncio.AbstractEventLoop]
    try:
        running_loop = asyncio.get_running_loop()
    except RuntimeError:
        running_loop = None

    if running_loop is loop:
        # Posting from the same event-loop as the poster task and queue are in.
        # Therefore, it is the same thread, and all calls here are thread-safe.
        # Special thread-safe cross-event-loop methods make no effect here.
        queue.put_nowait(event)
    else:
        # No event-loop or another event-loop - assume another thread.
        # Use the cross-thread thread-safe methods. Block until enqueued there.
        future = asyncio.run_coroutine_threadsafe(queue.put(event), loop=loop)
        future.result()  # block, wait, re-raise.


def event(
        objs: Union[bodies.Body, Iterable[bodies.Body]],
        *,
        type: str,
        reason: str,
        message: str = '',
) -> None:
    settings: configuration.OperatorSettings = settings_var.get()
    if settings.posting.enabled:
        for obj in cast(Iterator[bodies.Body], dicts.walk(objs)):
            ref = bodies.build_object_reference(obj)
            enqueue(ref=ref, type=type, reason=reason, message=message)


def info(
        objs: Union[bodies.Body, Iterable[bodies.Body]],
        *,
        reason: str,
        message: str = '',
) -> None:
    settings: configuration.OperatorSettings = settings_var.get()
    if settings.posting.enabled and settings.posting.level <= logging.INFO:
        for obj in cast(Iterator[bodies.Body], dicts.walk(objs)):
            ref = bodies.build_object_reference(obj)
            enqueue(ref=ref, type='Normal', reason=reason, message=message)


def warn(
        objs: Union[bodies.Body, Iterable[bodies.Body]],
        *,
        reason: str,
        message: str = '',
) -> None:
    settings: configuration.OperatorSettings = settings_var.get()
    if settings.posting.level <= logging.WARNING:
        for obj in cast(Iterator[bodies.Body], dicts.walk(objs)):
            ref = bodies.build_object_reference(obj)
            enqueue(ref=ref, type='Warning', reason=reason, message=message)


def exception(
        objs: Union[bodies.Body, Iterable[bodies.Body]],
        *,
        reason: str = '',
        message: str = '',
        exc: Optional[BaseException] = None,
) -> None:
    if exc is None:
        _, exc, _ = sys.exc_info()
    reason = reason if reason else type(exc).__name__
    message = f'{message} {exc}' if message and exc else f'{exc}' if exc else f'{message}'
    settings: configuration.OperatorSettings = settings_var.get()
    if settings.posting.enabled and settings.posting.level <= logging.ERROR:
        for obj in cast(Iterator[bodies.Body], dicts.walk(objs)):
            ref = bodies.build_object_reference(obj)
            enqueue(ref=ref, type='Error', reason=reason, message=message)


async def poster(
        *,
        event_queue: K8sEventQueue,
        backbone: references.Backbone,
        settings: configuration.OperatorSettings,
) -> NoReturn:
    """
    Post events in the background as they are queued.

    When the events come from the logging system, they have
    their reason, type, and other fields adjusted to meet Kubernetes's concepts.

    When the events are explicitly defined via `kopf.event` and similar calls,
    they have these special fields defined already.

    In either case, we pass the queued events directly to the K8s client
    (or a client wrapper/adapter), with no extra processing.

    This task is defined in this module only because all other tasks are here,
    so we keep all forever-running tasks together.
    """
    resource = await backbone.wait_for(references.EVENTS)
    while True:
        posted_event = await event_queue.get()
        await events.post_event(
            ref=posted_event.ref,
            type=posted_event.type,
            reason=posted_event.reason,
            message=posted_event.message,
            resource=resource,
            settings=settings,
            logger=logger,
        )


class K8sPoster(logging.Handler):
    """
    A handler to post all log messages as K8s events.
    """
    if sys.version_info[:2] < (3, 13):
        # Disable this optimisation for Python >= 3.13.
        # The `handle` no longer support having `None` as lock.
        def createLock(self) -> None:
            # Save some time on unneeded locks. Events are posted in the background.
            # We only put events to the queue, which is already lock-protected.
            self.lock = None

    def filter(self, record: logging.LogRecord) -> bool:
        # Only those which have a k8s object referred (see: `ObjectLogger`).
        # Otherwise, we have nothing to post, and nothing to do.
        # TODO: remove all bool() -- they were needed for Python 3.12 & MyPy 1.8.0 wrong inference.
        settings: Optional[configuration.OperatorSettings]
        settings = getattr(record, 'settings', None)
        level_ok = settings is not None and bool(record.levelno >= settings.posting.level)
        enabled = settings is not None and bool(settings.posting.enabled)
        has_ref = hasattr(record, 'k8s_ref')
        skipped = hasattr(record, 'k8s_skip') and bool(getattr(record, 'k8s_skip'))
        return enabled and level_ok and has_ref and not skipped and bool(super().filter(record))

    def emit(self, record: logging.LogRecord) -> None:
        # Same try-except as in e.g. `logging.StreamHandler`.
        try:
            ref = getattr(record, 'k8s_ref')
            type = (
                "Debug" if record.levelno <= logging.DEBUG else
                "Normal" if record.levelno <= logging.INFO else
                "Warning" if record.levelno <= logging.WARNING else
                "Error" if record.levelno <= logging.ERROR else
                "Fatal" if record.levelno <= logging.FATAL else
                logging.getLevelName(record.levelno).capitalize())
            reason = 'Logging'
            message = self.format(record)
            enqueue(
                ref=ref,
                type=type,
                reason=reason,
                message=message)
        except Exception:
            self.handleError(record)


loggers.logger.addHandler(K8sPoster())



================================================
FILE: kopf/_core/engines/probing.py
================================================
import asyncio
import datetime
import logging
import urllib.parse
from collections.abc import MutableMapping
from typing import Optional

import aiohttp.web

from kopf._cogs.configs import configuration
from kopf._cogs.structs import ephemera, ids
from kopf._core.actions import execution, lifecycles
from kopf._core.engines import activities
from kopf._core.intents import causes, registries

logger = logging.getLogger(__name__)

LOCALHOST: str = 'localhost'
HTTP_PORT: int = 80


async def health_reporter(
        endpoint: str,
        *,
        memo: ephemera.AnyMemo,
        indices: ephemera.Indices,
        registry: registries.OperatorRegistry,
        settings: configuration.OperatorSettings,
        ready_flag: Optional[asyncio.Event] = None,  # used for testing
) -> None:
    """
    Simple HTTP(S)/TCP server to report the operator's health to K8s probes.

    Runs forever until cancelled (which happens if any other root task
    is cancelled or failed). Once it will stop responding for any reason,
    Kubernetes will assume the pod is not alive anymore, and will restart it.
    """
    probing_container: MutableMapping[ids.HandlerId, execution.Result] = {}
    probing_timestamp: Optional[datetime.datetime] = None
    probing_max_age = datetime.timedelta(seconds=10.0)
    probing_lock = asyncio.Lock()

    async def get_health(
            request: aiohttp.web.Request,
    ) -> aiohttp.web.Response:
        nonlocal probing_container, probing_timestamp, probing_max_age, probing_lock

        # Recollect the data on-demand, and only if is is older that a reasonable caching period.
        # Protect against multiple parallel requests performing the same heavy activity.
        now = datetime.datetime.now(datetime.timezone.utc)
        if probing_timestamp is None or now - probing_timestamp >= probing_max_age:
            async with probing_lock:
                now = datetime.datetime.now(datetime.timezone.utc)
                if probing_timestamp is None or now - probing_timestamp >= probing_max_age:

                    activity_results = await activities.run_activity(
                        lifecycle=lifecycles.all_at_once,
                        registry=registry,
                        settings=settings,
                        activity=causes.Activity.PROBE,
                        indices=indices,
                        memo=memo,
                    )
                    probing_container.clear()
                    probing_container.update(activity_results)
                    probing_timestamp = datetime.datetime.now(datetime.timezone.utc)

        return aiohttp.web.json_response(probing_container)

    parts = urllib.parse.urlsplit(endpoint)
    if parts.scheme == 'http':
        host = parts.hostname or LOCALHOST
        port = parts.port or HTTP_PORT
        path = parts.path
    else:
        raise Exception(f"Unsupported scheme: {endpoint}")

    app = aiohttp.web.Application()
    app.add_routes([aiohttp.web.get(path, get_health)])

    runner = aiohttp.web.AppRunner(app, handle_signals=False, shutdown_timeout=1.0)
    await runner.setup()

    site = aiohttp.web.TCPSite(runner, host, port)
    await site.start()

    # Log with the actual URL: normalised, with hostname/port set.
    url = urllib.parse.urlunsplit([parts.scheme, f'{host}:{port}', path, '', ''])
    logger.debug(f"Serving health status at {url}")
    if ready_flag is not None:
        ready_flag.set()

    try:
        # Sleep forever. No activity is needed.
        await asyncio.Event().wait()
    finally:
        # On any reason of exit, stop reporting the health.
        await asyncio.shield(runner.cleanup())



================================================
FILE: kopf/_core/intents/__init__.py
================================================
[Empty file]


================================================
FILE: kopf/_core/intents/callbacks.py
================================================
"""
Callback signatures for typing.

Since these signatures contain a lot of copy-pasted kwargs and are
not so important for the codebase, they are moved to this separate module.

As a rule of thumb, for every kwarg named ``whatever``, there should be
a corresponding type or class ``kopf.Whatever`` with all the typing tricks
(``Union[...]``, ``Optional[...]``, partial ``Any`` values, etc) included.
"""
import datetime
from collections.abc import Collection
from typing import TYPE_CHECKING, Any, Callable, Optional, TypeVar, Union

from kopf._cogs.configs import configuration
from kopf._cogs.helpers import typedefs
from kopf._cogs.structs import bodies, diffs, ephemera, patches, references, reviews
from kopf._core.actions import invocation
from kopf._core.intents import stoppers

if not TYPE_CHECKING:  # pragma: nocover
    # Define unspecified protocols for the runtime annotations -- to avoid "quoting".
    ActivityFn = Callable[..., invocation.SyncOrAsync[Optional[object]]]
    IndexingFn = Callable[..., invocation.SyncOrAsync[Optional[object]]]
    WatchingFn = Callable[..., invocation.SyncOrAsync[Optional[object]]]
    ChangingFn = Callable[..., invocation.SyncOrAsync[Optional[object]]]
    WebhookFn = Callable[..., invocation.SyncOrAsync[Optional[object]]]
    DaemonFn = Callable[..., invocation.SyncOrAsync[Optional[object]]]
    TimerFn = Callable[..., invocation.SyncOrAsync[Optional[object]]]
    WhenFilterFn = Callable[..., bool]  # strictly sync, no async!
    MetaFilterFn = Callable[..., bool]  # strictly sync, no async!
else:
    from mypy_extensions import Arg, DefaultNamedArg, KwArg, NamedArg

    # TODO:1: Split to specialised LoginFn, ProbeFn, StartupFn, etc. -- with different result types.
    # TODO:2: Try using ParamSpec to support index type checking in callbacks
    #         when PEP 612 is released (https://www.python.org/dev/peps/pep-0612/)
    ActivityFn = Callable[
        [
            NamedArg(configuration.OperatorSettings, "settings"),
            NamedArg(ephemera.Index[Any, Any], "*"),
            NamedArg(int, "retry"),
            NamedArg(datetime.datetime, "started"),
            NamedArg(datetime.timedelta, "runtime"),
            NamedArg(typedefs.Logger, "logger"),
            NamedArg(Any, "memo"),
            DefaultNamedArg(Any, "param"),
            KwArg(Any),
        ],
        invocation.SyncOrAsync[Optional[object]]
    ]

    IndexingFn = Callable[
        [
            NamedArg(bodies.Annotations, "annotations"),
            NamedArg(bodies.Labels, "labels"),
            NamedArg(bodies.Body, "body"),
            NamedArg(bodies.Meta, "meta"),
            NamedArg(bodies.Spec, "spec"),
            NamedArg(bodies.Status, "status"),
            NamedArg(references.Resource, "resource"),
            NamedArg(Optional[str], "uid"),
            NamedArg(Optional[str], "name"),
            NamedArg(Optional[str], "namespace"),
            NamedArg(patches.Patch, "patch"),
            NamedArg(typedefs.Logger, "logger"),
            NamedArg(Any, "memo"),
            DefaultNamedArg(Any, "param"),
            KwArg(Any),
        ],
        invocation.SyncOrAsync[Optional[object]]
    ]

    WatchingFn = Callable[
        [
            NamedArg(str, "type"),
            NamedArg(bodies.RawEvent, "event"),
            NamedArg(bodies.Annotations, "annotations"),
            NamedArg(bodies.Labels, "labels"),
            NamedArg(bodies.Body, "body"),
            NamedArg(bodies.Meta, "meta"),
            NamedArg(bodies.Spec, "spec"),
            NamedArg(bodies.Status, "status"),
            NamedArg(references.Resource, "resource"),
            NamedArg(Optional[str], "uid"),
            NamedArg(Optional[str], "name"),
            NamedArg(Optional[str], "namespace"),
            NamedArg(patches.Patch, "patch"),
            NamedArg(typedefs.Logger, "logger"),
            NamedArg(Any, "memo"),
            DefaultNamedArg(Any, "param"),
            KwArg(Any),
        ],
        invocation.SyncOrAsync[Optional[object]]
    ]

    ChangingFn = Callable[
        [
            NamedArg(int, "retry"),
            NamedArg(datetime.datetime, "started"),
            NamedArg(datetime.timedelta, "runtime"),
            NamedArg(bodies.Annotations, "annotations"),
            NamedArg(bodies.Labels, "labels"),
            NamedArg(bodies.Body, "body"),
            NamedArg(bodies.Meta, "meta"),
            NamedArg(bodies.Spec, "spec"),
            NamedArg(bodies.Status, "status"),
            NamedArg(references.Resource, "resource"),
            NamedArg(Optional[str], "uid"),
            NamedArg(Optional[str], "name"),
            NamedArg(Optional[str], "namespace"),
            NamedArg(patches.Patch, "patch"),
            NamedArg(str, "reason"),
            NamedArg(diffs.Diff, "diff"),
            NamedArg(Optional[Union[bodies.BodyEssence, Any]], "old"),
            NamedArg(Optional[Union[bodies.BodyEssence, Any]], "new"),
            NamedArg(typedefs.Logger, "logger"),
            NamedArg(Any, "memo"),
            DefaultNamedArg(Any, "param"),
            KwArg(Any),
        ],
        invocation.SyncOrAsync[Optional[object]]
    ]

    WebhookFn = Callable[
        [
            NamedArg(bool, "dryrun"),
            NamedArg(list[str], "warnings"),  # mutable!
            NamedArg(Optional[str], "subresource"),
            NamedArg(reviews.UserInfo, "userinfo"),
            NamedArg(reviews.SSLPeer, "sslpeer"),
            NamedArg(reviews.Headers, "headers"),
            NamedArg(bodies.Labels, "labels"),
            NamedArg(bodies.Annotations, "annotations"),
            NamedArg(bodies.Body, "body"),
            NamedArg(bodies.Meta, "meta"),
            NamedArg(bodies.Spec, "spec"),
            NamedArg(bodies.Status, "status"),
            NamedArg(references.Resource, "resource"),
            NamedArg(Optional[str], "uid"),
            NamedArg(Optional[str], "name"),
            NamedArg(Optional[str], "namespace"),
            NamedArg(patches.Patch, "patch"),
            NamedArg(typedefs.Logger, "logger"),
            NamedArg(Any, "memo"),
            DefaultNamedArg(Any, "param"),
            KwArg(Any),
        ],
        invocation.SyncOrAsync[Optional[object]]
    ]

    DaemonFn = Callable[
        [
            NamedArg(stoppers.DaemonStopped, "stopped"),
            NamedArg(int, "retry"),
            NamedArg(datetime.datetime, "started"),
            NamedArg(datetime.timedelta, "runtime"),
            NamedArg(bodies.Annotations, "annotations"),
            NamedArg(bodies.Labels, "labels"),
            NamedArg(bodies.Body, "body"),
            NamedArg(bodies.Meta, "meta"),
            NamedArg(bodies.Spec, "spec"),
            NamedArg(bodies.Status, "status"),
            NamedArg(references.Resource, "resource"),
            NamedArg(Optional[str], "uid"),
            NamedArg(Optional[str], "name"),
            NamedArg(Optional[str], "namespace"),
            NamedArg(patches.Patch, "patch"),
            NamedArg(typedefs.Logger, "logger"),
            NamedArg(Any, "memo"),
            DefaultNamedArg(Any, "param"),
            KwArg(Any),
        ],
        invocation.SyncOrAsync[Optional[object]]
    ]

    TimerFn = Callable[
        [
            NamedArg(ephemera.Index[Any, Any], "*"),
            NamedArg(bodies.Annotations, "annotations"),
            NamedArg(bodies.Labels, "labels"),
            NamedArg(bodies.Body, "body"),
            NamedArg(bodies.Meta, "meta"),
            NamedArg(bodies.Spec, "spec"),
            NamedArg(bodies.Status, "status"),
            NamedArg(references.Resource, "resource"),
            NamedArg(Optional[str], "uid"),
            NamedArg(Optional[str], "name"),
            NamedArg(Optional[str], "namespace"),
            NamedArg(patches.Patch, "patch"),
            NamedArg(typedefs.Logger, "logger"),
            NamedArg(Any, "memo"),
            DefaultNamedArg(Any, "param"),
            KwArg(Any),
        ],
        invocation.SyncOrAsync[Optional[object]]
    ]

    WhenFilterFn = Callable[
        [
            NamedArg(str, "type"),
            NamedArg(bodies.RawEvent, "event"),
            NamedArg(bodies.Annotations, "annotations"),
            NamedArg(bodies.Labels, "labels"),
            NamedArg(bodies.Body, "body"),
            NamedArg(bodies.Meta, "meta"),
            NamedArg(bodies.Spec, "spec"),
            NamedArg(bodies.Status, "status"),
            NamedArg(references.Resource, "resource"),
            NamedArg(Optional[str], "uid"),
            NamedArg(Optional[str], "name"),
            NamedArg(Optional[str], "namespace"),
            NamedArg(patches.Patch, "patch"),
            NamedArg(diffs.Diff, "diff"),
            NamedArg(Optional[Union[bodies.BodyEssence, Any]], "old"),
            NamedArg(Optional[Union[bodies.BodyEssence, Any]], "new"),
            NamedArg(typedefs.Logger, "logger"),
            NamedArg(Any, "memo"),
            DefaultNamedArg(Any, "param"),
            KwArg(Any),
        ],
        bool  # strictly sync, no async!
    ]

    MetaFilterFn = Callable[
        [
            Arg(Any, "value"),
            NamedArg(str, "type"),
            NamedArg(bodies.Annotations, "annotations"),
            NamedArg(bodies.Labels, "labels"),
            NamedArg(bodies.Body, "body"),
            NamedArg(bodies.Meta, "meta"),
            NamedArg(bodies.Spec, "spec"),
            NamedArg(bodies.Status, "status"),
            NamedArg(references.Resource, "resource"),
            NamedArg(Optional[str], "uid"),
            NamedArg(Optional[str], "name"),
            NamedArg(Optional[str], "namespace"),
            NamedArg(patches.Patch, "patch"),
            NamedArg(typedefs.Logger, "logger"),
            NamedArg(Any, "memo"),
            DefaultNamedArg(Any, "param"),
            KwArg(Any),
        ],
        bool  # strictly sync, no async!
    ]

SpawningFn = Union[DaemonFn, TimerFn]
_FnT = TypeVar('_FnT', WhenFilterFn, MetaFilterFn)


def not_(fn: _FnT) -> _FnT:
    def not_fn(*args: Any, **kwargs: Any) -> bool:
        return not fn(*args, **kwargs)
    return not_fn


def all_(fns: Collection[_FnT]) -> _FnT:
    def all_fn(*args: Any, **kwargs: Any) -> bool:
        return all(fn(*args, **kwargs) for fn in fns)
    return all_fn


def any_(fns: Collection[_FnT]) -> _FnT:
    def any_fn(*args: Any, **kwargs: Any) -> bool:
        return any(fn(*args, **kwargs) for fn in fns)
    return any_fn


def none_(fns: Collection[_FnT]) -> _FnT:
    def none_fn(*args: Any, **kwargs: Any) -> bool:
        return not any(fn(*args, **kwargs) for fn in fns)
    return none_fn



================================================
FILE: kopf/_core/intents/causes.py
================================================
"""
Detection of the event causes, based on the resource state.

The low-level watch-events are highly limited in information on what
caused them, and they only notify that the object was changed somehow:

* ``ADDED`` for the newly created objects (or for the first-time listing).
* ``MODIFIED`` for the changes of any field, be that metadata, spec, or status.
* ``DELETED`` for the actual deletion of the object post-factum.

The conversion of low-level *events* to high level *causes* is done by
checking the object's state and comparing it to the saved last-seen state.

This allows to track which specific fields were changed, and if are those
changes are important enough to call the handlers: e.g. the ``status`` changes
are ignored, so as some selected system fields of the ``metadata``.

For deletion, the cause is detected when the object is just marked for deletion,
not when it is actually deleted (as the events notify): so that the handlers
could execute on the yet-existing object (and its children, if created).
"""
import dataclasses
import enum
from collections.abc import Mapping
from typing import Any, Optional

from kopf._cogs.configs import configuration
from kopf._cogs.structs import bodies, diffs, ephemera, finalizers, \
                               ids, patches, references, reviews
from kopf._core.actions import execution
from kopf._core.intents import stoppers


class Activity(str, enum.Enum):
    STARTUP = 'startup'
    CLEANUP = 'cleanup'
    AUTHENTICATION = 'authentication'
    PROBE = 'probe'


class WebhookType(str, enum.Enum):
    VALIDATING = 'validating'
    MUTATING = 'mutating'

    def __str__(self) -> str:
        return str(self.value)


# Constants for cause types, to prevent a direct usage of strings, and typos.
# They are not exposed by the framework, but are used internally. See also: `kopf.on`.
class Reason(str, enum.Enum):
    CREATE = 'create'
    UPDATE = 'update'
    DELETE = 'delete'
    RESUME = 'resume'
    NOOP = 'noop'
    FREE = 'free'
    GONE = 'gone'

    def __str__(self) -> str:
        return str(self.value)


# These sets are checked in a few places, so we keep them centralised:
# the user-facing causes (for handlers) and internally facing (for the reactor).
HANDLER_REASONS = (
    Reason.CREATE,
    Reason.UPDATE,
    Reason.DELETE,
    Reason.RESUME,
)
REACTOR_REASONS = (
    Reason.NOOP,
    Reason.FREE,
    Reason.GONE,
)
ALL_REASONS = HANDLER_REASONS + REACTOR_REASONS

# The human-readable names of these causes. Will be capitalised when needed.
TITLES = {
    Reason.CREATE.value: 'creation',
    Reason.UPDATE.value: 'updating',
    Reason.DELETE.value: 'deletion',
    Reason.RESUME.value: 'resuming',
}


@dataclasses.dataclass
class BaseCause(execution.Cause):
    """
    Base non-specific cause as used in the framework's reactor in most cases.

    IMPORTANT! Indices overwrite any other kwargs, even the existing ones.

    Why so? Here is why: for forwards & backwards compatibility.
    If an handler uses an index named "children", and Kopf introduces
    a new kwarg "children", the handler's code could break on the upgrade.
    To prevent this, Kopf overwrites the framework's kwarg "children"
    with the operator's index "children" and lets the developers rename it
    when (and if) they want the new kwarg.

    Naming the new indices the same as the known/existing kwargs
    harms only the developers who do so, so this is considered safe.
    """
    indices: ephemera.Indices
    memo: ephemera.AnyMemo

    @property
    def _kwargs(self) -> Mapping[str, Any]:
        kwargs = dict(super()._kwargs)
        del kwargs['indices']
        return kwargs

    @property
    def _super_kwargs(self) -> Mapping[str, Any]:
        return self.indices


@dataclasses.dataclass
class ActivityCause(BaseCause):
    activity: Activity
    settings: configuration.OperatorSettings


@dataclasses.dataclass
class ResourceCause(BaseCause):
    resource: references.Resource
    patch: patches.Patch
    body: bodies.Body

    @property
    def _kwargs(self) -> Mapping[str, Any]:
        return dict(
            super()._kwargs,
            spec=self.body.spec,
            meta=self.body.metadata,
            status=self.body.status,
            uid=self.body.metadata.uid,
            name=self.body.metadata.name,
            namespace=self.body.metadata.namespace,
            labels=self.body.metadata.labels,
            annotations=self.body.metadata.annotations,
        )


@dataclasses.dataclass
class WebhookCause(ResourceCause):
    dryrun: bool
    reason: Optional[WebhookType]  # None means "all" or expects the webhook id
    webhook: Optional[ids.HandlerId]  # None means "all"
    headers: Mapping[str, str]
    sslpeer: Mapping[str, Any]
    userinfo: reviews.UserInfo
    warnings: list[str]  # mutable!
    operation: Optional[reviews.Operation]  # None if not provided for some reason
    subresource: Optional[str]  # e.g. "status", "scale"; None for the main resource body
    old: Optional[bodies.Body] = None
    new: Optional[bodies.Body] = None
    diff: Optional[diffs.Diff] = None

    @property
    def _kwargs(self) -> Mapping[str, Any]:
        kwargs = dict(super()._kwargs)
        del kwargs['reason']
        del kwargs['webhook']
        return kwargs


@dataclasses.dataclass
class IndexingCause(ResourceCause):
    """
    The raw event received from the API.
    """
    pass


@dataclasses.dataclass
class WatchingCause(ResourceCause):
    """
    The raw event received from the API.

    It is a read-only mapping with some extra properties and methods.
    """
    type: bodies.RawEventType
    event: bodies.RawEvent


@dataclasses.dataclass
class SpawningCause(ResourceCause):
    """
    An internal daemon is spawning: tasks, threads, timers.

    Used only on the first appearance of a resource as a container for resource-
    specific objects (loggers, etc).
    """
    reset: bool

    @property
    def _kwargs(self) -> Mapping[str, Any]:
        kwargs = dict(super()._kwargs)
        del kwargs['reset']
        return kwargs


@dataclasses.dataclass
class ChangingCause(ResourceCause):
    """
    The cause is what has caused the whole reaction as a chain of handlers.

    Unlike the low-level Kubernetes watch-events, the cause is aware
    of actual field changes, including multi-handler changes.
    """
    initial: bool
    reason: Reason
    diff: diffs.Diff = diffs.EMPTY
    old: Optional[bodies.BodyEssence] = None
    new: Optional[bodies.BodyEssence] = None

    @property
    def _kwargs(self) -> Mapping[str, Any]:
        kwargs = dict(super()._kwargs)
        del kwargs['initial']
        return kwargs

    @property
    def deleted(self) -> bool:
        """ Used to conditionally skip/select the @on.resume handlers if the object is deleted. """
        return finalizers.is_deletion_ongoing(self.body)


@dataclasses.dataclass
class DaemonCause(ResourceCause):
    """
    An exceptional case of a container for daemon invocation kwargs.

    Regular causes are usually short-term, triggered by a watch-stream event,
    and disappear once the event is processed. The processing includes
    daemon spawning: the original cause and its temporary watch-event
    should not be remembered though the whole life cycle of a daemon.

    Instead, a new artificial daemon-cause is used (this class), which
    passes the kwarg values to the invocation routines. It only contains
    the long-living kwargs: loggers, per-daemon stoppers, body-views
    (with only the latest bodies as contained values), etc.

    Unlike other causes, it is created not in the processing routines once
    per event, but in the daemon spawning routines once per daemon (or a timer).
    Therefore, it is not "detected", but is created directly as an instance.
    """
    stopper: stoppers.DaemonStopper  # a signaller for the termination and its reason.

    @property
    def _kwargs(self) -> Mapping[str, Any]:
        kwargs = dict(super()._kwargs)
        del kwargs['stopper']
        return kwargs

    @property
    def _sync_kwargs(self) -> Mapping[str, Any]:
        return dict(super()._sync_kwargs, stopped=self.stopper.sync_waiter)

    @property
    def _async_kwargs(self) -> Mapping[str, Any]:
        return dict(super()._async_kwargs, stopped=self.stopper.async_waiter)


def detect_watching_cause(
        raw_event: bodies.RawEvent,
        body: bodies.Body,
        **kwargs: Any,
) -> WatchingCause:
    return WatchingCause(
        event=raw_event,
        type=raw_event['type'],
        body=body,
        **kwargs)


def detect_spawning_cause(
        body: bodies.Body,
        **kwargs: Any,
) -> SpawningCause:
    return SpawningCause(
        body=body,
        **kwargs)


def detect_changing_cause(
        *,
        finalizer: str,
        raw_event: bodies.RawEvent,
        body: bodies.Body,
        old: Optional[bodies.BodyEssence] = None,
        new: Optional[bodies.BodyEssence] = None,
        diff: Optional[diffs.Diff] = None,
        initial: bool = False,
        **kwargs: Any,
) -> ChangingCause:
    """
    Detect the cause of the event to be handled.

    This is a purely computational function with no side-effects.
    The causes are then consumed by `custom_object_handler`,
    which performs the actual handler invocation, logging, patching,
    and other side-effects.
    """

    # Put them back to the pass-through kwargs (to avoid code duplication).
    kwargs |= dict(body=body, old=old, new=new, initial=initial)
    if diff is not None:
        kwargs |= dict(diff=diff)

    # The object was really deleted from the cluster. But we do not care anymore.
    if raw_event['type'] == 'DELETED':
        return ChangingCause(reason=Reason.GONE, **kwargs)

    # The finalizer has been just removed. We are fully done.
    deletion_is_ongoing = finalizers.is_deletion_ongoing(body=body)
    deletion_is_blocked = finalizers.is_deletion_blocked(body=body, finalizer=finalizer)
    if deletion_is_ongoing and not deletion_is_blocked:
        return ChangingCause(reason=Reason.FREE, **kwargs)

    if deletion_is_ongoing:
        return ChangingCause(reason=Reason.DELETE, **kwargs)

    # For an object seen for the first time (i.e. just-created), call the creation handlers,
    # then mark the state as if it was seen when the creation has finished.
    # Creation never mixes with resuming, even if an object is detected on startup (first listing).
    if old is None:  # i.e. we have no essence stored
        kwargs['initial'] = False
        return ChangingCause(reason=Reason.CREATE, **kwargs)

    # Cases with no essence changes are usually ignored (NOOP). But for the not-yet-resumed objects,
    # we simulate a fake cause to invoke the resuming handlers. For cases with the essence changes,
    # the resuming handlers will be mixed-in to the regular cause handling ("cuckoo-style")
    # due to the ``initial=True`` flag on the cause, regardless of the reason.
    if not diff and initial:
        return ChangingCause(reason=Reason.RESUME, **kwargs)

    # The previous step triggers one more patch operation without actual changes. Ignore it.
    # Either the last-seen state or the status field has changed.
    if not diff:
        return ChangingCause(reason=Reason.NOOP, **kwargs)

    # And what is left, is the update operation on one of the useful fields of the existing object.
    return ChangingCause(reason=Reason.UPDATE, **kwargs)



================================================
FILE: kopf/_core/intents/filters.py
================================================
import enum
from collections.abc import Mapping
from typing import Any, Union

from kopf._core.intents import callbacks


class MetaFilterToken(enum.Enum):
    """ Tokens for filtering by annotations/labels. """
    PRESENT = enum.auto()
    ABSENT = enum.auto()


# For exporting to the top-level package.
ABSENT = MetaFilterToken.ABSENT
PRESENT = MetaFilterToken.PRESENT

# Filters for handler specifications (not the same as the object's values).
MetaFilter = Mapping[str, Union[str, MetaFilterToken, callbacks.MetaFilterFn]]

# Filters for old/new values of a field.
# NB: `Any` covers all other values, but we want to highlight that they are specially treated.
ValueFilter = Union[None, Any, MetaFilterToken, callbacks.MetaFilterFn]



================================================
FILE: kopf/_core/intents/handlers.py
================================================
import dataclasses
import warnings
from collections.abc import Collection
from typing import Optional, cast

from kopf._cogs.structs import dicts, diffs, references
from kopf._core.actions import execution
from kopf._core.intents import callbacks, causes, filters


@dataclasses.dataclass(frozen=True)
class ActivityHandler(execution.Handler):
    fn: callbacks.ActivityFn  # typing clarification
    activity: Optional[causes.Activity]
    _fallback: bool = False  # non-public!

    def __str__(self) -> str:
        return f"Activity {self.id!r}"


@dataclasses.dataclass(frozen=True)
class ResourceHandler(execution.Handler):
    selector: Optional[references.Selector]  # None is used only in sub-handlers
    labels: Optional[filters.MetaFilter]
    annotations: Optional[filters.MetaFilter]
    when: Optional[callbacks.WhenFilterFn]
    field: Optional[dicts.FieldPath]
    value: Optional[filters.ValueFilter]

    def adjust_cause(self, cause: execution.CauseT) -> execution.CauseT:
        if self.field is not None and isinstance(cause, causes.ChangingCause):
            old = dicts.resolve(cause.old, self.field, None)
            new = dicts.resolve(cause.new, self.field, None)
            diff = diffs.reduce(cause.diff, self.field)
            new_cause = dataclasses.replace(cause, old=old, new=new, diff=diff)
            return cast(execution.CauseT, new_cause)  # TODO: mypy bug?
        else:
            return cause


@dataclasses.dataclass(frozen=True)
class WebhookHandler(ResourceHandler):
    fn: callbacks.WebhookFn  # typing clarification
    reason: causes.WebhookType
    operations: Optional[Collection[str]]
    subresource: Optional[str]
    persistent: Optional[bool]
    side_effects: Optional[bool]
    ignore_failures: Optional[bool]

    def __str__(self) -> str:
        return f"Webhook {self.id!r}"

    @property
    def operation(self) -> Optional[str]:  # deprecated
        warnings.warn("handler.operation is deprecated, use handler.operations", DeprecationWarning)
        if not self.operations:
           return None
        elif len(self.operations) == 1:
            return list(self.operations)[0]
        else:
            raise ValueError(
                f"{len(self.operations)} operations in the handler. Use it as handler.operations."
            )


@dataclasses.dataclass(frozen=True)
class IndexingHandler(ResourceHandler):
    fn: callbacks.IndexingFn  # typing clarification

    def __str__(self) -> str:
        return f"Indexer {self.id!r}"


@dataclasses.dataclass(frozen=True)
class WatchingHandler(ResourceHandler):
    fn: callbacks.WatchingFn  # typing clarification


@dataclasses.dataclass(frozen=True)
class ChangingHandler(ResourceHandler):
    fn: callbacks.ChangingFn  # typing clarification
    reason: Optional[causes.Reason]
    initial: Optional[bool]
    deleted: Optional[bool]  # used for mixed-in (initial==True) @on.resume handlers only.
    requires_finalizer: Optional[bool]
    field_needs_change: Optional[bool]  # to identify on-field/on-update with support for old=/new=.
    old: Optional[filters.ValueFilter]
    new: Optional[filters.ValueFilter]


@dataclasses.dataclass(frozen=True)
class SpawningHandler(ResourceHandler):
    requires_finalizer: Optional[bool]
    initial_delay: Optional[float]


@dataclasses.dataclass(frozen=True)
class DaemonHandler(SpawningHandler):
    fn: callbacks.DaemonFn  # typing clarification
    cancellation_backoff: Optional[float]  # how long to wait before actual cancellation.
    cancellation_timeout: Optional[float]  # how long to wait before giving up on cancellation.
    cancellation_polling: Optional[float]  # how often to check for cancellation status.

    def __str__(self) -> str:
        return f"Daemon {self.id!r}"


@dataclasses.dataclass(frozen=True)
class TimerHandler(SpawningHandler):
    fn: callbacks.TimerFn  # typing clarification
    sharp: Optional[bool]
    idle: Optional[float]
    interval: Optional[float]

    def __str__(self) -> str:
        return f"Timer {self.id!r}"



================================================
FILE: kopf/_core/intents/piggybacking.py
================================================
"""
Rudimentary piggybacking on the known K8s API clients for authentication.

Kopf is not a client library, and avoids bringing too much logic
for proper authentication, especially all the complex auth-providers.

Instead, it uses the existing clients, triggers the (re-)authentication
in them, and extracts the basic credentials for its own use.

.. seealso::
    :mod:`credentials` and :func:`authentication`.
"""
import os
from collections.abc import Sequence
from typing import Any, Optional

import yaml

from kopf._cogs.helpers import typedefs
from kopf._cogs.structs import credentials

# Keep as constants to make them patchable. Higher priority is more preferred.
PRIORITY_OF_CLIENT: int = 10
PRIORITY_OF_PYKUBE: int = 20

# Rudimentary logins are added only if the clients are absent, so the priorities can overlap.
PRIORITY_OF_KUBECONFIG: int = 10
PRIORITY_OF_SERVICE_ACCOUNT: int = 20


def has_client() -> bool:
    try:
        import kubernetes
    except ImportError:
        return False
    else:
        return True


def has_pykube() -> bool:
    try:
        import pykube
    except ImportError:
        return False
    else:
        return True


# We keep the official client library auto-login only because it was
# an implied behavior before switching to pykube -- to keep it so (implied).
def login_via_client(
        *,
        logger: typedefs.Logger,
        **_: Any,
) -> Optional[credentials.ConnectionInfo]:

    # Keep imports in the function, as module imports are mocked in some tests.
    try:
        import kubernetes.config
    except ImportError:
        return None

    try:
        kubernetes.config.load_incluster_config()  # cluster env vars
        logger.debug("Client is configured in cluster with service account.")
    except kubernetes.config.ConfigException as e1:
        try:
            kubernetes.config.load_kube_config()  # developer's config files
            logger.debug("Client is configured via kubeconfig file.")
        except kubernetes.config.ConfigException as e2:
            raise credentials.LoginError("Cannot authenticate the client library "
                                         "neither in-cluster, nor via kubeconfig.")

    # We do not even try to understand how it works and why. Just load it, and extract the results.
    # For kubernetes client >= 12.0.0 use the new 'get_default_copy' method
    if callable(getattr(kubernetes.client.Configuration, 'get_default_copy', None)):
        config = kubernetes.client.Configuration.get_default_copy()
    else:
        config = kubernetes.client.Configuration()

    # For auth-providers, this method is monkey-patched with the auth-provider's one.
    # We need the actual auth-provider's token, so we call it instead of accessing api_key.
    # Other keys (token, tokenFile) also end up being retrieved via this method.
    header: Optional[str] = config.get_api_key_with_prefix('authorization')
    parts: Sequence[str] = header.split(' ', 1) if header else []
    scheme, token = ((None, None) if len(parts) == 0 else
                     (None, parts[0]) if len(parts) == 1 else
                     (parts[0], parts[1]))  # RFC-7235, Appendix C.

    # Interpret the config object for our own minimalistic credentials.
    # Note: kubernetes client has no concept of a "current" context's namespace.
    return credentials.ConnectionInfo(
        server=config.host,
        ca_path=config.ssl_ca_cert,  # can be a temporary file
        insecure=not config.verify_ssl,
        username=config.username or None,  # an empty string when not defined
        password=config.password or None,  # an empty string when not defined
        scheme=scheme,
        token=token,
        certificate_path=config.cert_file,  # can be a temporary file
        private_key_path=config.key_file,  # can be a temporary file
        priority=PRIORITY_OF_CLIENT,
    )


def login_via_pykube(
        *,
        logger: typedefs.Logger,
        **_: Any,
) -> Optional[credentials.ConnectionInfo]:

    # Keep imports in the function, as module imports are mocked in some tests.
    try:
        import pykube
    except ImportError:
        return None

    # Read the pykube config either way for later interpretation.
    config: pykube.KubeConfig
    try:
        config = pykube.KubeConfig.from_service_account()
        logger.debug("Pykube is configured in cluster with service account.")
    except FileNotFoundError:
        try:
            config = pykube.KubeConfig.from_file()
            logger.debug("Pykube is configured via kubeconfig file.")
        except (pykube.PyKubeError, FileNotFoundError):
            raise credentials.LoginError("Cannot authenticate pykube "
                                         "neither in-cluster, nor via kubeconfig.")

    # We don't know how this token will be retrieved, we just get it afterwards.
    provider_token = None
    if config.user.get('auth-provider'):
        api = pykube.HTTPClient(config)
        api.get(version='', base='/')  # ignore the response status
        provider_token = config.user.get('auth-provider', {}).get('config', {}).get('access-token')

    # Interpret the config object for our own minimalistic credentials.
    ca: Optional[pykube.config.BytesOrFile] = config.cluster.get('certificate-authority')
    cert: Optional[pykube.config.BytesOrFile] = config.user.get('client-certificate')
    pkey: Optional[pykube.config.BytesOrFile] = config.user.get('client-key')
    return credentials.ConnectionInfo(
        server=config.cluster.get('server'),
        ca_path=ca.filename() if ca else None,  # can be a temporary file
        insecure=config.cluster.get('insecure-skip-tls-verify'),
        username=config.user.get('username'),
        password=config.user.get('password'),
        token=config.user.get('token') or provider_token,
        certificate_path=cert.filename() if cert else None,  # can be a temporary file
        private_key_path=pkey.filename() if pkey else None,  # can be a temporary file
        default_namespace=config.namespace,
        priority=PRIORITY_OF_PYKUBE,
    )


def has_service_account() -> bool:
    return os.path.exists('/var/run/secrets/kubernetes.io/serviceaccount/token')


def login_with_service_account(**_: Any) -> Optional[credentials.ConnectionInfo]:
    """
    A minimalistic login handler that can get raw data from a service account.

    Authentication capabilities can be limited to keep the code short & simple.
    No parsing or sophisticated multi-step token retrieval is performed.

    This login function is intended to make Kopf runnable in trivial cases
    when neither pykube-ng nor the official client library are installed.
    """

    # As per https://kubernetes.io/docs/tasks/run-application/access-api-from-pod/
    token_path = '/var/run/secrets/kubernetes.io/serviceaccount/token'
    ns_path = '/var/run/secrets/kubernetes.io/serviceaccount/namespace'
    ca_path = '/var/run/secrets/kubernetes.io/serviceaccount/ca.crt'

    if os.path.exists(token_path):
        with open(token_path, encoding='utf-8') as f:
            token = f.read().strip()

        namespace: Optional[str] = None
        if os.path.exists(ns_path):
            with open(ns_path, encoding='utf-8') as f:
                namespace = f.read().strip()

        return credentials.ConnectionInfo(
            server='https://kubernetes.default.svc',
            ca_path=ca_path if os.path.exists(ca_path) else None,
            token=token or None,
            default_namespace=namespace or None,
            priority=PRIORITY_OF_SERVICE_ACCOUNT,
        )
    else:
        return None


def has_kubeconfig() -> bool:
    env_var_set = bool(os.environ.get('KUBECONFIG'))
    file_exists = os.path.exists(os.path.expanduser('~/.kube/config'))
    return env_var_set or file_exists


def login_with_kubeconfig(**_: Any) -> Optional[credentials.ConnectionInfo]:
    """
    A minimalistic login handler that can get raw data from a kubeconfig file.

    Authentication capabilities can be limited to keep the code short & simple.
    No parsing or sophisticated multi-step token retrieval is performed.

    This login function is intended to make Kopf runnable in trivial cases
    when neither pykube-ng nor the official client library are installed.
    """

    # As per https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/
    kubeconfig = os.environ.get('KUBECONFIG')
    if not kubeconfig and os.path.exists(os.path.expanduser('~/.kube/config')):
        kubeconfig = '~/.kube/config'
    if not kubeconfig:
        return None

    paths = [path.strip() for path in kubeconfig.split(os.pathsep)]
    paths = [os.path.expanduser(path) for path in paths if path]

    # As prescribed: if the file is absent or non-deserialisable, then fail. The first value wins.
    current_context: Optional[str] = None
    contexts: dict[Any, Any] = {}
    clusters: dict[Any, Any] = {}
    users: dict[Any, Any] = {}
    for path in paths:

        with open(path, encoding='utf-8') as f:
            config = yaml.safe_load(f.read()) or {}

        if current_context is None:
            current_context = config.get('current-context')
        for item in config.get('contexts', []):
            if item['name'] not in contexts:
                contexts[item['name']] = item.get('context') or {}
        for item in config.get('clusters', []):
            if item['name'] not in clusters:
                clusters[item['name']] = item.get('cluster') or {}
        for item in config.get('users', []):
            if item['name'] not in users:
                users[item['name']] = item.get('user') or {}

    # Once fully parsed, use the current context only.
    if current_context is None:
        raise credentials.LoginError('Current context is not set in kubeconfigs.')
    context = contexts[current_context]
    cluster = clusters[context['cluster']]
    user = users[context['user']]

    # Unlike pykube's login, we do not make a fake API request to refresh the token.
    provider_token = user.get('auth-provider', {}).get('config', {}).get('access-token')

    # Map the retrieved fields into the credentials object.
    return credentials.ConnectionInfo(
        server=cluster.get('server'),
        ca_path=cluster.get('certificate-authority'),
        ca_data=cluster.get('certificate-authority-data'),
        insecure=cluster.get('insecure-skip-tls-verify'),
        certificate_path=user.get('client-certificate'),
        certificate_data=user.get('client-certificate-data'),
        private_key_path=user.get('client-key'),
        private_key_data=user.get('client-key-data'),
        username=user.get('username'),
        password=user.get('password'),
        token=user.get('token') or provider_token,
        default_namespace=context.get('namespace'),
        priority=PRIORITY_OF_KUBECONFIG,
    )



================================================
FILE: kopf/_core/intents/registries.py
================================================
"""
A registry of the handlers, attached to the resources or events.

The global registry is populated by the `kopf.on` decorators, and is used
to register the resources being watched and handled, and to attach
the handlers to the specific causes (create/update/delete/field-change).

The simple registry is part of the global registry (for each individual
resource), and also used for the sub-handlers within a top-level handler.

Both are used in the `kopf._core.actions.execution` to retrieve the list
of the handlers to be executed on each reaction cycle.
"""
import abc
import enum
import functools
from collections.abc import Collection, Container, Iterable, Iterator, \
                            Mapping, MutableMapping, Sequence
from types import FunctionType, MethodType
from typing import Any, Callable, Generic, Optional, TypeVar, cast

from kopf._cogs.structs import dicts, ids, references
from kopf._core.actions import execution
from kopf._core.intents import causes, filters, handlers, piggybacking

# We only type-check for known classes of handlers/callbacks, and ignore any custom subclasses.
CauseT = TypeVar('CauseT', bound=execution.Cause)
HandlerT = TypeVar('HandlerT', bound=execution.Handler)
ResourceHandlerT = TypeVar('ResourceHandlerT', bound=handlers.ResourceHandler)


class GenericRegistry(Generic[HandlerT]):
    """ A generic base class of a simple registry (with no handler getters). """
    _handlers: list[HandlerT]

    def __init__(self) -> None:
        super().__init__()
        self._handlers = []

    def append(self, handler: HandlerT) -> None:
        self._handlers.append(handler)

    def get_all_handlers(self) -> Collection[HandlerT]:
        return list(self._handlers)


class ActivityRegistry(GenericRegistry[handlers.ActivityHandler]):

    def get_handlers(
            self,
            activity: causes.Activity,
    ) -> Sequence[handlers.ActivityHandler]:
        return list(_deduplicated(self.iter_handlers(activity=activity)))

    def iter_handlers(
            self,
            activity: causes.Activity,
    ) -> Iterator[handlers.ActivityHandler]:
        found: bool = False

        # Regular handlers go first.
        for handler in self._handlers:
            if handler.activity is None or handler.activity == activity and not handler._fallback:
                yield handler
                found = True

        # Fallback handlers -- only if there were no matching regular handlers.
        if not found:
            for handler in self._handlers:
                if handler.activity is None or handler.activity == activity and handler._fallback:
                    yield handler


class ResourceRegistry(GenericRegistry[ResourceHandlerT], Generic[ResourceHandlerT, CauseT]):

    def get_all_selectors(self) -> frozenset[references.Selector]:
        return frozenset(
            handler.selector
            for handler in self.get_all_handlers()
            if handler.selector is not None  # None is reserved for sub-handlers
        )

    def has_handlers(
            self,
            resource: references.Resource,
    ) -> bool:
        for handler in self._handlers:
            if _matches_resource(handler, resource):
                return True
        return False

    def get_handlers(
            self,
            cause: CauseT,
            excluded: Container[ids.HandlerId] = frozenset(),
    ) -> Sequence[ResourceHandlerT]:
        return list(_deduplicated(self.iter_handlers(cause=cause, excluded=excluded)))

    @abc.abstractmethod
    def iter_handlers(
            self,
            cause: CauseT,
            excluded: Container[ids.HandlerId] = frozenset(),
    ) -> Iterator[ResourceHandlerT]:
        raise NotImplementedError

    def get_extra_fields(
            self,
            resource: references.Resource,
    ) -> set[dicts.FieldPath]:
        return set(self.iter_extra_fields(resource=resource))

    def iter_extra_fields(
            self,
            resource: references.Resource,
    ) -> Iterator[dicts.FieldPath]:
        for handler in self._handlers:
            if _matches_resource(handler, resource):
                if handler.field:
                    yield handler.field


class IndexingRegistry(ResourceRegistry[handlers.IndexingHandler, causes.IndexingCause]):

    def iter_handlers(
            self,
            cause: causes.IndexingCause,
            excluded: Container[ids.HandlerId] = frozenset(),
    ) -> Iterator[handlers.IndexingHandler]:
        for handler in self._handlers:
            if handler.id not in excluded:
                if match(handler=handler, cause=cause):
                    yield handler


class WatchingRegistry(ResourceRegistry[handlers.WatchingHandler, causes.WatchingCause]):

    def iter_handlers(
            self,
            cause: causes.WatchingCause,
            excluded: Container[ids.HandlerId] = frozenset(),
    ) -> Iterator[handlers.WatchingHandler]:
        for handler in self._handlers:
            if handler.id not in excluded:
                if match(handler=handler, cause=cause):
                    yield handler


class SpawningRegistry(ResourceRegistry[handlers.SpawningHandler, causes.SpawningCause]):

    def iter_handlers(
            self,
            cause: causes.SpawningCause,
            excluded: Container[ids.HandlerId] = frozenset(),
    ) -> Iterator[handlers.SpawningHandler]:
        for handler in self._handlers:
            if handler.id not in excluded:
                if match(handler=handler, cause=cause):
                    yield handler

    def requires_finalizer(
            self,
            cause: causes.SpawningCause,
            excluded: Container[ids.HandlerId] = frozenset(),
    ) -> bool:
        """
        Check whether a finalizer should be added to the given resource or not.
        """
        # check whether the body matches a deletion handler
        for handler in self._handlers:
            if handler.id not in excluded:
                if handler.requires_finalizer and match(handler=handler, cause=cause):
                    return True
        return False


class ChangingRegistry(ResourceRegistry[handlers.ChangingHandler, causes.ChangingCause]):

    def iter_handlers(
            self,
            cause: causes.ChangingCause,
            excluded: Container[ids.HandlerId] = frozenset(),
    ) -> Iterator[handlers.ChangingHandler]:
        for handler in self._handlers:
            if handler.id not in excluded:
                if handler.reason is None or handler.reason == cause.reason:
                    if handler.initial and not cause.initial:
                        pass  # skip initial handlers in non-initial causes.
                    elif handler.initial and cause.deleted and not handler.deleted:
                        pass  # skip initial handlers on deletion, unless explicitly marked as used.
                    elif match(handler=handler, cause=cause):
                        yield handler

    def requires_finalizer(
            self,
            cause: causes.ResourceCause,
            excluded: Container[ids.HandlerId] = frozenset(),
    ) -> bool:
        """
        Check whether a finalizer should be added to the given resource or not.
        """
        # check whether the body matches a deletion handler
        for handler in self._handlers:
            if handler.id not in excluded:
                if handler.requires_finalizer and prematch(handler=handler, cause=cause):
                    return True
        return False

    def prematch(
            self,
            cause: causes.ChangingCause,
    ) -> bool:
        for handler in self._handlers:
            if prematch(handler=handler, cause=cause):
                return True
        return False

    def get_resource_handlers(
            self,
            resource: references.Resource,
    ) -> Sequence[handlers.ChangingHandler]:
        found_handlers: list[handlers.ChangingHandler] = []
        for handler in self._handlers:
            if _matches_resource(handler, resource):
                found_handlers.append(handler)
        return list(_deduplicated(found_handlers))


class WebhooksRegistry(ResourceRegistry[handlers.WebhookHandler, causes.WebhookCause]):

    def iter_handlers(
            self,
            cause: causes.WebhookCause,
            excluded: Container[ids.HandlerId] = frozenset(),
    ) -> Iterator[handlers.WebhookHandler]:
        for handler in self._handlers:
            if handler.id not in excluded:
                # Only the handlers for the hinted webhook, if possible; if not hinted, then all.
                matching_reason = cause.reason is None or cause.reason == handler.reason
                matching_webhook = cause.webhook is None or cause.webhook == handler.id
                if matching_reason and matching_webhook:
                    # For deletion, exclude all mutation handlers unless explicitly enabled.
                    non_mutating = handler.reason != causes.WebhookType.MUTATING
                    non_deletion = cause.operation != 'DELETE'
                    explicitly_for_deletion = set(handler.operations or []) == {'DELETE'}
                    if non_mutating or non_deletion or explicitly_for_deletion:
                        # Filter by usual criteria: labels, annotations, fields, callbacks.
                        if match(handler=handler, cause=cause):
                            yield handler


class OperatorRegistry:
    """
    A global registry is used for handling of multiple resources & activities.

    It is usually populated by the ``@kopf.on...`` decorators, but can also
    be explicitly created and used in the embedded operators.
    """
    def __init__(self) -> None:
        super().__init__()
        self._activities = ActivityRegistry()
        self._indexing = IndexingRegistry()
        self._watching = WatchingRegistry()
        self._spawning = SpawningRegistry()
        self._changing = ChangingRegistry()
        self._webhooks = WebhooksRegistry()


class SmartOperatorRegistry(OperatorRegistry):
    def __init__(self) -> None:
        super().__init__()

        if piggybacking.has_pykube():
            self._activities.append(handlers.ActivityHandler(
                id=ids.HandlerId('login_via_pykube'),
                fn=piggybacking.login_via_pykube,
                activity=causes.Activity.AUTHENTICATION,
                errors=execution.ErrorsMode.IGNORED,
                param=None, timeout=None, retries=None, backoff=None,
                _fallback=True,
            ))
        if piggybacking.has_client():
            self._activities.append(handlers.ActivityHandler(
                id=ids.HandlerId('login_via_client'),
                fn=piggybacking.login_via_client,
                activity=causes.Activity.AUTHENTICATION,
                errors=execution.ErrorsMode.IGNORED,
                param=None, timeout=None, retries=None, backoff=None,
                _fallback=True,
            ))

        # As a last resort, fall back to rudimentary logins if no advanced ones are available.
        thirdparties_present = piggybacking.has_pykube() or piggybacking.has_client()
        if not thirdparties_present and piggybacking.has_kubeconfig():
            self._activities.append(handlers.ActivityHandler(
                id=ids.HandlerId('login_with_kubeconfig'),
                fn=piggybacking.login_with_kubeconfig,
                activity=causes.Activity.AUTHENTICATION,
                errors=execution.ErrorsMode.IGNORED,
                param=None, timeout=None, retries=None, backoff=None,
                _fallback=True,
            ))
        if not thirdparties_present and piggybacking.has_service_account():
            self._activities.append(handlers.ActivityHandler(
                id=ids.HandlerId('login_with_service_account'),
                fn=piggybacking.login_with_service_account,
                activity=causes.Activity.AUTHENTICATION,
                errors=execution.ErrorsMode.IGNORED,
                param=None, timeout=None, retries=None, backoff=None,
                _fallback=True,
            ))


def generate_id(
        fn: Callable[..., Any],
        id: Optional[str],
        prefix: Optional[str] = None,
        suffix: Optional[str] = None,
) -> ids.HandlerId:
    real_id: str
    real_id = id if id is not None else get_callable_id(fn)
    real_id = real_id if not suffix else f'{real_id}/{suffix}'
    real_id = real_id if not prefix else f'{prefix}/{real_id}'
    return cast(ids.HandlerId, real_id)


def get_callable_id(c: Callable[..., Any]) -> str:
    """ Get an reasonably good id of any commonly used callable. """
    if c is None:
        raise ValueError("Cannot build a persistent id of None.")
    elif isinstance(c, functools.partial):
        return get_callable_id(c.func)
    elif hasattr(c, '__wrapped__'):  # @functools.wraps()
        return get_callable_id(getattr(c, '__wrapped__'))
    elif isinstance(c, FunctionType) and c.__name__ == '<lambda>':
        # The best we can do to keep the id stable across the process restarts,
        # assuming at least no code changes. The code changes are not detectable.
        line = c.__code__.co_firstlineno
        path = c.__code__.co_filename
        return f'lambda:{path}:{line}'
    elif isinstance(c, (FunctionType, MethodType)):
        return str(getattr(c, '__qualname__', getattr(c, '__name__', repr(c))))
    else:
        raise ValueError(f"Cannot get id of {c!r}.")


def _deduplicated(
        src: Iterable[HandlerT],
) -> Iterator[HandlerT]:
    """
    Yield the handlers deduplicated.

    The same handler function should not be invoked more than once for one
    single event/cause, even if it is registered with multiple decorators
    (e.g. different filtering criteria or different but same-effect causes).

    One of the ways how this could happen::

        @kopf.on.create(...)
        @kopf.on.resume(...)
        def fn(**kwargs): pass

    In normal cases, the function will be called either on resource creation
    or on operator restart for the pre-existing (already handled) resources.
    When a resource is created during the operator downtime, it is
    both creation and resuming at the same time: the object is new (not yet
    handled) **AND** it is detected as per-existing before operator start.
    But `fn()` should be called only once for this cause.
    """
    seen_ids: set[tuple[int, ids.HandlerId]] = set()
    for handler in src:
        key = (id(handler.fn), handler.id)
        if key in seen_ids:
            pass
        else:
            seen_ids.add(key)
            yield handler


def prematch(
        handler: handlers.ResourceHandler,
        cause: causes.ResourceCause,
) -> bool:
    # Kwargs are lazily evaluated on the first _actual_ use, and shared for all filters since then.
    kwargs: MutableMapping[str, Any] = {}
    return (
        _matches_resource(handler, cause.resource) and
        _matches_subresource(handler, cause) and
        _matches_labels(handler, cause, kwargs) and
        _matches_annotations(handler, cause, kwargs) and
        _matches_field_values(handler, cause, kwargs) and
        _matches_filter_callback(handler, cause, kwargs)  # the callback comes in the end!
    )


def match(
        handler: handlers.ResourceHandler,
        cause: causes.ResourceCause,
) -> bool:
    # Kwargs are lazily evaluated on the first _actual_ use, and shared for all filters since then.
    kwargs: MutableMapping[str, Any] = {}
    return (
        _matches_resource(handler, cause.resource) and
        _matches_subresource(handler, cause) and
        _matches_labels(handler, cause, kwargs) and
        _matches_annotations(handler, cause, kwargs) and
        _matches_field_values(handler, cause, kwargs) and
        _matches_field_changes(handler, cause, kwargs) and
        _matches_filter_callback(handler, cause, kwargs)  # the callback comes in the end!
    )


def _matches_resource(
        handler: handlers.ResourceHandler,
        resource: references.Resource,
) -> bool:
    return (handler.selector is None or
            handler.selector.check(resource))


def _matches_subresource(
        handler: handlers.ResourceHandler,
        cause: causes.ResourceCause,
) -> bool:
    if not isinstance(handler, handlers.WebhookHandler):
        return True
    if not isinstance(cause, causes.WebhookCause):
        return True
    return (handler.subresource == '*' or
            handler.subresource == cause.subresource)  # incl. None==None case


def _matches_labels(
        handler: handlers.ResourceHandler,
        cause: causes.ResourceCause,
        kwargs: MutableMapping[str, Any],
) -> bool:
    return (not handler.labels or
            _matches_metadata(pattern=handler.labels,
                              content=cause.body.get('metadata', {}).get('labels', {}),
                              kwargs=kwargs, cause=cause))


def _matches_annotations(
        handler: handlers.ResourceHandler,
        cause: causes.ResourceCause,
        kwargs: MutableMapping[str, Any],
) -> bool:
    return (not handler.annotations or
            _matches_metadata(pattern=handler.annotations,
                              content=cause.body.get('metadata', {}).get('annotations', {}),
                              kwargs=kwargs, cause=cause))


def _matches_metadata(
        *,
        pattern: filters.MetaFilter,  # from the handler
        content: Mapping[str, str],  # from the body
        kwargs: MutableMapping[str, Any],
        cause: causes.ResourceCause,
) -> bool:
    for key, value in pattern.items():
        if value is filters.MetaFilterToken.ABSENT and key not in content:
            continue
        elif value is filters.MetaFilterToken.PRESENT and key in content:
            continue
        elif callable(value):
            if not kwargs:
                kwargs.update(cause.kwargs)
            if value(content.get(key, None), **kwargs):
                continue
            else:
                return False
        elif key not in content:
            return False
        elif value != content[key]:
            return False
        else:
            continue
    return True


def _matches_field_values(
        handler: handlers.ResourceHandler,
        cause: causes.ResourceCause,
        kwargs: MutableMapping[str, Any],
) -> bool:
    if not handler.field:
        return True

    if not kwargs:
        kwargs.update(cause.kwargs)

    absent = _UNSET.token  # or any other identifyable object
    if isinstance(cause, causes.ChangingCause):
        # For on.update/on.field, so as for on.create/resume/delete for uniformity and simplicity:
        old = dicts.resolve(cause.old, handler.field, absent)
        new = dicts.resolve(cause.new, handler.field, absent)
        values = [new, old]  # keep "new" first, to avoid "old" callbacks if "new" works.
    else:
        # For event-watching, timers/daemons (could also work for on.create/resume/delete):
        val = dicts.resolve(cause.body, handler.field, absent)
        values = [val]
    return (
        (handler.value is None and any(value is not absent for value in values)) or
        (handler.value is filters.PRESENT and any(value is not absent for value in values)) or
        (handler.value is filters.ABSENT and any(value is absent for value in values)) or
        (callable(handler.value) and any(handler.value(value, **kwargs) for value in values)) or
        (any(handler.value == value for value in values))
    )


def _matches_field_changes(
        handler: handlers.ResourceHandler,
        cause: causes.ResourceCause,
        kwargs: MutableMapping[str, Any],
) -> bool:
    if not isinstance(handler, handlers.ChangingHandler):
        return True
    if not isinstance(cause, causes.ChangingCause):
        return True
    if not handler.field:
        return True

    if not kwargs:
        kwargs.update(cause.kwargs)

    absent = _UNSET.token  # or any other identifyable object
    old = dicts.resolve(cause.old, handler.field, absent)
    new = dicts.resolve(cause.new, handler.field, absent)
    return ((
        not handler.field_needs_change or
        old != new  # ... or there IS a change.
    ) and (
        (handler.old is None) or
        (handler.old is filters.ABSENT and old is absent) or
        (handler.old is filters.PRESENT and old is not absent) or
        (callable(handler.old) and handler.old(old, **kwargs)) or
        (handler.old == old)
    ) and (
        (handler.new is None) or
        (handler.new is filters.ABSENT and new is absent) or
        (handler.new is filters.PRESENT and new is not absent) or
        (callable(handler.new) and handler.new(new, **kwargs)) or
        (handler.new == new)
    ))


def _matches_filter_callback(
        handler: handlers.ResourceHandler,
        cause: causes.ResourceCause,
        kwargs: MutableMapping[str, Any],
) -> bool:
    if handler.when is None:
        return True
    if not kwargs:
        kwargs.update(cause.kwargs)
    return handler.when(**kwargs)


class _UNSET(enum.Enum):
    token = enum.auto()


_default_registry: Optional[OperatorRegistry] = None


def get_default_registry() -> OperatorRegistry:
    """
    Get the default registry to be used by the decorators and the reactor
    unless the explicit registry is provided to them.
    """
    global _default_registry
    if _default_registry is None:
        _default_registry = SmartOperatorRegistry()
    return _default_registry


def set_default_registry(registry: OperatorRegistry) -> None:
    """
    Set the default registry to be used by the decorators and the reactor
    unless the explicit registry is provided to them.
    """
    global _default_registry
    _default_registry = registry



================================================
FILE: kopf/_core/intents/stoppers.py
================================================
import enum

from kopf._cogs.aiokits import aioenums


class DaemonStoppingReason(enum.Flag):
    """
    A reason or reasons of daemon being terminated.

    Daemons are signalled to exit usually for two reasons: the operator itself
    is exiting or restarting, so all daemons of all resources must stop;
    or the individual resource was deleted, but the operator continues running.

    No matter the reason, the daemons must exit, so one and only one stop-flag
    is used. Some daemons can check the reason of exiting if it is important.

    There can be multiple reasons combined (in rare cases, all of them).
    """
    DONE = enum.auto()  # whatever the reason and the status, the asyncio task has exited.
    FILTERS_MISMATCH = enum.auto()  # the resource does not match the filters anymore.
    RESOURCE_DELETED = enum.auto()  # the resource was deleted, the asyncio task is still awaited.
    OPERATOR_PAUSING = enum.auto()  # the operator is pausing, the asyncio task is still awaited.
    OPERATOR_EXITING = enum.auto()  # the operator is exiting, the asyncio task is still awaited.
    DAEMON_SIGNALLED = enum.auto()  # the stopper flag was set, the asyncio task is still awaited.
    DAEMON_CANCELLED = enum.auto()  # the asyncio task was cancelled, the thread can be running.
    DAEMON_ABANDONED = enum.auto()  # we gave up on the asyncio task, the thread can be running.


DaemonStopper = aioenums.FlagSetter[DaemonStoppingReason]
DaemonStopped = aioenums.FlagWaiter[DaemonStoppingReason]
SyncDaemonStopperChecker = aioenums.SyncFlagWaiter[DaemonStoppingReason]  # deprecated
AsyncDaemonStopperChecker = aioenums.AsyncFlagWaiter[DaemonStoppingReason]  # deprecated



================================================
FILE: kopf/_core/reactor/__init__.py
================================================
"""
The reactor groups all modules to watch & process the low- & high-level events.

The low-level events are the kubernetes watch streams, received on every
object change, including the metadata, status, etc.

The high-level events are the actually identified changes in the objects,
such as their creation, deletion, update both in general and per-field.
"""



================================================
FILE: kopf/_core/reactor/inventory.py
================================================
"""
An internal in-memory storage of structured records about resource objects.

Each object gets at most one record in the inventory of an operator's memories.
The information is stored strictly in-memory and is not persistent.
On the operator restart, all the memories are lost.
The information is not exposed to the operator developers (except for memos).

It is used internally to track allocated system resources for each Kubernetes
object, even if that object does not show up in the event streams for long time.

In the future, additional never-ending tasks can be running to maintain
the operator's memories and inventory and garbage-collect all outdated records.

The inventory memories are data structures, but they are a part of the reactor
because they store specialised data structures of specialised reactor modules
(e.g. daemons, admission, indexing, etc). For cohesion, these data structures
must be kept together with their owning modules rather than mirrored in structs.
"""
import copy
import dataclasses
from collections.abc import Iterator, MutableMapping
from typing import Optional

from kopf._cogs.structs import bodies, ephemera
from kopf._core.actions import throttlers
from kopf._core.engines import admission, daemons, indexing


@dataclasses.dataclass(frozen=False)
class ResourceMemory:
    """ A system memo about a single resource/object. Usually stored in `Memories`. """
    memo: ephemera.AnyMemo = dataclasses.field(default_factory=lambda: ephemera.AnyMemo(ephemera.Memo()))
    error_throttler: throttlers.Throttler = dataclasses.field(default_factory=throttlers.Throttler)
    indexing_memory: indexing.IndexingMemory = dataclasses.field(default_factory=indexing.IndexingMemory)
    daemons_memory: daemons.DaemonsMemory = dataclasses.field(default_factory=daemons.DaemonsMemory)

    # For resuming handlers tracking and deciding on should they be called or not.
    noticed_by_listing: bool = False
    fully_handled_once: bool = False


class ResourceMemories(admission.MemoGetter, daemons.DaemonsMemoriesIterator):
    """
    A container of all memos about every existing resource in a single operator.

    Distinct operator tasks have their own memory containers, which
    do not overlap. This solves the problem if storing the per-resource
    entries in the global or context variables.

    The memos can store anything the resource handlers need to persist within
    a single process/operator lifetime, but not persisted on the resource.
    For example, the runtime system resources: flags, threads, tasks, etc.
    Or the scalar values, which have meaning only for this operator process.

    The container is relatively async-safe: one individual resource is always
    handled sequentially, never in parallel with itself (different resources
    are handled in parallel through), so the same key will not be added/deleted
    in the background during the operation, so the locking is not needed.
    """
    _items: MutableMapping[str, ResourceMemory]

    def __init__(self) -> None:
        super().__init__()
        self._items = {}

    def iter_all_memories(self) -> Iterator[ResourceMemory]:
        yield from self._items.values()

    def iter_all_daemon_memories(self) -> Iterator[daemons.DaemonsMemory]:
        for memory in self._items.values():
            yield memory.daemons_memory

    async def recall_memo(
            self,
            raw_body: bodies.RawBody,
            *,
            memobase: Optional[ephemera.AnyMemo] = None,
            ephemeral: bool = False,
    ) -> ephemera.AnyMemo:
        memory = await self.recall(raw_body=raw_body, memobase=memobase, ephemeral=ephemeral)
        return memory.memo

    async def recall(
            self,
            raw_body: bodies.RawBody,
            *,
            memobase: Optional[ephemera.AnyMemo] = None,
            noticed_by_listing: bool = False,
            ephemeral: bool = False,
    ) -> ResourceMemory:
        """
        Either find a resource's memory, or create and remember a new one.

        Keep the last-seen body up to date for all the handlers.

        Ephemeral memos are not remembered now
        (later: will be remembered for short time, and then garbage-collected).
        They are used by admission webhooks before the resource is created --
        to not waste RAM for what might never exist. The persistent memo
        will be created *after* the resource creation really happens.
        """
        key = self._build_key(raw_body)
        if key in self._items:
            memory = self._items[key]
        else:
            if memobase is None:
                memory = ResourceMemory(noticed_by_listing=noticed_by_listing)
            else:
                memo = copy.copy(memobase)
                memory = ResourceMemory(noticed_by_listing=noticed_by_listing, memo=memo)
            if not ephemeral:
                self._items[key] = memory
        return memory

    async def forget(
            self,
            raw_body: bodies.RawBody,
    ) -> None:
        """
        Forget the resource's memory if it exists; or ignore if it does not.
        """
        key = self._build_key(raw_body)
        if key in self._items:
            del self._items[key]

    def _build_key(
            self,
            raw_body: bodies.RawBody,
    ) -> str:
        """
        Construct an immutable persistent key of a resource.

        Generally, a uid is sufficient, as it is unique within the cluster.
        But it can be e.g. plural/namespace/name triplet, or anything else,
        even of different types (as long as it satisfies the type checkers).

        But it must be consistent within a single process lifetime.
        """
        return raw_body.get('metadata', {}).get('uid') or ''



================================================
FILE: kopf/_core/reactor/observation.py
================================================
"""
Keeping track of the cluster setup: namespaces, resources (custom and builtin).

The outcome of observation are "insights" -- a description of the cluster setup,
including the "backbone" -- core resources to be used by the operator/framework.

The resource specifications can be partial or even fuzzy (e.g. by categories),
with zero, one, or more actual resources matching the specification (selector).
Similarly, the namespaces can be specified by patterns or negations of them.

The actual resources & namespaces in the cluster are permanently observed
and matched against the specifications. Only those that do match are served.

If there are no permissions to observe the CRDs or namespaces, the framework
attempts the best possible fallback scenario:

* For CRDs, only the initial scan is done; no runtime observation is performed.
* For namespaces, the namespace patterns are treated as exact namespace names.

A warning is logged unless ``settings.scanning.disabled`` is set to true
to declare this restricted mode as the desired mode of operation.
"""
import asyncio
import functools
import logging
from collections.abc import Collection, Iterable
from typing import Optional

from kopf._cogs.aiokits import aiotoggles
from kopf._cogs.clients import errors, fetching, scanning
from kopf._cogs.configs import configuration
from kopf._cogs.structs import bodies, references
from kopf._core.intents import handlers, registries
from kopf._core.reactor import queueing

logger = logging.getLogger(__name__)


async def namespace_observer(
        *,
        clusterwide: bool,
        namespaces: Collection[references.NamespacePattern],
        insights: references.Insights,
        settings: configuration.OperatorSettings,
) -> None:
    exact_namespaces = references.select_specific_namespaces(namespaces)
    resource = await insights.backbone.wait_for(references.NAMESPACES)

    # Populate the namespaces atomically (instead of notifying on every item from the watch-stream).
    if not settings.scanning.disabled and not clusterwide:
        try:
            objs, _ = await fetching.list_objs(
                settings=settings,
                resource=resource,
                namespace=None,
                logger=logger,
            )
            async with insights.revised:
                revise_namespaces(raw_bodies=objs, insights=insights, namespaces=namespaces)
                insights.revised.notify_all()
        except errors.APIForbiddenError:
            logger.warning("Not enough permissions to list namespaces. "
                           "Falling back to a list of namespaces which are assumed to exist: "
                           f"{exact_namespaces!r}")
            async with insights.revised:
                insights.namespaces.update(exact_namespaces)
                insights.revised.notify_all()
    else:
        async with insights.revised:
            insights.namespaces.update({None} if clusterwide else exact_namespaces)
            insights.revised.notify_all()

    # Notify those waiting for the initial listing (e.g. CLI commands).
    insights.ready_namespaces.set()

    if not settings.scanning.disabled and not clusterwide:
        try:
            await queueing.watcher(
                settings=settings,
                resource=resource,
                namespace=None,
                processor=functools.partial(process_discovered_namespace_event,
                                            namespaces=namespaces,
                                            insights=insights))
        except errors.APIForbiddenError:
            logger.warning("Not enough permissions to watch for namespaces: "
                           "changes (deletion/creation) will not be noticed; "
                           "the namespaces are only refreshed on operator restarts.")
            await asyncio.Event().wait()
    else:
        await asyncio.Event().wait()


async def resource_observer(
        *,
        settings: configuration.OperatorSettings,
        registry: registries.OperatorRegistry,
        insights: references.Insights,
) -> None:

    # Scan only the resource-related handlers, ignore activies & co.
    all_handlers: list[handlers.ResourceHandler] = []
    all_handlers.extend(registry._webhooks.get_all_handlers())
    all_handlers.extend(registry._indexing.get_all_handlers())
    all_handlers.extend(registry._watching.get_all_handlers())
    all_handlers.extend(registry._spawning.get_all_handlers())
    all_handlers.extend(registry._changing.get_all_handlers())
    groups = {handler.selector.group for handler in all_handlers if handler.selector is not None}
    groups.update({selector.group for selector in insights.backbone.selectors})

    # Prepopulate the resources before the dimension watchers start, so that each initially listed
    # namespace would start a watcher, and each initially listed CRD is already on the list.
    group_filter = None if None in groups else {group for group in groups if group is not None}
    resources = await scanning.scan_resources(groups=group_filter, settings=settings, logger=logger)
    async with insights.revised:
        revise_resources(resources=resources, insights=insights, registry=registry, group=None)
        await insights.backbone.fill(resources=resources)
        insights.revised.notify_all()

    # Notify those waiting for the initial listing (e.g. CLI commands).
    insights.ready_resources.set()

    # The resource watcher starts with an initial listing, and later reacts to any changes. However,
    # the existing resources are known already, so there will be no changes on the initial listing.
    resource = await insights.backbone.wait_for(references.CRDS)
    if not settings.scanning.disabled:
        try:
            await queueing.watcher(
                settings=settings,
                resource=resource,
                namespace=None,
                processor=functools.partial(process_discovered_resource_event,
                                            settings=settings,
                                            registry=registry,
                                            insights=insights))
        except errors.APIForbiddenError:
            logger.warning("Not enough permissions to watch for resources: "
                           "changes (creation/deletion/updates) will not be noticed; "
                           "the resources are only refreshed on operator restarts.")
            await asyncio.Event().wait()
    else:
        await asyncio.Event().wait()


async def process_discovered_namespace_event(
        *,
        raw_event: bodies.RawEvent,
        namespaces: Collection[references.NamespacePattern],
        insights: references.Insights,
        # Must be accepted whether used or not -- as passed by watcher()/worker().
        stream_pressure: Optional[asyncio.Event] = None,  # None for tests
        resource_indexed: Optional[aiotoggles.Toggle] = None,  # None for tests & observation
        operator_indexed: Optional[aiotoggles.ToggleSet] = None,  # None for tests & observation
) -> None:
    if raw_event['type'] is None:
        return

    async with insights.revised:
        revise_namespaces(raw_events=[raw_event], insights=insights, namespaces=namespaces)
        insights.revised.notify_all()


async def process_discovered_resource_event(
        *,
        raw_event: bodies.RawEvent,
        settings: configuration.OperatorSettings,
        registry: registries.OperatorRegistry,
        insights: references.Insights,
        # Must be accepted whether used or not -- as passed by watcher()/worker().
        stream_pressure: Optional[asyncio.Event] = None,  # None for tests
        resource_indexed: Optional[aiotoggles.Toggle] = None,  # None for tests & observation
        operator_indexed: Optional[aiotoggles.ToggleSet] = None,  # None for tests & observation
) -> None:
    # Ignore the initial listing, as all custom resources were already noticed by API listing.
    # This prevents numerous unneccessary API requests at the the start of the operator.
    if raw_event['type'] is None:
        return

    # Re-scan the whole dimension of resources if any single one of them changes. By this, we make
    # K8s's /apis/ endpoint the source of truth for all resources & versions & preferred versions,
    # instead of mimicking K8s in interpreting them ourselves (a probable source of bugs).
    # As long as it is atomic (for asyncio, i.e. sync), the existing tasks will not be affected.
    group = raw_event['object']['spec']['group']
    resources = await scanning.scan_resources(groups={group}, settings=settings, logger=logger)
    async with insights.revised:
        revise_resources(resources=resources, insights=insights, registry=registry, group=group)
        await insights.backbone.fill(resources=resources)
        insights.revised.notify_all()


def revise_namespaces(
        *,
        insights: references.Insights,
        namespaces: Collection[references.NamespacePattern],
        raw_events: Collection[bodies.RawEvent] = (),
        raw_bodies: Collection[bodies.RawBody] = (),
) -> None:
    all_events = list(raw_events) + [bodies.RawEvent(type=None, object=obj) for obj in raw_bodies]
    for raw_event in all_events:
        namespace = references.NamespaceName(raw_event['object']['metadata']['name'])
        matched = any(references.match_namespace(namespace, pattern) for pattern in namespaces)
        deleted = is_deleted(raw_event)
        if deleted:
            insights.namespaces.discard(namespace)
        elif matched:
            insights.namespaces.add(namespace)


def revise_resources(
        *,
        group: Optional[str],
        insights: references.Insights,
        registry: registries.OperatorRegistry,
        resources: Collection[references.Resource],
) -> None:

    # Scan only the resource-related handlers grouped by purpose; ignore activities & co.
    webhook_selectors = registry._webhooks.get_all_selectors()
    indexed_selectors = registry._indexing.get_all_selectors()
    watched_selectors = (
        registry._indexing.get_all_selectors() |
        registry._watching.get_all_selectors() |
        registry._spawning.get_all_selectors() |
        registry._changing.get_all_selectors()
    )
    patched_selectors = (
        registry._spawning.get_all_selectors() |
        registry._changing.get_all_selectors()
    )

    # Note: indexed and webhook resources are not checked for ambiguity or empty matching:
    # - the indexed resources are not served/watched directly and are only used as an utility cache;
    # - the webhook resources are PASSIVELY matched per HTTP request, so ambiguity is not a problem.
    # Ambiguity is a potential problem only for regular resource handlers because the operators
    # ACTIVELY trigger them and produce irreversible side-effects --- even if improperly configured.
    _update_resources(insights.webhook_resources, webhook_selectors, group=group, source=resources)
    _update_resources(insights.indexed_resources, indexed_selectors, group=group, source=resources)
    _update_resources(insights.watched_resources, watched_selectors, group=group, source=resources)
    _disable_ambiguous_selectors(resources=insights.watched_resources, selectors=watched_selectors)
    _disable_mismatched_selectors(resources=insights.watched_resources, selectors=watched_selectors)
    _disable_unsuitable_resources(resources=insights.watched_resources, selectors=patched_selectors)


def _update_resources(
        resources: set[references.Resource],
        selectors: Iterable[references.Selector],
        *,
        group: Optional[str],
        source: Collection[references.Resource],
) -> None:
    """
    Update all or the group's resources from the source of resources.

    This also excludes the resources that continue to exist but stop matching
    the selectors: e.g. by category --- if a CRD's categories were modified.

    WARNING: We do not block the CRDs by adding finalizers (for simplicity),
    so it can be so that we miss the CRD deletion event and continue
    the watching attempts (and fail with HTTP 404).
    """

    # Exclude previously served resources that are gone now.
    group_resources = {resource for resource in resources if group in [None, resource.group]}
    resources.difference_update(group_resources)

    # Include or re-include the resources that are [still] served.
    for selector in selectors:
        resources.update(selector.select(source))


def _disable_ambiguous_selectors(
        *,
        resources: set[references.Resource],
        selectors: Iterable[references.Selector],
) -> None:
    """
    Detect ambiguous selectors and stop serving/watching them.

    Ambiguous selectors are those matching 2+ distinct resources.
    For example, if a selector specifies "pods" and there are resources
    "pods.v1" and "pods.v1beta1.metrics.k8s.io" (but only if non-v1 resources
    cannot be filtered out completely; otherwise, implicitly prefer v1).
    """
    for selector in selectors:
        selected = selector.select(resources)
        if selector.is_specific and len(selected) > 1:
            logger.warning("Ambiguous resources will not be served (try specifying API groups):"
                           f" {selector} => {selected}")
            resources.difference_update(selected)


def _disable_mismatched_selectors(
        *,
        resources: set[references.Resource],
        selectors: frozenset[references.Selector],
) -> None:
    """
    Warn for handlers that specify nonexistent resources.

    This can be due to a typo or a misconfiguration or CRDs are not yet created.
    """
    selector_names = ", ".join(
        f"{selector}"
        for selector in selectors
        if not selector.select(resources)
    )
    if selector_names:
        logger.warning("Unresolved resources cannot be served (try creating their CRDs):"
                       f" {selector_names}")


def _disable_unsuitable_resources(
        *,
        resources: set[references.Resource],
        selectors: frozenset[references.Selector],
) -> None:

    # For both watching & patching, only look at watched resources, ignore webhook-only resources.
    nonwatchable_resources = {resource for resource in resources
                              if 'watch' not in resource.verbs or 'list' not in resource.verbs}
    nonpatchable_resources = {resource for resource in resources
                              if 'patch' not in resource.verbs} - nonwatchable_resources

    # For patching, only react if there are handlers that store a state (i.e. not on-event/index).
    patching_required = any(selector.select(nonpatchable_resources) for selector in selectors)

    if nonwatchable_resources:
        logger.warning(f"Non-watchable resources will not be served: {nonwatchable_resources}")
        resources.difference_update(nonwatchable_resources)
    if nonpatchable_resources and patching_required:
        logger.warning(f"Non-patchable resources will not be served: {nonpatchable_resources}")
        resources.difference_update(nonpatchable_resources)


def is_deleted(raw_event: bodies.RawEvent) -> bool:
    marked_as_deleted = bool(raw_event['object'].get('metadata', {}).get('deletionTimestamp'))
    really_is_deleted = raw_event['type'] == 'DELETED'
    return marked_as_deleted or really_is_deleted



================================================
FILE: kopf/_core/reactor/orchestration.py
================================================
"""
Orchestrating the tasks for served resources & namespaces.

The resources & namespaces are observed in :mod:`.observation`, where they
are stored in the "insights" -- a description of the current cluster setup.
They are used as the input for the orchestration.

For every combination of every actual resource & every actual namespace,
there is a watcher task and a few optional peering tasks/toggles.
The tasks are started when new values are added to any of these dimension,
or stopped when some existing values are removed.

There are several kinds of tasks:

* Regular watchers (watch-streams) -- the main one.
* Peering watchers (watch-streams).
* Peering keep-alives (pingers).

The peering tasks are started only when the peering is enabled at all.
For peering, the resource is not used, only the namespace is of importance.

Some special watchers for the meta-level resources -- i.e. for dimensions --
are started and stopped separately, not as part of the the orchestration.
"""
import asyncio
import dataclasses
import functools
import itertools
import logging
from collections.abc import Collection, Container, Iterable, MutableMapping
from typing import Any, NamedTuple, Optional, Protocol

from kopf._cogs.aiokits import aiotasks, aiotoggles
from kopf._cogs.configs import configuration
from kopf._cogs.structs import bodies, references
from kopf._core.engines import peering
from kopf._core.reactor import queueing

logger = logging.getLogger(__name__)


class EnsembleKey(NamedTuple):
    resource: references.Resource
    namespace: references.Namespace


# Differs from queueing.WatchStreamProcessor by the resource=… kwarg.
class ResourceWatchStreamProcessor(Protocol):
    async def __call__(
            self,
            *,
            resource: references.Resource,
            raw_event: bodies.RawEvent,
            stream_pressure: Optional[asyncio.Event] = None,  # None for tests
            resource_indexed: Optional[aiotoggles.Toggle] = None,  # None for tests & observation
            operator_indexed: Optional[aiotoggles.ToggleSet] = None,  # None for tests & observation
    ) -> None: ...


@dataclasses.dataclass
class Ensemble:

    # Global synchronisation point on the cache pre-populating stage and overall cache readiness.
    # Note: there is no need for ToggleSet; it is checked by emptiness of items inside.
    #       ToggleSet is used because it is the closest equivalent of such a primitive.
    operator_indexed: aiotoggles.ToggleSet

    # Multidimentional pausing: for every namespace, and a few for the whole cluster (for CRDs).
    operator_paused: aiotoggles.ToggleSet
    peering_missing: aiotoggles.Toggle
    conflicts_found: dict[EnsembleKey, aiotoggles.Toggle] = dataclasses.field(default_factory=dict)

    # Multidimensional tasks -- one for every combination of relevant dimensions.
    watcher_tasks: dict[EnsembleKey, aiotasks.Task] = dataclasses.field(default_factory=dict)
    peering_tasks: dict[EnsembleKey, aiotasks.Task] = dataclasses.field(default_factory=dict)
    pinging_tasks: dict[EnsembleKey, aiotasks.Task] = dataclasses.field(default_factory=dict)

    def get_keys(self) -> Collection[EnsembleKey]:
        return (frozenset(self.watcher_tasks) |
                frozenset(self.peering_tasks) |
                frozenset(self.pinging_tasks) |
                frozenset(self.conflicts_found))

    def get_tasks(self, keys: Container[EnsembleKey]) -> Collection[aiotasks.Task]:
        return {task
                for tasks in [self.watcher_tasks, self.peering_tasks, self.pinging_tasks]
                for key, task in tasks.items() if key in keys}

    def get_flags(self, keys: Container[EnsembleKey]) -> Collection[aiotoggles.Toggle]:
        return {toggle for key, toggle in self.conflicts_found.items() if key in keys}

    def del_keys(self, keys: Container[EnsembleKey]) -> None:
        d: MutableMapping[EnsembleKey, Any]
        for d in [self.watcher_tasks, self.peering_tasks, self.pinging_tasks]:
            for key in set(d):
                if key in keys:
                    del d[key]
        for d in [self.conflicts_found]:  # separated for easier type inferrence
            for key in set(d):
                if key in keys:
                    del d[key]


async def ochestrator(
        *,
        processor: ResourceWatchStreamProcessor,
        settings: configuration.OperatorSettings,
        identity: peering.Identity,
        insights: references.Insights,
        operator_paused: aiotoggles.ToggleSet,
) -> None:
    peering_missing = await operator_paused.make_toggle(name='peering CRD is missing')
    ensemble = Ensemble(
        peering_missing=peering_missing,
        operator_paused=operator_paused,
        operator_indexed=aiotoggles.ToggleSet(all),
    )
    try:
        async with insights.revised:
            while True:
                await insights.revised.wait()
                await adjust_tasks(
                    processor=processor,
                    insights=insights,
                    settings=settings,
                    identity=identity,
                    ensemble=ensemble,
                )
    except asyncio.CancelledError:
        tasks = ensemble.get_tasks(ensemble.get_keys())
        await aiotasks.stop(tasks, title="streaming", logger=logger, interval=10)
        raise


# Directly corresponds to one iteration of an orchestrator, but it is extracted for testability:
# for a simulation of the insights (inputs) and an assertion of the tasks & toggles (outputs).
async def adjust_tasks(
        *,
        processor: ResourceWatchStreamProcessor,
        insights: references.Insights,
        settings: configuration.OperatorSettings,
        identity: peering.Identity,
        ensemble: Ensemble,
) -> None:
    peering_selectors = peering.guess_selectors(settings=settings)
    peering_resources = {insights.backbone[s] for s in peering_selectors if s in insights.backbone}

    # Pause or resume all streams if the peering CRDs are absent but required.
    # Ignore the CRD absence in auto-detection mode: pause only when (and if) the CRDs are added.
    await ensemble.peering_missing.turn_to(settings.peering.mandatory and not peering_resources)

    # Stop & start the tasks to match the task matrix with the cluster insights.
    # As a rule of thumb, stop the tasks first, start later -- not vice versa!
    await terminate_redundancies(ensemble=ensemble,
                                 remaining_resources=insights.watched_resources | peering_resources,
                                 remaining_namespaces=insights.namespaces | {None})
    await spawn_missing_peerings(ensemble=ensemble,
                                 settings=settings,
                                 identity=identity,
                                 resources=peering_resources,
                                 namespaces=insights.namespaces)
    await spawn_missing_watchers(ensemble=ensemble,
                                 settings=settings,
                                 processor=processor,
                                 indexed_resources=insights.indexed_resources,
                                 watched_resources=insights.watched_resources,
                                 watched_namespaces=insights.namespaces)


async def terminate_redundancies(
        *,
        remaining_resources: Collection[references.Resource],
        remaining_namespaces: Collection[references.Namespace],
        ensemble: Ensemble,
) -> None:
    # Do not distinguish the keys: even for the case when the peering CRD is served by the operator,
    # for the peering CRD or namespace deletion, both tasks are stopped together, never apart.
    redundant_keys = {key for key in ensemble.get_keys()
                      if key.namespace not in remaining_namespaces
                      or key.resource not in remaining_resources}
    redundant_tasks = ensemble.get_tasks(redundant_keys)
    redundant_flags = ensemble.get_flags(redundant_keys)
    await aiotasks.stop(redundant_tasks, title="streaming", logger=logger, interval=10, quiet=True)
    await ensemble.operator_paused.drop_toggles(redundant_flags)
    ensemble.del_keys(redundant_keys)


async def spawn_missing_peerings(
        *,
        settings: configuration.OperatorSettings,
        identity: peering.Identity,
        resources: Collection[references.Resource],
        namespaces: Collection[references.Namespace],
        ensemble: Ensemble,
) -> None:
    for resource, namespace in itertools.product(resources, namespaces):
        namespace = namespace if resource.namespaced else None
        dkey = EnsembleKey(resource=resource, namespace=namespace)
        if dkey not in ensemble.peering_tasks:
            what = f"{settings.peering.name}@{namespace}"
            is_preactivated = settings.peering.mandatory
            conflicts_found = await ensemble.operator_paused.make_toggle(is_preactivated, name=what)
            ensemble.conflicts_found[dkey] = conflicts_found
            ensemble.pinging_tasks[dkey] = aiotasks.create_guarded_task(
                name=f"peering keep-alive for {what}", logger=logger, cancellable=True,
                coro=peering.keepalive(
                    namespace=namespace,
                    resource=resource,
                    settings=settings,
                    identity=identity))
            ensemble.peering_tasks[dkey] = aiotasks.create_guarded_task(
                name=f"peering observer for {what}", logger=logger, cancellable=True,
                coro=queueing.watcher(
                    settings=settings,
                    resource=resource,
                    namespace=namespace,
                    processor=functools.partial(peering.process_peering_event,
                                                conflicts_found=conflicts_found,
                                                namespace=namespace,
                                                resource=resource,
                                                settings=settings,
                                                identity=identity)))

    # Ensure that all guarded tasks got control for a moment to enter the guard.
    await asyncio.sleep(0)


async def spawn_missing_watchers(
        *,
        processor: ResourceWatchStreamProcessor,
        settings: configuration.OperatorSettings,
        indexed_resources: Container[references.Resource],  # only "if in", never "for in"!
        watched_resources: Iterable[references.Resource],
        watched_namespaces: Iterable[references.Namespace],
        ensemble: Ensemble,
) -> None:

    # Block the operator globally until specialised per-resource-kind blockers are created.
    # NB: Must be created before the point of parallelisation!
    operator_blocked = await ensemble.operator_indexed.make_toggle(name="orchestration blocker")

    # Spawn watchers and create the specialised per-resource-kind blockers.
    for resource, namespace in itertools.product(watched_resources, watched_namespaces):
        namespace = namespace if resource.namespaced else None
        dkey = EnsembleKey(resource=resource, namespace=namespace)
        if dkey not in ensemble.watcher_tasks:
            what = f"{resource}@{namespace}"
            resource_indexed: Optional[aiotoggles.Toggle] = None
            if resource in indexed_resources:
                resource_indexed = await ensemble.operator_indexed.make_toggle(name=what)
            ensemble.watcher_tasks[dkey] = aiotasks.create_guarded_task(
                name=f"watcher for {what}", logger=logger, cancellable=True,
                coro=queueing.watcher(
                    operator_paused=ensemble.operator_paused,
                    operator_indexed=ensemble.operator_indexed,
                    resource_indexed=resource_indexed,
                    settings=settings,
                    resource=resource,
                    namespace=namespace,
                    processor=functools.partial(processor, resource=resource)))

    # Unblock globally, let the specialised per-resource-kind blockers hold the readiness.
    await ensemble.operator_indexed.drop_toggle(operator_blocked)

    # Ensure that all guarded tasks got control for a moment to enter the guard.
    await asyncio.sleep(0)



================================================
FILE: kopf/_core/reactor/processing.py
================================================
"""
Conversion of low-level events to high-level causes, and handling them.

These functions are invoked from `kopf._core.reactor.processing`,
which are the actual event loop of the operator process.

The conversion of the low-level events to the high-level causes is done by
checking the object's state and comparing it to the preserved last-seen state.

The framework itself makes the necessary changes to the object, -- such as the
finalizers attachment, last-seen state updates, and handler status tracking, --
thus provoking the low-level watch-events and additional queueing calls.
But these internal changes are filtered out from the cause detection
and therefore do not trigger the user-defined handlers.
"""
import asyncio
import time
from collections.abc import Collection
from typing import Optional

from kopf._cogs.aiokits import aiotoggles
from kopf._cogs.configs import configuration
from kopf._cogs.structs import bodies, diffs, ephemera, finalizers, patches, references
from kopf._core.actions import application, execution, lifecycles, loggers, progression, throttlers
from kopf._core.engines import daemons, indexing, posting
from kopf._core.intents import causes, registries
from kopf._core.reactor import inventory, subhandling


async def process_resource_event(
        lifecycle: execution.LifeCycleFn,
        indexers: indexing.OperatorIndexers,
        registry: registries.OperatorRegistry,
        settings: configuration.OperatorSettings,
        memories: inventory.ResourceMemories,
        memobase: ephemera.AnyMemo,
        resource: references.Resource,
        raw_event: bodies.RawEvent,
        event_queue: posting.K8sEventQueue,
        stream_pressure: Optional[asyncio.Event] = None,  # None for tests
        resource_indexed: Optional[aiotoggles.Toggle] = None,  # None for tests & observation
        operator_indexed: Optional[aiotoggles.ToggleSet] = None,  # None for tests & observation
) -> None:
    """
    Handle a single custom object low-level watch-event.

    Convert the low-level events, as provided by the watching/queueing tasks,
    to the high-level causes, and then call the cause-handling logic.
    """

    # Recall what is stored about that object. Share it in little portions with the consumers.
    # And immediately forget it if the object is deleted from the cluster (but keep in memory).
    raw_type, raw_body = raw_event['type'], raw_event['object']
    memory = await memories.recall(raw_body, noticed_by_listing=raw_type is None, memobase=memobase)
    if memory.daemons_memory.live_fresh_body is not None:
        memory.daemons_memory.live_fresh_body._replace_with(raw_body)
    if raw_type == 'DELETED':
        await memories.forget(raw_body)

    # Convert to a heavy mapping-view wrapper only now, when heavy processing begins.
    # Raw-event streaming, queueing, and batching use regular lightweight dicts.
    # Why here? 1. Before it splits into multiple causes & handlers for the same object's body;
    # 2. After it is batched (queueing); 3. While the "raw" parsed JSON is still known;
    # 4. Same as where a patch object of a similar wrapping semantics is created.
    live_fresh_body = memory.daemons_memory.live_fresh_body
    body = live_fresh_body if live_fresh_body is not None else bodies.Body(raw_body)
    patch = patches.Patch()

    # Different loggers for different cases with different verbosity and exposure.
    local_logger = loggers.LocalObjectLogger(body=body, settings=settings)
    terse_logger = loggers.TerseObjectLogger(body=body, settings=settings)
    event_logger = loggers.ObjectLogger(body=body, settings=settings)

    # Throttle the non-handler-related errors. The regular event watching/batching continues
    # to prevent queue overfilling, but the processing is skipped (events are ignored).
    # Choice of place: late enough to have a per-resource memory for a throttler; also, a logger.
    # But early enough to catch environment errors from K8s API, and from most of the complex code.
    async with throttlers.throttled(
        throttler=memory.error_throttler,
        logger=local_logger,
        delays=settings.batching.error_delays,
        wakeup=stream_pressure,
    ) as should_run:
        if should_run:

            # Each object has its own prefixed logger, to distinguish parallel handling.
            posting.event_queue_loop_var.set(asyncio.get_running_loop())
            posting.event_queue_var.set(event_queue)  # till the end of this object's task.

            # [Pre-]populate the indices. This must be lightweight.
            await indexing.index_resource(
                registry=registry,
                indexers=indexers,
                settings=settings,
                resource=resource,
                raw_event=raw_event,
                body=body,
                memo=memory.memo,
                memory=memory.indexing_memory,
                logger=terse_logger,
            )

            # Wait for all other individual resources and all other resource kinds' lists to finish.
            # If this one has changed while waiting for the global readiness, let it be reprocessed.
            if operator_indexed is not None and resource_indexed is not None:
                await operator_indexed.drop_toggle(resource_indexed)
            if operator_indexed is not None:
                await operator_indexed.wait_for(True)  # other resource kinds & objects.
            if stream_pressure is not None and stream_pressure.is_set():
                return

            # Do the magic -- do the job.
            delays, matched = await process_resource_causes(
                lifecycle=lifecycle,
                indexers=indexers,
                registry=registry,
                settings=settings,
                resource=resource,
                raw_event=raw_event,
                body=body,
                patch=patch,
                memory=memory,
                local_logger=local_logger,
                event_logger=event_logger,
            )

            # Whatever was done, apply the accumulated changes to the object, or sleep-n-touch for delays.
            # But only once, to reduce the number of API calls and the generated irrelevant events.
            # And only if the object is at least supposed to exist (not "GONE"), even if actually does not.
            if raw_event['type'] != 'DELETED':
                applied = await application.apply(
                    settings=settings,
                    resource=resource,
                    body=body,
                    patch=patch,
                    logger=local_logger,
                    delays=delays,
                    stream_pressure=stream_pressure,
                )
                if applied and matched:
                    local_logger.debug("Handling cycle is finished, waiting for new changes.")


async def process_resource_causes(
        lifecycle: execution.LifeCycleFn,
        indexers: indexing.OperatorIndexers,
        registry: registries.OperatorRegistry,
        settings: configuration.OperatorSettings,
        resource: references.Resource,
        raw_event: bodies.RawEvent,
        body: bodies.Body,
        patch: patches.Patch,
        memory: inventory.ResourceMemory,
        local_logger: loggers.ObjectLogger,
        event_logger: loggers.ObjectLogger,
) -> tuple[Collection[float], bool]:

    finalizer = settings.persistence.finalizer
    extra_fields = (
        # NB: indexing handlers are useless here, they are handled on their own.
        registry._watching.get_extra_fields(resource=resource) |
        registry._changing.get_extra_fields(resource=resource) |
        registry._spawning.get_extra_fields(resource=resource))
    old = settings.persistence.diffbase_storage.fetch(body=body)
    new = settings.persistence.diffbase_storage.build(body=body, extra_fields=extra_fields)
    old = settings.persistence.progress_storage.clear(essence=old) if old is not None else None
    new = settings.persistence.progress_storage.clear(essence=new) if new is not None else None
    diff = diffs.diff(old, new)

    # Detect what are we going to do on this processing cycle.
    watching_cause = causes.detect_watching_cause(
        raw_event=raw_event,
        resource=resource,
        indices=indexers.indices,
        logger=local_logger,
        patch=patch,
        body=body,
        memo=memory.memo,
    ) if registry._watching.has_handlers(resource=resource) else None

    spawning_cause = causes.detect_spawning_cause(
        resource=resource,
        indices=indexers.indices,
        logger=event_logger,
        patch=patch,
        body=body,
        memo=memory.memo,
        reset=bool(diff),  # only essential changes reset idling, not every event
    ) if registry._spawning.has_handlers(resource=resource) else None

    changing_cause = causes.detect_changing_cause(
        finalizer=finalizer,
        raw_event=raw_event,
        resource=resource,
        indices=indexers.indices,
        logger=event_logger,
        patch=patch,
        body=body,
        old=old,
        new=new,
        diff=diff,
        memo=memory.memo,
        initial=memory.noticed_by_listing and not memory.fully_handled_once,
    ) if registry._changing.has_handlers(resource=resource) else None

    # If there are any handlers for this resource kind in general, but not for this specific object
    # due to filters, then be blind to it, store no state, and log nothing about the handling cycle.
    if changing_cause is not None and not registry._changing.prematch(cause=changing_cause):
        changing_cause = None

    # Block the object from deletion if we have anything to do in its end of life:
    # specifically, if there are daemons to kill or mandatory on-deletion handlers to call.
    # The high-level handlers are prevented if this event cycle is dedicated to the finalizer.
    # The low-level handlers (on-event spying & daemon spawning) are still executed asap.
    deletion_is_ongoing = finalizers.is_deletion_ongoing(body=body)
    deletion_is_blocked = finalizers.is_deletion_blocked(body=body, finalizer=finalizer)
    deletion_must_be_blocked = (
        (spawning_cause is not None and
         registry._spawning.requires_finalizer(
             cause=spawning_cause,
             excluded=memory.daemons_memory.forever_stopped,
         ))
        or
        (changing_cause is not None and
         registry._changing.requires_finalizer(
             cause=changing_cause,
         )))

    if deletion_must_be_blocked and not deletion_is_blocked and not deletion_is_ongoing:
        local_logger.debug("Adding the finalizer, thus preventing the actual deletion.")
        finalizers.block_deletion(body=body, patch=patch, finalizer=finalizer)
        changing_cause = None  # prevent further high-level processing this time

    if not deletion_must_be_blocked and deletion_is_blocked:
        local_logger.debug("Removing the finalizer, as there are no handlers requiring it.")
        finalizers.allow_deletion(body=body, patch=patch, finalizer=finalizer)
        changing_cause = None  # prevent further high-level processing this time

    # Invoke all the handlers that should or could be invoked at this processing cycle.
    # The low-level spies go ASAP always. However, the daemons are spawned before the high-level
    # handlers and killed after them: the daemons should live throughout the full object lifecycle.
    if watching_cause is not None:
        await process_watching_cause(
            lifecycle=lifecycles.all_at_once,
            registry=registry,
            settings=settings,
            cause=watching_cause,
        )

    spawning_delays: Collection[float] = []
    if spawning_cause is not None:
        spawning_delays = await process_spawning_cause(
            registry=registry,
            settings=settings,
            memory=memory,
            cause=spawning_cause,
        )

    changing_delays: Collection[float] = []
    if changing_cause is not None:
        changing_delays = await process_changing_cause(
            lifecycle=lifecycle,
            registry=registry,
            settings=settings,
            memory=memory,
            cause=changing_cause,
        )

    # Release the object if everything is done, and it is marked for deletion.
    # But not when it has already gone.
    if deletion_is_ongoing and deletion_is_blocked and not spawning_delays and not changing_delays:
        local_logger.debug("Removing the finalizer, thus allowing the actual deletion.")
        finalizers.allow_deletion(body=body, patch=patch, finalizer=finalizer)

    delays = list(spawning_delays) + list(changing_delays)
    return (delays, changing_cause is not None)


async def process_watching_cause(
        lifecycle: execution.LifeCycleFn,
        registry: registries.OperatorRegistry,
        settings: configuration.OperatorSettings,
        cause: causes.WatchingCause,
) -> None:
    """
    Handle a received event, log but ignore all errors.

    This is a lightweight version of the cause handling, but for the raw events,
    without any progress persistence. Multi-step calls are also not supported.
    If the handler fails, it fails and is never retried.

    Note: K8s-event posting is skipped for `kopf.on.event` handlers,
    as they should be silent. Still, the messages are logged normally.
    """
    handlers = registry._watching.get_handlers(cause=cause)
    outcomes = await execution.execute_handlers_once(
        lifecycle=lifecycle,
        settings=settings,
        handlers=handlers,
        cause=cause,
        state=progression.State.from_scratch().with_handlers(handlers),
        default_errors=execution.ErrorsMode.IGNORED,
    )

    # Store the results, but not the handlers' progress.
    progression.deliver_results(outcomes=outcomes, patch=cause.patch)


async def process_spawning_cause(
        registry: registries.OperatorRegistry,
        settings: configuration.OperatorSettings,
        memory: inventory.ResourceMemory,
        cause: causes.SpawningCause,
) -> Collection[float]:
    """
    Spawn/kill all the background tasks of a resource.

    The spawning and killing happens in parallel with the resource-changing
    handlers invocation (even if it takes a few cycles). For this, the signal
    to terminate is sent to the daemons immediately, but the actual check
    of their shutdown is performed only when all the on-deletion handlers
    have succeeded (or after they were invoked if they are optional;
    or immediately if there were no on-deletion handlers to invoke at all).

    The resource remains blocked by the finalizers until all the daemons exit
    (except those marked as tolerating being orphaned).
    """

    # Refresh the up-to-date body & essential timestamp for all the daemons/timers.
    if memory.daemons_memory.live_fresh_body is None:
        memory.daemons_memory.live_fresh_body = cause.body
    if cause.reset:
        memory.daemons_memory.idle_reset_time = time.monotonic()

    if finalizers.is_deletion_ongoing(cause.body):
        stopping_delays = await daemons.stop_daemons(
            settings=settings,
            daemons=memory.daemons_memory.running_daemons,
        )
        return stopping_delays

    else:
        handlers = registry._spawning.get_handlers(
            cause=cause,
            excluded=memory.daemons_memory.forever_stopped,
        )
        spawning_delays = await daemons.spawn_daemons(
            settings=settings,
            daemons=memory.daemons_memory.running_daemons,
            cause=cause,
            memory=memory.daemons_memory,
            handlers=handlers,
        )
        matching_delays = await daemons.match_daemons(
            settings=settings,
            daemons=memory.daemons_memory.running_daemons,
            handlers=handlers,
        )
        return list(spawning_delays) + list(matching_delays)


async def process_changing_cause(
        lifecycle: execution.LifeCycleFn,
        registry: registries.OperatorRegistry,
        settings: configuration.OperatorSettings,
        memory: inventory.ResourceMemory,
        cause: causes.ChangingCause,
) -> Collection[float]:
    """
    Handle a detected cause, as part of the bigger handler routine.
    """
    logger = cause.logger
    patch = cause.patch  # TODO get rid of this alias
    body = cause.body  # TODO get rid of this alias
    delays: Collection[float] = []
    done: Optional[bool] = None
    skip: Optional[bool] = None

    # Regular causes invoke the handlers.
    if cause.reason in causes.HANDLER_REASONS:
        title = causes.TITLES.get(cause.reason.value, repr(cause.reason.value))

        resource_registry = registry._changing
        owned_handlers = resource_registry.get_resource_handlers(resource=cause.resource)
        cause_handlers = resource_registry.get_handlers(cause=cause)
        storage = settings.persistence.progress_storage
        state = progression.State.from_storage(body=cause.body, storage=storage, handlers=owned_handlers)
        state = state.with_purpose(cause.reason).with_handlers(cause_handlers)

        # Report the causes that have been superseded (intercepted, overridden) by the current one.
        # The mix-in causes (i.e. resuming) is re-purposed if its handlers are still selected.
        # To the next cycle, all extras are purged or re-purposed, so the message does not repeat.
        for extra_purpose, counters in state.extras.items():  # usually 0..1 items, rarely 2+.
            extra_title = causes.TITLES.get(extra_purpose, repr(extra_purpose))
            logger.info(f"{extra_title.capitalize()} is superseded by {title.lower()}: "
                        f"{counters.success} succeeded; "
                        f"{counters.failure} failed; "
                        f"{counters.running} left to the moment.")
            state = state.with_purpose(purpose=cause.reason, handlers=cause_handlers)

        # Purge the now-irrelevant handlers if they were not re-purposed (extras are recalculated!).
        # The current cause continues afterwards, and overrides its own pre-purged handler states.
        # TODO: purge only the handlers that fell out of current purpose; but it is not critical
        if state.extras:
            state.purge(body=cause.body, patch=cause.patch,
                        storage=storage, handlers=owned_handlers)

        # Inform on the current cause/event on every processing cycle. Even if there are
        # no handlers -- to show what has happened and why the diff-base is patched.
        logger.debug(f"{title.capitalize()} is in progress: {body!r}")
        if cause.diff and cause.old is not None and cause.new is not None:
            logger.debug(f"{title.capitalize()} diff: {cause.diff!r}")

        if cause_handlers:
            outcomes = await execution.execute_handlers_once(
                lifecycle=lifecycle,
                settings=settings,
                handlers=cause_handlers,
                cause=cause,
                state=state,
                extra_context=subhandling.subhandling_context,
            )
            state = state.with_outcomes(outcomes)
            state.store(body=cause.body, patch=cause.patch, storage=storage)
            progression.deliver_results(outcomes=outcomes, patch=cause.patch)

            if state.done:
                counters = state.counts  # calculate only once
                logger.info(f"{title.capitalize()} is processed: "
                            f"{counters.success} succeeded; "
                            f"{counters.failure} failed.")
                state.purge(body=cause.body, patch=cause.patch,
                            storage=storage, handlers=owned_handlers)

            done = state.done
            delays = state.delays
        else:
            skip = True

    # Regular causes also do some implicit post-handling when all handlers are done.
    if done or skip:
        if cause.new is not None and cause.old != cause.new:
            settings.persistence.diffbase_storage.store(body=body, patch=patch, essence=cause.new)

        # Once all handlers have succeeded at least once for any reason, or if there were none,
        # prevent further resume-handlers (which otherwise happens on each watch-stream re-listing).
        memory.fully_handled_once = True

    # Informational causes just print the log lines.
    if cause.reason == causes.Reason.GONE:
        logger.debug("Deleted, really deleted, and we are notified.")

    if cause.reason == causes.Reason.FREE:
        logger.debug("Deletion, but we are done with it, and we do not care.")

    if cause.reason == causes.Reason.NOOP:
        logger.debug("Something has changed, but we are not interested (the essence is the same).")

    # The delay is then consumed by the main handling routine (in different ways).
    return delays



================================================
FILE: kopf/_core/reactor/queueing.py
================================================
"""
Kubernetes watching/streaming and the per-object queueing system.

The framework can handle multiple resources at once.
Every custom resource type is "watched" (as in ``kubectl get --watch``)
in a separate asyncio task in the never-ending loop.

The events for this resource type (of all its objects) are then pushed
to the per-object queues, which are created and destroyed dynamically.
The per-object queues are created on demand.

Every object is identified by its uid, and is handled sequentially:
i.e. the low-level events are processed in the order of their arrival.
Other objects are handled in parallel in their own sequential tasks.

To prevent the memory leaks over the long run, the queues and the workers
of each object are destroyed if no new events arrive for some time.
The destruction delay (usually a few seconds, maybe minutes) is needed
to prevent the often queue/worker destruction and re-creation
in case the events are for any reason delayed by Kubernetes.

The conversion of the low-level watch-events to the high-level causes
is done in the `kopf._core.reactor.processing` routines.
"""
import asyncio
import contextlib
import enum
import logging
from collections.abc import MutableMapping
from typing import TYPE_CHECKING, NamedTuple, NewType, Optional, Protocol, Union

from kopf._cogs.aiokits import aiotasks, aiotoggles
from kopf._cogs.clients import watching
from kopf._cogs.configs import configuration
from kopf._cogs.structs import bodies, references

logger = logging.getLogger(__name__)


class WatchStreamProcessor(Protocol):
    async def __call__(
            self,
            *,
            raw_event: bodies.RawEvent,
            stream_pressure: Optional[asyncio.Event] = None,  # None for tests
            resource_indexed: Optional[aiotoggles.Toggle] = None,  # None for tests & observation
            operator_indexed: Optional[aiotoggles.ToggleSet] = None,  # None for tests & observation
    ) -> None: ...


# An end-of-stream marker sent from the watcher to the workers.
# See: https://www.python.org/dev/peps/pep-0484/#support-for-singleton-types-in-unions
class EOS(enum.Enum):
    token = enum.auto()


if TYPE_CHECKING:
    WatchEventQueue = asyncio.Queue[Union[bodies.RawEvent, EOS]]
else:
    WatchEventQueue = asyncio.Queue


class Stream(NamedTuple):
    """ A single object's stream of watch-events, with some extra helpers. """
    backlog: WatchEventQueue
    pressure: asyncio.Event  # means: "hurry up, there are new events queued again"


ObjectUid = NewType('ObjectUid', str)
ObjectRef = tuple[references.Resource, ObjectUid]
Streams = MutableMapping[ObjectRef, Stream]


def get_uid(raw_event: bodies.RawEvent) -> ObjectUid:
    """
    Retrieve or simulate an identifier of an object unique both in time & space.

    It is used as a key in mappings of framework-internal system resources,
    such as tasks and queues. It is never exposed to the users, even in logs.
    The keys are only persistent during a lifetime of a single process.
    They can be safely changed across different versions.

    In most cases, UIDs are sufficient -- as populated by K8s itself.
    However, some resources have no UIDs: e.g. ``v1/ComponentStatus``:

    .. code-block:: yaml

        apiVersion: v1
        kind: ComponentStatus
        metadata:
          creationTimestamp: null
          name: controller-manager
          selfLink: /api/v1/componentstatuses/controller-manager
        conditions:
        - message: ok
          status: "True"
          type: Healthy

    Note that ``selfLink`` is deprecated and will stop being populated
    since K8s 1.20. Other fields are not always sufficient to ensure uniqueness
    both in space and time: in the example above, the creation time is absent.

    In this function, we do our best to provide a fallback scenario in case
    UIDs are absent. All in all, having slightly less unique identifiers
    is better than failing the whole resource handling completely.
    """
    if 'uid' in raw_event['object']['metadata']:
        uid = raw_event['object']['metadata']['uid']
    else:
        ids = [
            raw_event['object'].get('kind'),
            raw_event['object'].get('apiVersion'),
            raw_event['object']['metadata'].get('name'),
            raw_event['object']['metadata'].get('namespace'),
            raw_event['object']['metadata'].get('creationTimestamp'),
        ]
        uid = '//'.join([s or '-' for s in ids])
    return ObjectUid(uid)


async def watcher(
        *,
        namespace: references.Namespace,
        settings: configuration.OperatorSettings,
        resource: references.Resource,
        processor: WatchStreamProcessor,
        operator_paused: Optional[aiotoggles.ToggleSet] = None,  # None for tests & observation
        operator_indexed: Optional[aiotoggles.ToggleSet] = None,  # None for tests & observation
        resource_indexed: Optional[aiotoggles.Toggle] = None,  # None for tests & non-indexable
) -> None:
    """
    The watchers watches for the resource events via the API, and spawns the workers for every object.

    All resources and objects are done in parallel, but one single object is handled sequentially
    (otherwise, concurrent handling of multiple events of the same object could cause data damage).

    The watcher is as non-blocking and async, as possible. It does neither call any external routines,
    nor it makes the API calls via the sync libraries.

    The watcher is generally a never-ending task (unless an error happens or it is cancelled).
    The workers, on the other hand, are limited approximately to the life-time of an object's event.

    Watchers spend their time in the infinite watch stream, not in task waiting.
    The only valid way for a worker to wake up the watcher is to cancel it:
    this will terminate any i/o operation with `asyncio.CancelledError`, where
    we can make a decision on whether it was a real cancellation, or our own.
    """

    # In case of a failed worker, stop the watcher, and escalate to the operator to stop it.
    watcher_task = asyncio.current_task()
    worker_error: Optional[BaseException] = None
    def exception_handler(exc: BaseException) -> None:
        nonlocal worker_error, watcher_task
        if worker_error is None:
            worker_error = exc
            if watcher_task is not None:  # never happens, but is needed for type-checking.
                watcher_task.cancel()

    # All per-object workers are handled as fire-and-forget jobs via the scheduler,
    # and communicated via the per-object event queues.
    signaller = asyncio.Condition()
    scheduler = aiotasks.Scheduler(limit=settings.batching.worker_limit,
                                   exception_handler=exception_handler)
    streams: Streams = {}

    try:
        # Either use the existing object's queue, or create a new one together with the per-object job.
        # "Fire-and-forget": we do not wait for the result; the job destroys itself when it is fully done.
        stream = watching.infinite_watch(
            settings=settings,
            resource=resource, namespace=namespace,
            operator_paused=operator_paused,
        )
        async for raw_event in stream:

            # If the listing is over (even if it was empty), the resource kind is pre-indexed.
            # At this moment, only the individual workers/processors can block the global readiness.
            if raw_event is watching.Bookmark.LISTED:
                if operator_indexed is not None and resource_indexed is not None:
                    await operator_indexed.drop_toggle(resource_indexed)

            # Whatever is bookmarked there, don't let it go to the multiplexer. Handle it above.
            if isinstance(raw_event, watching.Bookmark):
                continue

            # Multiplex the raw events to per-resource workers/queues. Start the new ones if needed.
            key: ObjectRef = (resource, get_uid(raw_event))
            try:
                # Feed the worker, as fast as possible, no extra activities.
                streams[key].pressure.set()  # interrupt current sleeps, if any.
                await streams[key].backlog.put(raw_event)
            except KeyError:

                # Block the operator's readiness for individual resource's index handlers.
                # But NOT when the readiness is already achieved once! After that, ignore it.
                # NB: Strictly before the worker starts -- the processor can be too slow, too late.
                resource_object_indexed: Optional[aiotoggles.Toggle] = None
                if operator_indexed is not None and operator_indexed.is_on():
                    operator_indexed = None
                if operator_indexed is not None and resource_indexed is not None:
                    resource_object_indexed = await operator_indexed.make_toggle(name=f"{key!r}")

                # Start the worker, and feed it initially. Starting can be moderately slow.
                streams[key] = Stream(backlog=asyncio.Queue(), pressure=asyncio.Event())
                streams[key].pressure.set()  # interrupt current sleeps, if any.
                await streams[key].backlog.put(raw_event)
                await scheduler.spawn(
                    name=f'worker for {key}',
                    coro=worker(
                        signaller=signaller,
                        resource_indexed=resource_object_indexed,
                        operator_indexed=operator_indexed,
                        processor=processor,
                        settings=settings,
                        streams=streams,
                        key=key,
                    ))

    except asyncio.CancelledError:
        if worker_error is None:
            raise
        else:
            raise RuntimeError("Event processing has failed with an unrecoverable error. "
                               "This seems to be a framework bug. "
                               "The operator will stop to prevent damage.") from worker_error
    finally:
        # Allow the existing workers to finish gracefully before killing them.
        # Ensure the depletion is done even if the watcher is double-cancelled (e.g. in tests).
        depletion_task = asyncio.create_task(_wait_for_depletion(
            signaller=signaller,
            scheduler=scheduler,
            streams=streams,
            settings=settings,
        ))
        while not depletion_task.done():
            with contextlib.suppress(asyncio.CancelledError):
                await asyncio.shield(depletion_task)

        # Terminate all the fire-and-forget per-object jobs if they are still running.
        # Ensure the scheduler is closed even if the watcher is double-cancelled (e.g. in tests).
        closing_task = asyncio.create_task(scheduler.close())
        while not closing_task.done():
            with contextlib.suppress(asyncio.CancelledError):
                await asyncio.shield(closing_task)


async def worker(
        *,
        signaller: asyncio.Condition,
        processor: WatchStreamProcessor,
        settings: configuration.OperatorSettings,
        resource_indexed: Optional[aiotoggles.Toggle],  # None for tests & observation
        operator_indexed: Optional[aiotoggles.ToggleSet],  # None for tests & observation
        streams: Streams,
        key: ObjectRef,
) -> None:
    """
    The per-object workers consume the object's events and invoke the processors/handlers.

    The processor is expected to be an async coroutine, always the one from the framework.
    In fact, it is either a peering processor, which monitors the peer operators,
    or a generic resource processor, which internally calls the registered synchronous processors.

    The per-object worker is a time-limited task, which ends as soon as all the object's events
    have been handled. The watcher will spawn a new job when and if the new events arrive.

    To prevent the queue/job deletion and re-creation to happen too often, the jobs wait some
    reasonable, but small enough time (a few seconds) before actually finishing --
    in case the new events are there, but the API or the watcher task lags a bit.
    """
    backlog = streams[key].backlog
    pressure = streams[key].pressure
    shouldstop = False
    try:
        while not shouldstop:

            # Try ASAP, but give it a few seconds for the new events to arrive.
            # If the queue is empty for some time, then finish the object's worker.
            # If the queue is filled, use the latest event only (within a short time window).
            # If an EOS marker is received, handle the last real event, then finish the worker ASAP.
            try:
                raw_event = await asyncio.wait_for(
                    backlog.get(),
                    timeout=settings.batching.idle_timeout)
            except asyncio.TimeoutError:
                # A tricky part! Under high-load or with synchronous blocks of asyncio event-loop,
                # it is possible that the timeout happens while the queue is filled: depending on
                # the order in which the coros/waiters are checked once control returns to asyncio.
                # As a work-around, we double-check the queue and exit only if it is truly empty;
                # if not, run as normally. IMPORTANT: There MUST be NO async/await-code between
                # "break" and "finally", so that the queue is not populated again.
                # TODO: LATER: Test the described scenario. I have found no ways to simulate
                #  a timeout while the queue is filled -- neither with pure Python nor with mocks.
                if backlog.empty():
                    break
                else:
                    continue
            else:
                try:
                    while True:
                        prev_event = raw_event
                        next_event = await asyncio.wait_for(
                            backlog.get(),
                            timeout=settings.batching.batch_window)
                        shouldstop = shouldstop or isinstance(next_event, EOS)
                        raw_event = prev_event if isinstance(next_event, EOS) else next_event
                except asyncio.TimeoutError:
                    pass  # the batch accumulation is over, we can proceed to the processing

            # Exit gracefully and immediately on the end-of-stream marker sent by the watcher.
            if isinstance(raw_event, EOS):
                break

            # Try the processor. In case of errors, show the error, but continue the processing.
            pressure.clear()
            await processor(
                raw_event=raw_event,
                stream_pressure=pressure,
                resource_indexed=resource_indexed,
                operator_indexed=operator_indexed,
            )

    except Exception:
        # Log the error for every worker: there can be several of them failing at the same time,
        # but only one will trigger the watcher's failure -- others could be lost if not logged.
        logger.exception(f"Event processing has failed with an unrecoverable error for {key}.")
        raise

    finally:
        # Whether an exception or a break or a success, notify the caller, and garbage-collect our queue.
        # The queue must not be left in the queue-cache without a corresponding job handling this queue.
        try:
            del streams[key]
        except KeyError:
            pass  # already absent

        # Notify the depletion routine about the changes in the workers'/streams' overall state.
        # * This should happen STRICTLY AFTER the removal from the streams[], and
        # * This should happen A MOMENT BEFORE the job ends (within the scheduler's close_timeout).
        async with signaller:
            signaller.notify_all()


async def _wait_for_depletion(
        *,
        signaller: asyncio.Condition,
        scheduler: aiotasks.Scheduler,
        settings: configuration.OperatorSettings,
        streams: Streams,
) -> None:

    # Notify all the workers to finish now. Wake them up if they are waiting in the queue-getting.
    for stream in streams.values():
        await stream.backlog.put(EOS.token)

    # Wait for the queues to be depleted, but only if there are some workers running.
    # Continue with the tasks termination if the timeout is reached, no matter the queues.
    # NB: the scheduler is checked for a case of mocked workers; otherwise, the streams are enough.
    async with signaller:
        try:
            await asyncio.wait_for(
                signaller.wait_for(lambda: not streams or scheduler.empty()),
                timeout=settings.batching.exit_timeout)
        except asyncio.TimeoutError:
            pass  # if not depleted as configured, proceed with what's left and let it fail

    # The last check if the termination is going to be graceful or not.
    if streams:
        logger.warning(f"Unprocessed streams left for {list(streams.keys())!r}.")



================================================
FILE: kopf/_core/reactor/running.py
================================================
import asyncio
import functools
import logging
import signal
import threading
import warnings
from collections.abc import Collection, Coroutine, MutableSequence, Sequence
from typing import Optional

from kopf._cogs.aiokits import aioadapters, aiobindings, aiotasks, aiotoggles, aiovalues
from kopf._cogs.clients import auth
from kopf._cogs.configs import configuration
from kopf._cogs.helpers import versions
from kopf._cogs.structs import credentials, ephemera, references, reviews
from kopf._core.actions import execution, lifecycles
from kopf._core.engines import activities, admission, daemons, indexing, peering, posting, probing
from kopf._core.intents import causes, registries
from kopf._core.reactor import inventory, observation, orchestration, processing

logger = logging.getLogger(__name__)


def run(
        *,
        loop: Optional[asyncio.AbstractEventLoop] = None,
        lifecycle: Optional[execution.LifeCycleFn] = None,
        indexers: Optional[indexing.OperatorIndexers] = None,
        registry: Optional[registries.OperatorRegistry] = None,
        settings: Optional[configuration.OperatorSettings] = None,
        memories: Optional[inventory.ResourceMemories] = None,
        insights: Optional[references.Insights] = None,
        identity: Optional[peering.Identity] = None,
        standalone: Optional[bool] = None,
        priority: Optional[int] = None,
        peering_name: Optional[str] = None,
        liveness_endpoint: Optional[str] = None,
        clusterwide: bool = False,
        namespaces: Collection[references.NamespacePattern] = (),
        namespace: Optional[references.NamespacePattern] = None,  # deprecated
        stop_flag: Optional[aioadapters.Flag] = None,
        ready_flag: Optional[aioadapters.Flag] = None,
        vault: Optional[credentials.Vault] = None,
        memo: Optional[object] = None,
        _command: Optional[Coroutine[None, None, None]] = None,
) -> None:
    """
    Run the whole operator synchronously.

    If the loop is not specified, the operator runs in the event loop
    of the current _context_ (by asyncio's default, the current thread).
    See: https://docs.python.org/3/library/asyncio-policy.html for details.

    Alternatively, use `asyncio.run(kopf.operator(...))` with the same options.
    It will take care of a new event loop's creation and finalization for this
    call. See: :func:`asyncio.run`.
    """
    coro = operator(
        lifecycle=lifecycle,
        indexers=indexers,
        registry=registry,
        settings=settings,
        memories=memories,
        insights=insights,
        identity=identity,
        standalone=standalone,
        clusterwide=clusterwide,
        namespaces=namespaces,
        namespace=namespace,
        priority=priority,
        peering_name=peering_name,
        liveness_endpoint=liveness_endpoint,
        stop_flag=stop_flag,
        ready_flag=ready_flag,
        vault=vault,
        memo=memo,
        _command=_command,
    )
    try:
        if loop is not None:
            loop.run_until_complete(coro)
        else:
            asyncio.run(coro)
    except asyncio.CancelledError:
        pass


async def operator(
        *,
        lifecycle: Optional[execution.LifeCycleFn] = None,
        indexers: Optional[indexing.OperatorIndexers] = None,
        registry: Optional[registries.OperatorRegistry] = None,
        settings: Optional[configuration.OperatorSettings] = None,
        memories: Optional[inventory.ResourceMemories] = None,
        insights: Optional[references.Insights] = None,
        identity: Optional[peering.Identity] = None,
        standalone: Optional[bool] = None,
        priority: Optional[int] = None,
        peering_name: Optional[str] = None,
        liveness_endpoint: Optional[str] = None,
        clusterwide: bool = False,
        namespaces: Collection[references.NamespacePattern] = (),
        namespace: Optional[references.NamespacePattern] = None,  # deprecated
        stop_flag: Optional[aioadapters.Flag] = None,
        ready_flag: Optional[aioadapters.Flag] = None,
        vault: Optional[credentials.Vault] = None,
        memo: Optional[object] = None,
        _command: Optional[Coroutine[None, None, None]] = None,
) -> None:
    """
    Run the whole operator asynchronously.

    This function should be used to run an operator in an asyncio event-loop
    if the operator is orchestrated explicitly and manually.

    It is efficiently `spawn_tasks` + `run_tasks` with some safety.
    """
    existing_tasks = await aiotasks.all_tasks()
    operator_tasks = await spawn_tasks(
        lifecycle=lifecycle,
        indexers=indexers,
        registry=registry,
        settings=settings,
        memories=memories,
        insights=insights,
        identity=identity,
        standalone=standalone,
        clusterwide=clusterwide,
        namespaces=namespaces,
        namespace=namespace,
        priority=priority,
        peering_name=peering_name,
        liveness_endpoint=liveness_endpoint,
        stop_flag=stop_flag,
        ready_flag=ready_flag,
        vault=vault,
        memo=memo,
        _command=_command,
    )
    await run_tasks(operator_tasks, ignored=existing_tasks)


async def spawn_tasks(
        *,
        lifecycle: Optional[execution.LifeCycleFn] = None,
        indexers: Optional[indexing.OperatorIndexers] = None,
        registry: Optional[registries.OperatorRegistry] = None,
        settings: Optional[configuration.OperatorSettings] = None,
        memories: Optional[inventory.ResourceMemories] = None,
        insights: Optional[references.Insights] = None,
        identity: Optional[peering.Identity] = None,
        standalone: Optional[bool] = None,
        priority: Optional[int] = None,
        peering_name: Optional[str] = None,
        liveness_endpoint: Optional[str] = None,
        clusterwide: bool = False,
        namespaces: Collection[references.NamespacePattern] = (),
        namespace: Optional[references.NamespacePattern] = None,  # deprecated
        stop_flag: Optional[aioadapters.Flag] = None,
        ready_flag: Optional[aioadapters.Flag] = None,
        vault: Optional[credentials.Vault] = None,
        memo: Optional[object] = None,
        _command: Optional[Coroutine[None, None, None]] = None,
) -> Collection[aiotasks.Task]:
    """
    Spawn all the tasks needed to run the operator.

    The tasks are properly inter-connected with the synchronisation primitives.
    """
    loop = asyncio.get_running_loop()

    if namespaces and namespace:
        raise TypeError("Either namespaces= or namespace= can be passed. Got both.")
    elif namespace:
        warnings.warn("namespace= is deprecated; use namespaces=[...]", DeprecationWarning)
        namespaces = [namespace]

    if clusterwide and namespaces:
        raise TypeError("The operator can be either cluster-wide or namespaced, not both.")
    if not clusterwide and not namespaces:
        warnings.warn("Absence of either namespaces or cluster-wide flag will become an error soon."
                      " For now, switching to the cluster-wide mode for backward compatibility.",
                      FutureWarning)
        clusterwide = True

    # All tasks of the operator are synced via these primitives and structures:
    lifecycle = lifecycle if lifecycle is not None else lifecycles.get_default_lifecycle()
    registry = registry if registry is not None else registries.get_default_registry()
    settings = settings if settings is not None else configuration.OperatorSettings()
    memories = memories if memories is not None else inventory.ResourceMemories()
    indexers = indexers if indexers is not None else indexing.OperatorIndexers()
    insights = insights if insights is not None else references.Insights()
    identity = identity if identity is not None else peering.detect_own_id(manual=False)
    vault = vault if vault is not None else credentials.Vault()
    memo = memo if memo is not None else ephemera.Memo()
    memo = ephemera.AnyMemo(memo)
    event_queue: posting.K8sEventQueue = asyncio.Queue()
    signal_flag: aiotasks.Future = asyncio.Future()
    started_flag: asyncio.Event = asyncio.Event()
    operator_paused = aiotoggles.ToggleSet(any)
    tasks: MutableSequence[aiotasks.Task] = []

    # Map kwargs into the settings object.
    settings.peering.clusterwide = clusterwide
    if peering_name is not None:
        settings.peering.mandatory = True
        settings.peering.name = peering_name
    if standalone is not None:
        settings.peering.standalone = standalone
    if priority is not None:
        settings.peering.priority = priority

    # Prepopulate indexers with empty indices -- to be available startup handlers.
    indexers.ensure(registry._indexing.get_all_handlers())

    # Global credentials store for this operator, also for CRD-reading & peering mode detection.
    auth.vault_var.set(vault)

    # Special case: pass the settings container through the user-side handlers (no explicit args).
    # Toolkits have to keep the original operator context somehow, and the only way is contextvars.
    posting.settings_var.set(settings)

    # A few common background forever-running infrastructural tasks (irregular root tasks).
    tasks.append(asyncio.create_task(
        name="stop-flag checker",
        coro=_stop_flag_checker(
            signal_flag=signal_flag,
            stop_flag=stop_flag)))
    tasks.append(asyncio.create_task(
        name="ultimate termination",
        coro=_ultimate_termination(
            settings=settings,
            stop_flag=stop_flag)))
    tasks.append(asyncio.create_task(
        name="startup/cleanup activities",
        coro=_startup_cleanup_activities(
            root_tasks=tasks,  # used as a "live" view, populated later.
            ready_flag=ready_flag,
            started_flag=started_flag,
            registry=registry,
            settings=settings,
            indices=indexers.indices,
            vault=vault,
            memo=memo)))  # to purge & finalize the caches in the end.

    # Kill all the daemons gracefully when the operator exits (so that they are not "hung").
    tasks.append(aiotasks.create_guarded_task(
        name="daemon killer", flag=started_flag, logger=logger,
        coro=daemons.daemon_killer(
            settings=settings,
            memories=memories,
            operator_paused=operator_paused)))

    # Keeping the credentials fresh and valid via the authentication handlers on demand.
    tasks.append(aiotasks.create_guarded_task(
        name="credentials retriever", flag=started_flag, logger=logger,
        coro=activities.authenticator(
            registry=registry,
            settings=settings,
            indices=indexers.indices,
            vault=vault,
            memo=memo)))

    # K8s-event posting. Events are queued in-memory and posted in the background.
    # NB: currently, it is a global task, but can be made per-resource or per-object.
    tasks.append(aiotasks.create_guarded_task(
        name="poster of events", flag=started_flag, logger=logger,
        coro=posting.poster(
            settings=settings,
            backbone=insights.backbone,
            event_queue=event_queue)))

    # Liveness probing -- so that Kubernetes would know that the operator is alive.
    if liveness_endpoint:
        tasks.append(aiotasks.create_guarded_task(
            name="health reporter", flag=started_flag, logger=logger,
            coro=probing.health_reporter(
                registry=registry,
                settings=settings,
                endpoint=liveness_endpoint,
                indices=indexers.indices,
                memo=memo)))

    # Admission webhooks run as either a server or a tunnel or a fixed config.
    # The webhook manager automatically adjusts the cluster configuration at runtime.
    container: aiovalues.Container[reviews.WebhookClientConfig] = aiovalues.Container()
    tasks.append(aiotasks.create_guarded_task(
        name="admission insights chain", flag=started_flag, logger=logger,
        coro=aiobindings.condition_chain(
            source=insights.revised, target=container.changed)))
    tasks.append(aiotasks.create_guarded_task(
        name="admission validating configuration manager", flag=started_flag, logger=logger,
        coro=admission.validating_configuration_manager(
            container=container, settings=settings, registry=registry, insights=insights)))
    tasks.append(aiotasks.create_guarded_task(
        name="admission mutating configuration manager", flag=started_flag, logger=logger,
        coro=admission.mutating_configuration_manager(
            container=container, settings=settings, registry=registry, insights=insights)))
    tasks.append(aiotasks.create_guarded_task(
        name="admission webhook server", flag=started_flag, logger=logger,
        coro=admission.admission_webhook_server(
            container=container, settings=settings, registry=registry, insights=insights,
            webhookfn=functools.partial(admission.serve_admission_request,
                                        settings=settings, registry=registry, insights=insights,
                                        memories=memories, memobase=memo,
                                        indices=indexers.indices))))

    # Permanent observation of what resource kinds and namespaces are available in the cluster.
    # Spawn and cancel dimensional tasks as they come and go; dimensions = resources x namespaces.
    tasks.append(aiotasks.create_guarded_task(
        name="resource observer", flag=started_flag, logger=logger,
        coro=observation.resource_observer(
            insights=insights,
            registry=registry,
            settings=settings)))
    tasks.append(aiotasks.create_guarded_task(
        name="namespace observer", flag=started_flag, logger=logger,
        coro=observation.namespace_observer(
            clusterwide=clusterwide,
            namespaces=namespaces,
            insights=insights,
            settings=settings)))

    # Explicit command is a hack for the CLI to run coroutines in an operator-like environment.
    # If not specified, then use the normal resource processing. It is not exposed publicly (yet).
    if _command is not None:
        tasks.append(aiotasks.create_guarded_task(
            name="the command", flag=started_flag, logger=logger, finishable=True,
            coro=_command))
    else:
        tasks.append(aiotasks.create_guarded_task(
            name="multidimensional multitasker", flag=started_flag, logger=logger,
            coro=orchestration.ochestrator(
                settings=settings,
                insights=insights,
                identity=identity,
                operator_paused=operator_paused,
                processor=functools.partial(processing.process_resource_event,
                                            lifecycle=lifecycle,
                                            registry=registry,
                                            settings=settings,
                                            indexers=indexers,
                                            memories=memories,
                                            memobase=memo,
                                            event_queue=event_queue))))

    # Ensure that all guarded tasks got control for a moment to enter the guard.
    await asyncio.sleep(0)

    # On Ctrl+C or pod termination, cancel all tasks gracefully.
    if threading.current_thread() is threading.main_thread():
        # Handle NotImplementedError when ran on Windows since asyncio only supports Unix signals
        try:
            loop.add_signal_handler(signal.SIGINT, signal_flag.set_result, signal.SIGINT)
            loop.add_signal_handler(signal.SIGTERM, signal_flag.set_result, signal.SIGTERM)
        except NotImplementedError:
            logger.warning("OS signals are ignored: can't add signal handler in Windows.")

    else:
        logger.warning("OS signals are ignored: running not in the main thread.")

    return tasks


async def run_tasks(
        root_tasks: Collection[aiotasks.Task],
        *,
        ignored: Collection[aiotasks.Task] = frozenset(),
) -> None:
    """
    Orchestrate the tasks and terminate them gracefully when needed.

    The root tasks are expected to run forever. Their number is limited. Once
    any of them exits, the whole operator and all other root tasks should exit.

    The root tasks, in turn, can spawn multiple sub-tasks of various purposes.
    They can be awaited, monitored, or fired-and-forgot.

    The hung tasks are those that were spawned during the operator runtime,
    and were not cancelled/exited on the root tasks termination. They are given
    some extra time to finish, after which they are forcely terminated too.

    .. note::
        Due to implementation details, every task created after the operator's
        startup is assumed to be a task or a sub-task of the operator.
        In the end, all tasks are forcely cancelled. Even if those tasks were
        created by other means. There is no way to trace who spawned what.
        Only the tasks that existed before the operator startup are ignored
        (for example, those that spawned the operator itself).
    """

    # Run the infinite tasks until one of them fails/exits (they never exit normally).
    # If the operator is cancelled, propagate the cancellation to all the sub-tasks.
    # There is no graceful period: cancel as soon as possible, but allow them to finish.
    try:
        root_done, root_pending = await aiotasks.wait(root_tasks, return_when=asyncio.FIRST_COMPLETED)
    except asyncio.CancelledError:
        await aiotasks.stop(root_tasks, title="Root", logger=logger, cancelled=True, interval=10)
        hung_tasks = await aiotasks.all_tasks(ignored=ignored)
        await aiotasks.stop(hung_tasks, title="Hung", logger=logger, cancelled=True, interval=1)
        raise

    # If the operator is intact, but one of the root tasks has exited (successfully or not),
    # cancel all the remaining root tasks, and gracefully exit other spawned sub-tasks.
    root_cancelled, _ = await aiotasks.stop(root_pending, title="Root", logger=logger)

    # After the root tasks are all gone, cancel any spawned sub-tasks (e.g. handlers).
    # If the operator is cancelled, propagate the cancellation to all the sub-tasks.
    # TODO: an assumption! the loop is not fully ours! find a way to cancel only our spawned tasks.
    hung_tasks = await aiotasks.all_tasks(ignored=ignored)
    try:
        hung_done, hung_pending = await aiotasks.wait(hung_tasks, timeout=5)
    except asyncio.CancelledError:
        await aiotasks.stop(hung_tasks, title="Hung", logger=logger, cancelled=True, interval=1)
        raise

    # If the operator is intact, but the timeout is reached, forcely cancel the sub-tasks.
    hung_cancelled, _ = await aiotasks.stop(hung_pending, title="Hung", logger=logger, interval=1)

    # If succeeded or if cancellation is silenced, re-raise from failed tasks (if any).
    await aiotasks.reraise(root_done | root_cancelled | hung_done | hung_cancelled)


async def _stop_flag_checker(
        signal_flag: aiotasks.Future,
        stop_flag: Optional[aioadapters.Flag],
) -> None:
    """
    A top-level task for external stopping by setting a stop-flag. Once set,
    this task will exit, and thus all other top-level tasks will be cancelled.
    """

    # Selects the flags to be awaited (if set).
    flags = []
    if signal_flag is not None:
        flags.append(signal_flag)
    if stop_flag is not None:
        flags.append(asyncio.create_task(aioadapters.wait_flag(stop_flag), name="stop-flag waiter"))

    # Wait until one of the stoppers is set/raised.
    try:
        done, pending = await asyncio.wait(flags, return_when=asyncio.FIRST_COMPLETED)
        future = done.pop()
        result = await future
    except asyncio.CancelledError:
        pass  # operator is stopping for any other reason
    else:
        if result is None:
            logger.info("Stop-flag is raised. Operator is stopping.")
        elif isinstance(result, signal.Signals):
            logger.info(f"Signal {result.name!s} is received. Operator is stopping.")
        else:
            logger.info(f"Stop-flag is set to {result!r}. Operator is stopping.")


async def _ultimate_termination(
        *,
        settings: configuration.OperatorSettings,
        stop_flag: Optional[aioadapters.Flag],
) -> None:
    """
    Ensure that SIGKILL is sent regardless of the operator's stopping routines.

    Try to be gentle and kill only the thread with the operator, not the whole
    process or a process group. If this is the main thread (as in most cases),
    this would imply the process termination too.

    Intentional stopping via a stop-flag is ignored.
    """
    # Sleep forever, or until cancelled, which happens when the operator begins its shutdown.
    try:
        await asyncio.Event().wait()
    except asyncio.CancelledError:
        if not aioadapters.check_flag(stop_flag):
            if settings.process.ultimate_exiting_timeout is not None:
                loop = asyncio.get_running_loop()
                loop.call_later(settings.process.ultimate_exiting_timeout,
                                signal.pthread_kill, threading.get_ident(), signal.SIGKILL)


async def _startup_cleanup_activities(
        root_tasks: Sequence[aiotasks.Task],  # mutated externally!
        ready_flag: Optional[aioadapters.Flag],
        started_flag: asyncio.Event,
        registry: registries.OperatorRegistry,
        settings: configuration.OperatorSettings,
        indices: ephemera.Indices,
        vault: credentials.Vault,
        memo: ephemera.AnyMemo,
) -> None:
    """
    Startup and cleanup activities.

    This task spends most of its time in forever sleep, only running
    in the beginning and in the end.

    The root tasks do not actually start until the ready-flag is set,
    which happens after the startup handlers finished successfully.

    Beside calling the startup/cleanup handlers, it performs a few operator-wide
    cleanups too (those that cannot be handled by garbage collection).
    """
    logger.debug(f"Starting Kopf {versions.version or '(unknown version)'}.")

    # Execute the startup activity before any root task starts running (due to readiness flag).
    try:
        await activities.run_activity(
            lifecycle=lifecycles.all_at_once,
            registry=registry,
            settings=settings,
            activity=causes.Activity.STARTUP,
            indices=indices,
            memo=memo,
        )
    except asyncio.CancelledError:
        logger.warning("Startup activity is only partially executed due to cancellation.")
        raise

    # Notify the caller that we are ready to be executed. This unfreezes all the root tasks.
    started_flag.set()
    await aioadapters.raise_flag(ready_flag)

    # Sleep forever, or until cancelled, which happens when the operator begins its shutdown.
    try:
        await asyncio.Event().wait()
    except asyncio.CancelledError:
        pass

    # Wait for all other root tasks to exit before cleaning up.
    # Beware: on explicit operator cancellation, there is no graceful period at all.
    try:
        current_task = asyncio.current_task()
        awaited_tasks = {task for task in root_tasks if task is not current_task}
        await aiotasks.wait(awaited_tasks)
    except asyncio.CancelledError:
        logger.warning("Cleanup activity is not executed at all due to cancellation.")
        raise

    # Execute the cleanup activity after all other root tasks are presumably done.
    try:
        await activities.run_activity(
            lifecycle=lifecycles.all_at_once,
            registry=registry,
            settings=settings,
            activity=causes.Activity.CLEANUP,
            indices=indices,
            memo=memo,
        )
        await vault.close()
    except asyncio.CancelledError:
        logger.warning("Cleanup activity is only partially executed due to cancellation.")
        raise



================================================
FILE: kopf/_core/reactor/subhandling.py
================================================
import collections.abc
import contextlib
from collections.abc import AsyncIterator, Iterable
from contextvars import ContextVar
from typing import Optional

from kopf._cogs.configs import configuration
from kopf._cogs.structs import ids
from kopf._core.actions import execution, invocation, lifecycles, progression
from kopf._core.intents import callbacks, causes, handlers as handlers_, registries

# The task-local context; propagated down the stack instead of multiple kwargs.
# Used in `@kopf.subhandler` and `kopf.execute()` to add/get the sub-handlers.
subregistry_var: ContextVar[registries.ChangingRegistry] = ContextVar('subregistry_var')
subexecuted_var: ContextVar[bool] = ContextVar('subexecuted_var')


@contextlib.asynccontextmanager
async def subhandling_context() -> AsyncIterator[None]:
    with invocation.context([
        (subregistry_var, registries.ChangingRegistry()),
        (subexecuted_var, False),
    ]):
        # Go for normal handler invocation.
        yield

        # If the sub-handlers are not called explicitly, run them implicitly
        # as if it was done inside of the handler (still under the try-finally clause).
        if not subexecuted_var.get():
            await execute()


async def execute(
        *,
        fns: Optional[Iterable[callbacks.ChangingFn]] = None,
        handlers: Optional[Iterable[handlers_.ChangingHandler]] = None,
        registry: Optional[registries.ChangingRegistry] = None,
        lifecycle: Optional[execution.LifeCycleFn] = None,
        cause: Optional[execution.Cause] = None,
) -> None:
    """
    Execute the handlers in an isolated lifecycle.

    This function is just a public wrapper for `execute` with multiple
    ways to specify the handlers: either as the raw functions, or as the
    pre-created handlers, or as a registry (as used in the object handling).

    If no explicit functions or handlers or registry are passed,
    the sub-handlers of the current handler are assumed, as accumulated
    in the per-handler registry with ``@kopf.subhandler``.

    If the call to this method for the sub-handlers is not done explicitly
    in the handler, it is done implicitly after the handler is exited.
    One way or another, it is executed for the sub-handlers.
    """

    # Restore the current context as set in the handler execution cycle.
    lifecycle = lifecycle if lifecycle is not None else execution.sublifecycle_var.get()
    lifecycle = lifecycle if lifecycle is not None else lifecycles.get_default_lifecycle()
    cause = cause if cause is not None else execution.cause_var.get()
    parent_handler: execution.Handler = execution.handler_var.get()
    parent_prefix = parent_handler.id if parent_handler is not None else None

    # Validate the inputs; the function signatures cannot put these kind of restrictions, so we do.
    if len([v for v in [fns, handlers, registry] if v is not None]) > 1:
        raise TypeError("Only one of the fns, handlers, registry can be passed. Got more.")

    elif fns is not None and isinstance(fns, collections.abc.Mapping):
        subregistry = registries.ChangingRegistry()
        for id, fn in fns.items():
            real_id = registries.generate_id(fn=fn, id=id, prefix=parent_prefix)
            handler = handlers_.ChangingHandler(
                fn=fn, id=real_id, param=None,
                errors=None, timeout=None, retries=None, backoff=None,
                selector=None, labels=None, annotations=None, when=None,
                initial=None, deleted=None, requires_finalizer=None,
                reason=None, field=None, value=None, old=None, new=None,
                field_needs_change=None,
            )
            subregistry.append(handler)

    elif fns is not None and isinstance(fns, collections.abc.Iterable):
        subregistry = registries.ChangingRegistry()
        for fn in fns:
            real_id = registries.generate_id(fn=fn, id=None, prefix=parent_prefix)
            handler = handlers_.ChangingHandler(
                fn=fn, id=real_id, param=None,
                errors=None, timeout=None, retries=None, backoff=None,
                selector=None, labels=None, annotations=None, when=None,
                initial=None, deleted=None, requires_finalizer=None,
                reason=None, field=None, value=None, old=None, new=None,
                field_needs_change=None,
            )
            subregistry.append(handler)

    elif fns is not None:
        raise ValueError(f"fns must be a mapping or an iterable, got {fns.__class__}.")

    elif handlers is not None:
        subregistry = registries.ChangingRegistry()
        for handler in handlers:
            subregistry.append(handler)

    # Use the registry as is; assume that the caller knows what they do.
    elif registry is not None:
        subregistry = registry

    # Prevent double implicit execution.
    elif subexecuted_var.get():
        return

    # If no explicit args were passed, use the accumulated handlers from `@kopf.subhandler`.
    else:
        subexecuted_var.set(True)
        subregistry = subregistry_var.get()

    # The sub-handlers are only for upper-level causes, not for lower-level events.
    if not isinstance(cause, causes.ChangingCause):
        raise RuntimeError("Sub-handlers of event-handlers are not supported and have "
                           "no practical use (there are no retries or state tracking).")

    # Execute the real handlers (all or a few or one of them, as per the lifecycle).
    settings: configuration.OperatorSettings = execution.subsettings_var.get()
    owned_handlers = subregistry.get_resource_handlers(resource=cause.resource)
    cause_handlers = subregistry.get_handlers(cause=cause)
    storage = settings.persistence.progress_storage
    state = progression.State.from_storage(body=cause.body, storage=storage, handlers=owned_handlers)
    state = state.with_purpose(cause.reason).with_handlers(cause_handlers)
    outcomes = await execution.execute_handlers_once(
        lifecycle=lifecycle,
        settings=settings,
        handlers=cause_handlers,
        cause=cause,
        state=state,
        extra_context=subhandling_context,
    )
    state = state.with_outcomes(outcomes)
    state.store(body=cause.body, patch=cause.patch, storage=storage)
    progression.deliver_results(outcomes=outcomes, patch=cause.patch)

    # Enrich all parents with references to sub-handlers of any level deep (sub-sub-handlers, etc).
    # There is at least one container, as this function can be called only from a handler.
    subrefs_containers: Iterable[set[ids.HandlerId]] = execution.subrefs_var.get()
    for key in state:
        for subrefs_container in subrefs_containers:
            subrefs_container.add(key)

    # Escalate `HandlerChildrenRetry` if the execute should be continued on the next iteration.
    if not state.done:
        raise execution.HandlerChildrenRetry(delay=state.delay)



================================================
FILE: kopf/_kits/__init__.py
================================================
"""
Toolkits to improve the developer experience in the context of Kopf.

They are not needed to use the framework or to run the operator
(unlike all other packages), but they can make the development
of the operators much easier.

Some things can be considered as the clients' responsibilities
rather than the operator framework's responsibilities.
In that case, the decision point is whether the functions work
"in the context of Kopf" at least to some extent
(e.g. by using its contextual information of the current handler).
"""



================================================
FILE: kopf/_kits/hierarchies.py
================================================
"""
All the functions to properly build the object hierarchies.
"""
import collections.abc
import enum
import warnings
from collections.abc import Iterable, Iterator, Mapping, MutableMapping
from typing import Any, Optional, Union, cast

from kopf._cogs.helpers import thirdparty
from kopf._cogs.structs import bodies, dicts
from kopf._core.actions import execution
from kopf._core.intents import causes

K8sObject = Union[MutableMapping[Any, Any], thirdparty.PykubeObject, thirdparty.KubernetesModel]
K8sObjects = Union[K8sObject, Iterable[K8sObject]]


class _UNSET(enum.Enum):
    token = enum.auto()


def append_owner_reference(
        objs: K8sObjects,
        owner: Optional[bodies.Body] = None,
        *,
        controller: Optional[bool] = True,
        block_owner_deletion: Optional[bool] = True,
) -> None:
    """
    Append an owner reference to the resource(s), if it is not yet there.

    Note: the owned objects are usually not the one being processed,
    so the whole body can be modified, no patches are needed.
    """
    real_owner = _guess_owner(owner)
    owner_ref = bodies.build_owner_reference(
        real_owner, controller=controller, block_owner_deletion=block_owner_deletion
    )
    for obj in cast(Iterator[K8sObject], dicts.walk(objs)):
        # Pykube is yielded as a usual dict, no need to specially treat it.
        if isinstance(obj, collections.abc.MutableMapping):
            refs = obj.setdefault('metadata', {}).setdefault('ownerReferences', [])
            if not any(ref.get('uid') == owner_ref['uid'] for ref in refs):
                refs.append(owner_ref)
        elif isinstance(obj, thirdparty.KubernetesModel):
            if obj.metadata is None:
                obj.metadata = thirdparty.V1ObjectMeta()
            if obj.metadata.owner_references is None:
                obj.metadata.owner_references = []
            refs = obj.metadata.owner_references
            if not any(ref.uid == owner_ref['uid'] for ref in refs):
                refs.append(thirdparty.V1OwnerReference(
                    api_version=owner_ref['apiVersion'],
                    kind=owner_ref['kind'],
                    name=owner_ref['name'],
                    uid=owner_ref['uid'],
                    controller=owner_ref['controller'],
                    block_owner_deletion=owner_ref['blockOwnerDeletion'],
                ))
        else:
            raise TypeError(f"K8s object class is not supported: {type(obj)}")


def remove_owner_reference(
        objs: K8sObjects,
        owner: Optional[bodies.Body] = None,
) -> None:
    """
    Remove an owner reference to the resource(s), if it is there.

    Note: the owned objects are usually not the one being processed,
    so the whole body can be modified, no patches are needed.
    """
    real_owner = _guess_owner(owner)
    owner_ref = bodies.build_owner_reference(real_owner)
    for obj in cast(Iterator[K8sObject], dicts.walk(objs)):
        # Pykube is yielded as a usual dict, no need to specially treat it.
        if isinstance(obj, collections.abc.MutableMapping):
            refs = obj.setdefault('metadata', {}).setdefault('ownerReferences', [])
            if any(ref.get('uid') == owner_ref['uid'] for ref in refs):
                refs[:] = [ref for ref in refs if ref.get('uid') != owner_ref['uid']]
        elif isinstance(obj, thirdparty.KubernetesModel):
            if obj.metadata is None:
                obj.metadata = thirdparty.V1ObjectMeta()
            if obj.metadata.owner_references is None:
                obj.metadata.owner_references = []
            refs = obj.metadata.owner_references
            if any(ref.uid == owner_ref['uid'] for ref in refs):
                refs[:] = [ref for ref in refs if ref.uid != owner_ref['uid']]
        else:
            raise TypeError(f"K8s object class is not supported: {type(obj)}")


def label(
        objs: K8sObjects,
        labels: Union[Mapping[str, Union[None, str]], _UNSET] = _UNSET.token,
        *,
        forced: bool = False,
        nested: Optional[Union[str, Iterable[dicts.FieldSpec]]] = None,
        force: Optional[bool] = None,  # deprecated
) -> None:
    """
    Apply the labels to the object(s).
    """
    nested = [nested] if isinstance(nested, str) else nested
    if force is not None:
        warnings.warn("force= is deprecated in kopf.label(); use forced=...", DeprecationWarning)
        forced = force

    # Try to use the current object being handled if possible.
    if isinstance(labels, _UNSET):
        real_owner = _guess_owner(None)
        labels = real_owner.get('metadata', {}).get('labels', {})
    if isinstance(labels, _UNSET):
        raise RuntimeError("Impossible error: labels are not resolved.")  # for type-checking

    # Set labels based on the explicitly specified or guessed ones.
    for obj in cast(Iterator[K8sObject], dicts.walk(objs, nested=nested)):
        # Pykube is yielded as a usual dict, no need to specially treat it.
        if isinstance(obj, collections.abc.MutableMapping):
            obj_labels = obj.setdefault('metadata', {}).setdefault('labels', {})
        elif isinstance(obj, thirdparty.KubernetesModel):
            if obj.metadata is None:
                obj.metadata = thirdparty.V1ObjectMeta()
            if obj.metadata.labels is None:
                obj.metadata.labels = {}
            obj_labels = obj.metadata.labels
        else:
            raise TypeError(f"K8s object class is not supported: {type(obj)}")

        for key, val in labels.items():
            if forced:
                obj_labels[key] = val
            else:
                obj_labels.setdefault(key, val)


def harmonize_naming(
        objs: K8sObjects,
        name: Union[None, str, _UNSET] = _UNSET.token,
        *,
        forced: bool = False,
        strict: bool = False,
) -> None:
    """
    Adjust the names or prefixes of the objects.

    In strict mode, the provided name is used as is. It can be helpful
    if the object is referred by that name in other objects.

    In non-strict mode (the default), the object uses the provided name
    as a prefix, while the suffix is added by Kubernetes remotely.
    The actual name should be taken from Kubernetes response
    (this is the recommended scenario).

    If the objects already have their own names, auto-naming is not applied,
    and the existing names are used as is.
    """

    # Try to use the current object being handled if possible.
    if isinstance(name, _UNSET):
        real_owner = _guess_owner(None)
        name = real_owner.get('metadata', {}).get('name', None)
    if isinstance(name, _UNSET):
        raise RuntimeError("Impossible error: the name is not resolved.")  # for type-checking
    if name is None:
        raise LookupError("Name must be set explicitly: couldn't find it automatically.")

    # Set name/prefix based on the explicitly specified or guessed name.
    for obj in cast(Iterator[K8sObject], dicts.walk(objs)):
        # Pykube is yielded as a usual dict, no need to specially treat it.
        if isinstance(obj, collections.abc.MutableMapping):
            noname = 'metadata' not in obj or not set(obj['metadata']) & {'name', 'generateName'}
            if forced or noname:
                if strict:
                    obj.setdefault('metadata', {})['name'] = name
                    if 'generateName' in obj['metadata']:
                        del obj['metadata']['generateName']
                else:
                    obj.setdefault('metadata', {})['generateName'] = f'{name}-'
                    if 'name' in obj['metadata']:
                        del obj['metadata']['name']
        elif isinstance(obj, thirdparty.KubernetesModel):
            if obj.metadata is None:
                obj.metadata = thirdparty.V1ObjectMeta()
            noname = obj.metadata.name is None and obj.metadata.generate_name is None
            if forced or noname:
                if strict:
                    obj.metadata.name = name
                    if obj.metadata.generate_name is not None:
                        obj.metadata.generate_name = None
                else:
                    obj.metadata.generate_name = f'{name}-'
                    if obj.metadata.name is not None:
                        obj.metadata.name = None
        else:
            raise TypeError(f"K8s object class is not supported: {type(obj)}")


def adjust_namespace(
        objs: K8sObjects,
        namespace: Union[None, str, _UNSET] = _UNSET.token,
        *,
        forced: bool = False,
) -> None:
    """
    Adjust the namespace of the objects.

    If the objects already have the namespace set, it will be preserved.

    It is a common practice to keep the children objects in the same
    namespace as their owner, unless explicitly overridden at time of creation.
    """

    # Try to use the current object being handled if possible.
    if isinstance(namespace, _UNSET):
        real_owner = _guess_owner(None)
        namespace = real_owner.get('metadata', {}).get('namespace', None)
    if isinstance(namespace, _UNSET):
        raise RuntimeError("Impossible error: the namespace is not resolved.")  # for type-checking

    # Set namespace based on the explicitly specified or guessed namespace.
    for obj in cast(Iterator[K8sObject], dicts.walk(objs)):
        # Pykube is yielded as a usual dict, no need to specially treat it.
        if isinstance(obj, collections.abc.MutableMapping):
            if forced or obj.get('metadata', {}).get('namespace') is None:
                obj.setdefault('metadata', {})['namespace'] = namespace
        elif isinstance(obj, thirdparty.KubernetesModel):
            if obj.metadata is None:
                obj.metadata = thirdparty.V1ObjectMeta()
            if forced or obj.metadata.namespace is None:
                obj.metadata.namespace = namespace
        else:
            raise TypeError(f"K8s object class is not supported: {type(obj)}")


def adopt(
        objs: K8sObjects,
        owner: Optional[bodies.Body] = None,
        *,
        forced: bool = False,
        strict: bool = False,
        nested: Optional[Union[str, Iterable[dicts.FieldSpec]]] = None,
) -> None:
    """
    The children should be in the same namespace, named after their parent, and owned by it.
    """
    real_owner = _guess_owner(owner)
    real_owner_name = real_owner.get('metadata', {}).get('name', None)
    real_owner_namespace = real_owner.get('metadata', {}).get('namespace', None)
    real_owner_labels = real_owner.get('metadata', {}).get('labels', {})
    append_owner_reference(objs, owner=real_owner)
    harmonize_naming(objs, forced=forced, strict=strict, name=real_owner_name)
    adjust_namespace(objs, forced=forced, namespace=real_owner_namespace)
    label(objs, forced=forced, nested=nested, labels=real_owner_labels)


def _guess_owner(
        owner: Optional[bodies.Body],
) -> bodies.Body:
    if owner is not None:
        return owner

    try:
        cause = execution.cause_var.get()
    except LookupError:
        pass
    else:
        if cause is not None and isinstance(cause, causes.ResourceCause):
            return cause.body

    raise LookupError("Owner must be set explicitly, since running outside of a handler.")



================================================
FILE: kopf/_kits/loops.py
================================================
import asyncio
import contextlib
from collections.abc import Generator
from typing import Optional


@contextlib.contextmanager
def proper_loop(suggested_loop: Optional[asyncio.AbstractEventLoop] = None) -> Generator[None, None, None]:
    """
    Ensure that we have the proper loop, either suggested or properly managed.

    A "properly managed" loop is the one we own and therefore close.
    If ``uvloop`` is installed, it is used.
    Otherwise, the event loop policy remains unaffected.

    This loop manager is usually used in CLI only, not deeper than that;
    i.e. not even in ``kopf.run()``, since uvloop is only auto-managed for CLI.
    """
    original_policy = asyncio.get_event_loop_policy()
    if suggested_loop is None:  # the pure CLI use, not a KopfRunner or other code
        try:
            import uvloop
        except ImportError:
            pass
        else:
            asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())

    try:
        yield

    finally:
        try:
            import uvloop
        except ImportError:
            pass
        else:
            asyncio.set_event_loop_policy(original_policy)



================================================
FILE: kopf/_kits/runner.py
================================================
import asyncio
import concurrent.futures
import contextlib
import threading
import types
from typing import TYPE_CHECKING, Any, Literal, Optional, cast

import click.testing

from kopf import cli
from kopf._cogs.configs import configuration
from kopf._core.intents import registries

_ExcType = BaseException
_ExcInfo = tuple[type[_ExcType], _ExcType, types.TracebackType]

if TYPE_CHECKING:
    ResultFuture = concurrent.futures.Future[click.testing.Result]
    class _AbstractKopfRunner(contextlib.AbstractContextManager["_AbstractKopfRunner"]):
        pass
else:
    ResultFuture = concurrent.futures.Future
    class _AbstractKopfRunner(contextlib.AbstractContextManager):
        pass


class KopfRunner(_AbstractKopfRunner):
    """
    A context manager to run a Kopf-based operator in parallel with the tests.

    Usage::

        from kopf.testing import KopfRunner

        with KopfRunner(['run', '-A', '--verbose', 'examples/01-minimal/example.py']) as runner:
            # do something while the operator is running.
            time.sleep(3)

        assert runner.exit_code == 0
        assert runner.exception is None
        assert 'And here we are!' in runner.output

    All the args & kwargs are passed directly to Click's invocation method.
    See: `click.testing.CliRunner`.
    All properties proxy directly to Click's `click.testing.Result` object
    when it is available (i.e. after the context manager exits).

    CLI commands have to be invoked in parallel threads, never in processes:

    First, with multiprocessing, they are unable to pickle and pass
    exceptions (specifically, their traceback objects)
    from a child thread (Kopf's CLI) to the parent thread (pytest).

    Second, mocking works within one process (all threads),
    but not across processes --- the mock's calls (counts, arrgs) are lost.
    """
    _future: ResultFuture

    def __init__(
            self,
            *args: Any,
            reraise: bool = True,
            timeout: Optional[float] = None,
            registry: Optional[registries.OperatorRegistry] = None,
            settings: Optional[configuration.OperatorSettings] = None,
            **kwargs: Any,
    ):
        super().__init__()
        self.args = args
        self.kwargs = kwargs
        self.reraise = reraise
        self.timeout = timeout
        self.registry = registry
        self.settings = settings
        self._stop = threading.Event()
        self._ready = threading.Event()  # NB: not asyncio.Event!
        self._thread = threading.Thread(target=self._target)
        self._future = concurrent.futures.Future()

    def __enter__(self) -> "KopfRunner":
        self._thread.start()
        self._ready.wait()  # should be nanosecond-fast
        return self

    def __exit__(
            self,
            exc_type: Optional[type[BaseException]],
            exc_val: Optional[BaseException],
            exc_tb: Optional[types.TracebackType],
    ) -> Literal[False]:

        # When the `with` block ends, shut down the parallel thread & loop
        # by cancelling all the tasks. Do not wait for the tasks to finish,
        # but instead wait for the thread+loop (CLI command) to finish.
        self._stop.set()
        self._thread.join(timeout=self.timeout)

        # If the thread is not finished, it is a bigger problem than exceptions.
        if self._thread.is_alive():
            raise Exception("The operator didn't stop, still running.")

        # Re-raise the exceptions of the threading & invocation logic.
        e = self._future.exception()
        if e is not None:
            if exc_val is None:
                raise e
            else:
                raise e from exc_val
        e = self._future.result().exception
        if e is not None and self.reraise:
            if exc_val is None:
                raise e
            else:
                raise e from exc_val

        return False

    def _target(self) -> None:

        # Every thread must have its own loop. The parent thread (pytest)
        # needs to know when the loop is set up, to be able to shut it down.
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        self._ready.set()

        # Execute the requested CLI command in the thread & thread's loop.
        # Remember the result & exception for re-raising in the parent thread.
        try:
            ctxobj = cli.CLIControls(
                registry=self.registry,
                settings=self.settings,
                stop_flag=self._stop,
                loop=loop)
            runner = click.testing.CliRunner()
            result = runner.invoke(cli.main, *self.args, **self.kwargs, obj=ctxobj)
        except BaseException as e:
            self._future.set_exception(e)
        else:
            self._future.set_result(result)
        finally:

            # Shut down the API-watching streams.
            loop.run_until_complete(loop.shutdown_asyncgens())

            # Shut down the transports and prevent ResourceWarning: unclosed transport.
            # See: https://docs.aiohttp.org/en/stable/client_advanced.html#graceful-shutdown
            # TODO: Try a hack: https://github.com/aio-libs/aiohttp/issues/1925#issuecomment-575754386
            loop.run_until_complete(asyncio.sleep(1.0))

            loop.close()

    @property
    def future(self) -> ResultFuture:
        return self._future

    @property
    def output(self) -> str:
        return self.future.result().output

    @property
    def stdout(self) -> str:
        return self.future.result().stdout

    @property
    def stdout_bytes(self) -> bytes:
        return self.future.result().stdout_bytes

    @property
    def stderr(self) -> str:
        return self.future.result().stderr

    @property
    def stderr_bytes(self) -> bytes:
        return self.future.result().stderr_bytes or b''

    @property
    def exit_code(self) -> int:
        return self.future.result().exit_code

    @property
    def exception(self) -> _ExcType:
        return cast(_ExcType, self.future.result().exception)

    @property
    def exc_info(self) -> _ExcInfo:
        return cast(_ExcInfo, self.future.result().exc_info)



================================================
FILE: kopf/_kits/webhacks.py
================================================
import functools
from collections.abc import AsyncGenerator, AsyncIterator
from typing import Any, Callable, TypeVar, cast

from kopf._cogs.structs import reviews

_SelfT = TypeVar('_SelfT')
_ServerFn = TypeVar('_ServerFn', bound=Callable[..., AsyncIterator[reviews.WebhookClientConfig]])


class WebhookContextManagerMeta(type):
    """
    Auto-decorate all ``__call__`` functions to persist their generators.

    Another way is via ``__init_subclass__``, but that requires monkey-patching
    and will not work on slotted classes (when they will be made slotted).
    """
    def __new__(
            cls,
            name: str,
            bases: tuple[type, ...],
            namespace: dict[str, Any],
            **kwargs: Any,
    ) -> "WebhookContextManagerMeta":
        if '__call__' in namespace:
            namespace['__call__'] = WebhookContextManager._persisted(namespace['__call__'])
        return super().__new__(cls, name, bases, namespace, **kwargs)


class WebhookContextManager(metaclass=WebhookContextManagerMeta):
    """
    A hack to make webhook servers/tunnels re-entrant.

    Generally, the webhook servers/tunnels are used only once per operator run,
    so the re-entrant servers/tunnels are not needed. This hack solves
    a problem that exists only in the unit-tests (because they are fast):

    When the garbage collection is postponed (as in PyPy), the server/tunnel
    frees the system resources (e.g. sockets) much later than the test is over
    (in ``finally:`` when the ``GeneratorExit`` is thrown into the generator).

    As a result, the resources (sockets) are released while the unrelated tests
    are running, and inject the exceptions into one of those unrelated tests
    (e.g. ``ResourceWarning: unclosed transport`` or alike).

    The obvious solution — the context managers — would break the protocol,
    which is promised to be a single callable that yields the client configs.

    To keep the backwards compatibility while cleaning up the resources on time:

    - Support the _optional_ async context manager protocol, _if_ implemented.
    - Implement a minimalistic context manager for the provided servers/tunnels.
    - Keep them fully functional when the context manager is not used.

    So, the servers/tunnels are left with the ``finally:`` block for cleanup.
    But the iterator-generator is remembered and force-closed on exit from
    the context manager — the same way as the garbage collector would close it.

    See more at :doc:`/admission`.
    """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)
        self.__generators: list[AsyncGenerator[reviews.WebhookClientConfig, None]] = []

    async def __aenter__(self: _SelfT) -> _SelfT:
        return self

    async def __aexit__(self, *_: Any) -> None:
        # The order of cleanups is probably irrelevant, but prefer LIFO just in case:
        # the innermost and latest super() calls freed first, the outermost and earliest — last.
        for generator in reversed(self.__generators):
            try:
                await generator.aclose()
            except (GeneratorExit, StopAsyncIteration, StopIteration):
                pass
        self.__generators[:] = []

    @staticmethod
    def _persisted(wrapped: _ServerFn) -> _ServerFn:
        @functools.wraps(wrapped)
        async def wrapper(
                self: "WebhookContextManager",
                fn: reviews.WebhookFn,
        ) -> AsyncIterator[reviews.WebhookClientConfig]:
            iterator = wrapped(self, fn)
            if isinstance(iterator, AsyncGenerator):
                self.__generators.append(iterator)
            async for value in iterator:
                yield value
            if isinstance(iterator, AsyncGenerator):
                self.__generators.remove(iterator)
        return cast(_ServerFn, wrapper)



================================================
FILE: kopf/_kits/webhooks.py
================================================
"""
Several webhooks servers & tunnels supported out of the box.
"""
import asyncio
import base64
import contextlib
import functools
import ipaddress
import json
import logging
import os
import pathlib
import socket
import ssl
import tempfile
import urllib.parse
from collections.abc import AsyncIterator, Collection, Iterable
from typing import TYPE_CHECKING, Optional, Union

import aiohttp.web

from kopf._cogs.clients import api, scanning
from kopf._cogs.configs import configuration
from kopf._cogs.structs import reviews
from kopf._core.engines import admission
from kopf._kits import webhacks

logger = logging.getLogger(__name__)

if TYPE_CHECKING:
    StrPath = Union[str, os.PathLike[str]]
else:
    StrPath = Union[str, os.PathLike]


class MissingDependencyError(ImportError):
    """ A server/tunnel is used which requires an optional dependency. """


class WebhookServer(webhacks.WebhookContextManager):
    """
    A local HTTP/HTTPS endpoint.

    Currently, the server is based on ``aiohttp``, but the implementation
    can change in the future without warning.

    This server is also used by specialised tunnels when they need
    a local endpoint to be tunneled.

    * ``addr``, ``port`` is where to listen for connections
      (defaults to ``localhost`` and ``9443``).
    * ``path`` is the root path for a webhook server
      (defaults to no root path).
    * ``host`` is an optional override of the hostname for webhook URLs;
      if not specified, the ``addr`` will be used.

    Kubernetes requires HTTPS, so HTTPS is the default mode of the server.
    This webhook server supports SSL both for the server certificates
    and for client certificates (e.g., for authentication) at the same time:

    * ``cadata``, ``cafile`` is the CA bundle to be passed as a "client config"
      to the webhook configuration objects, to be used by clients/apiservers
      when talking to the webhook server; it is not used in the server itself.
    * ``cadump`` is a path to save the resulting CA bundle to be used
      by clients, i.e. apiservers; it can be passed to ``curl --cacert ...``;
      if ``cafile`` is provided, it contains the same content.
    * ``certfile``, ``pkeyfile`` define the server's endpoint certificate;
      if not specified, a self-signed certificate and CA will be generated
      for both ``addr`` & ``host`` as SANs (but only ``host`` for CommonName).
    * ``password`` is either for decrypting the provided ``pkeyfile``,
      or for encrypting and decrypting the generated private key.
    * ``extra_sans`` are put into the self-signed certificate as SANs (DNS/IP)
      in addition to the host & addr (in case some other endpoints exist).
    * ``verify_mode``, ``verify_cafile``, ``verify_capath``, ``verify_cadata``
      will be loaded into the SSL context for verifying the client certificates
      when provided and if provided by the clients, i.e. apiservers or curl;
      (`ssl.SSLContext.verify_mode`, `ssl.SSLContext.load_verify_locations`).
    * ``insecure`` flag disables HTTPS and runs an HTTP webhook server.
      This is used in ngrok for a local endpoint, but can be used for debugging
      or when the certificate-generating dependencies/extras are not installed.
    """
    DEFAULT_HOST: Optional[str] = None

    addr: Optional[str]  # None means "all interfaces"
    port: Optional[int]  # None means random port
    host: Optional[str]
    path: Optional[str]

    cadata: Optional[bytes]  # -> .webhooks.*.clientConfig.caBundle
    cafile: Optional[StrPath]
    cadump: Optional[StrPath]

    context: Optional[ssl.SSLContext]
    insecure: bool
    certfile: Optional[StrPath]
    pkeyfile: Optional[StrPath]
    password: Optional[str]

    extra_sans: Iterable[str]

    verify_mode: Optional[ssl.VerifyMode]
    verify_cafile: Optional[StrPath]
    verify_capath: Optional[StrPath]
    verify_cadata: Optional[Union[str, bytes]]

    def __init__(
            self,
            *,
            # Listening socket, root URL path, and the reported URL hostname:
            addr: Optional[str] = None,
            port: Optional[int] = None,
            path: Optional[str] = None,
            host: Optional[str] = None,
            # The CA bundle to be passed to "client configs":
            cadata: Optional[bytes] = None,
            cafile: Optional[StrPath] = None,
            cadump: Optional[StrPath] = None,
            # A pre-configured SSL context (if any):
            context: Optional[ssl.SSLContext] = None,
            # The server's own certificate, or lack of it (loaded into the context):
            insecure: bool = False,  # http is needed for ngrok
            certfile: Optional[StrPath] = None,
            pkeyfile: Optional[StrPath] = None,
            password: Optional[str] = None,
            # Generated certificate's extra info.
            extra_sans: Iterable[str] = (),
            # Verification of client certificates (loaded into the context):
            verify_mode: Optional[ssl.VerifyMode] = None,
            verify_cafile: Optional[StrPath] = None,
            verify_capath: Optional[StrPath] = None,
            verify_cadata: Optional[Union[str, bytes]] = None,
    ) -> None:
        super().__init__()
        self.addr = addr
        self.port = port
        self.path = path
        self.host = host
        self.cadata = cadata
        self.cafile = cafile
        self.cadump = cadump
        self.context = context
        self.insecure = insecure
        self.certfile = certfile
        self.pkeyfile = pkeyfile
        self.password = password
        self.extra_sans = extra_sans
        self.verify_mode = verify_mode
        self.verify_cafile = verify_cafile
        self.verify_capath = verify_capath
        self.verify_cadata = verify_cadata

    async def __call__(self, fn: reviews.WebhookFn) -> AsyncIterator[reviews.WebhookClientConfig]:

        # Redefine as a coroutine instead of a partial to avoid warnings from aiohttp.
        async def _serve_fn(request: aiohttp.web.Request) -> aiohttp.web.Response:
            return await self._serve(fn, request)

        cadata, context = self._build_ssl()
        path = self.path.rstrip('/') if self.path else ''
        app = aiohttp.web.Application()
        app.add_routes([aiohttp.web.post(f"{path}/{{id:.*}}", _serve_fn)])
        runner = aiohttp.web.AppRunner(app, handle_signals=False)
        await runner.setup()
        try:
            # Note: reuse_port is mostly (but not only) for fast-running tests with SSL sockets;
            # multi-threaded sockets are not really used -- high load is not expected for webhooks.
            addr = self.addr or None  # None is aiohttp's "any interface"
            port = self.port or self._allocate_free_port()
            site = aiohttp.web.TCPSite(runner, addr, port, ssl_context=context, reuse_port=True)
            await site.start()

            # Log with the actual URL: normalised, with hostname/port set.
            schema = 'http' if context is None else 'https'
            url = self._build_url(schema, addr or '*', port, self.path or '')
            logger.debug(f"Listening for webhooks at {url}")
            host = self.host or self.DEFAULT_HOST or self._get_accessible_addr(self.addr)
            url = self._build_url(schema, host, port, self.path or '')
            logger.debug(f"Accessing the webhooks at {url}")

            client_config = reviews.WebhookClientConfig(url=url)
            if cadata is not None:
                client_config['caBundle'] = base64.b64encode(cadata).decode('ascii')

            yield client_config
            await asyncio.Event().wait()
        finally:
            # On any reason of exit, stop serving the endpoint.
            await runner.cleanup()

    @staticmethod
    async def _serve(
            fn: reviews.WebhookFn,
            request: aiohttp.web.Request,
    ) -> aiohttp.web.Response:
        """
        Serve a single admission request: an aiohttp-specific implementation.

        Mind 2 different ways the errors are reported:

        * Directly by the webhook's response, i.e. to the apiservers.
          This means that the webhook request was done improperly;
          the original API request might be good, but we could not confirm that.
        * In ``.response.status``, as apiservers send it to the requesting user.
          This means that the original API operation was done improperly,
          while the webhooks are functional.
        """
        # The extra information that is passed down to handlers for authentication/authorization.
        # Note: this is an identity of an apiserver, not of the user that sends an API request.
        headers = dict(request.headers)
        sslpeer = request.transport.get_extra_info('peercert') if request.transport else None
        webhook = request.match_info.get('id')
        try:
            text = await request.text()
            data = json.loads(text)
            response = await fn(data, webhook=webhook, sslpeer=sslpeer, headers=headers)
            return aiohttp.web.json_response(response)
        except admission.AmbiguousResourceError as e:
            raise aiohttp.web.HTTPConflict(reason=str(e) or None)
        except admission.UnknownResourceError as e:
            raise aiohttp.web.HTTPNotFound(reason=str(e) or None)
        except admission.WebhookError as e:
            raise aiohttp.web.HTTPBadRequest(reason=str(e) or None)
        except json.JSONDecodeError as e:
            raise aiohttp.web.HTTPBadRequest(reason=str(e) or None)

    @staticmethod
    def _allocate_free_port() -> int:
        with contextlib.closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as s:
            s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
            s.bind(('', 0))  # '' is a special IPv4 form for "any interface"
            return int(s.getsockname()[1])

    @staticmethod
    def _get_accessible_addr(addr: Optional[str]) -> str:
        """
        Convert a "catch-all" listening address to the accessible hostname.

        "Catch-all" interfaces like `0.0.0.0` or `::/0` can be used
        for listening to utilise all interfaces, but cannot be accessed.
        Some other real ("specified") address must be used for that.

        If the address is not IPv4/IPv6 address or is a regular "specified"
        address, it is used as is. Only the special addressed are overridden.
        """
        if addr is None:
            return 'localhost'  # and let the system resolved it to IPv4/IPv6
        try:
            ipv4 = ipaddress.IPv4Address(addr)
        except ipaddress.AddressValueError:
            pass
        else:
            return '127.0.0.1' if ipv4.is_unspecified else addr
        try:
            ipv6 = ipaddress.IPv6Address(addr)
        except ipaddress.AddressValueError:
            pass
        else:
            return '::1' if ipv6.is_unspecified else addr
        return addr

    @staticmethod
    def _build_url(schema: str, host: str, port: int, path: str) -> str:
        try:
            ipv6 = ipaddress.IPv6Address(host)
        except ipaddress.AddressValueError:
            pass
        else:
            host = f'[{ipv6}]'
        is_default_port = ((schema == 'http' and port == 80) or
                           (schema == 'https' and port == 443))
        netloc = host if is_default_port else f'{host}:{port}'
        return urllib.parse.urlunsplit([schema, netloc, path, '', ''])

    def _build_ssl(self) -> tuple[Optional[bytes], Optional[ssl.SSLContext]]:
        """
        A macros to construct an SSL context, possibly generating SSL certs.

        Returns a CA bundle to be passed to the "client configs",
        and a properly initialised SSL context to be used by the server.
        Or ``None`` for both if an HTTP server is needed.
        """
        cadata = self.cadata
        context = self.context
        if self.insecure and self.context is not None:
            raise ValueError("Insecure mode cannot have an SSL context specified.")

        # Read the provided CA bundle for webhooks' "client config"; not used by the server itself.
        if cadata is None and self.cafile is not None:
            cadata = pathlib.Path(self.cafile).read_bytes()

        # Kubernetes does not work with HTTP, so we do not bother and always run HTTPS too.
        # Except when explicitly said to be insecure, e.g. by ngrok (free plan only supports HTTP).
        if context is None and not self.insecure:
            context = ssl.create_default_context(purpose=ssl.Purpose.CLIENT_AUTH)

        if context is not None:

            # Load a CA for verifying the client certificates (if provided) by this server.
            if self.verify_mode is not None:
                context.verify_mode = self.verify_mode
            if self.verify_cafile or self.verify_capath or self.verify_cadata:
                logger.debug("Loading a CA for client certificate verification.")
                context.load_verify_locations(
                    self.verify_cafile,
                    self.verify_capath,
                    self.verify_cadata,
                )
                if context.verify_mode == ssl.CERT_NONE:
                    context.verify_mode = ssl.CERT_OPTIONAL

            # Load the specified server's certificate, or generate a self-signed one if possible.
            # If cafile/cadata are not defined, use the server's certificate as a CA for clients.
            if self.certfile is not None and self.pkeyfile is not None:
                logger.debug("Using a provided certificate for HTTPS.")
                context.load_cert_chain(
                    self.certfile,
                    self.pkeyfile,
                    self.password,
                )
                if cadata is None and self.certfile is not None:
                    cadata = pathlib.Path(self.certfile).read_bytes()
            else:
                logger.debug("Generating a self-signed certificate for HTTPS.")
                host = self.host or self.DEFAULT_HOST
                addr = self._get_accessible_addr(self.addr)
                hostnames = [host or addr, addr] + list(self.extra_sans)
                certdata, pkeydata = self.build_certificate(hostnames, self.password)
                with tempfile.NamedTemporaryFile() as certf, tempfile.NamedTemporaryFile() as pkeyf:
                    certf.write(certdata)
                    pkeyf.write(pkeydata)
                    certf.flush()
                    pkeyf.flush()
                    context.load_cert_chain(certf.name, pkeyf.name, self.password)

                # For a self-signed certificate, the CA bundle is the certificate itself,
                # regardless of what cafile/cadata are provided from outside.
                cadata = certdata

        # Dump the provided or self-signed CA (but not the key!), e.g. for `curl --cacert ...`
        if self.cadump is not None and cadata is not None:
            pathlib.Path(self.cadump).write_bytes(cadata)

        return cadata, context

    @staticmethod
    def build_certificate(
            hostnames: Collection[str],
            password: Optional[str] = None,
    ) -> tuple[bytes, bytes]:
        """
        Build a self-signed certificate with SANs (subject alternative names).

        Returns a tuple of the certificate and its private key (PEM-formatted).

        The certificate is "minimally sufficient", without much of the extra
        information on the subject besides its common and alternative names.
        However, IP addresses are properly recognised and normalised for better
        compatibility with strict SSL clients (like apiservers of Kubernetes).
        The first non-IP hostname becomes the certificate's common name --
        by convention, non-configurable. If no hostnames are found, the first
        IP address is used as a fallback. Magic IPs like 0.0.0.0 are excluded.

        ``certbuilder`` is used as an implementation because it is lightweight:
        2.9 MB vs. 8.7 MB for cryptography. Still, it is too heavy to include
        as a normal runtime dependency (for 8.8 MB of Kopf itself), so it is
        only available as the ``kopf[dev]`` extra for development-mode dependencies.
        This can change in the future if self-signed certificates become used
        at runtime (e.g. in production/staging environments or other real clusters).
        """
        try:
            import certbuilder
            import oscrypto.asymmetric
        except ImportError:
            raise MissingDependencyError(
                "Using self-signed certificates requires an extra dependency: "
                "run `pip install certbuilder` or `pip install kopf[dev]`. "
                "Or pass `insecure=True` to a webhook server to use only HTTP. "
                "Or generate your own certificates and pass as certfile=/pkeyfile=. "
                "More: https://kopf.readthedocs.io/en/stable/admission/")

        # Detect which ones of the hostnames are probably IPv4/IPv6 addresses.
        # A side-effect: bring them all to their canonical forms.
        parsed_ips: dict[str, Union[ipaddress.IPv4Address, ipaddress.IPv6Address]] = {}
        for hostname in hostnames:
            try:
                parsed_ips[hostname] = ipaddress.IPv4Address(hostname)
            except ipaddress.AddressValueError:
                pass  # non-parsable IPs are considered to be regular hostnames
            try:
                parsed_ips[hostname] = ipaddress.IPv6Address(hostname)
            except ipaddress.AddressValueError:
                pass  # non-parsable IPs are considered to be regular hostnames

        # Later, only the normalised IPs are used as SANs, not the raw IPs.
        # Remove bindable but non-accessible addresses (like 0.0.0.0) form the SANs.
        true_hostnames = [hostname for hostname in hostnames if hostname not in parsed_ips]
        accessible_ips = [str(ip) for ip in parsed_ips.values() if not ip.is_unspecified]

        # Build a certificate as the framework believe is good enough for itself.
        subject = {'common_name': true_hostnames[0] if true_hostnames else accessible_ips[0]}
        public_key, private_key = oscrypto.asymmetric.generate_pair('rsa', bit_size=2048)
        builder = certbuilder.CertificateBuilder(subject, public_key)
        builder.ca = True
        builder.key_usage = {'digital_signature', 'key_encipherment', 'key_cert_sign', 'crl_sign'}
        builder.extended_key_usage = {'server_auth', 'client_auth'}
        builder.self_signed = True
        builder.subject_alt_ips = list(set(accessible_ips))  # deduplicate
        builder.subject_alt_domains = list(set(true_hostnames) | set(accessible_ips))  # deduplicate
        certificate = builder.build(private_key)
        cert_pem: bytes = certbuilder.pem_armor_certificate(certificate)
        pkey_pem: bytes = oscrypto.asymmetric.dump_private_key(private_key, password, target_ms=10)
        return cert_pem, pkey_pem


class WebhookK3dServer(WebhookServer):
    """
    A tunnel from inside of K3d/K3s to its host where the operator is running.

    With this tunnel, a developer can develop the webhooks when fully offline,
    since all the traffic is local and never leaves the host machine.

    The forwarding is maintained by K3d itself. This tunnel only replaces
    the endpoints for the Kubernetes webhook and injects an SSL certificate
    with proper CN/SANs --- to match Kubernetes's SSL validity expectations.
    """
    DEFAULT_HOST = 'host.k3d.internal'


class WebhookMinikubeServer(WebhookServer):
    """
    A tunnel from inside of Minikube to its host where the operator is running.

    With this tunnel, a developer can develop the webhooks when fully offline,
    since all the traffic is local and never leaves the host machine.

    The forwarding is maintained by Minikube itself. This tunnel only replaces
    the endpoints for the Kubernetes webhook and injects an SSL certificate
    with proper CN/SANs --- to match Kubernetes's SSL validity expectations.
    """
    DEFAULT_HOST = 'host.minikube.internal'


class WebhookDockerDesktopServer(WebhookServer):
    """A tunnel from inside of Docker Desktop to its host where the operator is
    running.

    With this tunnel, a developer can develop the webhooks when fully
    offline, since all the traffic is local and never leaves the host
    machine.

    The forwarding is maintained by Docker Desktop itself. This tunnel
    only replaces the endpoints for the Kubernetes webhook and injects
    an SSL certificate with proper CN/SANs --- to match Kubernetes's SSL
    validity expectations.
    """
    DEFAULT_HOST = "host.docker.internal"


class WebhookNgrokTunnel(webhacks.WebhookContextManager):
    """
    Tunnel admission webhook request via an external tunnel: ngrok_.

    .. _ngrok: https://ngrok.com/

    ``addr``, ``port``, and ``path`` have the same meaning as in
    `kopf.WebhookServer`: where to listen for connections locally.
    Ngrok then tunnels this endpoint remotely with.

    Mind that the ngrok webhook tunnel runs the local webhook server
    in an insecure (HTTP) mode. For secure (HTTPS) mode, a paid subscription
    and properly issued certificates are needed. This goes beyond Kopf's scope.
    If needed, implement your own ngrok tunnel.

    Besides, ngrok tunnel does not report any CA to the webhook client configs.
    It is expected that the default trust chain is sufficient for ngrok's certs.

    ``token`` can be used for paid subscriptions, which lifts some limitations.
    Otherwise, the free plan has a limit of 40 requests per minute
    (this should be enough for local development).

    ``binary``, if set, will use the specified ``ngrok`` binary path;
    otherwise, ``pyngrok`` downloads the binary at runtime (not recommended).

    .. warning::

        The public URL is not properly protected and a malicious user
        can send requests to a locally running operator. If the handlers
        only process the data and make no side effects, this should be fine.

        Despite ngrok provides basic auth ("username:password"),
        Kubernetes does not permit this information in the URLs.

        Ngrok partially "protects" the URLS by assigning them random hostnames.
        Additionally, you can add random paths. However, this is not "security",
        only a bit of safety for a short time (enough for development runs).
    """
    addr: Optional[str]  # None means "any interface"
    port: Optional[int]  # None means a random port
    path: Optional[str]
    token: Optional[str]
    region: Optional[str]
    binary: Optional[StrPath]

    def __init__(
            self,
            *,
            addr: Optional[str] = None,
            port: Optional[int] = None,
            path: Optional[str] = None,
            token: Optional[str] = None,
            region: Optional[str] = None,
            binary: Optional[StrPath] = None,
    ) -> None:
        super().__init__()
        self.addr = addr
        self.port = port
        self.path = path
        self.token = token
        self.region = region
        self.binary = binary

    async def __call__(self, fn: reviews.WebhookFn) -> AsyncIterator[reviews.WebhookClientConfig]:
        try:
            from pyngrok import conf, ngrok
        except ImportError:
            raise MissingDependencyError(
                "Using ngrok webhook tunnel requires an extra dependency: "
                "run `pip install pyngrok` or `pip install kopf[dev]`. "
                "More: https://kopf.readthedocs.io/en/stable/admission/")

        if self.binary is not None:
            conf.get_default().ngrok_path = str(self.binary)
        if self.region is not None:
            conf.get_default().region = self.region
        if self.token is not None:
            ngrok.set_auth_token(self.token)

        # Ngrok only supports HTTP with a free plan; HTTPS requires a paid subscription.
        tunnel: Optional[ngrok.NgrokTunnel] = None
        loop = asyncio.get_running_loop()
        async with WebhookServer(addr=self.addr, port=self.port,
                                 path=self.path, insecure=True) as server:
            # TODO: inverse try & async for?
            try:
                async for client_config in server(fn):

                    # Re-create the tunnel for each new local endpoint (if it did change at all).
                    if tunnel is not None:
                        await loop.run_in_executor(None, ngrok.disconnect, tunnel.public_url)
                    parsed = urllib.parse.urlparse(client_config['url'])
                    tunnel = await loop.run_in_executor(
                        None, functools.partial(ngrok.connect, f'{parsed.port}', bind_tls=True))

                    # Adjust for local webhook server specifics (no port, but with the same path).
                    # Report no CA bundle -- ngrok's certs (Let's Encrypt) are in a default trust chain.
                    assert tunnel is not None  # for mypy
                    url = f"{tunnel.public_url}{self.path or ''}"
                    logger.debug(f"Accessing the webhooks at {url}")
                    yield reviews.WebhookClientConfig(url=url)  # e.g. 'https://e5fc05f6494b.ngrok.io/xyz'
            finally:
                if tunnel is not None:
                    await loop.run_in_executor(None, ngrok.disconnect, tunnel.public_url)


class ClusterDetector:
    """
    A mixing for auto-server/auto-tunnel to detect the cluster type.

    The implementation of the server detection requires the least possible
    permissions or no permissions at all. In most cases, it will identify
    the server type by its SSL certificate meta-information (subject/issuer).
    SSL information is the most universal way for all typical local clusters.

    If SSL parsing fails, it will try to fetch the information from the cluster.
    However, it rarely contains any useful information about the cluster's
    surroundings and environment, but only about the cluster itself
    (though it helps with K3s).

    Note: the SSL certificate of the Kubernetes API is checked, not of webhooks.
    """
    @staticmethod
    async def guess_host() -> Optional[str]:
        try:
            import certvalidator
        except ImportError:
            raise MissingDependencyError(
                "Auto-guessing cluster types requires an extra dependency: "
                "run `pip install certvalidator` or `pip install kopf[dev]`. "
                "More: https://kopf.readthedocs.io/en/stable/admission/")

        hostname, cert = await api.read_sslcert()
        valcontext = certvalidator.ValidationContext(extra_trust_roots=[cert])
        validator = certvalidator.CertificateValidator(cert, validation_context=valcontext)
        certpath = validator.validate_tls(hostname)
        issuer_cn = certpath.first.issuer.native.get('common_name', '')
        subject_cn = certpath.first.subject.native.get('common_name', '')
        subject_org = certpath.first.subject.native.get('organization_name', '')
        subject_alt_names = [name for name in certpath.first.subject_alt_name_value.native]

        if subject_cn == 'k3s' or subject_org == 'k3s' or issuer_cn.startswith('k3s-'):
            return WebhookK3dServer.DEFAULT_HOST
        elif subject_cn == 'minikube' or issuer_cn == 'minikubeCA':
            return WebhookMinikubeServer.DEFAULT_HOST
        elif 'docker-for-desktop' in subject_alt_names:
            return WebhookDockerDesktopServer.DEFAULT_HOST
        else:
            # The default timeouts & backoffs are used to retrieve the cluster
            # version, not those from the operator. It is too difficult to get
            # the settings here in webhooks. The "proper" way is to retrieve
            # the version in observation routines and pass it via contextvars,
            # but this is too overcomplicated for a dev-mode helping utility.
            settings = configuration.OperatorSettings()
            versioninfo = await scanning.read_version(settings=settings, logger=logger)
            if '+k3s' in versioninfo.get('gitVersion', ''):
                return WebhookK3dServer.DEFAULT_HOST
        return None


class WebhookAutoServer(ClusterDetector, WebhookServer):
    """
    A locally listening webserver which attempts to guess its proper hostname.

    The choice is happening between supported webhook servers only
    (K3d/K3d and Minikube at the moment). In all other cases,
    a regular local server is started without hostname overrides.

    If automatic tunneling is possible, consider `WebhookAutoTunnel` instead.
    """
    async def __call__(self, fn: reviews.WebhookFn) -> AsyncIterator[reviews.WebhookClientConfig]:
        host = self.DEFAULT_HOST = await self.guess_host()
        if host is None:
            logger.debug(f"Cluster detection failed, running a simple local server.")
        else:
            logger.debug(f"Cluster detection found the hostname: {host}")
        async for client_config in super().__call__(fn):
            yield client_config


class WebhookAutoTunnel(ClusterDetector, webhacks.WebhookContextManager):
    """
    The same as `WebhookAutoServer`, but with possible tunneling.

    Generally, tunneling gives more possibilities to run in any environment,
    but it must not happen without a permission from the developers,
    and is not possible if running in a completely isolated/local/CI/CD cluster.
    Therefore, developers should activated automatic setup explicitly.

    If automatic tunneling is prohibited or impossible, use `WebhookAutoServer`.

    .. note::

        Automatic server/tunnel detection is highly limited in configuration
        and provides only the most common options of all servers & tunners:
        specifically, listening ``addr:port/path``.
        All other options are specific to their servers/tunnels
        and the auto-guessing logic cannot use/accept/pass them.
    """
    addr: Optional[str]  # None means "any interface"
    port: Optional[int]  # None means a random port
    path: Optional[str]

    def __init__(
            self,
            *,
            addr: Optional[str] = None,
            port: Optional[int] = None,
            path: Optional[str] = None,
    ) -> None:
        super().__init__()
        self.addr = addr
        self.port = port
        self.path = path

    async def __call__(self, fn: reviews.WebhookFn) -> AsyncIterator[reviews.WebhookClientConfig]:
        server: Union[WebhookNgrokTunnel, WebhookServer]
        host = await self.guess_host()
        if host is None:
            logger.debug(f"Cluster detection failed, using an ngrok tunnel.")
            server = WebhookNgrokTunnel(addr=self.addr, port=self.port, path=self.path)
        else:
            logger.debug(f"Cluster detection found the hostname: {host}")
            server = WebhookServer(addr=self.addr, port=self.port, path=self.path, host=host)
        async with server:
            async for client_config in server(fn):
                yield client_config



================================================
FILE: tests/conftest.py
================================================
import asyncio
import dataclasses
import importlib
import io
import json
import logging
import re
import sys
import time
from unittest.mock import AsyncMock, Mock

import aiohttp.web
import pytest

import kopf
from kopf._cogs.clients.auth import APIContext
from kopf._cogs.configs.configuration import OperatorSettings
from kopf._cogs.structs.credentials import ConnectionInfo, Vault, VaultKey
from kopf._cogs.structs.references import Resource, Selector
from kopf._core.actions.loggers import ObjectPrefixingTextFormatter, configure
from kopf._core.engines.posting import settings_var
from kopf._core.intents.registries import OperatorRegistry
from kopf._core.reactor.inventory import ResourceMemories


def pytest_configure(config):
    config.addinivalue_line('markers', "e2e: end-to-end tests with real operators.")

    # Unexpected warnings should fail the tests. Use `-Wignore` to explicitly disable it.
    config.addinivalue_line('filterwarnings', 'error')

    # Warnings from the testing tools out of our control should not fail the tests.
    config.addinivalue_line('filterwarnings', 'ignore:The loop argument:DeprecationWarning:aiohttp')
    config.addinivalue_line('filterwarnings', 'ignore:The loop argument:DeprecationWarning:asyncio')
    config.addinivalue_line('filterwarnings', 'ignore:is deprecated, use current_thread:DeprecationWarning:threading')

    # TODO: Remove when fixed in https://github.com/pytest-dev/pytest-asyncio/issues/460:
    config.addinivalue_line('filterwarnings', 'ignore:There is no current event loop:DeprecationWarning:pytest_asyncio')

    # Python 3.12 transitional period:
    config.addinivalue_line('filterwarnings', 'ignore:datetime*:DeprecationWarning:dateutil')
    config.addinivalue_line('filterwarnings', 'ignore:datetime*:DeprecationWarning:freezegun')
    config.addinivalue_line('filterwarnings', 'ignore:.*:DeprecationWarning:_pydevd_.*')


def pytest_addoption(parser):
    parser.addoption("--only-e2e", action="store_true", help="Execute end-to-end tests only.")
    parser.addoption("--with-e2e", action="store_true", help="Include end-to-end tests.")


# This logic is not applied if pytest is started explicitly on ./examples/.
# In that case, regular pytest behaviour applies -- this is intended.
def pytest_collection_modifyitems(config, items):

    # Put all e2e tests to the end, as they are assumed to be slow.
    def _is_e2e(item):
        path = item.location[0]
        return path.startswith('tests/e2e/') or path.startswith('examples/')
    etc = [item for item in items if not _is_e2e(item)]
    e2e = [item for item in items if _is_e2e(item)]

    # Mark all e2e tests, no matter how they were detected. Just for filtering.
    mark_e2e = pytest.mark.e2e
    for item in e2e:
        item.add_marker(mark_e2e)

    # Minikube tests are heavy and require a cluster. Skip them by default,
    # so that the contributors can run pytest without initial tweaks.
    mark_skip = pytest.mark.skip(reason="E2E tests are not enabled. "
                                        "Use --with-e2e/--only-e2e to enable.")
    if not config.getoption('--with-e2e') and not config.getoption('--only-e2e'):
        for item in e2e:
            item.add_marker(mark_skip)

    # Minify the test-plan if only e2e are requested (all other should be skipped).
    if config.getoption('--only-e2e'):
        items[:] = e2e
    else:
        items[:] = etc + e2e


@pytest.fixture(params=[
    ('kopf.dev', 'v1', 'kopfpeerings', True),
    ('zalando.org', 'v1', 'kopfpeerings', True),
], ids=['kopf-dev-namespaced', 'zalando-org-namespaced'])
def namespaced_peering_resource(request):
    return Resource(*request.param[:3], namespaced=request.param[3])


@pytest.fixture(params=[
    ('kopf.dev', 'v1', 'clusterkopfpeerings', False),
    ('zalando.org', 'v1', 'clusterkopfpeerings', False),
], ids=['kopf-dev-cluster', 'zalando-org-cluster'])
def cluster_peering_resource(request):
    return Resource(*request.param[:3], namespaced=request.param[3])


@pytest.fixture(params=[
    ('kopf.dev', 'v1', 'clusterkopfpeerings', False),
    ('zalando.org', 'v1', 'clusterkopfpeerings', False),
    ('kopf.dev', 'v1', 'kopfpeerings', True),
    ('zalando.org', 'v1', 'kopfpeerings', True),
], ids=['kopf-dev-cluster', 'zalando-org-cluster', 'kopf-dev-namespaced', 'zalando-org-namespaced'])
def peering_resource(request):
    return Resource(*request.param[:3], namespaced=request.param[3])


@pytest.fixture()
def namespaced_resource():
    """ The resource used in the tests. Usually mocked, so it does not matter. """
    return Resource('kopf.dev', 'v1', 'kopfexamples', namespaced=True)


@pytest.fixture()
def cluster_resource():
    """ The resource used in the tests. Usually mocked, so it does not matter. """
    return Resource('kopf.dev', 'v1', 'kopfexamples', namespaced=False)


@pytest.fixture(params=[True, False], ids=['namespaced', 'cluster'])
def resource(request):
    """ The resource used in the tests. Usually mocked, so it does not matter. """
    return Resource('kopf.dev', 'v1', 'kopfexamples', namespaced=request.param)


@pytest.fixture()
def selector(resource):
    """ The selector used in the tests. Usually mocked, so it does not matter. """
    return Selector(group=resource.group, version=resource.version, plural=resource.plural)


@pytest.fixture()
def peering_namespace(peering_resource):
    return 'ns' if peering_resource.namespaced else None


@pytest.fixture()
def namespace(resource):
    return 'ns' if resource.namespaced else None


@pytest.fixture()
def settings():
    return OperatorSettings()


@pytest.fixture()
def memories():
    return ResourceMemories()


@pytest.fixture()
def logger():
    return logging.getLogger('fake-logger')


@pytest.fixture()
def settings_via_contextvar(settings):
    token = settings_var.set(settings)
    try:
        yield
    finally:
        settings_var.reset(token)


#
# Mocks for Kopf's internal but global variables.
#


@pytest.fixture
def registry_factory():
    # For most tests: not SmartOperatorRegistry, but the empty one!
    # For e2e tests: overridden to SmartOperatorRegistry.
    return OperatorRegistry


@pytest.fixture(autouse=True)
def registry(registry_factory):
    """
    Ensure that the tests have a fresh new global (not re-used) registry.
    """
    old_registry = kopf.get_default_registry()
    new_registry = registry_factory()
    kopf.set_default_registry(new_registry)
    yield new_registry
    kopf.set_default_registry(old_registry)


#
# Mocks for Kubernetes API clients (any of them). Reasons:
# 1. We do not test the clients, we test the layers on top of them,
#    so everything low-level should be mocked and assumed to be functional.
# 2. No external calls must be made under any circumstances.
#    The unit-tests must be fully isolated from the environment.
#


@dataclasses.dataclass(frozen=True, eq=False)
class K8sMocks:
    get: Mock
    post: Mock
    patch: Mock
    delete: Mock
    stream: Mock
    sleep: Mock


@pytest.fixture()
def k8s_mocked(mocker, resp_mocker):
    # We mock on the level of our own K8s API wrappers, not the HTTP client.
    mocker.patch('kopf._cogs.clients.api.request', side_effect=Exception('Must not be called!'))

    async def itr(*_, **__):
        await asyncio.sleep(0)  # give up control
        if False:  # make it an iterator without yielding anything.
            yield

    return K8sMocks(
        get=mocker.patch('kopf._cogs.clients.api.get', return_value={}),
        post=mocker.patch('kopf._cogs.clients.api.post', return_value={}),
        patch=mocker.patch('kopf._cogs.clients.api.patch', return_value={}),
        delete=mocker.patch('kopf._cogs.clients.api.delete', return_value={}),
        stream=mocker.patch('kopf._cogs.clients.api.stream', side_effect=itr),
        sleep=mocker.patch('kopf._cogs.aiokits.aiotime.sleep', return_value=None),
    )


@pytest.fixture()
async def enforced_context(fake_vault, mocker):
    """
    Patchable context/session for some tests, e.g. with local exceptions.

    The local exceptions are supposed to simulate either the code issues,
    or the connection issues. `aresponses` does not allow to raise arbitrary
    exceptions on the client side, but only to return the erroneous responses.

    This test forces the re-authenticating decorators to always use one specific
    session for the duration of the test, so that the patches would have effect.
    """
    _, item = fake_vault.select()
    context = APIContext(item.info)
    mocker.patch(f'{APIContext.__module__}.{APIContext.__name__}', return_value=context)
    async with context.session:
        yield context


@pytest.fixture()
async def enforced_session(enforced_context: APIContext):
    yield enforced_context.session


# Note: Unused `fake_vault` is to ensure that the client wrappers have the credentials.
# Note: Unused `enforced_session` is to ensure that the session is closed for every test.
@pytest.fixture()
def resp_mocker(fake_vault, enforced_session, aresponses):
    """
    A factory of server-side callbacks for `aresponses` with mocking/spying.

    The value of the fixture is a function, which return a coroutine mock.
    That coroutine mock should be passed to `aresponses.add` as a response
    callback function. When called, it calls the mock defined by the function's
    arguments (specifically, return_value or side_effects).

    The difference from passing the responses directly to `aresponses.add`
    is that it is possible to assert on whether the response was handled
    by that callback at all (i.e. HTTP URL & method matched), especially
    if there are multiple responses registered.

    Sample usage::

        def test_me(resp_mocker):
            response = aiohttp.web.json_response({'a': 'b'})
            callback = resp_mocker(return_value=response)
            aresponses.add(hostname, '/path/', 'get', callback)
            do_something()
            assert callback.called
            assert callback.call_count == 1
    """
    def resp_maker(*args, **kwargs):
        actual_response = AsyncMock(*args, **kwargs)
        async def resp_mock_effect(request):
            nonlocal actual_response

            # The request's content can be read inside of the handler only. We preserve
            # the data into a conventional field, so that they could be asserted later.
            try:
                request.data = await request.json()
            except json.JSONDecodeError:
                request.data = await request.text()

            # Get a response/error as it was intended (via return_value/side_effect).
            response = actual_response()
            if asyncio.iscoroutine(response):
                response = await response
            return response

        return AsyncMock(side_effect=resp_mock_effect)
    return resp_maker


@pytest.fixture()
def version_api(resp_mocker, aresponses, hostname, resource):
    result = {'resources': [{
        'name': resource.plural,
        'namespaced': True,
    }]}
    version_url = resource.get_url().rsplit('/', 1)[0]  # except the plural name
    list_mock = resp_mocker(return_value=aiohttp.web.json_response(result))
    aresponses.add(hostname, version_url, 'get', list_mock)


@pytest.fixture()
def stream(fake_vault, resp_mocker, aresponses, hostname, resource, version_api):
    """ A mock for the stream of events as if returned by K8s client. """

    def feed(*args, namespace=None):
        for arg in args:

            # Prepare the stream response pre-rendered (for simplicity, no actual streaming).
            if isinstance(arg, (list, tuple)):
                stream_text = '\n'.join(json.dumps(event) for event in arg)
                stream_resp = aresponses.Response(text=stream_text)
            else:
                stream_resp = arg

            # List is requested for every watch, so we simulate it empty.
            list_data = {'items': [], 'metadata': {'resourceVersion': '0'}}
            list_resp = aiohttp.web.json_response(list_data)
            list_url = resource.get_url(namespace=namespace)

            # The stream is not empty, but is as fed.
            stream_query = {'watch': 'true', 'resourceVersion': '0'}
            stream_url = resource.get_url(namespace=namespace, params=stream_query)

            # Note: `aresponses` excludes a response once it is matched (side-effect-like).
            # So we just accumulate them there, as many as needed.
            aresponses.add(hostname, stream_url, 'get', stream_resp, match_querystring=True)
            aresponses.add(hostname, list_url, 'get', list_resp, match_querystring=True)

    # TODO: One day, find a better way to terminate a ``while-true`` reconnection cycle.
    def close(*, namespace=None):
        """
        A way to stop the stream from reconnecting: say it that the resource version is gone
        (we know a priori that it stops on this condition, and escalates to `infinite_stream`).
        """
        feed([{'type': 'ERROR', 'object': {'code': 410}}], namespace=namespace)

    return Mock(spec_set=['feed', 'close'], feed=feed, close=close)


#
# Mocks for login & checks. Used in specifialised login tests,
# and in all CLI tests (since login is implicit with CLI commands).
#

@pytest.fixture()
def hostname():
    """ A fake hostname to be used in all aiohttp/aresponses tests. """
    return 'fake-host'


@dataclasses.dataclass(frozen=True, eq=False, order=False)
class LoginMocks:
    pykube_in_cluster: Mock = None
    pykube_from_file: Mock = None
    pykube_from_env: Mock = None
    client_in_cluster: Mock = None
    client_from_file: Mock = None


@pytest.fixture()
def login_mocks(mocker):
    """
    Make all client libraries potentially optional, but do not skip the tests:
    skipping the tests is the tests' decision, not this mocking fixture's one.
    """
    kwargs = {}
    try:
        import pykube
    except ImportError:
        pass
    else:
        cfg = pykube.KubeConfig({
            'current-context': 'self',
            'clusters': [{'name': 'self',
                          'cluster': {'server': 'localhost'}}],
            'contexts': [{'name': 'self',
                          'context': {'cluster': 'self', 'namespace': 'default'}}],
        })
        kwargs |= dict(
            pykube_in_cluster=mocker.patch.object(pykube.KubeConfig, 'from_service_account', return_value=cfg),
            pykube_from_file=mocker.patch.object(pykube.KubeConfig, 'from_file', return_value=cfg),
            pykube_from_env=mocker.patch.object(pykube.KubeConfig, 'from_env', return_value=cfg),
        )
    try:
        import kubernetes
    except ImportError:
        pass
    else:
        kwargs |= dict(
            client_in_cluster=mocker.patch.object(kubernetes.config, 'load_incluster_config'),
            client_from_file=mocker.patch.object(kubernetes.config, 'load_kube_config'),
        )
    return LoginMocks(**kwargs)


@pytest.fixture(autouse=True)
def clean_kubernetes_client():
    try:
        import kubernetes
    except ImportError:
        pass  # absent client is already "clean" (or not "dirty" at least).
    else:
        kubernetes.client.configuration.Configuration.set_default(None)


# Aresponses/aiohttp must be closed strictly after the vault. See the docstring.
@pytest.fixture()
async def _fake_vault(mocker, hostname, aresponses):
    """
    A hack around pytest's internal flaw in order to close the vault in the end.

    We cannot keep both the ContextVar and vault closing in the same fixture.
    Pytest runs every async setup and every async teardown in a separate task
    (a separate ``run_until_complete()``). The `vault_var` remains invisible
    to tests (with API calls) and even to the fixture's finalizing part.
    Sync (global) context vars do work and propagate fine — hence 2 fixtures.

    Without the proper vault finalization, the cached TCP sessions/connections
    remain open, so the aresponses/aiohttp test server takes time before exiting
    (15 seconds of keep-alive timeout by default).
    """
    key = VaultKey('fixture')
    info = ConnectionInfo(server=f'https://{hostname}')
    vault = Vault({key: info})
    mocker.patch.object(vault._guard, 'wait_for')
    try:
        yield vault
    finally:
        await vault.close()


@pytest.fixture()
def fake_vault(_fake_vault):
    """
    Provide a freshly created and populated authentication vault for every test.

    Most of the tests expect some credentials to be at least provided
    (even if not used). So, we create and set the vault as if every coroutine
    is invoked from the central `operator` method (where it is set normally).

    Any blocking activities are mocked, so that the tests do not hang.
    """
    from kopf._cogs.clients import auth

    token = auth.vault_var.set(_fake_vault)
    try:
        yield _fake_vault
    finally:
        auth.vault_var.reset(token)

#
# Simulating that Kubernetes client libraries are not installed.
#

def _with_module_present(name: str):
    # If the module is required, it should either be installed,
    # or skip the test: we cannot simulate its presence (unlike its absence).
    yield pytest.importorskip(name)


def _with_module_absent(name: str):

    class ProhibitedImportFinder:
        def find_spec(self, fullname, path, target=None):
            if fullname.split('.')[0] == name:
                raise ImportError("Import is prohibited for tests.")

    try:
        mod_before = importlib.import_module(name)
    except ImportError:
        yield
        return  # nothing to patch & restore.

    # Remove any cached modules.
    preserved = {}
    for fullname, mod in list(sys.modules.items()):
        if fullname.split('.')[0] == name:
            preserved[fullname] = mod
            del sys.modules[fullname]

    # Inject the prohibition for loading this module. And restore when done.
    finder = ProhibitedImportFinder()
    sys.meta_path.insert(0, finder)
    try:
        yield
    finally:
        sys.meta_path.remove(finder)
        sys.modules |= preserved

        # Verify if it works and that we didn't break the importing machinery.
        mod_after = importlib.import_module(name)
        assert mod_after is mod_before


@pytest.fixture(params=[True], ids=['with-client'])  # for hinting suffixes
def kubernetes():
    yield from _with_module_present('kubernetes')


@pytest.fixture(params=[False], ids=['no-client'])  # for hinting suffixes
def no_kubernetes():
    yield from _with_module_absent('kubernetes')


@pytest.fixture(params=[False, True], ids=['no-client', 'with-client'])
def any_kubernetes(request):
    if request.param:
        yield from _with_module_present('kubernetes')
    else:
        yield from _with_module_absent('kubernetes')


@pytest.fixture(params=[True], ids=['with-pykube'])  # for hinting suffixes
def pykube():
    yield from _with_module_present('pykube')


@pytest.fixture(params=[False], ids=['no-pykube'])  # for hinting suffixes
def no_pykube():
    yield from _with_module_absent('pykube')


@pytest.fixture(params=[False, True], ids=['no-pykube', 'with-pykube'])
def any_pykube(request):
    if request.param:
        yield from _with_module_present('pykube')
    else:
        yield from _with_module_absent('pykube')


@pytest.fixture()
def no_pyngrok():
    yield from _with_module_absent('pyngrok')


@pytest.fixture()
def no_oscrypto():
    yield from _with_module_absent('oscrypto')


@pytest.fixture()
def no_certbuilder():
    yield from _with_module_absent('certbuilder')


@pytest.fixture()
def no_certvalidator():
    yield from _with_module_absent('certvalidator')


#
# Helpers for the timing checks.
#

@pytest.fixture()
def timer():
    return Timer()


class Timer:
    """
    A helper context manager to measure the time of the code-blocks.
    Also, supports direct comparison with time-deltas and the numbers of seconds.

    Usage:

        with Timer() as timer:
            do_something()
            print(f"Executing for {timer.seconds}s already.")
            do_something_else()

        print(f"Executed in {timer.seconds}s.")
        assert timer < 5.0
    """

    def __init__(self):
        super().__init__()
        self._ts = None
        self._te = None

    @property
    def seconds(self):
        if self._ts is None:
            return None
        elif self._te is None:
            return time.perf_counter() - self._ts
        else:
            return self._te - self._ts

    def __repr__(self):
        status = 'new' if self._ts is None else 'running' if self._te is None else 'finished'
        return f'<Timer: {self.seconds}s ({status})>'

    def __enter__(self):
        self._ts = time.perf_counter()
        self._te = None
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self._te = time.perf_counter()

    async def __aenter__(self):
        return self.__enter__()

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        return self.__exit__(exc_type, exc_val, exc_tb)

    def __int__(self):
        return int(self.seconds)

    def __float__(self):
        return float(self.seconds)

#
# Helpers for the logging checks.
#


@pytest.fixture()
def logstream(caplog):
    """ Prefixing is done at the final output. We have to intercept it. """

    logger = logging.getLogger()
    handlers = list(logger.handlers)

    # Setup all log levels of sub-libraries. A sife-effect: the handlers are also added.
    configure(verbose=True)

    # Remove any stream handlers added in the step above. But keep the caplog's handlers.
    for handler in list(logger.handlers):
        if isinstance(handler, logging.StreamHandler) and handler.stream is sys.stderr:
            logger.removeHandler(handler)

    # Inject our stream-intercepting handler.
    stream = io.StringIO()
    handler = logging.StreamHandler(stream)
    formatter = ObjectPrefixingTextFormatter('prefix %(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)

    try:
        with caplog.at_level(logging.DEBUG):
            yield stream
    finally:
        logger.removeHandler(handler)
        logger.handlers[:] = handlers  # undo `configure()`


@pytest.fixture()
def assert_logs(caplog):
    """
    A function to assert the logs are present (by pattern).

    The listed message patterns MUST be present, in the order specified.
    Some other log messages can also be present, but they are ignored.
    """
    def assert_logs_fn(patterns, prohibited=[], strict=False):
        __traceback_hide__ = True
        remaining_patterns = list(patterns)
        for message in caplog.messages:
            # The expected pattern is at position 0.
            # Looking-ahead: if one of the following patterns matches, while the
            # 0th does not, then the log message is missing, and we fail the test.
            for idx, pattern in enumerate(remaining_patterns):
                m = re.search(pattern, message)
                if m:
                    if idx == 0:
                        remaining_patterns[:1] = []
                        break  # out of `remaining_patterns` cycle
                    else:
                        skipped_patterns = remaining_patterns[:idx]
                        raise AssertionError(f"Several patterns were skipped: {skipped_patterns!r}")
                elif strict:
                    raise AssertionError(f"Unexpected log message: {message!r}")

            # Check that the prohibited patterns do not appear in any message.
            for pattern in prohibited:
                m = re.search(pattern, message)
                if m:
                    raise AssertionError(f"Prohibited log pattern found: {message!r} ~ {pattern!r}")

        # If all patterns have been matched in order, we are done.
        # if some are left, but the messages are over, then we fail.
        if remaining_patterns:
            raise AssertionError(f"Several patterns were missed: {remaining_patterns!r}")

    return assert_logs_fn


#
# Helpers for asyncio checks.
#
@pytest.fixture()
async def loop():
    yield asyncio.get_running_loop()


@pytest.fixture(autouse=True)
def _no_asyncio_pending_tasks(loop: asyncio.AbstractEventLoop):
    """
    Ensure there are no unattended asyncio tasks after the test.

    It looks  both in the test's main event-loop, and in all other event-loops,
    such as the background thread of `KopfRunner` (used in e2e tests).

    Current solution uses some internals of asyncio, since there is no public
    interface for that. The warnings are printed only at the end of pytest.

    An alternative way: set event-loop's exception handler, force garbage
    collection after every test, and check messages from `asyncio.Task.__del__`.
    This, however, requires intercepting all event-loop creation in the code.
    """
    before = _get_all_tasks()

    # Run the test.
    yield

    # Let the pytest-asyncio's async2sync wrapper to finish all callbacks. Otherwise, it raises:
    #   <Task pending name='Task-2' coro=<<async_generator_athrow without __name__>()>>
    loop.run_until_complete(asyncio.sleep(0))

    # Detect all leftover tasks.
    after = _get_all_tasks()
    remains = after - before
    if remains:
        pytest.fail(f"Unattended asyncio tasks detected: {remains!r}")


def _get_all_tasks() -> set[asyncio.Task]:
    """Similar to `asyncio.all_tasks`, but for all event loops at once."""
    i = 0
    while True:
        try:
            if sys.version_info >= (3, 12):
                tasks = asyncio.tasks._eager_tasks | set(asyncio.tasks._scheduled_tasks)
            else:
                tasks = list(asyncio.tasks._all_tasks)
        except RuntimeError:
            i += 1
            if i >= 1000:
                raise  # we are truly unlucky today; try again tomorrow
        else:
            break
    return {t for t in tasks if not t.done()}



================================================
FILE: tests/test_absent_modules.py
================================================
"""
Verify that the module-prohibiting fixtures do work as expected.
Otherwise, the tests are useless or can show false-positives.
"""
import pytest


@pytest.mark.usefixtures('no_kubernetes')
def test_client_uninstalled_has_effect():
    with pytest.raises(ImportError):
        import kubernetes


@pytest.mark.usefixtures('no_pykube')
def test_pykube_uninstalled_has_effect():
    with pytest.raises(ImportError):
        import pykube



================================================
FILE: tests/test_async.py
================================================
"""
Just to make sure that asyncio tests are configured properly.
"""
import asyncio

_async_was_executed = False


async def test_async_tests_are_enabled(timer):
    global _async_was_executed
    _async_was_executed = True  # asserted in a sync-test below.

    with timer as t:
        await asyncio.sleep(0.5)

    assert t.seconds > 0.5  # real sleep


async def test_async_mocks_are_enabled(timer, mocker):
    p = mocker.patch('asyncio.sleep')
    with timer as t:
        await asyncio.sleep(1.0)

    assert p.call_count > 0
    assert p.await_count > 0
    assert t.seconds < 0.01  # mocked sleep


def test_async_test_was_executed_and_awaited():
    assert _async_was_executed



================================================
FILE: tests/test_filtering_helpers.py
================================================
import kopf


def _never1(*_, **__):
    return False


def _never2(*_, **__):
    return False


def _always1(*_, **__):
    return True


def _always2(*_, **__):
    return True


def test_notfn_when_true():
    combined = kopf.not_(_always1)
    result = combined()
    assert result is False


def test_notfn_when_false():
    combined = kopf.not_(_never1)
    result = combined()
    assert result is True


def test_allfn_when_all_are_true():
    combined = kopf.all_([_always1, _always2])
    result = combined()
    assert result is True


def test_allfn_when_one_is_false():
    combined = kopf.all_([_always1, _never1])
    result = combined()
    assert result is False


def test_allfn_when_all_are_false():
    combined = kopf.all_([_never1, _never2])
    result = combined()
    assert result is False


def test_allfn_when_no_functions():
    combined = kopf.all_([])
    result = combined()
    assert result is True


def test_anyfn_when_all_are_true():
    combined = kopf.any_([_always1, _always2])
    result = combined()
    assert result is True


def test_anyfn_when_one_is_false():
    combined = kopf.any_([_always1, _never1])
    result = combined()
    assert result is True


def test_anyfn_when_all_are_false():
    combined = kopf.any_([_never1, _never2])
    result = combined()
    assert result is False


def test_anyfn_when_no_functions():
    combined = kopf.any_([])
    result = combined()
    assert result is False


def test_nonefn_when_all_are_true():
    combined = kopf.none_([_always1, _always2])
    result = combined()
    assert result is False


def test_nonefn_when_one_is_false():
    combined = kopf.none_([_always1, _never1])
    result = combined()
    assert result is False


def test_nonefn_when_all_are_false():
    combined = kopf.none_([_never1, _never2])
    result = combined()
    assert result is True


def test_nonefn_when_no_functions():
    combined = kopf.none_([])
    result = combined()
    assert result is True



================================================
FILE: tests/test_finalizers.py
================================================
import pytest

from kopf._cogs.structs.finalizers import allow_deletion, block_deletion, \
                                          is_deletion_blocked, is_deletion_ongoing


def test_finalizer_is_fqdn(settings):
    assert settings.persistence.finalizer.startswith('kopf.zalando.org/')


@pytest.mark.parametrize('expected, body', [
    pytest.param(True, {'metadata': {'deletionTimestamp': '2020-12-31T23:59:59'}}, id='time'),
    pytest.param(False, {'metadata': {'deletionTimestamp': None}}, id='none'),
    pytest.param(False, {'metadata': {}}, id='no-field'),
    pytest.param(False, {}, id='no-metadata'),
])
def test_is_deleted(expected, body):
    result = is_deletion_ongoing(body=body)
    assert result == expected


@pytest.mark.parametrize('expected, body', [
    pytest.param(False, {}, id='no-metadata'),
    pytest.param(False, {'metadata': {}}, id='no-finalizers'),
    pytest.param(False, {'metadata': {'finalizers': []}}, id='empty'),
    pytest.param(False, {'metadata': {'finalizers': ['other']}}, id='others'),
    pytest.param(True, {'metadata': {'finalizers': ['fin']}}, id='normal'),
    pytest.param(True, {'metadata': {'finalizers': ['other', 'fin']}}, id='mixed'),
])
def test_has_finalizers(expected, body):
    result = is_deletion_blocked(body=body, finalizer='fin')
    assert result == expected


def test_append_finalizers_to_others():
    body = {'metadata': {'finalizers': ['other1', 'other2']}}
    patch = {}
    block_deletion(body=body, patch=patch, finalizer='fin')
    assert patch == {'metadata': {'finalizers': ['other1', 'other2', 'fin']}}


def test_append_finalizers_to_empty():
    body = {}
    patch = {}
    block_deletion(body=body, patch=patch, finalizer='fin')
    assert patch == {'metadata': {'finalizers': ['fin']}}


def test_append_finalizers_when_present():
    body = {'metadata': {'finalizers': ['other1', 'fin', 'other2']}}
    patch = {}
    block_deletion(body=body, patch=patch, finalizer='fin')
    assert patch == {}


@pytest.mark.parametrize('finalizer', [
    pytest.param('fin', id='normal'),
])
def test_remove_finalizers_keeps_others(finalizer):
    body = {'metadata': {'finalizers': ['other1', finalizer, 'other2']}}
    patch = {}
    allow_deletion(body=body, patch=patch, finalizer='fin')
    assert patch == {'metadata': {'finalizers': ['other1', 'other2']}}


def test_remove_finalizers_when_absent():
    body = {'metadata': {'finalizers': ['other1', 'other2']}}
    patch = {}
    allow_deletion(body=body, patch=patch, finalizer='fin')
    assert patch == {}


def test_remove_finalizers_when_empty():
    body = {}
    patch = {}
    allow_deletion(body=body, patch=patch, finalizer='fin')
    assert patch == {}



================================================
FILE: tests/test_it.py
================================================
def test_importing_works():
    import kopf
    assert kopf



================================================
FILE: tests/test_liveness.py
================================================
import asyncio

import aiohttp
import pytest

from kopf._cogs.structs.ephemera import Memo
from kopf._core.engines.indexing import OperatorIndexers
from kopf._core.engines.probing import health_reporter
from kopf._core.intents.causes import Activity
from kopf._core.intents.handlers import ActivityHandler
from kopf._core.intents.registries import OperatorRegistry


@pytest.fixture()
async def liveness_registry():
    return OperatorRegistry()


@pytest.fixture()
async def liveness_url(settings, liveness_registry, unused_tcp_port_factory):

    # The server startup is not instant, so we need a readiness flag.
    ready_flag = asyncio.Event()

    port = unused_tcp_port_factory()
    server = asyncio.create_task(
        health_reporter(
            endpoint=f'http://:{port}/xyz',
            registry=liveness_registry,
            settings=settings,
            ready_flag=ready_flag,
            indices=OperatorIndexers().indices,
            memo=Memo(),
        )
    )

    # Generally there is no or minimal timeout, except if the runner/server raise on start up.
    # In that case, escalate their error from the task instead of hanging here forever.
    try:
        await asyncio.wait_for(ready_flag.wait(), timeout=1)
        yield f'http://localhost:{port}/xyz'
    finally:
        server.cancel()
        try:
            await server
        except asyncio.CancelledError:
            pass  # cancellations are expected at this point


async def test_liveness_for_just_status(liveness_url):
    async with aiohttp.ClientSession() as session:
        async with session.get(liveness_url) as response:
            data = await response.json()
            assert isinstance(data, dict)


async def test_liveness_with_reporting(liveness_url, liveness_registry):

    def fn1(**kwargs):
        return {'x': 100}

    def fn2(**kwargs):
        return {'y': '200'}

    liveness_registry._activities.append(ActivityHandler(
        fn=fn1, id='id1', activity=Activity.PROBE,
        param=None, errors=None, timeout=None, retries=None, backoff=None,
    ))
    liveness_registry._activities.append(ActivityHandler(
        fn=fn2, id='id2', activity=Activity.PROBE,
        param=None, errors=None, timeout=None, retries=None, backoff=None,
    ))

    async with aiohttp.ClientSession() as session:
        async with session.get(liveness_url) as response:
            data = await response.json()
            assert isinstance(data, dict)
            assert data == {'id1': {'x': 100}, 'id2': {'y': '200'}}


async def test_liveness_data_is_cached(liveness_url, liveness_registry):
    counter = 0

    def fn1(**kwargs):
        nonlocal counter
        counter += 1
        return {'counter': counter}

    liveness_registry._activities.append(ActivityHandler(
        fn=fn1, id='id1', activity=Activity.PROBE,
        param=None, errors=None, timeout=None, retries=None, backoff=None,
    ))

    async with aiohttp.ClientSession() as session:
        async with session.get(liveness_url) as response:
            data = await response.json()
            assert isinstance(data, dict)
            assert data == {'id1': {'counter': 1}}
        async with session.get(liveness_url) as response:
            data = await response.json()
            assert isinstance(data, dict)
            assert data == {'id1': {'counter': 1}}  # not 2!



================================================
FILE: tests/test_thirdparty.py
================================================
import types

import pytest

from kopf._cogs.helpers.thirdparty import KubernetesModel


@pytest.mark.parametrize('name', ['V1Pod', 'V1ObjectMeta', 'V1PodSpec', 'V1PodTemplateSpec'])
def test_kubernetes_model_classes_detection(kubernetes, name):
    cls = getattr(kubernetes.client, name)
    assert issubclass(cls, KubernetesModel)


@pytest.mark.parametrize('name', ['CoreV1Api', 'ApiClient', 'Configuration'])
def test_kubernetes_other_classes_detection(kubernetes, name):
    cls = getattr(kubernetes.client, name)
    assert not issubclass(cls, KubernetesModel)


@pytest.mark.parametrize('cls', [object, types.SimpleNamespace])
def test_non_kubernetes_classes_detection(kubernetes, cls):
    assert not issubclass(cls, KubernetesModel)



================================================
FILE: tests/test_versions.py
================================================
import json
from typing import Any

import pytest

from kopf._cogs.clients.auth import APIContext, authenticated


def test_package_version():
    import kopf
    assert hasattr(kopf, '__version__')
    assert kopf.__version__  # not empty, not null


@pytest.mark.parametrize('version, useragent', [
    ('1.2.3', 'kopf/1.2.3'),
    ('1.2rc', 'kopf/1.2rc'),
    (None, 'kopf/unknown'),
])
async def test_http_user_agent_version(
        aresponses, hostname, fake_vault, mocker, version, useragent):

    mocker.patch('kopf._cogs.helpers.versions.version', version)

    @authenticated
    async def get_it(url: str, *, context: APIContext) -> dict[str, Any]:
        response = await context.session.get(url)
        return await response.json()

    async def responder(request):
        return aresponses.Response(
            content_type='application/json',
            text=json.dumps(dict(request.headers)))

    aresponses.add(hostname, '/', 'get', responder)
    returned_headers = await get_it(f"http://{hostname}/")
    assert returned_headers['User-Agent'] == useragent
    await fake_vault.close()  # to prevent ResourceWarnings for unclosed connectors



================================================
FILE: tests/admission/conftest.py
================================================
import asyncio
import dataclasses
import gc
import warnings

import pyngrok.conf
import pyngrok.ngrok
import pytest

from kopf._cogs.structs.references import Insights, Resource
from kopf._cogs.structs.reviews import CreateOptions, Request, RequestKind, RequestPayload, \
                                       RequestResource, UserInfo, WebhookFn
from kopf._core.engines.indexing import OperatorIndexers
from kopf._kits.webhooks import WebhookServer


# TODO: LATER: Fix this issue some day later.
@pytest.fixture()
def no_serverside_resource_warnings():
    """
    Hide an irrelevant ResourceWarning on the server side:

    It happens when a client disconnects from the webhook server,
    and the server closes the transport for that client. The garbage
    collector calls ``__del__()`` on the SSL proto object, despite
    it is not close to the moment.
    """
    with warnings.catch_warnings():
        warnings.filterwarnings('ignore',
                                category=ResourceWarning,
                                module='asyncio.sslproto',
                                message='unclosed transport')
        yield

        # Provoke the garbage collection of SSL sockets to trigger the warnings.
        # Otherwise, in PyPy, these warnings leak to other tests due to delayed gc.
        gc.collect()


# TODO: LATER: Fix this issue after aiohttp 4.0.0 is used.
@pytest.fixture()
async def no_clientside_resource_warnings():
    """
    Hide an irrelevant ResourceWarning on the client side.

    https://docs.aiohttp.org/en/stable/client_advanced.html#graceful-shutdown
    """
    yield
    await asyncio.sleep(0.100)


@pytest.fixture()
async def no_sslproto_warnings(no_serverside_resource_warnings, no_clientside_resource_warnings):
    pass


# cert generation is somewhat slow (~1s)
@pytest.fixture(scope='module')
def certpkey():
    cert, pkey = WebhookServer.build_certificate(['localhost', '127.0.0.1'])
    return cert, pkey


@pytest.fixture()
def certfile(tmpdir, certpkey):
    path = tmpdir.join('cert.pem')
    path.write_binary(certpkey[0])
    return str(path)


@pytest.fixture()
def pkeyfile(tmpdir, certpkey):
    path = tmpdir.join('pkey.pem')
    path.write_binary(certpkey[1])
    return str(path)


@pytest.fixture()
def adm_request(resource, namespace):
    return Request(
        apiVersion='admission.k8s.io/v1',
        kind='AdmissionReview',
        request=RequestPayload(
            uid='uid1',
            kind=RequestKind(group=resource.group, version=resource.version, kind=resource.kind),
            resource=RequestResource(group=resource.group, version=resource.version, resource=resource.plural),
            subResource=None,
            requestKind=RequestKind(group=resource.group, version=resource.version, kind=resource.kind),
            requestResource=RequestResource(group=resource.group, version=resource.version, resource=resource.plural),
            requestSubResource=None,
            userInfo=UserInfo(username='user1', uid='useruid1', groups=['group1']),
            name='name1',
            namespace=namespace,
            operation='CREATE',
            options=CreateOptions(apiVersion='meta.k8s.io/v1', kind='CreateOptions'),
            object={'spec': {'field': 'value'}},
            oldObject=None,
            dryRun=False,
        ))


@dataclasses.dataclass(frozen=True)
class Responder:
    fn: WebhookFn
    fut: asyncio.Future  # asyncio.Future[Response]


@pytest.fixture()
async def responder() -> Responder:
    fut = asyncio.Future()
    async def fn(*_, **__):
        return await fut
    return Responder(fn=fn, fut=fut)


@pytest.fixture()
async def insights(settings, resource):
    val_resource = Resource('admissionregistration.k8s.io', 'v1', 'validatingwebhookconfigurations')
    mut_resource = Resource('admissionregistration.k8s.io', 'v1', 'mutatingwebhookconfigurations')
    insights = Insights()
    insights.watched_resources.add(resource)
    insights.webhook_resources.add(resource)
    await insights.backbone.fill(resources=[val_resource, mut_resource])
    insights.ready_resources.set()
    return insights


@pytest.fixture()
def indices():
    indexers = OperatorIndexers()
    return indexers.indices


@pytest.fixture(autouse=True)
def pyngrok_mock(mocker):
    mocker.patch.object(pyngrok.conf, 'get_default')
    mocker.patch.object(pyngrok.ngrok, 'set_auth_token')
    mocker.patch.object(pyngrok.ngrok, 'connect')
    mocker.patch.object(pyngrok.ngrok, 'disconnect')
    pyngrok.ngrok.connect.return_value.public_url = 'https://nowhere'
    return pyngrok



================================================
FILE: tests/admission/test_admission_manager.py
================================================
import pytest

import kopf
from kopf._cogs.aiokits.aiovalues import Container
from kopf._cogs.clients.errors import APIConflictError, APIError, \
                                      APIForbiddenError, APIUnauthorizedError
from kopf._cogs.structs.references import MUTATING_WEBHOOK, VALIDATING_WEBHOOK
from kopf._core.engines.admission import configuration_manager
from kopf._core.intents.causes import WebhookType


@pytest.mark.parametrize('reason', set(WebhookType))
@pytest.mark.parametrize('selector', {VALIDATING_WEBHOOK, MUTATING_WEBHOOK})
async def test_nothing_happens_if_not_managed(
        mocker, settings, registry, insights, selector, reason, k8s_mocked):

    container = Container()
    mocker.patch.object(insights.ready_resources, 'wait')  # before the general Event.wait!
    mocker.patch.object(insights.backbone, 'wait_for')
    mocker.patch.object(container, 'as_changed')
    mocker.patch('asyncio.Event.wait')

    settings.admission.managed = None
    await configuration_manager(
        reason=reason,
        selector=selector,
        registry=registry,
        settings=settings,
        insights=insights,
        container=container,
    )

    assert not insights.ready_resources.wait.called
    assert not insights.backbone.wait_for.called
    assert not k8s_mocked.post.called
    assert not k8s_mocked.patch.called
    assert not container.as_changed.called


@pytest.mark.parametrize('reason', set(WebhookType))
@pytest.mark.parametrize('selector', {VALIDATING_WEBHOOK, MUTATING_WEBHOOK})
async def test_creation_is_attempted(
        mocker, settings, registry, insights, selector, resource, reason, k8s_mocked):

    container = Container()
    mocker.patch.object(container, 'as_changed', return_value=aiter([]))

    settings.admission.managed = 'xyz'
    await configuration_manager(
        reason=reason,
        selector=selector,
        registry=registry,
        settings=settings,
        insights=insights,
        container=container,
    )

    assert k8s_mocked.post.call_count == 1
    assert k8s_mocked.post.call_args_list[0][1]['url'].startswith('/apis/admissionregistration.k8s.io/')
    assert k8s_mocked.post.call_args_list[0][1]['payload']['metadata']['name'] == 'xyz'


@pytest.mark.parametrize('reason', set(WebhookType))
@pytest.mark.parametrize('selector', {VALIDATING_WEBHOOK, MUTATING_WEBHOOK})
async def test_creation_ignores_if_exists_already(
        mocker, settings, registry, insights, selector, resource, reason, k8s_mocked):

    container = Container()
    mocker.patch.object(container, 'as_changed', return_value=aiter([]))
    k8s_mocked.post.side_effect = APIConflictError({}, status=409)

    settings.admission.managed = 'xyz'
    await configuration_manager(
        reason=reason,
        selector=selector,
        registry=registry,
        settings=settings,
        insights=insights,
        container=container,
    )

    assert k8s_mocked.post.call_count == 1
    assert k8s_mocked.post.call_args_list[0][1]['url'].startswith('/apis/admissionregistration.k8s.io/')
    assert k8s_mocked.post.call_args_list[0][1]['payload']['metadata']['name'] == 'xyz'


@pytest.mark.parametrize('error', {APIError, APIForbiddenError, APIUnauthorizedError})
@pytest.mark.parametrize('reason', set(WebhookType))
@pytest.mark.parametrize('selector', {VALIDATING_WEBHOOK, MUTATING_WEBHOOK})
async def test_creation_escalates_on_errors(
        mocker, settings, registry, insights, selector, resource, reason, k8s_mocked, error):

    container = Container()
    mocker.patch.object(container, 'as_changed', return_value=aiter([]))
    k8s_mocked.post.side_effect = error({}, status=400)

    with pytest.raises(error):
        settings.admission.managed = 'xyz'
        await configuration_manager(
            reason=reason,
            selector=selector,
            registry=registry,
            settings=settings,
            insights=insights,
            container=container,
        )

    assert k8s_mocked.post.call_count == 1
    assert k8s_mocked.post.call_args_list[0][1]['url'].startswith('/apis/admissionregistration.k8s.io/')
    assert k8s_mocked.post.call_args_list[0][1]['payload']['metadata']['name'] == 'xyz'


@pytest.mark.parametrize('reason', set(WebhookType))
@pytest.mark.parametrize('selector', {VALIDATING_WEBHOOK, MUTATING_WEBHOOK})
async def test_patching_on_changes(
        mocker, settings, registry, insights, selector, resource, reason, k8s_mocked):

    @kopf.on.validate(*resource, registry=registry)
    def fn_v(**_): pass

    @kopf.on.mutate(*resource, registry=registry)
    def fn_m(**_): pass

    container = Container()
    mocker.patch.object(container, 'as_changed', return_value=aiter([
        {'url': 'https://hostname1/'},
        {'url': 'https://hostname2/'},
    ]))

    settings.admission.managed = 'xyz'
    await configuration_manager(
        reason=reason,
        selector=selector,
        registry=registry,
        settings=settings,
        insights=insights,
        container=container,
    )

    assert k8s_mocked.patch.call_count == 3
    assert k8s_mocked.patch.call_args_list[0][1]['url'].startswith('/apis/admissionregistration.k8s.io/')
    assert k8s_mocked.patch.call_args_list[0][1]['url'].endswith('/xyz')
    assert k8s_mocked.patch.call_args_list[1][1]['url'].startswith('/apis/admissionregistration.k8s.io/')
    assert k8s_mocked.patch.call_args_list[1][1]['url'].endswith('/xyz')
    assert k8s_mocked.patch.call_args_list[2][1]['url'].startswith('/apis/admissionregistration.k8s.io/')
    assert k8s_mocked.patch.call_args_list[2][1]['url'].endswith('/xyz')

    patch = k8s_mocked.patch.call_args_list[0][1]['payload']
    assert patch['webhooks']
    assert patch['webhooks'][0]['clientConfig']['url'].startswith('https://hostname1/')
    assert patch['webhooks'][0]['rules']
    assert patch['webhooks'][0]['rules'][0]['resources'] == ['kopfexamples']

    patch = k8s_mocked.patch.call_args_list[1][1]['payload']
    assert patch['webhooks']
    assert patch['webhooks'][0]['clientConfig']['url'].startswith('https://hostname2/')
    assert patch['webhooks'][0]['rules']
    assert patch['webhooks'][0]['rules'][0]['resources'] == ['kopfexamples']


@pytest.mark.parametrize('reason', set(WebhookType))
@pytest.mark.parametrize('selector', {VALIDATING_WEBHOOK, MUTATING_WEBHOOK})
async def test_patching_purges_non_permanent_webhooks(
        mocker, settings, registry, insights, selector, resource, reason, k8s_mocked):

    @kopf.on.validate(*resource, registry=registry, persistent=False)
    def fn_v(**_): pass

    @kopf.on.mutate(*resource, registry=registry, persistent=False)
    def fn_m(**_): pass

    container = Container()
    mocker.patch.object(container, 'as_changed', return_value=aiter([
        {'url': 'https://hostname/'},
    ]))

    settings.admission.managed = 'xyz'
    await configuration_manager(
        reason=reason,
        selector=selector,
        registry=registry,
        settings=settings,
        insights=insights,
        container=container,
    )

    assert k8s_mocked.patch.call_count == 2
    patch = k8s_mocked.patch.call_args_list[-1][1]['payload']
    assert not patch['webhooks']


@pytest.mark.parametrize('reason', set(WebhookType))
@pytest.mark.parametrize('selector', {VALIDATING_WEBHOOK, MUTATING_WEBHOOK})
async def test_patching_leaves_permanent_webhooks(
        mocker, settings, registry, insights, selector, resource, reason, k8s_mocked):

    @kopf.on.validate(*resource, registry=registry, persistent=True)
    def fn_v(**_): pass

    @kopf.on.mutate(*resource, registry=registry, persistent=True)
    def fn_m(**_): pass

    container = Container()
    mocker.patch.object(container, 'as_changed', return_value=aiter([
        {'url': 'https://hostname/'},
    ]))

    settings.admission.managed = 'xyz'
    await configuration_manager(
        reason=reason,
        selector=selector,
        registry=registry,
        settings=settings,
        insights=insights,
        container=container,
    )

    assert k8s_mocked.patch.call_count == 2
    patch = k8s_mocked.patch.call_args_list[-1][1]['payload']
    assert patch['webhooks'][0]['clientConfig']['url'].startswith('https://hostname/')
    assert patch['webhooks'][0]['rules']
    assert patch['webhooks'][0]['rules'][0]['resources'] == ['kopfexamples']


async def aiter(src):
    for item in src:
        yield item



================================================
FILE: tests/admission/test_admission_server.py
================================================
import contextlib
from unittest.mock import Mock

import pytest

import kopf
from kopf._cogs.aiokits.aiovalues import Container
from kopf._core.engines.admission import admission_webhook_server


async def webhookfn(*_, **__):
    pass


async def test_requires_webserver_if_webhooks_are_defined(
        settings, registry, insights, resource):

    @kopf.on.validate(*resource, registry=registry)
    def fn_v(**_): pass

    @kopf.on.mutate(*resource, registry=registry)
    def fn_m(**_): pass

    container = Container()
    with pytest.raises(Exception) as err:
        settings.admission.server = None
        await admission_webhook_server(
            registry=registry,
            settings=settings,
            insights=insights,
            container=container,
            webhookfn=webhookfn,
        )

    assert "Admission handlers exist, but no admission server/tunnel" in str(err.value)


async def test_configures_client_configs(
        settings, registry, insights):

    async def server(_):
        yield {'url': 'https://hostname/'}

    container = Container()
    settings.admission.server = server
    await admission_webhook_server(
        registry=registry,
        settings=settings,
        insights=insights,
        container=container,
        webhookfn=webhookfn,
    )

    assert container.get_nowait() == {'url': 'https://hostname/'}


async def test_contextmanager_class(
        settings, registry, insights):

    aenter_mock = Mock()
    aexit_mock = Mock()
    container = Container()

    class CtxMgrServer:
        async def __aenter__(self) -> "CtxMgrServer":
            aenter_mock()
            return self

        async def __aexit__(self, exc_type, exc_val, exc_tb) -> None:
            aexit_mock()

        async def __call__(self, _):
            yield {'url': 'https://hostname/'}

    settings.admission.server = CtxMgrServer()
    await admission_webhook_server(
        registry=registry,
        settings=settings,
        insights=insights,
        container=container,
        webhookfn=webhookfn,
    )

    assert aenter_mock.call_count == 1
    assert aexit_mock.call_count == 1
    assert container.get_nowait() == {'url': 'https://hostname/'}


async def test_contextmanager_decorator(
        settings, registry, insights):

    aenter_mock = Mock()
    aexit_mock = Mock()
    container = Container()

    async def server(_):
        yield {'url': 'https://hostname/'}

    @contextlib.asynccontextmanager
    async def server_manager():
        aenter_mock()
        try:
            yield server
        finally:
            aexit_mock()

    settings.admission.server = server_manager()  # can be entered only once
    await admission_webhook_server(
        registry=registry,
        settings=settings,
        insights=insights,
        container=container,
        webhookfn=webhookfn,
    )

    assert aenter_mock.call_count == 1
    assert aexit_mock.call_count == 1
    assert container.get_nowait() == {'url': 'https://hostname/'}



================================================
FILE: tests/admission/test_certificates.py
================================================
import certvalidator
import pytest

from kopf._kits.webhooks import WebhookServer


def test_missing_oscrypto(no_oscrypto):
    with pytest.raises(ImportError) as err:
        WebhookServer.build_certificate(['...'])
    assert "pip install certbuilder" in str(err.value)


def test_missing_certbuilder(no_certbuilder):
    with pytest.raises(ImportError) as err:
        WebhookServer.build_certificate(['...'])
    assert "pip install certbuilder" in str(err.value)


def test_certificate_generation():
    names = ['hostname1', 'hostname2', '1.2.3.4', '0:0:0:0:0:0:0:1']
    cert, pkey = WebhookServer.build_certificate(names)
    context = certvalidator.ValidationContext(extra_trust_roots=[cert])
    validator = certvalidator.CertificateValidator(cert, validation_context=context)
    path = validator.validate_tls('hostname1')
    assert len(path) == 1  # self-signed
    assert path.first.ca
    assert path.first.self_issued
    assert set(path.first.valid_domains) == {'hostname1', 'hostname2', '1.2.3.4', '::1'}
    assert set(path.first.valid_ips) == {'1.2.3.4', '::1'}


@pytest.mark.parametrize('hostnames, common_name', [
    (['hostname1', 'hostname2'], 'hostname1'),
    (['hostname2', 'hostname1'], 'hostname2'),
    (['1.2.3.4', 'hostname1'], 'hostname1'),
    (['1.2.3.4', '2.3.4.5'], '1.2.3.4'),
])
def test_common_name_selection(hostnames, common_name):
    cert, pkey = WebhookServer.build_certificate(hostnames)
    context = certvalidator.ValidationContext(extra_trust_roots=[cert])
    validator = certvalidator.CertificateValidator(cert, validation_context=context)
    path = validator.validate_tls(common_name)
    assert path.first.subject.native['common_name'] == common_name



================================================
FILE: tests/admission/test_jsonpatch.py
================================================
from kopf._cogs.structs.patches import Patch


def test_addition_of_the_key():
    body = {'abc': 456}
    patch = Patch(body=body)
    patch['xyz'] = 123
    jsonpatch = patch.as_json_patch()
    assert jsonpatch == [
        {'op': 'add', 'path': '/xyz', 'value': 123},
    ]


def test_replacement_of_the_key():
    body = {'xyz': 456}
    patch = Patch(body=body)
    patch['xyz'] = 123
    jsonpatch = patch.as_json_patch()
    assert jsonpatch == [
        {'op': 'replace', 'path': '/xyz', 'value': 123},
    ]


def test_removal_of_the_key():
    patch = Patch()
    patch['xyz'] = None
    jsonpatch = patch.as_json_patch()
    assert jsonpatch == [
        {'op': 'remove', 'path': '/xyz'},
    ]


def test_addition_of_the_subkey():
    body = {'xyz': {'def': 456}}
    patch = Patch(body=body)
    patch['xyz'] = {'abc': 123}
    jsonpatch = patch.as_json_patch()
    assert jsonpatch == [
        {'op': 'add', 'path': '/xyz/abc', 'value': 123},
    ]

def test_replacement_of_the_subkey():
    body = {'xyz': {'abc': 456}}
    patch = Patch(body=body)
    patch['xyz'] = {'abc': 123}
    jsonpatch = patch.as_json_patch()
    assert jsonpatch == [
        {'op': 'replace', 'path': '/xyz/abc', 'value': 123},
    ]


def test_addition_of_the_sub_subkey():
    body = {'xyz': {'uvw': 123}}
    patch = Patch(body=body)
    patch['xyz'] = {'abc': {'def': {'ghi': 456}}}
    jsonpatch = patch.as_json_patch()
    assert jsonpatch == [
        {'op': 'add', 'path': '/xyz/abc', 'value': {'def': {'ghi': 456}}},
    ]


def test_removal_of_the_subkey():
    patch = Patch()
    patch['xyz'] = {'abc': None}
    jsonpatch = patch.as_json_patch()
    assert jsonpatch == [
        {'op': 'remove', 'path': '/xyz/abc'},
    ]


def test_escaping_of_key():
    patch = Patch()
    patch['~xyz/test'] = {'abc': None}
    jsonpatch = patch.as_json_patch()
    assert jsonpatch == [
        {'op': 'remove', 'path': '/~0xyz~1test/abc'}
    ]


def test_recursive_escape_of_key():
    patch = Patch()
    patch['x/y/~z'] = {'a/b/~0c': None}
    jsonpatch = patch.as_json_patch()
    assert jsonpatch == [
        {'op': 'remove', 'path': '/x~1y~1~0z/a~1b~1~00c'},
    ]



================================================
FILE: tests/admission/test_managed_webhooks.py
================================================
import pytest

import kopf
from kopf._cogs.structs.references import Resource
from kopf._core.engines.admission import build_webhooks


@pytest.fixture()
def handlers(resource, registry):

    @kopf.on.validate(*resource, registry=registry)
    def validate_fn(**_):
        pass

    @kopf.on.mutate(*resource, registry=registry)
    def mutate_fn(**_):
        pass

    return registry._webhooks.get_all_handlers()


@pytest.mark.parametrize('id, field, exp_name', [
    ('id', None, 'id.sfx'),
    ('id.', None, 'id..sfx'),
    ('id-', None, 'id-.sfx'),
    ('id_', None, 'id-.sfx'),
    ('id!', None, 'id21.sfx'),
    ('id%', None, 'id25.sfx'),
    ('id/sub', None, 'id.sub.sfx'),
    ('id', 'fld1.fld2', 'id.fld1.fld2.sfx'),
])
@pytest.mark.parametrize('decorator', [kopf.on.validate, kopf.on.mutate])
def test_name_is_normalised(registry, resource, decorator, id, field, exp_name):

    @decorator(*resource, id=id, field=field, registry=registry)
    def fn(**_):
        pass

    webhooks = build_webhooks(
        registry._webhooks.get_all_handlers(),
        resources=[resource],
        name_suffix='sfx',
        client_config={})

    assert len(webhooks) == 1
    assert webhooks[0]['name'] == exp_name


@pytest.mark.parametrize('id, field, exp_url', [
    ('id', None, 'https://hostname/p1/p2/id'),
    ('id.', None, 'https://hostname/p1/p2/id.'),
    ('id-', None, 'https://hostname/p1/p2/id-'),
    ('id_', None, 'https://hostname/p1/p2/id_'),
    ('id!', None, 'https://hostname/p1/p2/id%21'),
    ('id%', None, 'https://hostname/p1/p2/id%25'),
    ('id/sub', None, 'https://hostname/p1/p2/id/sub'),
    ('id', 'fld1.fld2', 'https://hostname/p1/p2/id/fld1.fld2'),
])
@pytest.mark.parametrize('decorator', [kopf.on.validate, kopf.on.mutate])
def test_url_is_suffixed(registry, resource, decorator, id, field, exp_url):

    @decorator(*resource, id=id, field=field, registry=registry)
    def fn(**_):
        pass

    webhooks = build_webhooks(
        registry._webhooks.get_all_handlers(),
        resources=[resource],
        name_suffix='sfx',
        client_config={'url': 'https://hostname/p1/p2'})

    assert len(webhooks) == 1
    assert webhooks[0]['clientConfig']['url'] == exp_url


@pytest.mark.parametrize('id, field, exp_path', [
    ('id', None, 'p1/p2/id'),
    ('id.', None, 'p1/p2/id.'),
    ('id-', None, 'p1/p2/id-'),
    ('id_', None, 'p1/p2/id_'),
    ('id!', None, 'p1/p2/id%21'),
    ('id%', None, 'p1/p2/id%25'),
    ('id/sub', None, 'p1/p2/id/sub'),
    ('id', 'fld1.fld2', 'p1/p2/id/fld1.fld2'),
])
@pytest.mark.parametrize('decorator', [kopf.on.validate, kopf.on.mutate])
def test_path_is_suffixed(registry, resource, decorator, id, field, exp_path):

    @decorator(*resource, id=id, field=field, registry=registry)
    def fn(**_):
        pass

    webhooks = build_webhooks(
        registry._webhooks.get_all_handlers(),
        resources=[resource],
        name_suffix='sfx',
        client_config={'service': {'path': 'p1/p2'}})

    assert len(webhooks) == 1
    assert webhooks[0]['clientConfig']['service']['path'] == exp_path


@pytest.mark.parametrize('opts, key, val', [
    (dict(side_effects=False), 'sideEffects', 'None'),
    (dict(side_effects=True), 'sideEffects', 'NoneOnDryRun'),
    (dict(ignore_failures=False), 'failurePolicy', 'Fail'),
    (dict(ignore_failures=True), 'failurePolicy', 'Ignore'),
])
@pytest.mark.parametrize('decorator', [kopf.on.validate, kopf.on.mutate])
def test_flat_options_are_mapped(registry, resource, decorator, opts, key, val):

    @decorator(*resource, registry=registry, **opts)
    def fn(**_):
        pass

    webhooks = build_webhooks(
        registry._webhooks.get_all_handlers(),
        resources=[resource],
        name_suffix='sfx',
        client_config={})

    assert len(webhooks) == 1
    assert webhooks[0][key] == val
    assert webhooks[0]['matchPolicy'] == 'Equivalent'
    assert webhooks[0]['timeoutSeconds'] == 30
    assert webhooks[0]['admissionReviewVersions'] == ['v1', 'v1beta1']


# Mind the different supported collection types for operations, all converted to JSON lists.
@pytest.mark.parametrize('opts, key, val', [
    (dict(), 'operations', ['*']),
    (dict(operations={'CREATE'}), 'operations', ['CREATE']),
    (dict(operations={'UPDATE'}), 'operations', ['UPDATE']),
    (dict(operations={'DELETE'}), 'operations', ['DELETE']),
    (dict(operations=['CREATE','UPDATE']), 'operations', ['CREATE','UPDATE']),
    (dict(operations=['CREATE','DELETE']), 'operations', ['CREATE','DELETE']),
    (dict(operations=['UPDATE','DELETE']), 'operations', ['UPDATE','DELETE']),
])
@pytest.mark.parametrize('decorator', [kopf.on.validate, kopf.on.mutate])
def test_rule_options_are_mapped(registry, resource, decorator, opts, key, val):

    @decorator(*resource, registry=registry, **opts)
    def fn(**_):
        pass

    webhooks = build_webhooks(
        registry._webhooks.get_all_handlers(),
        resources=[resource],
        name_suffix='sfx',
        client_config={})

    assert len(webhooks) == 1
    assert len(webhooks[0]['rules']) == 1
    assert webhooks[0]['rules'][0][key] == val
    assert webhooks[0]['rules'][0]['scope'] == '*'
    assert webhooks[0]['rules'][0]['apiGroups'] == [resource.group]
    assert webhooks[0]['rules'][0]['apiVersions'] == [resource.version]
    assert webhooks[0]['rules'][0]['resources'] == [resource.plural]


@pytest.mark.parametrize('decorator', [kopf.on.validate, kopf.on.mutate])
def test_subresource_specific(registry, resource, decorator):

    @decorator(*resource, registry=registry, subresource='xyz')
    def fn(**_):
        pass

    webhooks = build_webhooks(
        registry._webhooks.get_all_handlers(),
        resources=[resource],
        name_suffix='sfx',
        client_config={})

    assert len(webhooks) == 1
    assert len(webhooks[0]['rules']) == 1
    assert webhooks[0]['rules'][0]['resources'] == [f'{resource.plural}/xyz']


@pytest.mark.parametrize('decorator', [kopf.on.validate, kopf.on.mutate])
def test_subresource_catchall(registry, resource, decorator):

    @decorator(*resource, registry=registry, subresource='*')
    def fn(**_):
        pass

    webhooks = build_webhooks(
        registry._webhooks.get_all_handlers(),
        resources=[resource],
        name_suffix='sfx',
        client_config={})

    assert len(webhooks) == 1
    assert len(webhooks[0]['rules']) == 1
    assert webhooks[0]['rules'][0]['resources'] == [f'{resource.plural}/*']


@pytest.mark.parametrize('decorator', [kopf.on.validate, kopf.on.mutate])
def test_subresource_absent(registry, resource, decorator):

    @decorator(*resource, registry=registry)
    def fn(**_):
        pass

    webhooks = build_webhooks(
        registry._webhooks.get_all_handlers(),
        resources=[resource],
        name_suffix='sfx',
        client_config={})

    assert len(webhooks) == 1
    assert len(webhooks[0]['rules']) == 1
    assert webhooks[0]['rules'][0]['resources'] == [f'{resource.plural}']


@pytest.mark.parametrize('decorator', [kopf.on.validate, kopf.on.mutate])
def test_multiple_handlers(registry, resource, decorator):

    @decorator(*resource, registry=registry)
    def fn1(**_):
        pass

    @decorator(*resource, registry=registry)
    def fn2(**_):
        pass

    webhooks = build_webhooks(
        registry._webhooks.get_all_handlers(),
        resources=[resource],
        name_suffix='sfx',
        client_config={})

    assert len(webhooks) == 2
    assert len(webhooks[0]['rules']) == 1
    assert len(webhooks[1]['rules']) == 1


@pytest.mark.parametrize('decorator', [kopf.on.validate, kopf.on.mutate])
def test_irrelevant_resources_are_ignored(registry, resource, decorator):

    @decorator(*resource, registry=registry)
    def fn(**_):
        pass

    irrelevant_resource = Resource('grp', 'vers', 'plural')
    webhooks = build_webhooks(
        registry._webhooks.get_all_handlers(),
        resources=[irrelevant_resource],
        name_suffix='sfx',
        client_config={})

    assert len(webhooks) == 1
    assert len(webhooks[0]['rules']) == 0


@pytest.mark.parametrize('label_value, exp_expr', [
    (kopf.PRESENT, {'key': 'lbl', 'operator': 'Exists'}),
    (kopf.ABSENT, {'key': 'lbl', 'operator': 'DoesNotExist'}),
    ('val', {'key': 'lbl', 'operator': 'In', 'values': ['val']}),
])
@pytest.mark.parametrize('decorator', [kopf.on.validate, kopf.on.mutate])
def test_labels_specific_filter(registry, resource, decorator, label_value, exp_expr):

    @decorator(*resource, registry=registry, labels={'lbl': label_value})
    def fn(**_):
        pass

    irrelevant_resource = Resource('grp', 'vers', 'plural')
    webhooks = build_webhooks(
        registry._webhooks.get_all_handlers(),
        resources=[irrelevant_resource],
        name_suffix='sfx',
        client_config={})

    assert len(webhooks) == 1
    assert webhooks[0]['objectSelector'] == {'matchExpressions': [exp_expr]}


@pytest.mark.parametrize('decorator', [kopf.on.validate, kopf.on.mutate])
def test_labels_callable_filter(registry, resource, decorator):

    @decorator(*resource, registry=registry, labels={'lbl': lambda *_, **__: None})
    def fn(**_):
        pass

    irrelevant_resource = Resource('grp', 'vers', 'plural')
    webhooks = build_webhooks(
        registry._webhooks.get_all_handlers(),
        resources=[irrelevant_resource],
        name_suffix='sfx',
        client_config={})

    assert len(webhooks) == 1
    assert webhooks[0]['objectSelector'] is None



================================================
FILE: tests/admission/test_serving_ephemeral_memos.py
================================================
import pytest

from kopf._core.engines.admission import serve_admission_request


@pytest.mark.parametrize('operation', ['CREATE'])
async def test_memo_is_not_remembered_if_admission_is_for_creation(
        settings, registry, resource, memories, insights, indices, adm_request, operation):

    adm_request['request']['operation'] = operation
    await serve_admission_request(
        adm_request,
        settings=settings, registry=registry, insights=insights,
        memories=memories, memobase=object(), indices=indices,
    )
    known_memories = list(memories.iter_all_memories())
    assert not known_memories


@pytest.mark.parametrize('operation', ['UPDATE', 'DELETE', 'CONNECT', '*WHATEVER*'])
async def test_memo_is_remembered_if_admission_for_other_operations(
        settings, registry, resource, memories, insights, indices, adm_request, operation):

    adm_request['request']['operation'] = operation
    await serve_admission_request(
        adm_request,
        settings=settings, registry=registry, insights=insights,
        memories=memories, memobase=object(), indices=indices,
    )
    known_memories = list(memories.iter_all_memories())
    assert len(known_memories) == 1



================================================
FILE: tests/admission/test_serving_handler_selection.py
================================================
from unittest.mock import Mock

import pytest

import kopf
from kopf._cogs.structs.ids import HandlerId
from kopf._core.engines.admission import serve_admission_request
from kopf._core.intents.causes import WebhookType


async def test_all_handlers_with_no_id_or_reason_requested(
        settings, registry, resource, memories, insights, indices, adm_request):

    mock1 = Mock()
    mock2 = Mock()
    mock3 = Mock()
    mock4 = Mock()

    @kopf.on.validate(*resource)
    def fn1(**kwargs):
        mock1(**kwargs)

    @kopf.on.validate(*resource)
    def fn2(**kwargs):
        mock2(**kwargs)

    @kopf.on.mutate(*resource)
    def fn3(**kwargs):
        mock3(**kwargs)

    @kopf.on.mutate(*resource)
    def fn4(**kwargs):
        mock4(**kwargs)

    response = await serve_admission_request(
        adm_request,
        settings=settings, registry=registry, insights=insights,
        memories=memories, memobase=object(), indices=indices,
    )
    assert response['response']['allowed'] is True
    assert mock1.call_count == 1
    assert mock2.call_count == 1
    assert mock3.call_count == 1
    assert mock4.call_count == 1


@pytest.mark.parametrize('reason', set(WebhookType))
async def test_handlers_with_reason_requested(
        settings, registry, resource, memories, insights, indices, adm_request, reason):

    mock1 = Mock()
    mock2 = Mock()
    mock3 = Mock()
    mock4 = Mock()

    @kopf.on.validate(*resource)
    def fn1(**kwargs):
        mock1(**kwargs)

    @kopf.on.validate(*resource)
    def fn2(**kwargs):
        mock2(**kwargs)

    @kopf.on.mutate(*resource)
    def fn3(**kwargs):
        mock3(**kwargs)

    @kopf.on.mutate(*resource)
    def fn4(**kwargs):
        mock4(**kwargs)

    response = await serve_admission_request(
        adm_request, reason=reason,
        settings=settings, registry=registry, insights=insights,
        memories=memories, memobase=object(), indices=indices,
    )
    assert response['response']['allowed'] is True
    assert mock1.call_count == (1 if reason == WebhookType.VALIDATING else 0)
    assert mock2.call_count == (1 if reason == WebhookType.VALIDATING else 0)
    assert mock3.call_count == (1 if reason == WebhookType.MUTATING else 0)
    assert mock4.call_count == (1 if reason == WebhookType.MUTATING else 0)


async def test_handlers_with_webhook_requested(
        settings, registry, resource, memories, insights, indices, adm_request):

    mock1 = Mock()
    mock2 = Mock()
    mock3 = Mock()
    mock4 = Mock()

    @kopf.on.validate(*resource, id='fnX')
    def fn1(**kwargs):
        mock1(**kwargs)

    @kopf.on.validate(*resource)
    def fn2(**kwargs):
        mock2(**kwargs)

    @kopf.on.mutate(*resource)
    def fn3(**kwargs):
        mock3(**kwargs)

    @kopf.on.mutate(*resource, id='fnX')
    def fn4(**kwargs):
        mock4(**kwargs)

    response = await serve_admission_request(
        adm_request, webhook=HandlerId('fnX'),
        settings=settings, registry=registry, insights=insights,
        memories=memories, memobase=object(), indices=indices,
    )
    assert response['response']['allowed'] is True
    assert mock1.call_count == 1
    assert mock2.call_count == 0
    assert mock3.call_count == 0
    assert mock4.call_count == 1


@pytest.mark.parametrize('reason', set(WebhookType))
async def test_handlers_with_reason_and_webhook_requested(
        settings, registry, resource, memories, insights, indices, adm_request, reason):

    mock1 = Mock()
    mock2 = Mock()
    mock3 = Mock()
    mock4 = Mock()

    @kopf.on.validate(*resource, id='fnX')
    def fn1(**kwargs):
        mock1(**kwargs)

    @kopf.on.validate(*resource)
    def fn2(**kwargs):
        mock2(**kwargs)

    @kopf.on.mutate(*resource)
    def fn3(**kwargs):
        mock3(**kwargs)

    @kopf.on.mutate(*resource, id='fnX')
    def fn4(**kwargs):
        mock4(**kwargs)

    response = await serve_admission_request(
        adm_request, webhook=HandlerId('fnX'), reason=reason,
        settings=settings, registry=registry, insights=insights,
        memories=memories, memobase=object(), indices=indices,
    )
    assert response['response']['allowed'] is True
    assert mock1.call_count == (1 if reason == WebhookType.VALIDATING else 0)
    assert mock2.call_count == 0
    assert mock3.call_count == 0
    assert mock4.call_count == (1 if reason == WebhookType.MUTATING else 0)


@pytest.mark.parametrize('operation', ['CREATE', 'UPDATE', 'CONNECT', '*WHATEVER*'])
async def test_mutating_handlers_are_selected_for_nondeletion(
        settings, registry, resource, memories, insights, indices, adm_request, operation):

    v_mock = Mock()
    m_mock = Mock()

    @kopf.on.validate(*resource)
    def v_fn(**kwargs):
        v_mock(**kwargs)

    @kopf.on.mutate(*resource)
    def m_fn(**kwargs):
        m_mock(**kwargs)

    adm_request['request']['operation'] = operation
    response = await serve_admission_request(
        adm_request,
        settings=settings, registry=registry, insights=insights,
        memories=memories, memobase=object(), indices=indices,
    )
    assert response['response']['allowed'] is True
    assert v_mock.call_count == 1
    assert m_mock.call_count == 1


async def test_mutating_handlers_are_not_selected_for_deletion_by_default(
        settings, registry, resource, memories, insights, indices, adm_request):

    v_mock = Mock()
    m_mock = Mock()

    @kopf.on.validate(*resource)
    def v_fn(**kwargs):
        v_mock(**kwargs)

    @kopf.on.mutate(*resource)
    def m_fn(**kwargs):
        m_mock(**kwargs)

    adm_request['request']['operation'] = 'DELETE'
    response = await serve_admission_request(
        adm_request,
        settings=settings, registry=registry, insights=insights,
        memories=memories, memobase=object(), indices=indices,
    )
    assert response['response']['allowed'] is True
    assert v_mock.call_count == 1
    assert m_mock.call_count == 0


async def test_mutating_handlers_are_selected_for_deletion_if_explicitly_marked(
        settings, registry, resource, memories, insights, indices, adm_request):

    v_mock = Mock()
    m_mock = Mock()

    @kopf.on.validate(*resource)
    def v_fn(**kwargs):
        v_mock(**kwargs)

    @kopf.on.mutate(*resource, operations=['DELETE'])
    def m_fn(**kwargs):
        m_mock(**kwargs)

    adm_request['request']['operation'] = 'DELETE'
    response = await serve_admission_request(
        adm_request,
        settings=settings, registry=registry, insights=insights,
        memories=memories, memobase=object(), indices=indices,
    )
    assert response['response']['allowed'] is True
    assert v_mock.call_count == 1
    assert m_mock.call_count == 1


@pytest.mark.parametrize('handler_sub, request_sub, exp_calls', [
    ('*', 'no', 1),
    ('*', 'sb', 1),
    ('*', None, 1),
    ('sb', 'no', 0),
    ('sb', 'sb', 1),
    ('sb', None, 0),
    (None, 'no', 0),
    (None, 'sb', 0),
    (None, None, 1),
])
async def test_subresources(
        settings, registry, resource, memories, insights, indices, adm_request,
        handler_sub, request_sub, exp_calls):

    v_mock = Mock()
    m_mock = Mock()

    @kopf.on.validate(*resource, subresource=handler_sub)
    def v_fn(**kwargs):
        v_mock(**kwargs)

    @kopf.on.mutate(*resource, subresource=handler_sub)
    def m_fn(**kwargs):
        m_mock(**kwargs)

    adm_request['request']['subResource'] = request_sub
    adm_request['request']['requestSubResource'] = request_sub
    response = await serve_admission_request(
        adm_request,
        settings=settings, registry=registry, insights=insights,
        memories=memories, memobase=object(), indices=indices,
    )
    assert response['response']['allowed'] is True
    assert v_mock.call_count == exp_calls
    assert m_mock.call_count == exp_calls



================================================
FILE: tests/admission/test_serving_kwargs_passthrough.py
================================================
from unittest.mock import Mock

import pytest

import kopf
from kopf._core.engines.admission import serve_admission_request


@pytest.mark.parametrize('dryrun', [True, False])
async def test_dryrun_passed(
        settings, registry, resource, memories, insights, indices, adm_request, dryrun):
    mock = Mock()

    @kopf.on.validate(*resource)
    def fn(**kwargs):
        mock(**kwargs)

    adm_request['request']['dryRun'] = dryrun
    await serve_admission_request(
        adm_request,
        settings=settings, registry=registry, insights=insights,
        memories=memories, memobase=object(), indices=indices,
    )
    assert mock.call_count == 1
    assert mock.call_args[1]['dryrun'] == dryrun


async def test_headers_passed(
        settings, registry, resource, memories, insights, indices, adm_request):
    mock = Mock()

    @kopf.on.validate(*resource)
    def fn(**kwargs):
        mock(**kwargs)

    headers = {'X': '123', 'Y': '456'}
    await serve_admission_request(
        adm_request, headers=headers,
        settings=settings, registry=registry, insights=insights,
        memories=memories, memobase=object(), indices=indices,
    )
    assert mock.call_count == 1
    assert mock.call_args[1]['headers'] == headers


async def test_headers_not_passed_but_injected(
        settings, registry, resource, memories, insights, indices, adm_request):
    mock = Mock()

    @kopf.on.validate(*resource)
    def fn(**kwargs):
        mock(**kwargs)

    await serve_admission_request(
        adm_request,
        settings=settings, registry=registry, insights=insights,
        memories=memories, memobase=object(), indices=indices,
    )
    assert mock.call_count == 1
    assert mock.call_args[1]['headers'] == {}


async def test_sslpeer_passed(
        settings, registry, resource, memories, insights, indices, adm_request):
    mock = Mock()

    @kopf.on.validate(*resource)
    def fn(**kwargs):
        mock(**kwargs)

    sslpeer = {'X': '123', 'Y': '456'}
    await serve_admission_request(
        adm_request, sslpeer=sslpeer,
        settings=settings, registry=registry, insights=insights,
        memories=memories, memobase=object(), indices=indices,
    )
    assert mock.call_count == 1
    assert mock.call_args[1]['sslpeer'] == sslpeer


async def test_sslpeer_not_passed_but_injected(
        settings, registry, resource, memories, insights, indices, adm_request):
    mock = Mock()

    @kopf.on.validate(*resource)
    def fn(**kwargs):
        mock(**kwargs)

    await serve_admission_request(
        adm_request,
        settings=settings, registry=registry, insights=insights,
        memories=memories, memobase=object(), indices=indices,
    )
    assert mock.call_count == 1
    assert mock.call_args[1]['sslpeer'] == {}


async def test_userinfo_passed(
        settings, registry, resource, memories, insights, indices, adm_request):
    mock = Mock()

    @kopf.on.validate(*resource)
    def fn(**kwargs):
        mock(**kwargs)

    userinfo = {'X': '123', 'Y': '456'}
    adm_request['request']['userInfo'] = userinfo
    await serve_admission_request(
        adm_request,
        settings=settings, registry=registry, insights=insights,
        memories=memories, memobase=object(), indices=indices,
    )
    assert mock.call_count == 1
    assert mock.call_args[1]['userinfo'] == userinfo


async def test_subresource_passed(
        settings, registry, resource, memories, insights, indices, adm_request):
    mock = Mock()

    @kopf.on.validate(*resource, subresource='*')
    def fn(**kwargs):
        mock(**kwargs)

    adm_request['request']['subResource'] = 'xyz'
    await serve_admission_request(
        adm_request,
        settings=settings, registry=registry, insights=insights,
        memories=memories, memobase=object(), indices=indices,
    )
    assert mock.call_count == 1
    assert mock.call_args[1]['subresource'] == 'xyz'



================================================
FILE: tests/admission/test_serving_responses.py
================================================
import base64
import json

import pytest

import kopf
from kopf._core.actions.execution import PermanentError, TemporaryError
from kopf._core.engines.admission import AdmissionError, serve_admission_request


async def test_metadata_reflects_the_request(
        settings, registry, memories, insights, indices, adm_request):

    adm_request['apiVersion'] = 'any.group/any.version'
    adm_request['kind'] = 'AnyKindOfAdmissionReview'
    adm_request['request']['uid'] = 'anyuid'
    response = await serve_admission_request(
        adm_request,
        settings=settings, registry=registry, insights=insights,
        memories=memories, memobase=object(), indices=indices,
    )
    assert response['apiVersion'] == 'any.group/any.version'
    assert response['kind'] == 'AnyKindOfAdmissionReview'
    assert response['response']['uid'] == 'anyuid'


async def test_simple_response_with_no_handlers_allows_admission(
        settings, registry, memories, insights, indices, adm_request):

    response = await serve_admission_request(
        adm_request,
        settings=settings, registry=registry, insights=insights,
        memories=memories, memobase=object(), indices=indices,
    )
    assert 'warnings' not in response['response']
    assert 'patchType' not in response['response']
    assert 'patch' not in response['response']
    assert 'status' not in response['response']
    assert response['response']['allowed'] is True


@pytest.mark.parametrize('decorator', [kopf.on.validate, kopf.on.mutate])
async def test_simple_handler_allows_admission(
        settings, registry, resource, memories, insights, indices, adm_request,
        decorator):

    @decorator(*resource)
    def fn(**_):
        pass

    response = await serve_admission_request(
        adm_request,
        settings=settings, registry=registry, insights=insights,
        memories=memories, memobase=object(), indices=indices,
    )
    assert 'warnings' not in response['response']
    assert 'patchType' not in response['response']
    assert 'patch' not in response['response']
    assert 'status' not in response['response']
    assert response['response']['allowed'] is True


@pytest.mark.parametrize('error, exp_msg, exp_code', [
    (Exception("No!"), "No!", 500),
    (kopf.PermanentError("No!"), "No!", 500),
    (kopf.TemporaryError("No!"), "No!", 500),
    (kopf.AdmissionError("No!"), "No!", 500),
    (kopf.AdmissionError("No!", code=123), "No!", 123),
])
@pytest.mark.parametrize('decorator', [kopf.on.validate, kopf.on.mutate])
async def test_errors_deny_admission(
        settings, registry, resource, memories, insights, indices, adm_request,
        decorator, error, exp_msg, exp_code):

    @decorator(*resource)
    def fn(**_):
        raise error

    response = await serve_admission_request(
        adm_request,
        settings=settings, registry=registry, insights=insights,
        memories=memories, memobase=object(), indices=indices,
    )
    assert 'warnings' not in response['response']
    assert 'patchType' not in response['response']
    assert 'patch' not in response['response']
    assert response['response']['allowed'] is False
    assert response['response']['status'] == {'message': exp_msg, 'code': exp_code}


@pytest.mark.parametrize('error1, error2, exp_msg', [
    pytest.param(Exception("err1"), Exception("err2"), "err1", id='builtin-first-samerank'),
    pytest.param(TemporaryError("err1"), TemporaryError("err2"), "err1", id='temp-first-samerank'),
    pytest.param(PermanentError("err1"), PermanentError("err2"), "err1", id='perm-first-samerank'),
    pytest.param(AdmissionError("err1"), AdmissionError("err2"), "err1", id='adms-first-samerank'),
    pytest.param(Exception("err1"), TemporaryError("err2"), "err2", id='temp-over-builtin'),
    pytest.param(Exception("err1"), AdmissionError("err2"), "err2", id='adms-over-builtin'),
    pytest.param(Exception("err1"), PermanentError("err2"), "err2", id='perm-over-builtin'),
    pytest.param(TemporaryError("err1"), PermanentError("err2"), "err2", id='perm-over-temp'),
    pytest.param(TemporaryError("err1"), AdmissionError("err2"), "err2", id='adms-over-temp'),
    pytest.param(PermanentError("err1"), AdmissionError("err2"), "err2", id='adms-over-perm'),
])
@pytest.mark.parametrize('decorator', [kopf.on.validate, kopf.on.mutate])
async def test_errors_priorities(
        settings, registry, resource, memories, insights, indices, adm_request,
        decorator, error1, error2, exp_msg):

    @decorator(*resource)
    def fn1(**_):
        raise error1

    @decorator(*resource)
    def fn2(**_):
        raise error2

    response = await serve_admission_request(
        adm_request,
        settings=settings, registry=registry, insights=insights,
        memories=memories, memobase=object(), indices=indices,
    )
    assert 'warnings' not in response['response']
    assert 'patchType' not in response['response']
    assert 'patch' not in response['response']
    assert response['response']['allowed'] is False
    assert response['response']['status'] == {'message': exp_msg, 'code': 500}


@pytest.mark.parametrize('decorator', [kopf.on.validate, kopf.on.mutate])
async def test_warnings_are_returned_to_kubernetes(
        settings, registry, resource, memories, insights, indices, adm_request,
        decorator):

    @decorator(*resource)
    def fn(warnings, **_):
        warnings.append("oops!")

    response = await serve_admission_request(
        adm_request,
        settings=settings, registry=registry, insights=insights,
        memories=memories, memobase=object(), indices=indices,
    )
    assert 'patchType' not in response['response']
    assert 'patch' not in response['response']
    assert 'status' not in response['response']
    assert response['response']['warnings'] == ['oops!']
    assert response['response']['allowed'] is True


@pytest.mark.parametrize('decorator', [kopf.on.validate, kopf.on.mutate])
async def test_patch_is_returned_to_kubernetes(
        settings, registry, resource, memories, insights, indices, adm_request,
        decorator):

    @decorator(*resource)
    def fn(patch, **_):
        patch['xyz'] = 'added'
        patch.spec['field'] = 'replaced'

    response = await serve_admission_request(
        adm_request,
        settings=settings, registry=registry, insights=insights,
        memories=memories, memobase=object(), indices=indices,
    )
    assert 'warnings' not in response['response']
    assert 'status' not in response['response']
    assert response['response']['allowed'] is True
    assert response['response']['patchType'] == 'JSONPatch'
    assert json.loads(base64.b64decode(response['response']['patch'])) == [
        {'op': 'add', 'path': '/xyz', 'value': 'added'},
        {'op': 'replace', 'path': '/spec/field', 'value': 'replaced'},
    ]



================================================
FILE: tests/admission/test_webhook_detection.py
================================================
import pytest

from kopf._kits.webhooks import ClusterDetector, WebhookAutoServer, WebhookAutoTunnel


# Reproducing the realistic environment would be costly and difficult,
# so we mock all external(!) libraries to return the results as we expect them.
# This reduces the quality of the tests, but makes them simple.
@pytest.fixture(autouse=True)
def pathmock(mocker, fake_vault, enforced_context, aresponses, hostname):
    mocker.patch('ssl.get_server_certificate', return_value='')
    mocker.patch('certvalidator.ValidationContext')
    validator = mocker.patch('certvalidator.CertificateValidator')
    pathmock = validator.return_value.validate_tls.return_value
    pathmock.first.issuer.native = {}
    pathmock.first.subject.native = {}
    return pathmock


async def test_no_detection(hostname, aresponses):
    aresponses.add(hostname, '/version', 'get', {'gitVersion': 'v1.2.3'})
    hostname = await ClusterDetector().guess_host()
    assert hostname is None


async def test_dependencies(hostname, aresponses, no_certvalidator):
    aresponses.add(hostname, '/version', 'get', {'gitVersion': 'v1.2.3'})
    with pytest.raises(ImportError) as err:
        await ClusterDetector().guess_host()
    assert "pip install certvalidator" in str(err.value)


async def test_minikube_via_issuer_cn(pathmock):
    pathmock.first.issuer.native = {'common_name': 'minikubeCA'}
    hostname = await ClusterDetector().guess_host()
    assert hostname == 'host.minikube.internal'


async def test_minikube_via_subject_cn(pathmock):
    pathmock.first.subject.native = {'common_name': 'minikube'}
    hostname = await ClusterDetector().guess_host()
    assert hostname == 'host.minikube.internal'


async def test_k3d_via_issuer_cn(pathmock):
    pathmock.first.issuer.native = {'common_name': 'k3s-ca-server-12345'}
    hostname = await ClusterDetector().guess_host()
    assert hostname == 'host.k3d.internal'


async def test_k3d_via_subject_cn(pathmock):
    pathmock.first.subject.native = {'common_name': 'k3s'}
    hostname = await ClusterDetector().guess_host()
    assert hostname == 'host.k3d.internal'


async def test_k3d_via_subject_org(pathmock):
    pathmock.first.subject.native = {'organization_name': 'k3s'}
    hostname = await ClusterDetector().guess_host()
    assert hostname == 'host.k3d.internal'


async def test_k3d_via_version_infix(hostname, aresponses):
    aresponses.add(hostname, '/version', 'get', {'gitVersion': 'v1.20.4+k3s1'})
    hostname = await ClusterDetector().guess_host()
    assert hostname == 'host.k3d.internal'


async def test_server_detects(responder, aresponses, hostname, caplog, assert_logs):
    caplog.set_level(0)
    aresponses.add(hostname, '/version', 'get', {'gitVersion': 'v1.20.4+k3s1'})
    server = WebhookAutoServer(insecure=True)
    async with server:
        async for _ in server(responder.fn):
            break  # do not sleep
    assert_logs(["Cluster detection found the hostname: host.k3d.internal"])


async def test_server_works(
        responder, aresponses, hostname, caplog, assert_logs):
    caplog.set_level(0)
    aresponses.add(hostname, '/version', 'get', {'gitVersion': 'v1.20.4'})
    server = WebhookAutoServer(insecure=True)
    async with server:
        async for _ in server(responder.fn):
            break  # do not sleep
    assert_logs(["Cluster detection failed, running a simple local server"])


async def test_tunnel_detects(responder, pyngrok_mock, aresponses, hostname, caplog, assert_logs):
    caplog.set_level(0)
    aresponses.add(hostname, '/version', 'get', {'gitVersion': 'v1.20.4+k3s1'})
    server = WebhookAutoTunnel()
    async with server:
        async for _ in server(responder.fn):
            break  # do not sleep
    assert_logs(["Cluster detection found the hostname: host.k3d.internal"])


async def test_tunnel_works(responder, pyngrok_mock, aresponses, hostname, caplog, assert_logs):
    caplog.set_level(0)
    aresponses.add(hostname, '/version', 'get', {'gitVersion': 'v1.20.4'})
    server = WebhookAutoTunnel()
    async with server:
        async for _ in server(responder.fn):
            break  # do not sleep
    assert_logs(["Cluster detection failed, using an ngrok tunnel."])



================================================
FILE: tests/admission/test_webhook_ngrok.py
================================================
import asyncio

import pytest

from kopf._kits.webhooks import WebhookNgrokTunnel


async def test_missing_pyngrok(no_pyngrok, responder):
    with pytest.raises(ImportError) as err:
        server = WebhookNgrokTunnel()
        async with server:
            async for _ in server(responder.fn):
                break  # do not sleep
    assert "pip install pyngrok" in str(err.value)


async def test_ngrok_tunnel(
        certfile, pkeyfile, responder, pyngrok_mock):

    responder.fut.set_result({'hello': 'world'})
    server = WebhookNgrokTunnel(port=54321, path='/p1/p2',
                                region='xx', token='xyz', binary='/bin/ngrok')
    async with server:
        async for client_config in server(responder.fn):
            assert 'caBundle' not in client_config  # trust the default CA
            assert client_config['url'] == 'https://nowhere/p1/p2'
            break  # do not sleep

    assert pyngrok_mock.conf.get_default.called
    assert pyngrok_mock.conf.get_default.return_value.ngrok_path == '/bin/ngrok'
    assert pyngrok_mock.conf.get_default.return_value.region == 'xx'
    assert pyngrok_mock.ngrok.set_auth_token.called
    assert pyngrok_mock.ngrok.set_auth_token.call_args_list[0][0][0] == 'xyz'
    assert pyngrok_mock.ngrok.connect.called
    assert pyngrok_mock.ngrok.connect.call_args_list[0][0][0] == '54321'
    assert pyngrok_mock.ngrok.connect.call_args_list[0][1]['bind_tls'] == True
    assert pyngrok_mock.ngrok.disconnect.called

    await asyncio.get_running_loop().shutdown_asyncgens()



================================================
FILE: tests/admission/test_webhook_server.py
================================================
import base64
import json
import ssl

import aiohttp
import pytest

from kopf._core.engines.admission import AmbiguousResourceError, MissingDataError, \
                                         UnknownResourceError, WebhookError
from kopf._kits.webhooks import WebhookDockerDesktopServer, WebhookK3dServer, \
                                WebhookMinikubeServer, WebhookServer


async def test_starts_as_http_ipv4(responder):
    server = WebhookServer(addr='127.0.0.1', port=22533, path='/p1/p2', insecure=True)
    async with server:
        async for client_config in server(responder.fn):
            assert client_config['url'] == 'http://127.0.0.1:22533/p1/p2'
            assert 'caBundle' not in client_config
            break  # do not sleep


async def test_starts_as_http_ipv6(responder):
    server = WebhookServer(addr='::1', port=22533, path='/p1/p2', insecure=True)
    async with server:
        async for client_config in server(responder.fn):
            assert client_config['url'] == 'http://[::1]:22533/p1/p2'
            assert 'caBundle' not in client_config
            break  # do not sleep


async def test_unspecified_port_allocates_a_random_port(responder):
    server1 = WebhookServer(addr='127.0.0.1', path='/p1/p2', insecure=True)
    server2 = WebhookServer(addr='127.0.0.1', path='/p1/p2', insecure=True)
    async with server1, server2:
        async for client_config1 in server1(responder.fn):
            async for client_config2 in server2(responder.fn):
                assert client_config1['url'] != client_config2['url']
                break  # do not sleep
            break  # do not sleep


async def test_unspecified_addr_uses_all_interfaces(responder, caplog, assert_logs):
    caplog.set_level(0)
    server = WebhookServer(port=22533, path='/p1/p2', insecure=True)
    async with server:
        async for client_config in server(responder.fn):
            assert client_config['url'] == 'http://localhost:22533/p1/p2'
            break  # do not sleep
    assert_logs([r"Listening for webhooks at http://\*:22533/p1/p2"])


async def test_webhookserver_starts_as_https_with_selfsigned_cert(
        responder):
    server = WebhookServer(addr='127.0.0.1', port=22533, path='/p1/p2', host='somehost')
    async with server:
        async for client_config in server(responder.fn):
            assert client_config['url'] == 'https://somehost:22533/p1/p2'
            assert 'caBundle' in client_config  # regardless of the value
            break  # do not sleep


async def test_webhookserver_starts_as_https_with_provided_cert(
        certfile, pkeyfile, certpkey, responder):
    server = WebhookServer(port=22533, certfile=certfile, pkeyfile=pkeyfile)
    async with server:
        async for client_config in server(responder.fn):
            assert client_config['url'] == 'https://localhost:22533'
            assert base64.b64decode(client_config['caBundle']) == certpkey[0]
            break  # do not sleep


@pytest.mark.parametrize('cls, url', [
    (WebhookK3dServer, 'https://host.k3d.internal:22533/p1/p2'),
    (WebhookMinikubeServer, 'https://host.minikube.internal:22533/p1/p2'),
    (WebhookDockerDesktopServer, 'https://host.docker.internal:22533/p1/p2'),
])
async def test_webhookserver_flavours_inject_hostnames(
        certfile, pkeyfile, certpkey, responder, cls, url):
    server = cls(port=22533, certfile=certfile, pkeyfile=pkeyfile, path='/p1/p2')
    async with server:
        async for client_config in server(responder.fn):
            assert client_config['url'] == url
            break  # do not sleep


@pytest.mark.usefixtures('no_sslproto_warnings')
async def test_webhookserver_serves(
        certfile, pkeyfile, responder, adm_request):
    responder.fut.set_result({'hello': 'world'})
    server = WebhookServer(certfile=certfile, pkeyfile=pkeyfile)
    async with server:
        async for client_config in server(responder.fn):
            cadata = base64.b64decode(client_config['caBundle']).decode('ascii')
            sslctx = ssl.create_default_context(cadata=cadata)
            async with aiohttp.ClientSession() as client:
                async with client.post(client_config['url'], ssl=sslctx, json=adm_request) as resp:
                    text = await resp.text()
                    assert text == '{"hello": "world"}'
                    assert resp.status == 200
            break  # do not sleep


@pytest.mark.parametrize('code, error', [
    (500, Exception),
    (400, WebhookError),
    (400, WebhookError),
    (400, MissingDataError),
    (404, UnknownResourceError),
    (409, AmbiguousResourceError),
    (400, lambda: json.JSONDecodeError('...', '...', 0)),
])
@pytest.mark.usefixtures('no_sslproto_warnings')
async def test_webhookserver_errors(
        certfile, pkeyfile, responder, adm_request, code, error):
    responder.fut.set_exception(error())
    server = WebhookServer(certfile=certfile, pkeyfile=pkeyfile)
    async with server:
        async for client_config in server(responder.fn):
            cadata = base64.b64decode(client_config['caBundle']).decode('ascii')
            sslctx = ssl.create_default_context(cadata=cadata)
            async with aiohttp.ClientSession() as client:
                async with client.post(client_config['url'], ssl=sslctx, json=adm_request) as resp:
                    assert resp.status == code
            break  # do not sleep



================================================
FILE: tests/apis/test_api_requests.py
================================================
import asyncio
import textwrap

import aiohttp.web
import pytest

from kopf._cogs.clients.api import delete, get, patch, post, request, stream
from kopf._cogs.clients.errors import APIError


@pytest.mark.parametrize('method', ['get', 'post', 'patch', 'delete'])
async def test_raw_requests_work(
        resp_mocker, aresponses, hostname, method, settings, logger):

    mock = resp_mocker(return_value=aiohttp.web.json_response({}))
    aresponses.add(hostname, '/url', method, mock)
    response = await request(
        method=method,
        url='/url',
        payload={'fake': 'payload'},
        headers={'fake': 'headers'},
        settings=settings,
        logger=logger,
    )
    assert isinstance(response, aiohttp.ClientResponse)  # unparsed!
    assert mock.call_count == 1
    assert isinstance(mock.call_args[0][0], aiohttp.web.BaseRequest)
    assert mock.call_args[0][0].method.lower() == method
    assert mock.call_args[0][0].path == '/url'
    assert mock.call_args[0][0].data == {'fake': 'payload'}
    assert mock.call_args[0][0].headers['fake'] == 'headers'  # and other system headers


@pytest.mark.parametrize('method', ['get', 'post', 'patch', 'delete'])
async def test_raw_requests_are_not_parsed(
        resp_mocker, aresponses, hostname, method, settings, logger):

    mock = resp_mocker(return_value=aresponses.Response(text='BAD JSON!'))
    aresponses.add(hostname, '/url', method, mock)
    response = await request(method, '/url', settings=settings, logger=logger)
    assert isinstance(response, aiohttp.ClientResponse)


@pytest.mark.parametrize('method', ['get', 'post', 'patch', 'delete'])
async def test_server_errors_escalate(
        resp_mocker, aresponses, hostname, method, settings, logger):

    mock = resp_mocker(return_value=aiohttp.web.json_response({}, status=666, reason='oops'))
    aresponses.add(hostname, '/url', method, mock)
    with pytest.raises(APIError) as err:
        await request(method, '/url', settings=settings, logger=logger)
    assert err.value.status == 666


@pytest.mark.parametrize('method', ['get', 'post', 'patch', 'delete'])
async def test_relative_urls_are_prepended_with_server(
        resp_mocker, aresponses, hostname, method, settings, logger):

    mock = resp_mocker(return_value=aiohttp.web.json_response({}))
    aresponses.add(hostname, '/url', method, mock)
    await request(method, '/url', settings=settings, logger=logger)
    assert isinstance(mock.call_args[0][0], aiohttp.web.BaseRequest)
    assert str(mock.call_args[0][0].url) == f'http://{hostname}/url'


@pytest.mark.parametrize('method', ['get', 'post', 'patch', 'delete'])
async def test_absolute_urls_are_passed_through(
        resp_mocker, aresponses, hostname, method, settings, logger):

    mock = resp_mocker(return_value=aiohttp.web.json_response({}))
    aresponses.add(hostname, '/url', method, mock)
    aresponses.add('fakehost.localdomain', '/url', method, mock)
    await request(method, 'http://fakehost.localdomain/url', settings=settings, logger=logger)
    assert isinstance(mock.call_args[0][0], aiohttp.web.BaseRequest)
    assert str(mock.call_args[0][0].url) == 'http://fakehost.localdomain/url'


@pytest.mark.parametrize('fn, method', [
    (get, 'get'),
    (post, 'post'),
    (patch, 'patch'),
    (delete, 'delete'),
])
async def test_parsing_in_requests(
        resp_mocker, aresponses, hostname, fn, method, settings, logger):

    mock = resp_mocker(return_value=aiohttp.web.json_response({'fake': 'result'}))
    aresponses.add(hostname, '/url', method, mock)
    response = await fn(
        url='/url',
        payload={'fake': 'payload'},
        headers={'fake': 'headers'},
        settings=settings,
        logger=logger,
    )
    assert response == {'fake': 'result'}  # parsed!
    assert mock.call_count == 1
    assert isinstance(mock.call_args[0][0], aiohttp.web.BaseRequest)
    assert mock.call_args[0][0].method.lower() == method
    assert mock.call_args[0][0].path == '/url'
    assert mock.call_args[0][0].data == {'fake': 'payload'}
    assert mock.call_args[0][0].headers['fake'] == 'headers'  # and other system headers


@pytest.mark.parametrize('method', ['get'])  # the only supported method at the moment
async def test_parsing_in_streams(
        resp_mocker, aresponses, hostname, method, settings, logger):

    mock = resp_mocker(return_value=aresponses.Response(text=textwrap.dedent("""
        {"fake": "result1"}
        {"fake": "result2"}
    """)))
    aresponses.add(hostname, '/url', method, mock)

    items = []
    async for item in stream(
        url='/url',
        payload={'fake': 'payload'},
        headers={'fake': 'headers'},
        settings=settings,
        logger=logger,
    ):
        items.append(item)

    assert items == [{'fake': 'result1'}, {'fake': 'result2'}]
    assert mock.call_count == 1
    assert isinstance(mock.call_args[0][0], aiohttp.web.BaseRequest)
    assert mock.call_args[0][0].method.lower() == method
    assert mock.call_args[0][0].path == '/url'
    assert mock.call_args[0][0].data == {'fake': 'payload'}
    assert mock.call_args[0][0].headers['fake'] == 'headers'  # and other system headers


@pytest.mark.parametrize('fn, method', [
    (get, 'get'),
    (post, 'post'),
    (patch, 'patch'),
    (delete, 'delete'),
])
async def test_direct_timeout_in_requests(
        resp_mocker, aresponses, hostname, fn, method, settings, logger, timer):

    async def serve_slowly():
        await asyncio.sleep(1.0)
        return aiohttp.web.json_response({})

    mock = resp_mocker(side_effect=serve_slowly)
    aresponses.add(hostname, '/url', method, mock)

    with timer, pytest.raises(asyncio.TimeoutError):
        timeout = aiohttp.ClientTimeout(total=0.1)
        # aiohttp raises an asyncio.TimeoutError which is automatically retried.
        # To reduce the test duration we disable retries for this test.
        settings.networking.error_backoffs = None
        await fn('/url', timeout=timeout, settings=settings, logger=logger)

    assert 0.1 < timer.seconds < 0.2

    # Let the server request finish and release all resources (tasks).
    # TODO: Remove when fixed: https://github.com/aio-libs/aiohttp/issues/7551
    await asyncio.sleep(1.0)


@pytest.mark.parametrize('fn, method', [
    (get, 'get'),
    (post, 'post'),
    (patch, 'patch'),
    (delete, 'delete'),
])
async def test_settings_timeout_in_requests(
        resp_mocker, aresponses, hostname, fn, method, settings, logger, timer):

    async def serve_slowly():
        await asyncio.sleep(1.0)
        return aiohttp.web.json_response({})

    mock = resp_mocker(side_effect=serve_slowly)
    aresponses.add(hostname, '/url', method, mock)

    with timer, pytest.raises(asyncio.TimeoutError):
        settings.networking.request_timeout = 0.1
        # aiohttp raises an asyncio.TimeoutError which is automatically retried.
        # To reduce the test duration we disable retries for this test.
        settings.networking.error_backoffs = None
        await fn('/url', settings=settings, logger=logger)

    assert 0.1 < timer.seconds < 0.2

    # Let the server request finish and release all resources (tasks).
    # TODO: Remove when fixed: https://github.com/aio-libs/aiohttp/issues/7551
    await asyncio.sleep(1.0)


@pytest.mark.parametrize('method', ['get'])  # the only supported method at the moment
async def test_direct_timeout_in_streams(
        resp_mocker, aresponses, hostname, method, settings, logger, timer):

    async def serve_slowly():
        await asyncio.sleep(1.0)
        return "{}"

    mock = resp_mocker(side_effect=serve_slowly)
    aresponses.add(hostname, '/url', method, mock)

    with timer, pytest.raises(asyncio.TimeoutError):
        timeout = aiohttp.ClientTimeout(total=0.1)
        # aiohttp raises an asyncio.TimeoutError which is automatically retried.
        # To reduce the test duration we disable retries for this test.
        settings.networking.error_backoffs = None
        async for _ in stream('/url', timeout=timeout, settings=settings, logger=logger):
            pass

    assert 0.1 < timer.seconds < 0.2

    # Let the server request finish and release all resources (tasks).
    # TODO: Remove when fixed: https://github.com/aio-libs/aiohttp/issues/7551
    await asyncio.sleep(1.0)


@pytest.mark.parametrize('method', ['get'])  # the only supported method at the moment
async def test_settings_timeout_in_streams(
        resp_mocker, aresponses, hostname, method, settings, logger, timer):

    async def serve_slowly():
        await asyncio.sleep(1.0)
        return "{}"

    mock = resp_mocker(side_effect=serve_slowly)
    aresponses.add(hostname, '/url', method, mock)

    with timer, pytest.raises(asyncio.TimeoutError):
        settings.networking.request_timeout = 0.1
        # aiohttp raises an asyncio.TimeoutError which is automatically retried.
        # To reduce the test duration we disable retries for this test.
        settings.networking.error_backoffs = None
        async for _ in stream('/url', settings=settings, logger=logger):
            pass

    assert 0.1 < timer.seconds < 0.2

    # Let the server request finish and release all resources (tasks).
    # TODO: Remove when fixed: https://github.com/aio-libs/aiohttp/issues/7551
    await asyncio.sleep(1.0)


@pytest.mark.parametrize('delay, expected', [
    pytest.param(0.0, [], id='instant-none'),
    pytest.param(0.1, [{'fake': 'result1'}], id='fast-single'),
    pytest.param(9.9, [{'fake': 'result1'}, {'fake': 'result2'}], id='inf-double'),
])
@pytest.mark.parametrize('method', ['get'])  # the only supported method at the moment
async def test_stopper_in_streams(
        resp_mocker, aresponses, hostname, method, delay, expected, settings, logger):

    async def stream_slowly(request: aiohttp.web.Request) -> aiohttp.web.StreamResponse:
        response = aiohttp.web.StreamResponse()
        await response.prepare(request)
        try:
            await asyncio.sleep(0.05)
            await response.write(b'{"fake": "result1"}\n')
            await asyncio.sleep(0.15)
            await response.write(b'{"fake": "result2"}\n')
            await response.write_eof()
        except ConnectionError:
            pass  # the client side sometimes disconnects earlier, ignore it
        return response

    aresponses.add(hostname, '/url', method, stream_slowly)

    stopper = asyncio.Future()
    asyncio.get_running_loop().call_later(delay, stopper.set_result, None)

    items = []
    async for item in stream('/url', stopper=stopper, settings=settings, logger=logger):
        items.append(item)

    assert items == expected

    await asyncio.sleep(0.2)  # give the response some time to be cancelled and its tasks closed



================================================
FILE: tests/apis/test_default_namespace.py
================================================
from kopf._cogs.clients.api import get_default_namespace


async def test_default_namespace_when_unset(mocker, enforced_context):
    mocker.patch.object(enforced_context, 'default_namespace', None)
    ns = await get_default_namespace()
    assert ns is None


async def test_default_namespace_when_set(mocker, enforced_context):
    mocker.patch.object(enforced_context, 'default_namespace', 'xyz')
    ns = await get_default_namespace()
    assert ns == 'xyz'



================================================
FILE: tests/apis/test_error_retries.py
================================================
import asyncio

import aiohttp.web
import pytest

from kopf._cogs.clients.api import request
from kopf._cogs.clients.errors import APIError


@pytest.fixture(autouse=True)
def sleep(mocker):
    """Do not let it actually sleep, even if it is a 0-sleep."""
    return mocker.patch('asyncio.sleep')


@pytest.fixture()
def request_fn(mocker):
    return mocker.patch('aiohttp.ClientSession.request')


async def test_regular_errors_escalate_without_retries(
        caplog, assert_logs, settings, logger, resp_mocker, aresponses, hostname, request_fn):
    caplog.set_level(0)

    request_fn.side_effect = Exception("boo")

    settings.networking.error_backoffs = [1, 2, 3]
    with pytest.raises(Exception) as err:
        await request('get', '/url', settings=settings, logger=logger)

    assert str(err.value) == "boo"
    assert request_fn.call_count == 1
    assert_logs([], prohibited=["attempt", "escalating", "retry"])


@pytest.mark.parametrize('status', [400, 404, 499, 666, 999])
async def test_client_errors_escalate_without_retries(
        caplog, assert_logs, settings, logger, resp_mocker, aresponses, hostname, status):
    caplog.set_level(0)

    # side_effect instead of return_value -- to generate a new response on every call, not reuse it.
    mock = resp_mocker(side_effect=lambda: aiohttp.web.json_response({}, status=status, reason='oops'))
    aresponses.add(hostname, '/url', 'get', mock)  # repeat=N would copy the mock, lose all counts
    aresponses.add(hostname, '/url', 'get', mock)  # repeat=N would copy the mock, lose all counts
    aresponses.add(hostname, '/url', 'get', mock)  # repeat=N would copy the mock, lose all counts
    aresponses.add(hostname, '/url', 'get', mock)  # repeat=N would copy the mock, lose all counts

    settings.networking.error_backoffs = [1, 2, 3]
    with pytest.raises(APIError) as err:
        await request('get', '/url', settings=settings, logger=logger)

    assert err.value.status == status
    assert mock.call_count == 1
    assert_logs([], prohibited=["attempt", "escalating", "retry"])


@pytest.mark.parametrize('status', [500, 503, 599])
async def test_server_errors_escalate_with_retries(
        caplog, assert_logs, settings, logger, resp_mocker, aresponses, hostname, status):
    caplog.set_level(0)

    # side_effect instead of return_value -- to generate a new response on every call, not reuse it.
    mock = resp_mocker(side_effect=lambda: aiohttp.web.json_response({}, status=status, reason='oops'))
    aresponses.add(hostname, '/url', 'get', mock)  # repeat=N would copy the mock, lose all counts
    aresponses.add(hostname, '/url', 'get', mock)  # repeat=N would copy the mock, lose all counts
    aresponses.add(hostname, '/url', 'get', mock)  # repeat=N would copy the mock, lose all counts
    aresponses.add(hostname, '/url', 'get', mock)  # repeat=N would copy the mock, lose all counts

    settings.networking.error_backoffs = [0, 0, 0]
    with pytest.raises(APIError) as err:
        await request('get', '/url', settings=settings, logger=logger)

    assert err.value.status == status
    assert mock.call_count == 4
    assert_logs([
        "attempt #1/4 failed; will retry",
        "attempt #2/4 failed; will retry",
        "attempt #3/4 failed; will retry",
        "attempt #4/4 failed; escalating",
    ])


async def test_connection_errors_escalate_with_retries(
        caplog, assert_logs, settings, logger, resp_mocker, aresponses, hostname, request_fn):
    caplog.set_level(0)

    request_fn.side_effect = aiohttp.ClientConnectionError()

    settings.networking.error_backoffs = [0, 0, 0]
    with pytest.raises(aiohttp.ClientConnectionError):
        await request('get', '/url', settings=settings, logger=logger)

    assert request_fn.call_count == 4
    assert_logs([
        "attempt #1/4 failed; will retry",
        "attempt #2/4 failed; will retry",
        "attempt #3/4 failed; will retry",
        "attempt #4/4 failed; escalating",
    ])


async def test_timeout_errors_escalate_with_retries(
        caplog, assert_logs, settings, logger, resp_mocker, aresponses, hostname, request_fn):
    caplog.set_level(0)

    request_fn.side_effect = asyncio.TimeoutError()

    settings.networking.error_backoffs = [0, 0, 0]
    with pytest.raises(asyncio.TimeoutError):
        await request('get', '/url', settings=settings, logger=logger)

    assert request_fn.call_count == 4
    assert_logs([
        "attempt #1/4 failed; will retry",
        "attempt #2/4 failed; will retry",
        "attempt #3/4 failed; will retry",
        "attempt #4/4 failed; escalating",
    ])


async def test_retried_until_succeeded(
        caplog, assert_logs, settings, logger, resp_mocker, aresponses, hostname):
    caplog.set_level(0)

    mock = resp_mocker(side_effect=[
        aiohttp.web.json_response({}, status=505, reason='oops'),
        aiohttp.web.json_response({}, status=505, reason='oops'),
        aiohttp.web.json_response({}),
    ])
    aresponses.add(hostname, '/url', 'get', mock)  # repeat=N would copy the mock, lose all counts
    aresponses.add(hostname, '/url', 'get', mock)  # repeat=N would copy the mock, lose all counts
    aresponses.add(hostname, '/url', 'get', mock)  # repeat=N would copy the mock, lose all counts
    aresponses.add(hostname, '/url', 'get', mock)  # repeat=N would copy the mock, lose all counts

    settings.networking.error_backoffs = [0, 0, 0]
    await request('get', '/url', settings=settings, logger=logger)

    assert mock.call_count == 3  # 2 failures, 1 success; limited to 4 attempts.
    assert_logs([
        "attempt #1/4 failed; will retry",
        "attempt #2/4 failed; will retry",
        "attempt #3/4 succeeded",
    ], prohibited=[
        "attempt #4/4",
    ])


@pytest.mark.parametrize('backoffs, exp_calls', [
    ([], 1),
    ([0], 2),
    ([1], 2),
    ([0, 0], 3),
    ([1, 2], 3),
])
async def test_backoffs_as_lists(
        caplog, assert_logs, settings, logger, resp_mocker, aresponses, hostname, sleep,
        backoffs, exp_calls):
    caplog.set_level(0)

    # side_effect instead of return_value -- to generate a new response on every call, not reuse it.
    mock = resp_mocker(side_effect=lambda: aiohttp.web.json_response({}, status=500, reason='oops'))
    aresponses.add(hostname, '/url', 'get', mock)  # repeat=N would copy the mock, lose all counts
    aresponses.add(hostname, '/url', 'get', mock)  # repeat=N would copy the mock, lose all counts
    aresponses.add(hostname, '/url', 'get', mock)  # repeat=N would copy the mock, lose all counts
    aresponses.add(hostname, '/url', 'get', mock)  # repeat=N would copy the mock, lose all counts

    settings.networking.error_backoffs = backoffs
    with pytest.raises(APIError):
        await request('get', '/url', settings=settings, logger=logger)

    assert mock.call_count == exp_calls
    all_sleeps = [call[0][0] for call in sleep.call_args_list]
    assert all_sleeps == backoffs


async def test_backoffs_as_floats(
        caplog, assert_logs, settings, logger, resp_mocker, aresponses, hostname, sleep):
    caplog.set_level(0)

    # side_effect instead of return_value -- to generate a new response on every call, not reuse it.
    mock = resp_mocker(side_effect=lambda: aiohttp.web.json_response({}, status=500, reason='oops'))
    aresponses.add(hostname, '/url', 'get', mock)  # repeat=N would copy the mock, lose all counts
    aresponses.add(hostname, '/url', 'get', mock)  # repeat=N would copy the mock, lose all counts

    settings.networking.error_backoffs = 5.0
    with pytest.raises(APIError):
        await request('get', '/url', settings=settings, logger=logger)

    assert mock.call_count == 2
    all_sleeps = [call[0][0] for call in sleep.call_args_list]
    assert all_sleeps == [5.0]


async def test_backoffs_as_iterables(
        caplog, assert_logs, settings, logger, resp_mocker, aresponses, hostname, sleep):
    caplog.set_level(0)

    class Itr:
        def __iter__(self):
            return iter([1, 2, 3])

    # side_effect instead of return_value -- to generate a new response on every call, not reuse it.
    mock = resp_mocker(side_effect=lambda: aiohttp.web.json_response({}, status=500, reason='oops'))
    aresponses.add(hostname, '/url', 'get', mock)  # repeat=N would copy the mock, lose all counts
    aresponses.add(hostname, '/url', 'get', mock)  # repeat=N would copy the mock, lose all counts
    aresponses.add(hostname, '/url', 'get', mock)  # repeat=N would copy the mock, lose all counts
    aresponses.add(hostname, '/url', 'get', mock)  # repeat=N would copy the mock, lose all counts
    aresponses.add(hostname, '/url', 'get', mock)  # repeat=N would copy the mock, lose all counts
    aresponses.add(hostname, '/url', 'get', mock)  # repeat=N would copy the mock, lose all counts
    aresponses.add(hostname, '/url', 'get', mock)  # repeat=N would copy the mock, lose all counts
    aresponses.add(hostname, '/url', 'get', mock)  # repeat=N would copy the mock, lose all counts

    settings.networking.error_backoffs = Itr()  # to be reused on every attempt
    with pytest.raises(APIError):
        await request('get', '/url', settings=settings, logger=logger)
    with pytest.raises(APIError):
        await request('get', '/url', settings=settings, logger=logger)

    assert mock.call_count == 8
    all_sleeps = [call[0][0] for call in sleep.call_args_list]
    assert all_sleeps == [1, 2, 3, 1, 2, 3]



================================================
FILE: tests/apis/test_iterjsonlines.py
================================================
from unittest.mock import Mock

from kopf._cogs.clients.api import iter_jsonlines


async def test_empty_content():
    async def iter_chunked(n: int):
        if False:  # to make this function a generator
            yield b''

    content = Mock(iter_chunked=iter_chunked)
    lines = []
    async for line in iter_jsonlines(content):
        lines.append(line)

    assert lines == []


async def test_empty_chunk():
    async def iter_chunked(n: int):
        yield b''

    content = Mock(iter_chunked=iter_chunked)
    lines = []
    async for line in iter_jsonlines(content):
        lines.append(line)

    assert lines == []


async def test_one_chunk_one_line():
    async def iter_chunked(n: int):
        yield b'hello'

    content = Mock(iter_chunked=iter_chunked)
    lines = []
    async for line in iter_jsonlines(content):
        lines.append(line)

    assert lines == [b'hello']


async def test_one_chunk_two_lines():
    async def iter_chunked(n: int):
        yield b'hello\nworld'

    content = Mock(iter_chunked=iter_chunked)
    lines = []
    async for line in iter_jsonlines(content):
        lines.append(line)

    assert lines == [b'hello', b'world']


async def test_one_chunk_empty_lines():
    async def iter_chunked(n: int):
        yield b'\n\nhello\n\nworld\n\n'

    content = Mock(iter_chunked=iter_chunked)
    lines = []
    async for line in iter_jsonlines(content):
        lines.append(line)

    assert lines == [b'hello', b'world']


async def test_a_few_chunks_split():
    async def iter_chunked(n: int):
        yield b'\n\nhell'
        yield b'o\n\nwor'
        yield b'ld\n\n'

    content = Mock(iter_chunked=iter_chunked)
    lines = []
    async for line in iter_jsonlines(content):
        lines.append(line)

    assert lines == [b'hello', b'world']



================================================
FILE: tests/authentication/test_authentication.py
================================================
import pytest

from kopf._cogs.structs.credentials import ConnectionInfo, LoginError, Vault
from kopf._cogs.structs.ephemera import Memo
from kopf._core.engines.activities import authenticate
from kopf._core.engines.indexing import OperatorIndexers
from kopf._core.intents.causes import Activity
from kopf._core.intents.handlers import ActivityHandler
from kopf._core.intents.registries import OperatorRegistry


async def test_empty_registry_produces_no_credentials(settings):
    vault = Vault()
    registry = OperatorRegistry()

    await authenticate(
        registry=registry,
        settings=settings,
        vault=vault,
        memo=Memo(),
        indices=OperatorIndexers().indices,
    )

    assert vault.is_empty()
    with pytest.raises(LoginError):
        async for _, _ in vault:
            pass


async def test_noreturn_handler_produces_no_credentials(settings):
    vault = Vault()
    registry = OperatorRegistry()

    def login_fn(**_):
        pass

    # NB: id auto-detection does not work, as it is local to the test function.
    registry._activities.append(ActivityHandler(
        fn=login_fn, id='login_fn', activity=Activity.AUTHENTICATION,
        param=None, errors=None, timeout=None, retries=None, backoff=None,
    ))

    await authenticate(
        registry=registry,
        settings=settings,
        vault=vault,
        memo=Memo(),
        indices=OperatorIndexers().indices,
    )

    assert vault.is_empty()
    with pytest.raises(LoginError):
        async for _, _ in vault:
            pass


async def test_single_credentials_provided_to_vault(settings):
    info = ConnectionInfo(server='https://expected/')
    vault = Vault()
    registry = OperatorRegistry()

    def login_fn(**_):
        return info

    # NB: id auto-detection does not work, as it is local to the test function.
    registry._activities.append(ActivityHandler(
        fn=login_fn, id='login_fn', activity=Activity.AUTHENTICATION,
        param=None, errors=None, timeout=None, retries=None, backoff=None,
    ))

    await authenticate(
        registry=registry,
        settings=settings,
        vault=vault,
        memo=Memo(),
        indices=OperatorIndexers().indices,
    )

    assert not vault.is_empty()

    items = []
    async for key, info in vault:
        items.append((key, info))

    assert len(items) == 1
    assert items[0][0] == 'login_fn'
    assert items[0][1] is info



================================================
FILE: tests/authentication/test_connectioninfo.py
================================================
import datetime

from kopf._cogs.structs.credentials import ConnectionInfo, VaultKey


def test_key_as_string():
    key = VaultKey('some-key')
    assert isinstance(key, str)
    assert key == 'some-key'


def test_creation_with_minimal_fields():
    info = ConnectionInfo(
        server='https://localhost',
    )
    assert info.server == 'https://localhost'
    assert info.ca_path is None
    assert info.ca_data is None
    assert info.insecure is None
    assert info.username is None
    assert info.password is None
    assert info.scheme is None
    assert info.token is None
    assert info.certificate_path is None
    assert info.certificate_data is None
    assert info.private_key_path is None
    assert info.private_key_data is None
    assert info.default_namespace is None
    assert info.expiration is None


def test_creation_with_maximal_fields():
    info = ConnectionInfo(
        server='https://localhost',
        ca_path='/ca/path',
        ca_data=b'ca_data',
        insecure=True,
        username='username',
        password='password',
        scheme='scheme',
        token='token',
        certificate_path='/cert/path',
        certificate_data=b'cert_data',
        private_key_path='/pkey/path',
        private_key_data=b'pkey_data',
        default_namespace='default',
        expiration=datetime.datetime.max,
    )
    assert info.server == 'https://localhost'
    assert info.ca_path == '/ca/path'
    assert info.ca_data == b'ca_data'
    assert info.insecure is True
    assert info.username == 'username'
    assert info.password == 'password'
    assert info.scheme == 'scheme'
    assert info.token == 'token'
    assert info.certificate_path == '/cert/path'
    assert info.certificate_data == b'cert_data'
    assert info.private_key_path == '/pkey/path'
    assert info.private_key_data == b'pkey_data'
    assert info.default_namespace == 'default'
    assert info.expiration == datetime.datetime.max



================================================
FILE: tests/authentication/test_credentials.py
================================================
import base64
import ssl
import textwrap

import pytest

from kopf._cogs.clients.auth import APIContext, authenticated, vault_var
from kopf._cogs.structs.credentials import ConnectionInfo, Vault

# These are Minikube's locally geenrated certificates (CN=minikubeCA).
# They are not in any public use, and are regenerated regularly.
SAMPLE_MINIKUBE_CA = textwrap.dedent('''
-----BEGIN CERTIFICATE-----
MIIC5zCCAc+gAwIBAgIBATANBgkqhkiG9w0BAQsFADAVMRMwEQYDVQQDEwptaW5p
a3ViZUNBMB4XDTE5MDUyMTA5MTgzNloXDTI5MDUxOTA5MTgzNlowFTETMBEGA1UE
AxMKbWluaWt1YmVDQTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBANNU
9eyjlDfWZNSTXbdM9uebYseWWo6KeGGQ/OISrnVc6+AjuIv/fxtdCc8nVyLXBWu2
dlDjBqOsG2WfY7m1RVwsF0L2G8pgNZJ4eOww/PyDZzzIcB911eWiry528YB2PZQu
sN6sUItSZrHsin3dkcEZMUKcvVOY3FaNqXukCoMywZBO7QlLZasHhCCaanMFjxBx
WiqB4gxcyTlGRBSoa49agSW2r45873xmJ+JglI/tNjeobGLynYwrDvRWmrhVOFAj
QeiMr5lkzVO5cC2t84WdEihXVqFcZQUe0jfRHmmgpUxJRtiJMv7yudgnK2ALdhOY
eDVtV1wIyWgLpF2lZk8CAwEAAaNCMEAwDgYDVR0PAQH/BAQDAgKkMB0GA1UdJQQW
MBQGCCsGAQUFBwMCBggrBgEFBQcDATAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3
DQEBCwUAA4IBAQBdsqvuvK+8RJ5xqwGkdpSAK1U2LrZ3Hm0MzXoEo8GH79F1yubv
Ig1VRLHDIDY1d/fKrK4a7uulSFTFvpt6AGSB/225wJVBQUAALH1lPkTXq5TDi5jE
NqoXk3d61qf9StUEc1YehL6ZgkSknNU7ksAe5Ht0lfJlSa3DmACkI4CZJ1F5cztk
m2p3RZYkxizY2i/9P34f59F3XCNUSOW52aJgLhnMugEM0baOTHN0mcYZRGmrunT6
fs/5eZq6ZrXBu0nIkEZkEWAM2WoqDGxMlUao5IOnf289HyBxJTFGte5tysg1sJF0
JCH4VcilJllzUki594R1Yv8O5qtxkXXfXQNR
-----END CERTIFICATE-----
''').strip()

SAMPLE_MINIKUBE_CERT = textwrap.dedent('''
-----BEGIN CERTIFICATE-----
MIIDADCCAeigAwIBAgIBAjANBgkqhkiG9w0BAQsFADAVMRMwEQYDVQQDEwptaW5p
a3ViZUNBMB4XDTE5MTExMzA5NDMzNFoXDTIwMTExMzA5NDMzNFowMTEXMBUGA1UE
ChMOc3lzdGVtOm1hc3RlcnMxFjAUBgNVBAMTDW1pbmlrdWJlLXVzZXIwggEiMA0G
CSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCut0LegwHP4kJp9Uf89vjuslIMi6hv
BiBNhSr8wCZ9uuFUN2dvBnCPXX/xvxYxpBKh0WUKX7sYfKdaMjxVr1ndnwu63e5A
CW7919uRH3fhVV7rTntlO+rUeyHXNlSQue8oVlvO+8D+Qzlna02axt/5PwsPGD/G
lw7Ti0f/LjmmqTB0T6yCsyOH90d7pQ0yuiOyDK072Ns1vTf6hkrkiNQaRhUjEtqm
rMq/A4xpb7h+z6LWlRyv6/DBJsLmpDS99hqZbbj1U6IJ56r0JpQuq7CJxeE/F2t5
I1i0k8TJ4CQRrHvXovl6wI+zmOTmKMy5uDdKUEkfe6vck+x16gV2RK79AgMBAAGj
PzA9MA4GA1UdDwEB/wQEAwIFoDAdBgNVHSUEFjAUBggrBgEFBQcDAQYIKwYBBQUH
AwIwDAYDVR0TAQH/BAIwADANBgkqhkiG9w0BAQsFAAOCAQEAbo/PxDz7K9dCAcxi
S0RdZvOu2RYHqRmc/vN1B5JDHLcYcCet4iuJ44BuWZc/Oe1sXdy6DOZO2UQHAF0F
5cAaBnkqpz/yvUzkE3NvdXpU/9leo3O6XeKKi0Fi+hv09nhh/tJgh/XDxWfAAkeG
I3AQCkcHrqFrpBMxuWUXlnexwsvvbdEVkpVMVwSRfpsxLfxV62HCU90EU0823UKW
V1npcRXBtxK/jqWZbLd5buRul+V6PKa4KRY22d8it+9MDAMgPFQosG1ShhQropul
VJUvAQ14dPpiyQN4FRI3MljVykFe2cWI0rVwoboy9TEniaMPqr3MqujOlUv7KTpk
lKRP5w==
-----END CERTIFICATE-----
''').strip()

SAMPLE_MINIKUBE_PKEY = textwrap.dedent('''
-----BEGIN RSA PRIVATE KEY-----
MIIEowIBAAKCAQEArrdC3oMBz+JCafVH/Pb47rJSDIuobwYgTYUq/MAmfbrhVDdn
bwZwj11/8b8WMaQSodFlCl+7GHynWjI8Va9Z3Z8Lut3uQAlu/dfbkR934VVe6057
ZTvq1Hsh1zZUkLnvKFZbzvvA/kM5Z2tNmsbf+T8LDxg/xpcO04tH/y45pqkwdE+s
grMjh/dHe6UNMrojsgytO9jbNb03+oZK5IjUGkYVIxLapqzKvwOMaW+4fs+i1pUc
r+vwwSbC5qQ0vfYamW249VOiCeeq9CaULquwicXhPxdreSNYtJPEyeAkEax716L5
esCPs5jk5ijMubg3SlBJH3ur3JPsdeoFdkSu/QIDAQABAoIBAAk8FoS8V/Qs+WWw
WUW9qBq1wjB3kUeNA1gVmdgSL/alUhOpegYcSQbK4mBcwUeObI2xC64osTLyI8ZY
sWe2BQH5zhzqbhVkakFwj2J0T1nRsVquo0cOi7L/byJ49K9RpJp1NhUSqXjHBNm6
ijeMG3qJIoSBu507jsUPr5aFUvbEFCby+VvU/UljS1dK+5wm7QcGgRrcXc8ZCIuk
1P5YX6Tr9RYzYNjc/zB8czIyoISSSTk9uroYzvuMCgYTQ4WzWvX8UwchdfZyV7Gq
kdKjG6IkGxvlS0Lc8534LRlH8iJR+wPAlYKHBDa0Rc3qZFsvgjnbDCCa6YqLoNNn
ltsz7wECgYEA5IrnX8EHXyeaYnyiVI2xk6QTkmLrtovd1Ue2GwQ9BJgtaDrT0xil
UV5NV4VUu5Zid7cqmIyFyh+7jjIex+dpfXT94+wr1HXxNYbLHeCnCZWcomxaD8pU
Bh8B8wSRgjOFW33q6APDkFJPO92O96B+BczSgOFmEvuk8Kj7aYFahWUCgYEAw7Ta
YiD66I+eK8B2+lWfPNfpIddW7D3Dn9cSW2RVazMyinTsVBm3p63kpCQeVaaX5key
WiyY6phTvIfJ45pTzrS/kpA2zGcB1FFnB1xvM0bzpbIxTOQBGQUH1mSQmNT6+0VZ
+GdILRKedp4qg7BLh7VElSCYGVy1Yr62Rp01FbkCgYB3QZxWtQ05tBq1hb/XS1D8
b8PewUuqp/WL063NDzsf6KDZIMlkABpUCVdmciay9FhRi/zoOXue60wdeT3ipni/
hIrvok+EwD6r5bib0JyZPb7MaqncT4Hk581GmH2taWEPSveHNl+YMbsyy/xMby0T
rbuykOuIwFNjWWpHtb4cmQKBgGF4MUuuIUiyPpSLxrXm7ufeoL26AhCmskdpVjsu
PVymowVSNmGsbUuVz8nwMyt1TTHjg3BlxcMRGqNK/cHdmt/YJZFZQfGLW93irO19
m+Rt8esUVHl3FRTg7IZaj6mOaXG7mJOe3NOV8lYhcAsmQnfUT9P158q54ZzMXvvM
UCQBAoGBAMTbVdbybvzqOSIKGeqIVlX7L2Fp55lYC8MfTwQzjPeJMQ8YdeNquv+M
hweBrRg1DXxdaicfpCuqITU1WkqHd/NGNQX2h8VleiR6t22dZb8nBplO1l1e3XgY
lUXVsCYgw8yNCm10xGCelpJ4nxxPhf5apbz4F3nGORGfsv5C+x++
-----END RSA PRIVATE KEY-----
''').strip()


@authenticated
async def fn(context: APIContext):
    return context.session


@pytest.fixture(autouse=True)
def vault():
    vault = Vault()
    vault_var.set(vault)
    return vault


@pytest.fixture
def cabase64(tmpdir):
    return base64.encodebytes(SAMPLE_MINIKUBE_CA.encode('ascii'))


@pytest.fixture
def certbase64(tmpdir):
    return base64.encodebytes(SAMPLE_MINIKUBE_CERT.encode('ascii'))


@pytest.fixture
def pkeybase64(tmpdir):
    return base64.encodebytes(SAMPLE_MINIKUBE_PKEY.encode('ascii'))


@pytest.fixture
def cafile(tmpdir):
    path = tmpdir / 'ca.crt'
    path.write_text(SAMPLE_MINIKUBE_CA, encoding='utf-8')
    return str(path)


@pytest.fixture
def certfile(tmpdir):
    path = tmpdir / 'client.crt'
    path.write_text(SAMPLE_MINIKUBE_CERT, encoding='utf-8')
    return str(path)


@pytest.fixture
def pkeyfile(tmpdir):
    path = tmpdir / 'client.key'
    path.write_text(SAMPLE_MINIKUBE_PKEY, encoding='utf-8')
    return str(path)


async def test_basic_auth(vault):
    await vault.populate({
        'id': ConnectionInfo(
            server='http://localhost',
            username='username',
            password='password',
        ),
    })
    session = await fn()

    async with session:
        assert session._default_auth.login == 'username'
        assert session._default_auth.password == 'password'
        assert 'Authorization' not in session._default_headers


async def test_header_with_token_only(vault):
    await vault.populate({
        'id': ConnectionInfo(
            server='http://localhost',
            token='token',
        ),
    })
    session = await fn()

    async with session:
        assert session._default_auth is None
        assert session._default_headers['Authorization'] == 'Bearer token'


async def test_header_with_schema_only(vault):
    await vault.populate({
        'id': ConnectionInfo(
            server='http://localhost',
            scheme='Digest xyz',
        ),
    })
    session = await fn()

    async with session:
        assert session._default_auth is None
        assert session._default_headers['Authorization'] == 'Digest xyz'


async def test_header_with_schema_and_token(vault):
    await vault.populate({
        'id': ConnectionInfo(
            server='http://localhost',
            scheme='Digest',
            token='xyz',
        ),
    })
    session = await fn()

    async with session:
        assert session._default_auth is None
        assert session._default_headers['Authorization'] == 'Digest xyz'


async def test_ca_insecure(vault, cafile):
    await vault.populate({
        'id': ConnectionInfo(
            server='http://localhost',
            insecure=True,
        ),
    })
    session = await fn()

    async with session:
        ctx = session.connector._ssl
        assert ctx.verify_mode == ssl.CERT_NONE


async def test_ca_as_path(vault, cafile):
    await vault.populate({
        'id': ConnectionInfo(
            server='http://localhost',
            ca_path=cafile,
        ),
    })
    session = await fn()

    async with session:
        ctx = session.connector._ssl
        assert len(ctx.get_ca_certs()) == 1
        assert ctx.cert_store_stats()['x509'] == 1
        assert ctx.cert_store_stats()['x509_ca'] == 1


async def test_ca_as_data(vault, cabase64):
    await vault.populate({
        'id': ConnectionInfo(
            server='http://localhost',
            ca_data=cabase64,
        ),
    })
    session = await fn()

    async with session:
        ctx = session.connector._ssl
        assert len(ctx.get_ca_certs()) == 1
        assert ctx.cert_store_stats()['x509'] == 1
        assert ctx.cert_store_stats()['x509_ca'] == 1


# TODO: find a way to test that the client certificates/pkeys are indeed loaded.
# TODO: currently, we only test that the parsing/loading does not fail at all.
async def test_clientcert_as_path(vault, cafile, certfile, pkeyfile):
    await vault.populate({
        'id': ConnectionInfo(
            server='http://localhost',
            ca_path=cafile,
            certificate_path=certfile,
            private_key_path=pkeyfile,
        ),
    })
    session = await fn()

    async with session:
        pass


async def test_clientcert_as_data(vault, cafile, certbase64, pkeybase64):
    await vault.populate({
        'id': ConnectionInfo(
            server='http://localhost',
            ca_path=cafile,
            certificate_data=certbase64,
            private_key_data=pkeybase64,
        ),
    })
    session = await fn()

    async with session:
        pass



================================================
FILE: tests/authentication/test_login_kubeconfig.py
================================================
import os

import pytest
import yaml

from kopf._cogs.structs.credentials import LoginError
from kopf._core.intents.piggybacking import has_kubeconfig, login_with_kubeconfig

MINICONFIG = '''
    kind: Config
    current-context: ctx
    contexts:
      - name: ctx
        context:
          cluster: clstr
          user: usr
    clusters:
      - name: clstr
    users:
      - name: usr
'''


@pytest.mark.parametrize('envs', [{}, {'KUBECONFIG': ''}], ids=['absent', 'empty'])
def test_has_no_kubeconfig_when_nothing_is_provided(mocker, envs):
    exists_mock = mocker.patch('os.path.exists', return_value=False)
    mocker.patch.dict(os.environ, envs, clear=True)
    result = has_kubeconfig()
    assert result is False
    assert exists_mock.call_count == 1
    assert exists_mock.call_args_list[0][0][0].endswith('/.kube/config')


@pytest.mark.parametrize('envs', [{'KUBECONFIG': 'x'}], ids=['set'])
def test_has_kubeconfig_when_envvar_is_set_but_no_homedir(mocker, envs):
    exists_mock = mocker.patch('os.path.exists', return_value=False)
    mocker.patch.dict(os.environ, envs, clear=True)
    result = has_kubeconfig()
    assert result is True
    assert exists_mock.call_count == 1
    assert exists_mock.call_args_list[0][0][0].endswith('/.kube/config')


@pytest.mark.parametrize('envs', [{}, {'KUBECONFIG': ''}], ids=['absent', 'empty'])
def test_has_kubeconfig_when_homedir_exists_but_no_envvar(mocker, envs):
    exists_mock = mocker.patch('os.path.exists', return_value=True)
    mocker.patch.dict(os.environ, envs, clear=True)
    result = has_kubeconfig()
    assert result is True
    assert exists_mock.call_count == 1
    assert exists_mock.call_args_list[0][0][0].endswith('/.kube/config')


@pytest.mark.parametrize('envs', [{}, {'KUBECONFIG': ''}], ids=['absent', 'empty'])
def test_homedir_is_used_if_it_exists(tmpdir, mocker, envs):
    exists_mock = mocker.patch('os.path.exists', return_value=True)
    open_mock = mocker.patch('kopf._core.intents.piggybacking.open')
    open_mock.return_value.__enter__.return_value.read.return_value = MINICONFIG
    mocker.patch.dict(os.environ, envs, clear=True)
    credentials = login_with_kubeconfig()
    assert exists_mock.call_count == 1
    assert exists_mock.call_args_list[0][0][0].endswith('/.kube/config')
    assert open_mock.call_count == 1
    assert open_mock.call_args_list[0][0][0].endswith('/.kube/config')
    assert credentials is not None


@pytest.mark.parametrize('envs', [{}, {'KUBECONFIG': ''}], ids=['absent', 'empty'])
def test_homedir_is_ignored_if_it_is_absent(tmpdir, mocker, envs):
    exists_mock = mocker.patch('os.path.exists', return_value=False)
    open_mock = mocker.patch('kopf._core.intents.piggybacking.open')
    open_mock.return_value.__enter__.return_value.read.return_value = ''
    mocker.patch.dict(os.environ, envs, clear=True)
    credentials = login_with_kubeconfig()
    assert exists_mock.call_count == 1
    assert exists_mock.call_args_list[0][0][0].endswith('/.kube/config')
    assert open_mock.call_count == 0
    assert credentials is None


def test_absent_kubeconfig_fails(tmpdir, mocker):
    kubeconfig = tmpdir.join('config')
    mocker.patch.dict(os.environ, clear=True, KUBECONFIG=str(kubeconfig))
    with pytest.raises(IOError):
        login_with_kubeconfig()


def test_corrupted_kubeconfig_fails(tmpdir, mocker):
    kubeconfig = tmpdir.join('config')
    kubeconfig.write("""!!acb!.-//:""")  # invalid yaml
    mocker.patch.dict(os.environ, clear=True, KUBECONFIG=str(kubeconfig))
    with pytest.raises(yaml.YAMLError):
        login_with_kubeconfig()


def test_empty_kubeconfig_fails(tmpdir, mocker):
    kubeconfig = tmpdir.join('config')
    kubeconfig.write('')
    mocker.patch.dict(os.environ, clear=True, KUBECONFIG=str(kubeconfig))
    with pytest.raises(LoginError) as err:
        login_with_kubeconfig()
    assert "context is not set" in str(err.value)


def test_mini_kubeconfig_reading(tmpdir, mocker):
    kubeconfig = tmpdir.join('config')
    kubeconfig.write(MINICONFIG)

    mocker.patch.dict(os.environ, clear=True, KUBECONFIG=str(kubeconfig))
    credentials = login_with_kubeconfig()

    assert credentials is not None
    assert credentials.server is None
    assert credentials.insecure is None
    assert credentials.scheme is None
    assert credentials.token is None
    assert credentials.certificate_path is None
    assert credentials.certificate_data is None
    assert credentials.private_key_path is None
    assert credentials.private_key_data is None
    assert credentials.ca_path is None
    assert credentials.ca_data is None
    assert credentials.password is None
    assert credentials.username is None
    assert credentials.default_namespace is None


def test_full_kubeconfig_reading(tmpdir, mocker):
    kubeconfig = tmpdir.join('config')
    kubeconfig.write('''
        kind: Config
        current-context: ctx
        contexts:
          - name: ctx
            context:
              cluster: clstr
              user: usr
              namespace: ns
          - name: def
        clusters:
          - name: clstr
            cluster:
              server: https://hostname:1234/
              certificate-authority-data: base64dataA
              certificate-authority: /pathA
              insecure-skip-tls-verify: true
          - name: hij
        users:
          - name: usr
            user:
              username: uname
              password: passw
              client-certificate-data: base64dataC
              client-certificate: /pathC
              client-key-data: base64dataK
              client-key: /pathK
              token: tkn
          - name: klm
    ''')

    mocker.patch.dict(os.environ, clear=True, KUBECONFIG=str(kubeconfig))
    credentials = login_with_kubeconfig()

    assert credentials is not None
    assert credentials.server == 'https://hostname:1234/'
    assert credentials.insecure == True
    assert credentials.scheme is None
    assert credentials.token == 'tkn'
    assert credentials.certificate_path == '/pathC'
    assert credentials.certificate_data == 'base64dataC'
    assert credentials.private_key_path == '/pathK'
    assert credentials.private_key_data == 'base64dataK'
    assert credentials.ca_path == '/pathA'
    assert credentials.ca_data == 'base64dataA'
    assert credentials.password == 'passw'
    assert credentials.username == 'uname'
    assert credentials.default_namespace == 'ns'


def test_kubeconfig_with_provider_token(tmpdir, mocker):
    kubeconfig = tmpdir.join('config')
    kubeconfig.write('''
        kind: Config
        current-context: ctx
        contexts:
          - name: ctx
            context:
              cluster: clstr
              user: usr
        clusters:
          - name: clstr
        users:
          - name: usr
            user:
              auth-provider:
                config:
                  access-token: provtkn
    ''')

    mocker.patch.dict(os.environ, clear=True, KUBECONFIG=str(kubeconfig))
    credentials = login_with_kubeconfig()

    assert credentials is not None
    assert credentials.token == 'provtkn'


def test_merged_kubeconfigs_across_currentcontext(tmpdir, mocker):
    kubeconfig1 = tmpdir.join('config1')
    kubeconfig1.write('''
        kind: Config
        current-context: ctx
    ''')
    kubeconfig2 = tmpdir.join('config2')
    kubeconfig2.write('''
        kind: Config
        contexts:
          - name: ctx
            context:
              cluster: clstr
              user: usr
              namespace: ns
        clusters:
          - name: clstr
            cluster:
              server: srv
        users:
          - name: usr
            user:
              token: tkn
    ''')

    mocker.patch.dict(os.environ, clear=True, KUBECONFIG=f'{kubeconfig1}{os.pathsep}{kubeconfig2}')
    credentials = login_with_kubeconfig()

    assert credentials is not None
    assert credentials.default_namespace == 'ns'
    assert credentials.server == 'srv'
    assert credentials.token == 'tkn'


def test_merged_kubeconfigs_across_contexts(tmpdir, mocker):
    kubeconfig1 = tmpdir.join('config1')
    kubeconfig1.write('''
        kind: Config
        current-context: ctx
        contexts:
          - name: ctx
            context:
              cluster: clstr
              user: usr
              namespace: ns
    ''')
    kubeconfig2 = tmpdir.join('config2')
    kubeconfig2.write('''
        kind: Config
        clusters:
          - name: clstr
            cluster:
              server: srv
        users:
          - name: usr
            user:
              token: tkn
    ''')

    mocker.patch.dict(os.environ, clear=True, KUBECONFIG=f'{kubeconfig1}{os.pathsep}{kubeconfig2}')
    credentials = login_with_kubeconfig()

    assert credentials is not None
    assert credentials.default_namespace == 'ns'
    assert credentials.server == 'srv'
    assert credentials.token == 'tkn'


def test_merged_kubeconfigs_first_wins(tmpdir, mocker):
    kubeconfig1 = tmpdir.join('config1')
    kubeconfig1.write('''
        kind: Config
        current-context: ctx
        contexts:
          - name: ctx
            context:
              cluster: clstr
              user: usr
              namespace: ns1
        clusters:
          - name: clstr
            cluster:
              server: srv1
        users:
          - name: usr
            user:
              token: tkn1
    ''')
    kubeconfig2 = tmpdir.join('config2')
    kubeconfig2.write('''
        kind: Config
        current-context: ctx
        contexts:
          - name: ctx
            context:
              cluster: clstr
              user: usr
              namespace: ns2
        clusters:
          - name: clstr
            cluster:
              server: srv2
        users:
          - name: usr
            user:
              token: tkn2
    ''')

    mocker.patch.dict(os.environ, clear=True, KUBECONFIG=f'{kubeconfig1}{os.pathsep}{kubeconfig2}')
    credentials = login_with_kubeconfig()

    assert credentials is not None
    assert credentials.default_namespace == 'ns1'
    assert credentials.server == 'srv1'
    assert credentials.token == 'tkn1'



================================================
FILE: tests/authentication/test_login_serviceaccount.py
================================================
from kopf._core.intents.piggybacking import has_service_account, login_with_service_account

# As per https://kubernetes.io/docs/tasks/run-application/access-api-from-pod/
NAMESPACE_PATH = '/var/run/secrets/kubernetes.io/serviceaccount/namespace'
SA_TOKEN_PATH = '/var/run/secrets/kubernetes.io/serviceaccount/token'
CA_PATH = '/var/run/secrets/kubernetes.io/serviceaccount/ca.crt'


def test_has_no_serviceaccount_when_special_file_is_absent(mocker):
    exists_mock = mocker.patch('os.path.exists', return_value=False)
    result = has_service_account()
    assert result is False
    assert exists_mock.call_count == 1
    assert exists_mock.call_args_list[0][0][0] == SA_TOKEN_PATH


def test_has_serviceaccount_when_special_file_exists(mocker):
    exists_mock = mocker.patch('os.path.exists', return_value=True)
    result = has_service_account()
    assert result is True
    assert exists_mock.call_count == 1
    assert exists_mock.call_args_list[0][0][0] == SA_TOKEN_PATH


def test_serviceaccount_with_all_absent_files(mocker):
    exists_mock = mocker.patch('os.path.exists', return_value=False)  # all 3 of them.
    open_mock = mocker.patch('kopf._core.intents.piggybacking.open')
    open_mock.return_value.__enter__.return_value.read.return_value = ''
    credentials = login_with_service_account()
    assert credentials is None
    assert exists_mock.call_count == 1
    assert exists_mock.call_args_list[0][0][0] == SA_TOKEN_PATH
    assert not open_mock.called


def test_serviceaccount_with_all_present_files(mocker):
    exists_mock = mocker.patch('os.path.exists', return_value=True)  # all 3 of them.
    open_mock = mocker.patch('kopf._core.intents.piggybacking.open')
    open_mock.return_value.__enter__.return_value.read.side_effect=[' tkn ', ' ns ', RuntimeError]
    credentials = login_with_service_account()
    assert credentials is not None
    assert credentials.server == 'https://kubernetes.default.svc'
    assert credentials.default_namespace == 'ns'
    assert credentials.token == 'tkn'
    assert credentials.ca_path == CA_PATH
    assert exists_mock.call_count == 3
    assert exists_mock.call_args_list[0][0][0] == SA_TOKEN_PATH
    assert exists_mock.call_args_list[1][0][0] == NAMESPACE_PATH
    assert exists_mock.call_args_list[2][0][0] == CA_PATH
    assert open_mock.call_count == 2
    assert open_mock.call_args_list[0][0][0] == SA_TOKEN_PATH
    assert open_mock.call_args_list[1][0][0] == NAMESPACE_PATH
    # NB: the order is irrelevant and can be changed if needed.


def test_serviceaccount_with_only_the_token_file(mocker):
    # NB: the order is irrelevant and can be changed if needed.
    exists_mock = mocker.patch('os.path.exists', side_effect=[True, False, False])
    open_mock = mocker.patch('kopf._core.intents.piggybacking.open')
    open_mock.return_value.__enter__.return_value.read.side_effect=[' tkn ', RuntimeError]
    credentials = login_with_service_account()
    assert credentials is not None
    assert credentials.server == 'https://kubernetes.default.svc'
    assert credentials.default_namespace is None
    assert credentials.token == 'tkn'
    assert credentials.ca_path is None
    assert exists_mock.call_count == 3
    assert exists_mock.call_args_list[0][0][0] == SA_TOKEN_PATH
    assert exists_mock.call_args_list[1][0][0] == NAMESPACE_PATH
    assert exists_mock.call_args_list[2][0][0] == CA_PATH
    assert open_mock.call_count == 1
    assert open_mock.call_args_list[0][0][0] == SA_TOKEN_PATH



================================================
FILE: tests/authentication/test_reauthentication.py
================================================
from typing import Optional

import aiohttp.web

from kopf._cogs.clients.auth import APIContext, authenticated
from kopf._cogs.structs.credentials import ConnectionInfo


@authenticated
async def fn(
        x: int,
        *,
        context: Optional[APIContext],
) -> tuple[APIContext, int]:
    return context, x + 100


async def test_session_is_injected(
        fake_vault, resp_mocker, aresponses, hostname, resource, namespace):

    result = {}
    get_mock = resp_mocker(return_value=aiohttp.web.json_response(result))
    aresponses.add(hostname, resource.get_url(namespace=namespace, name='xyz'), 'get', get_mock)

    context, result = await fn(1)

    async with context.session:
        assert context is not None
        assert result == 101


async def test_session_is_passed_through(
        fake_vault, resp_mocker, aresponses, hostname, resource, namespace):

    result = {}
    get_mock = resp_mocker(return_value=aiohttp.web.json_response(result))
    aresponses.add(hostname, resource.get_url(namespace=namespace, name='xyz'), 'get', get_mock)

    explicit_context = APIContext(ConnectionInfo(server='http://irrelevant/'))
    context, result = await fn(1, context=explicit_context)

    async with context.session:
        assert context is explicit_context
        assert result == 101



================================================
FILE: tests/authentication/test_tempfiles.py
================================================
import gc
import os.path

from kopf._cogs.clients.auth import _TempFiles


def test_created():
    tempfiles = _TempFiles()
    path = tempfiles[b'hello']
    assert os.path.isfile(path)
    with open(path, 'rb') as f:
        assert f.read() == b'hello'


def test_reused():
    tempfiles = _TempFiles()
    path1 = tempfiles[b'hello']
    path2 = tempfiles[b'hello']
    assert path1 == path2


def test_differs():
    tempfiles = _TempFiles()
    path1 = tempfiles[b'hello']
    path2 = tempfiles[b'world']
    assert path1 != path2


def test_purged():
    tempfiles = _TempFiles()
    path1 = tempfiles[b'hello']
    path2 = tempfiles[b'world']
    assert os.path.isfile(path1)
    assert os.path.isfile(path2)

    tempfiles.purge()

    assert not os.path.isfile(path1)
    assert not os.path.isfile(path2)


def test_garbage_collected():
    tempfiles = _TempFiles()
    path1 = tempfiles[b'hello']
    path2 = tempfiles[b'world']
    assert os.path.isfile(path1)
    assert os.path.isfile(path2)

    del tempfiles
    gc.collect()
    gc.collect()
    gc.collect()

    assert not os.path.isfile(path1)
    assert not os.path.isfile(path2)



================================================
FILE: tests/authentication/test_vault.py
================================================
import datetime

import freezegun
import iso8601
import pytest

from kopf._cogs.structs.credentials import ConnectionInfo, LoginError, Vault, VaultKey


async def test_probits_evaluating_as_boolean():
    vault = Vault()
    with pytest.raises(NotImplementedError):
        bool(vault)


async def test_empty_at_creation():
    vault = Vault()
    assert vault.is_empty()


async def test_not_empty_when_populated():
    key1 = VaultKey('some-key')
    info1 = ConnectionInfo(server='https://expected/')
    vault = Vault()
    await vault.populate({key1: info1})
    assert not vault.is_empty()


async def test_yielding_after_creation(mocker):
    vault = Vault()
    mocker.patch.object(vault._guard, 'wait_for')

    with pytest.raises(LoginError):
        async for _, _ in vault:
            pass

    assert vault._guard.wait_for.called


async def test_yielding_after_population(mocker):
    key1 = VaultKey('some-key')
    info1 = ConnectionInfo(server='https://expected/')
    vault = Vault()
    mocker.patch.object(vault._guard, 'wait_for')

    await vault.populate({key1: info1})

    results = []
    async for key, info in vault:
        results.append((key, info))

    assert len(results) == 1
    assert results[0][0] == key1
    assert results[0][1] is info1


@freezegun.freeze_time('2020-01-01T00:00:00')
async def test_yielding_items_before_expiration(mocker):
    future = iso8601.parse_date('2020-01-01T00:00:00.000001')
    key1 = VaultKey('some-key')
    info1 = ConnectionInfo(server='https://expected/', expiration=future)
    vault = Vault()
    mocker.patch.object(vault._guard, 'wait_for')

    results = []
    await vault.populate({key1: info1})
    async for key, info in vault:
        results.append((key, info))

    assert len(results) == 1
    assert results[0][0] == key1
    assert results[0][1] is info1


@pytest.mark.parametrize('delta', [0, 1])
@freezegun.freeze_time('2020-01-01T00:00:00')
async def test_yielding_ignores_expired_items(mocker, delta):
    future = iso8601.parse_date('2020-01-01T00:00:00.000001')
    past = iso8601.parse_date('2020-01-01') - datetime.timedelta(microseconds=delta)
    key1 = VaultKey('some-key')
    key2 = VaultKey('other-key')
    info1 = ConnectionInfo(server='https://expected/', expiration=past)
    info2 = ConnectionInfo(server='https://expected/', expiration=future)
    vault = Vault()
    mocker.patch.object(vault._guard, 'wait_for')

    results = []
    await vault.populate({key1: info1, key2: info2})
    async for key, info in vault:
        results.append((key, info))

    assert len(results) == 1
    assert results[0][0] == key2
    assert results[0][1] is info2


@pytest.mark.parametrize('delta', [0, 1])
@freezegun.freeze_time('2020-01-01T00:00:00')
async def test_yielding_when_everything_is_expired(mocker, delta):
    past = iso8601.parse_date('2020-01-01') - datetime.timedelta(microseconds=delta)
    key1 = VaultKey('some-key')
    info1 = ConnectionInfo(server='https://expected/', expiration=past)
    vault = Vault()
    mocker.patch.object(vault._guard, 'wait_for')

    await vault.populate({key1: info1})
    with pytest.raises(LoginError):
        async for _, _ in vault:
            pass


async def test_invalidation_reraises_if_nothing_is_left_with_exception(mocker):
    exc = Exception("Sample error.")
    key1 = VaultKey('some-key')
    info1 = ConnectionInfo(server='https://expected/')
    vault = Vault()
    mocker.patch.object(vault._guard, 'wait_for')

    await vault.populate({key1: info1})
    with pytest.raises(Exception) as e:
        await vault.invalidate(key1, info1, exc=exc)

    assert isinstance(e.value, LoginError)
    assert e.value.__cause__ is exc
    assert vault._guard.wait_for.called


async def test_invalidation_continues_if_nothing_is_left_without_exception(mocker):
    key1 = VaultKey('some-key')
    info1 = ConnectionInfo(server='https://expected/')
    vault = Vault()
    mocker.patch.object(vault._guard, 'wait_for')

    await vault.populate({key1: info1})
    await vault.invalidate(key1, info1)

    assert vault._guard.wait_for.called


async def test_invalidation_continues_if_something_is_left():
    exc = Exception("Sample error.")
    key1 = VaultKey('key1')
    key2 = VaultKey('key2')
    info1 = ConnectionInfo(server='https://server1/')
    info2 = ConnectionInfo(server='https://server2/')
    vault = Vault()

    await vault.populate({key1: info1})
    await vault.populate({key2: info2})
    await vault.invalidate(key1, info1, exc=exc)  # no exception!

    results = []
    async for key, info in vault:
        results.append((key, info))

    assert len(results) == 1
    assert results[0][0] == key2
    assert results[0][1] is info2


async def test_invalidation_continues_if_items_is_replaced(mocker):
    key1 = VaultKey('some-key')
    info1 = ConnectionInfo(server='https://newer-valid-credentials/')
    info2 = ConnectionInfo(server='https://older-expired-credentials/')
    vault = Vault()
    mocker.patch.object(vault._guard, 'wait_for')

    await vault.populate({key1: info1})
    await vault.invalidate(key1, info2)

    results = []
    async for key, info in vault:
        results.append((key, info))

    assert len(results) == 1
    assert results[0][0] == key1
    assert results[0][1] is info1


async def test_yielding_after_invalidation(mocker):
    key1 = VaultKey('some-key')
    info1 = ConnectionInfo(server='https://expected/')
    vault = Vault()
    mocker.patch.object(vault._guard, 'wait_for')

    await vault.populate({key1: info1})
    await vault.invalidate(key1, info1)

    with pytest.raises(LoginError):
        async for _, _ in vault:
            pass


async def test_duplicates_are_remembered(mocker):
    key1 = VaultKey('some-key')
    info1 = ConnectionInfo(server='https://expected/')
    info2 = ConnectionInfo(server='https://expected/')  # another instance, same fields
    vault = Vault()
    mocker.patch.object(vault._guard, 'wait_for')

    await vault.populate({key1: info1})
    await vault.invalidate(key1, info1)
    await vault.populate({key1: info2})

    # There should be nothing to yield, despite the second populate() call.
    with pytest.raises(LoginError):
        async for _, _ in vault:
            pass


async def test_caches_from_factory(mocker):
    key1 = VaultKey('some-key')
    obj1 = object()
    info1 = ConnectionInfo(server='https://expected/')
    vault = Vault()
    await vault.populate({key1: info1})

    def factory(_: ConnectionInfo) -> object:
        return obj1

    factory_spy = mocker.MagicMock(spec=factory, wraps=factory)

    results = []
    async for key, info, obj in vault.extended(factory_spy):
        results.append((key, info, obj))

    assert len(results) == 1
    assert results[0][0] == key1
    assert results[0][1] is info1
    assert results[0][2] is obj1

    assert factory_spy.called


async def test_caches_with_same_purpose(mocker):
    key1 = VaultKey('some-key')
    obj1 = object()
    info1 = ConnectionInfo(server='https://expected/')
    vault = Vault()
    await vault.populate({key1: info1})

    def factory(_: ConnectionInfo) -> object:
        return obj1

    factory_spy = mocker.MagicMock(spec=factory, wraps=factory)

    async for _, _, _ in vault.extended(factory_spy, purpose='A'):
        pass

    async for _, _, _ in vault.extended(factory_spy, purpose='A'):
        pass

    assert factory_spy.call_count == 1  # called only once, not twice!


async def test_caches_with_different_purposes(mocker):
    key1 = VaultKey('some-key')
    obj1 = object()
    info1 = ConnectionInfo(server='https://expected/')
    vault = Vault()
    await vault.populate({key1: info1})

    def factory(_: ConnectionInfo) -> object:
        return obj1

    factory_spy = mocker.MagicMock(spec=factory, wraps=factory)

    async for _, _, _ in vault.extended(factory_spy, purpose='A'):
        pass

    async for _, _, _ in vault.extended(factory_spy, purpose='B'):
        pass

    assert factory_spy.call_count == 2  # once per purpose.



================================================
FILE: tests/basic-structs/test_causes.py
================================================
import pytest

from kopf._core.intents.causes import ActivityCause, ChangingCause, WatchingCause


@pytest.mark.parametrize('cls', [ActivityCause, WatchingCause, ChangingCause])
def test_cause_with_no_args(cls):
    with pytest.raises(TypeError):
        cls()


def test_activity_cause(mocker):
    memo = mocker.Mock()
    logger = mocker.Mock()
    indices = mocker.Mock()
    activity = mocker.Mock()
    settings = mocker.Mock()
    cause = ActivityCause(
        activity=activity,
        settings=settings,
        indices=indices,
        logger=logger,
        memo=memo,
    )
    assert cause.activity is activity
    assert cause.settings is settings
    assert cause.indices is indices
    assert cause.logger is logger
    assert cause.memo is memo


def test_watching_cause(mocker):
    logger = mocker.Mock()
    indices = mocker.Mock()
    resource = mocker.Mock()
    body = mocker.Mock()
    patch = mocker.Mock()
    memo = mocker.Mock()
    type = mocker.Mock()
    event = mocker.Mock()
    cause = WatchingCause(
        resource=resource,
        indices=indices,
        logger=logger,
        body=body,
        patch=patch,
        memo=memo,
        type=type,
        event=event,
    )
    assert cause.resource is resource
    assert cause.indices is indices
    assert cause.logger is logger
    assert cause.body is body
    assert cause.patch is patch
    assert cause.memo is memo
    assert cause.type is type
    assert cause.event is event


def test_changing_cause_with_all_args(mocker):
    logger = mocker.Mock()
    indices = mocker.Mock()
    resource = mocker.Mock()
    reason = mocker.Mock()
    initial = mocker.Mock()
    body = mocker.Mock()
    patch = mocker.Mock()
    memo = mocker.Mock()
    diff = mocker.Mock()
    old = mocker.Mock()
    new = mocker.Mock()
    cause = ChangingCause(
        resource=resource,
        indices=indices,
        logger=logger,
        reason=reason,
        initial=initial,
        body=body,
        patch=patch,
        memo=memo,
        diff=diff,
        old=old,
        new=new,
    )
    assert cause.resource is resource
    assert cause.indices is indices
    assert cause.logger is logger
    assert cause.reason is reason
    assert cause.initial is initial
    assert cause.body is body
    assert cause.patch is patch
    assert cause.memo is memo
    assert cause.diff is diff
    assert cause.old is old
    assert cause.new is new


def test_changing_cause_with_only_required_args(mocker):
    logger = mocker.Mock()
    indices = mocker.Mock()
    resource = mocker.Mock()
    reason = mocker.Mock()
    initial = mocker.Mock()
    body = mocker.Mock()
    patch = mocker.Mock()
    memo = mocker.Mock()
    cause = ChangingCause(
        resource=resource,
        indices=indices,
        logger=logger,
        reason=reason,
        initial=initial,
        body=body,
        patch=patch,
        memo=memo,
    )
    assert cause.resource is resource
    assert cause.indices is indices
    assert cause.logger is logger
    assert cause.reason is reason
    assert cause.initial is initial
    assert cause.body is body
    assert cause.patch is patch
    assert cause.memo is memo
    assert cause.diff is not None
    assert not cause.diff
    assert cause.old is None
    assert cause.new is None



================================================
FILE: tests/basic-structs/test_handlers.py
================================================
import pytest

from kopf._core.intents.handlers import ActivityHandler, ChangingHandler


@pytest.mark.parametrize('cls', [ActivityHandler, ChangingHandler])
def test_handler_with_no_args(cls):
    with pytest.raises(TypeError):
        cls()


def test_activity_handler_with_all_args(mocker):
    fn = mocker.Mock()
    id = mocker.Mock()
    param = mocker.Mock()
    errors = mocker.Mock()
    timeout = mocker.Mock()
    retries = mocker.Mock()
    backoff = mocker.Mock()
    activity = mocker.Mock()
    handler = ActivityHandler(
        fn=fn,
        id=id,
        param=param,
        errors=errors,
        timeout=timeout,
        retries=retries,
        backoff=backoff,
        activity=activity,
    )
    assert handler.fn is fn
    assert handler.id is id
    assert handler.param is param
    assert handler.errors is errors
    assert handler.timeout is timeout
    assert handler.retries is retries
    assert handler.backoff is backoff
    assert handler.activity is activity


def test_resource_handler_with_all_args(mocker):
    fn = mocker.Mock()
    id = mocker.Mock()
    param = mocker.Mock()
    selector = mocker.Mock()
    reason = mocker.Mock()
    errors = mocker.Mock()
    timeout = mocker.Mock()
    retries = mocker.Mock()
    backoff = mocker.Mock()
    initial = mocker.Mock()
    deleted = mocker.Mock()
    labels = mocker.Mock()
    annotations = mocker.Mock()
    when = mocker.Mock()
    field = mocker.Mock()
    value = mocker.Mock()
    old = mocker.Mock()
    new = mocker.Mock()
    field_needs_change = mocker.Mock()
    requires_finalizer = mocker.Mock()
    handler = ChangingHandler(
        fn=fn,
        id=id,
        param=param,
        selector=selector,
        reason=reason,
        errors=errors,
        timeout=timeout,
        retries=retries,
        backoff=backoff,
        initial=initial,
        deleted=deleted,
        labels=labels,
        annotations=annotations,
        when=when,
        field=field,
        value=value,
        old=old,
        new=new,
        field_needs_change=field_needs_change,
        requires_finalizer=requires_finalizer,
    )
    assert handler.fn is fn
    assert handler.id is id
    assert handler.param is param
    assert handler.selector is selector
    assert handler.reason is reason
    assert handler.errors is errors
    assert handler.timeout is timeout
    assert handler.retries is retries
    assert handler.backoff is backoff
    assert handler.initial is initial
    assert handler.deleted is deleted
    assert handler.labels is labels
    assert handler.annotations is annotations
    assert handler.when is when
    assert handler.field is field
    assert handler.value is value
    assert handler.old is old
    assert handler.new is new
    assert handler.field_needs_change is field_needs_change
    assert handler.requires_finalizer is requires_finalizer



================================================
FILE: tests/basic-structs/test_memories.py
================================================
from unittest.mock import Mock

from kopf._cogs.structs.bodies import Body
from kopf._cogs.structs.ephemera import Memo
from kopf._core.reactor.inventory import ResourceMemories, ResourceMemory

BODY: Body = {
    'metadata': {
        'uid': 'uid1',
    }
}


def test_creation_with_defaults():
    ResourceMemory()


async def test_recalling_creates_when_absent():
    memories = ResourceMemories()
    memory = await memories.recall(BODY)
    assert isinstance(memory, ResourceMemory)


async def test_recalling_reuses_when_present():
    memories = ResourceMemories()
    memory1 = await memories.recall(BODY)
    memory2 = await memories.recall(BODY)
    assert memory1 is memory2


async def test_forgetting_deletes_when_present():
    memories = ResourceMemories()
    memory1 = await memories.recall(BODY)
    await memories.forget(BODY)

    # Check by recalling -- it should be a new one.
    memory2 = await memories.recall(BODY)
    assert memory1 is not memory2


async def test_forgetting_ignores_when_absent():
    memories = ResourceMemories()
    await memories.forget(BODY)


async def test_memo_is_autocreated():
    memories = ResourceMemories()
    memory = await memories.recall(BODY)
    assert isinstance(memory.memo, Memo)


async def test_memo_is_shallow_copied():

    class MyMemo(Memo):
        def __copy__(self):
            mock()
            return MyMemo()

    mock = Mock()
    memobase = MyMemo()
    memories = ResourceMemories()
    memory = await memories.recall(BODY, memobase=memobase)
    assert mock.call_count == 1
    assert memory.memo is not memobase



================================================
FILE: tests/basic-structs/test_memos.py
================================================
import collections.abc

import pytest

from kopf._cogs.structs.ephemera import Memo


def test_creation_with_defaults():
    obj = Memo()
    assert isinstance(obj, collections.abc.MutableMapping)
    assert not set(obj)


def test_creation_with_dict():
    obj = Memo({'xyz': 100})
    assert isinstance(obj, collections.abc.MutableMapping)
    assert set(obj) == {'xyz'}


def test_creation_with_list():
    obj = Memo([('xyz', 100)])
    assert isinstance(obj, collections.abc.MutableMapping)
    assert set(obj) == {'xyz'}


def test_creation_with_memo():
    obj = Memo(Memo({'xyz': 100}))
    assert isinstance(obj, collections.abc.MutableMapping)
    assert set(obj) == {'xyz'}


def test_fields_are_keys():
    obj = Memo()
    obj.xyz = 100
    assert obj['xyz'] == 100


def test_keys_are_fields():
    obj = Memo()
    obj['xyz'] = 100
    assert obj.xyz == 100


def test_keys_deleted():
    obj = Memo()
    obj['xyz'] = 100
    del obj['xyz']
    assert obj == {}


def test_fields_deleted():
    obj = Memo()
    obj.xyz = 100
    del obj.xyz
    assert obj == {}


def test_raises_key_errors_on_get():
    obj = Memo()
    with pytest.raises(KeyError):
        obj['unexistent']


def test_raises_attribute_errors_on_get():
    obj = Memo()
    with pytest.raises(AttributeError):
        obj.unexistent


def test_raises_key_errors_on_del():
    obj = Memo()
    with pytest.raises(KeyError):
        del obj['unexistent']


def test_raises_attribute_errors_on_del():
    obj = Memo()
    with pytest.raises(AttributeError):
        del obj.unexistent


def test_shallow_copied_keys():
    obj1 = Memo({'xyz': 100})
    obj2 = Memo(obj1)
    obj1['xyz'] = 200
    assert obj2['xyz'] == 100


def test_shallow_copied_values():
    obj1 = Memo({'xyz': 100})
    obj2 = Memo(dat=obj1)
    obj1['xyz'] = 200
    assert obj2['dat']['xyz'] == 200



================================================
FILE: tests/basic-structs/test_resource.py
================================================
import pytest

from kopf._cogs.structs.references import Resource


def test_creation_with_no_args():
    with pytest.raises(TypeError):
        Resource()


def test_creation_with_all_kwargs():
    resource = Resource(
        group='group',
        version='version',
        plural='plural',
        kind='kind',
        singular='singular',
        shortcuts=['shortcut1', 'shortcut2'],
        categories=['category1', 'category2'],
        subresources=['sub1', 'sub2'],
        namespaced=True,
        preferred=True,
        verbs=['verb1', 'verb2'],
    )
    assert resource.group == 'group'
    assert resource.version == 'version'
    assert resource.plural == 'plural'
    assert resource.kind == 'kind'
    assert resource.singular == 'singular'
    assert resource.shortcuts == ['shortcut1', 'shortcut2']
    assert resource.categories == ['category1', 'category2']
    assert resource.subresources == ['sub1', 'sub2']
    assert resource.namespaced == True
    assert resource.preferred == True
    assert resource.verbs == ['verb1', 'verb2']


def test_url_for_a_list_of_clusterscoped_custom_resources_clusterwide():
    resource = Resource('group', 'version', 'plural', namespaced=False)
    url = resource.get_url(namespace=None)
    assert url == '/apis/group/version/plural'


def test_url_for_a_list_of_clusterscoped_custom_resources_in_a_namespace():
    resource = Resource('group', 'version', 'plural', namespaced=False)
    with pytest.raises(ValueError) as err:
        resource.get_url(namespace='ns')
    assert str(err.value) == "Specific namespaces are not supported for cluster-scoped resources."


def test_url_for_a_list_of_namespaced_custom_resources_clusterwide():
    resource = Resource('group', 'version', 'plural', namespaced=True)
    url = resource.get_url(namespace=None)
    assert url == '/apis/group/version/plural'


def test_url_for_a_list_of_namespaced_custom_resources_in_a_namespace():
    resource = Resource('group', 'version', 'plural', namespaced=True)
    url = resource.get_url(namespace='ns-a.b')
    assert url == '/apis/group/version/namespaces/ns-a.b/plural'


def test_url_for_a_specific_clusterscoped_custom_resource_clusterwide():
    resource = Resource('group', 'version', 'plural', namespaced=False)
    url = resource.get_url(namespace=None, name='name-a.b')
    assert url == '/apis/group/version/plural/name-a.b'


def test_url_for_a_specific_clusterscoped_custom_resource_in_a_namespace():
    resource = Resource('group', 'version', 'plural', namespaced=False)
    with pytest.raises(ValueError) as err:
        resource.get_url(namespace='ns', name='name-a.b')
    assert str(err.value) == "Specific namespaces are not supported for cluster-scoped resources."


def test_url_for_a_specific_namespaced_custom_resource_clusterwide():
    resource = Resource('group', 'version', 'plural', namespaced=True)
    with pytest.raises(ValueError) as err:
        resource.get_url(namespace=None, name='name-a.b')
    assert str(err.value) == "Specific namespaces are required for specific namespaced resources."


def test_url_for_a_specific_namespaced_custom_resource_in_a_namespace():
    resource = Resource('group', 'version', 'plural', namespaced=True)
    url = resource.get_url(namespace='ns-a.b', name='name-a.b')
    assert url == '/apis/group/version/namespaces/ns-a.b/plural/name-a.b'


def test_url_for_a_list_of_clusterscoped_corev1_resources_clusterwide():
    resource = Resource('', 'v1', 'plural', namespaced=False)
    url = resource.get_url(namespace=None)
    assert url == '/api/v1/plural'


def test_url_for_a_list_of_clusterscoped_corev1_resources_in_a_namespace():
    resource = Resource('', 'v1', 'plural', namespaced=False)
    with pytest.raises(ValueError) as err:
        resource.get_url(namespace='ns')
    assert str(err.value) == "Specific namespaces are not supported for cluster-scoped resources."


def test_url_for_a_list_of_namespaced_corev1_resources_clusterwide():
    resource = Resource('', 'v1', 'plural', namespaced=True)
    url = resource.get_url(namespace=None)
    assert url == '/api/v1/plural'


def test_url_for_a_list_of_namespaced_corev1_resources_in_a_namespace():
    resource = Resource('', 'v1', 'plural', namespaced=True)
    url = resource.get_url(namespace='ns-a.b')
    assert url == '/api/v1/namespaces/ns-a.b/plural'


def test_url_of_a_specific_clusterscoped_corev1_resource_clusterwide():
    resource = Resource('', 'v1', 'plural', namespaced=False)
    url = resource.get_url(namespace=None, name='name-a.b')
    assert url == '/api/v1/plural/name-a.b'


def test_url_of_a_specific_clusterscoped_corev1_resource_in_a_namespace():
    resource = Resource('', 'v1', 'plural', namespaced=False)
    with pytest.raises(ValueError) as err:
        resource.get_url(namespace='ns', name='name-a.b')
    assert str(err.value) == "Specific namespaces are not supported for cluster-scoped resources."


def test_url_of_a_specific_namespaced_corev1_resource_clusterwide():
    resource = Resource('', 'v1', 'plural', namespaced=True)
    with pytest.raises(ValueError) as err:
        resource.get_url(namespace=None, name='name-a.b')
    assert str(err.value) == "Specific namespaces are required for specific namespaced resources."


def test_url_of_a_specific_namespaced_corev1_resource_in_a_namespace():
    resource = Resource('', 'v1', 'plural', namespaced=True)
    url = resource.get_url(namespace='ns-a.b', name='name-a.b')
    assert url == '/api/v1/namespaces/ns-a.b/plural/name-a.b'


def test_url_with_arbitrary_params():
    resource = Resource('group', 'version', 'plural')
    url = resource.get_url(params=dict(watch='true', resourceVersion='abc%def xyz'))
    assert url == '/apis/group/version/plural?watch=true&resourceVersion=abc%25def+xyz'


def test_url_for_a_list_of_clusterscoped_custom_subresources_clusterwide():
    resource = Resource('group', 'version', 'plural', namespaced=False)
    with pytest.raises(ValueError) as err:
        resource.get_url(namespace=None, subresource='status')
    assert str(err.value) == "Subresources can be used only with specific resources by their name."


def test_url_for_a_list_of_clusterscoped_custom_subresources_in_a_namespace():
    resource = Resource('group', 'version', 'plural', namespaced=False)
    with pytest.raises(ValueError) as err:
        resource.get_url(namespace='ns', subresource='status')
    assert str(err.value) in {
        "Specific namespaces are not supported for cluster-scoped resources.",
        "Subresources can be used only with specific resources by their name.",
    }


def test_url_for_a_list_of_namespaced_custom_subresources_clusterwide():
    resource = Resource('group', 'version', 'plural', namespaced=True)
    with pytest.raises(ValueError) as err:
        resource.get_url(namespace=None, subresource='status')
    assert str(err.value) == "Subresources can be used only with specific resources by their name."


def test_url_for_a_list_of_namespaced_custom_subresources_in_a_namespace():
    resource = Resource('group', 'version', 'plural', namespaced=True)
    with pytest.raises(ValueError) as err:
        resource.get_url(namespace='ns-a.b', subresource='status')
    assert str(err.value) == "Subresources can be used only with specific resources by their name."


def test_url_for_a_specific_clusterscoped_custom_subresource_clusterwide():
    resource = Resource('group', 'version', 'plural', namespaced=False)
    url = resource.get_url(namespace=None, name='name-a.b', subresource='status')
    assert url == '/apis/group/version/plural/name-a.b/status'


def test_url_for_a_specific_clusterscoped_custom_subresource_in_a_namespace():
    resource = Resource('group', 'version', 'plural', namespaced=False)
    with pytest.raises(ValueError) as err:
        resource.get_url(namespace='ns', name='name-a.b', subresource='status')
    assert str(err.value) == "Specific namespaces are not supported for cluster-scoped resources."


def test_url_for_a_specific_namespaced_custom_subresource_clusterwide():
    resource = Resource('group', 'version', 'plural', namespaced=True)
    with pytest.raises(ValueError) as err:
        resource.get_url(namespace=None, name='name-a.b', subresource='status')
    assert str(err.value) == "Specific namespaces are required for specific namespaced resources."


def test_url_for_a_specific_namespaced_custom_subresource_in_a_namespace():
    resource = Resource('group', 'version', 'plural', namespaced=True)
    url = resource.get_url(namespace='ns-a.b', name='name-a.b', subresource='status')
    assert url == '/apis/group/version/namespaces/ns-a.b/plural/name-a.b/status'


def test_url_for_a_list_of_clusterscoped_corev1_subresources_clusterwide():
    resource = Resource('', 'v1', 'plural', namespaced=False)
    with pytest.raises(ValueError) as err:
        resource.get_url(namespace=None, subresource='status')
    assert str(err.value) == "Subresources can be used only with specific resources by their name."


def test_url_for_a_list_of_clusterscoped_corev1_subresources_in_a_namespace():
    resource = Resource('', 'v1', 'plural', namespaced=False)
    with pytest.raises(ValueError) as err:
        resource.get_url(namespace='ns', subresource='status')
    assert str(err.value) in {
        "Specific namespaces are not supported for cluster-scoped resources.",
        "Subresources can be used only with specific resources by their name.",
    }


def test_url_for_a_list_of_namespaced_corev1_subresources_clusterwide():
    resource = Resource('', 'v1', 'plural', namespaced=True)
    with pytest.raises(ValueError) as err:
        resource.get_url(namespace=None, subresource='status')
    assert str(err.value) == "Subresources can be used only with specific resources by their name."


def test_url_for_a_list_of_namespaced_corev1_subresources_in_a_namespace():
    resource = Resource('', 'v1', 'plural', namespaced=True)
    with pytest.raises(ValueError) as err:
        resource.get_url(namespace='ns-a.b', subresource='status')
    assert str(err.value) == "Subresources can be used only with specific resources by their name."


def test_url_for_a_specific_clusterscoped_corev1_subresource_clusterwide():
    resource = Resource('', 'v1', 'plural', namespaced=False)
    url = resource.get_url(namespace=None, name='name-a.b', subresource='status')
    assert url == '/api/v1/plural/name-a.b/status'


def test_url_for_a_specific_clusterscoped_corev1_subresource_in_a_namespace():
    resource = Resource('', 'v1', 'plural', namespaced=False)
    with pytest.raises(ValueError) as err:
        resource.get_url(namespace='ns', name='name-a.b', subresource='status')
    assert str(err.value) == "Specific namespaces are not supported for cluster-scoped resources."


def test_url_for_a_specific_namespaced_corev1_subresource_clusterwide():
    resource = Resource('', 'v1', 'plural', namespaced=True)
    with pytest.raises(ValueError) as err:
        resource.get_url(namespace=None, name='name-a.b', subresource='status')
    assert str(err.value) == "Specific namespaces are required for specific namespaced resources."


def test_url_for_a_specific_namespaced_corev1_subresource_in_a_namespace():
    resource = Resource('', 'v1', 'plural', namespaced=True)
    url = resource.get_url(namespace='ns-a.b', name='name-a.b', subresource='status')
    assert url == '/api/v1/namespaces/ns-a.b/plural/name-a.b/status'



================================================
FILE: tests/causation/test_detection.py
================================================
import copy
import json

import pytest

from kopf._cogs.structs.bodies import Body
from kopf._core.intents.causes import Reason, detect_changing_cause

LAST_SEEN_ANNOTATION = 'kopf.zalando.org/last-handled-configuration'
FINALIZER = 'fin'

# Encoded at runtime, so that we do not make any assumptions on json formatting.
SPEC_DATA = {'spec': {'field': 'value'}}
SPEC_JSON = json.dumps(SPEC_DATA, separators=(',', ':'))
ALT_DATA = {'spec': {'field': 'other'}}
ALT_JSON = json.dumps(ALT_DATA, separators=(',', ':'))

#
# The following factors contribute to the detection of the cause
# (and we combine all of them with the matching & mismatching fixtures):
# * Finalizers (presence or absence).
# * Deletion timestamp (presence or absence).
# * Annotation with the last-seen state (presence or absence).
# * Annotation with the last-seen state (difference with the real state).
#

deleted_events = pytest.mark.parametrize('event', [
    pytest.param('DELETED'),
])

regular_events = pytest.mark.parametrize('event', [
    pytest.param('ADDED'),
    pytest.param('MODIFIED'),
    pytest.param('FORWARD-COMPATIBILITY-PSEUDO-EVENT', id='COMPAT'),
])


all_finalizers = pytest.mark.parametrize('finalizers', [
    pytest.param({}, id='no-finalizers'),
    pytest.param({'finalizers': [FINALIZER]}, id='own-finalizer'),
    pytest.param({'finalizers': ['irrelevant', 'another']}, id='other-finalizers'),
    pytest.param({'finalizers': ['irrelevant', FINALIZER, 'another']}, id='mixed-finalizers'),
])

our_finalizers = pytest.mark.parametrize('finalizers', [
    pytest.param({'finalizers': [FINALIZER]}, id='own-finalizer'),
    pytest.param({'finalizers': ['irrelevant', FINALIZER, 'another']}, id='mixed-finalizers'),
])

no_finalizers = pytest.mark.parametrize('finalizers', [
    pytest.param({}, id='no-finalizers'),
    pytest.param({'finalizers': ['irrelevant', 'another']}, id='other-finalizers'),
])


all_deletions = pytest.mark.parametrize('deletion_ts', [
    pytest.param({}, id='no-deletion-ts'),
    pytest.param({'deletionTimestamp': None}, id='empty-deletion-ts'),
    pytest.param({'deletionTimestamp': 'some'}, id='real-deletion-ts'),
])

real_deletions = pytest.mark.parametrize('deletion_ts', [
    pytest.param({'deletionTimestamp': 'some'}, id='real-deletion-ts'),
])

no_deletions = pytest.mark.parametrize('deletion_ts', [
    pytest.param({}, id='no-deletion-ts'),
    pytest.param({'deletionTimestamp': None}, id='empty-deletion-ts'),
])


all_lastseen = pytest.mark.parametrize('old, annotations', [
    pytest.param(None, {}, id='no-annotations'),
    pytest.param(None, {'annotations': {}}, id='no-lastseen'),
    pytest.param(SPEC_DATA, {'annotations': {LAST_SEEN_ANNOTATION: SPEC_JSON}}, id='good-lastseen'),
    pytest.param(ALT_DATA, {'annotations': {LAST_SEEN_ANNOTATION: ALT_JSON}}, id='wrong-lastseen'),
])

absent_lastseen = pytest.mark.parametrize('old, annotations', [
    pytest.param(None, {}, id='no-annotations'),
    pytest.param(None, {'annotations': {}}, id='no-lastseen'),
])

matching_lastseen = pytest.mark.parametrize('old, annotations', [
    pytest.param(SPEC_DATA, {'annotations': {LAST_SEEN_ANNOTATION: SPEC_JSON}}, id='good-lastseen'),
])

mismatching_lastseen = pytest.mark.parametrize('old, annotations', [
    pytest.param(ALT_DATA, {'annotations': {LAST_SEEN_ANNOTATION: ALT_JSON}}, id='wrong-lastseen'),
])

all_requires_finalizer = pytest.mark.parametrize('requires_finalizer', [
    pytest.param(True, id='requires-finalizer'),
    pytest.param(False, id='doesnt-require-finalizer'),
])

requires_finalizer = pytest.mark.parametrize('requires_finalizer', [
    pytest.param(True, id='requires-finalizer'),
])

doesnt_require_finalizer = pytest.mark.parametrize('requires_finalizer', [
    pytest.param(False, id='doesnt-require-finalizer'),
])


@pytest.fixture
def content():
    return copy.deepcopy(SPEC_DATA)


#
# kwargs helpers -- to test them for all causes.
#

@pytest.fixture()
def kwargs():
    return dict(
        finalizer=FINALIZER,
        resource=object(),
        indices=object(),
        logger=object(),
        patch=object(),
        memo=object(),
    )


def check_kwargs(cause, kwargs):
    __traceback_hide__ = True
    assert cause.resource is kwargs['resource']
    assert cause.logger is kwargs['logger']
    assert cause.patch is kwargs['patch']
    assert cause.memo is kwargs['memo']


#
# The tests.
#

@all_requires_finalizer
@all_finalizers
@all_deletions
@deleted_events
def test_for_gone(
        kwargs, event, finalizers, deletion_ts, requires_finalizer):
    event = {'type': event, 'object': {'metadata': {}}}
    event['object']['metadata'] |= finalizers
    event['object']['metadata'] |= deletion_ts
    cause = detect_changing_cause(
        raw_event=event,
        body=Body(event['object']),
        **kwargs)
    assert cause.reason == Reason.GONE
    check_kwargs(cause, kwargs)


@all_requires_finalizer
@no_finalizers
@real_deletions
@regular_events
def test_for_free(
        kwargs, event, finalizers, deletion_ts, requires_finalizer):
    event = {'type': event, 'object': {'metadata': {}}}
    event['object']['metadata'] |= finalizers
    event['object']['metadata'] |= deletion_ts
    cause = detect_changing_cause(
        raw_event=event,
        body=Body(event['object']),
        **kwargs)
    assert cause.reason == Reason.FREE
    check_kwargs(cause, kwargs)


@all_requires_finalizer
@our_finalizers
@real_deletions
@regular_events
def test_for_delete(
        kwargs, event, finalizers, deletion_ts, requires_finalizer):
    event = {'type': event, 'object': {'metadata': {}}}
    event['object']['metadata'] |= finalizers
    event['object']['metadata'] |= deletion_ts
    cause = detect_changing_cause(
        raw_event=event,
        body=Body(event['object']),
        **kwargs)
    assert cause.reason == Reason.DELETE
    check_kwargs(cause, kwargs)


@requires_finalizer
@absent_lastseen
@our_finalizers
@no_deletions
@regular_events
def test_for_create(
        kwargs, event, finalizers, deletion_ts, old, annotations, content, requires_finalizer):
    event = {'type': event, 'object': {'metadata': {}}}
    event['object'] |= content
    event['object']['metadata'] |= finalizers
    event['object']['metadata'] |= deletion_ts
    event['object']['metadata'] |= annotations
    cause = detect_changing_cause(
        raw_event=event,
        body=Body(event['object']),
        old=old,
        **kwargs)
    assert cause.reason == Reason.CREATE
    check_kwargs(cause, kwargs)


@doesnt_require_finalizer
@no_finalizers
@no_deletions
@regular_events
def test_for_create_skip_acquire(
        kwargs, event, finalizers, deletion_ts, requires_finalizer):
    event = {'type': event, 'object': {'metadata': {}}}
    event['object']['metadata'] |= finalizers
    event['object']['metadata'] |= deletion_ts
    cause = detect_changing_cause(
        raw_event=event,
        body=Body(event['object']),
        **kwargs)
    assert cause.reason == Reason.CREATE
    check_kwargs(cause, kwargs)


@requires_finalizer
@matching_lastseen
@our_finalizers
@no_deletions
@regular_events
def test_for_no_op(
        kwargs, event, finalizers, deletion_ts, old, annotations, content, requires_finalizer):
    event = {'type': event, 'object': {'metadata': {}}}
    event['object'] |= content
    event['object']['metadata'] |= finalizers
    event['object']['metadata'] |= deletion_ts
    event['object']['metadata'] |= annotations
    cause = detect_changing_cause(
        raw_event=event,
        body=Body(event['object']),
        old=old,
        **kwargs)
    assert cause.reason == Reason.NOOP
    check_kwargs(cause, kwargs)


@requires_finalizer
@mismatching_lastseen
@our_finalizers
@no_deletions
@regular_events
def test_for_update(
        kwargs, event, finalizers, deletion_ts, old, annotations, content, requires_finalizer):
    event = {'type': event, 'object': {'metadata': {}}}
    event['object'] |= content
    event['object']['metadata'] |= finalizers
    event['object']['metadata'] |= deletion_ts
    event['object']['metadata'] |= annotations
    cause = detect_changing_cause(
        raw_event=event,
        body=Body(event['object']),
        diff=True,
        old=old,
        **kwargs)
    assert cause.reason == Reason.UPDATE
    check_kwargs(cause, kwargs)



================================================
FILE: tests/causation/test_kwargs.py
================================================
import dataclasses
import logging
from unittest.mock import Mock

import pytest

from kopf._cogs.configs.configuration import OperatorSettings
from kopf._cogs.structs import diffs
from kopf._cogs.structs.bodies import Body, BodyEssence
from kopf._cogs.structs.diffs import Diff
from kopf._cogs.structs.ephemera import Memo
from kopf._cogs.structs.patches import Patch
from kopf._core.engines.indexing import OperatorIndexer, OperatorIndexers
from kopf._core.intents.causes import Activity, ActivityCause, BaseCause, ChangingCause, \
                                      DaemonCause, IndexingCause, Reason, ResourceCause, \
                                      SpawningCause, WatchingCause, WebhookCause
from kopf._core.intents.stoppers import DaemonStopper

ALL_CAUSES = [
    BaseCause, ActivityCause, ResourceCause,
    WatchingCause, SpawningCause,
    ChangingCause, IndexingCause,
    WebhookCause, DaemonCause,
]
ALL_FIELDS = {
    field.name
    for cause_cls in ALL_CAUSES
    for field in dataclasses.fields(cause_cls)
} | {'stopped', 'body', 'spec', 'meta', 'status', 'name', 'namespace', 'labels', 'annotations'}


@pytest.mark.parametrize('cls', ALL_CAUSES)
@pytest.mark.parametrize('name', ALL_FIELDS)
@pytest.mark.parametrize('attr', ['kwargs', 'sync_kwargs', 'async_kwargs'])
def test_indices_overwrite_kwargs(cls: type[BaseCause], name, attr):
    indexers = OperatorIndexers()
    indexers['index1'] = OperatorIndexer()
    indexers['index2'] = OperatorIndexer()
    indexers[name] = OperatorIndexer()
    mocks = {field.name: Mock() for field in dataclasses.fields(cls)}
    mocks['indices'] = indexers.indices
    cause = cls(**mocks)
    kwargs = getattr(cause, attr)  # cause.kwargs / cause.sync_kwargs / cause.async_kwargs
    assert kwargs['index1'] is indexers['index1'].index
    assert kwargs['index2'] is indexers['index2'].index
    assert kwargs[name] is indexers[name].index


@pytest.mark.parametrize('activity', set(Activity))
@pytest.mark.parametrize('attr', ['kwargs', 'sync_kwargs', 'async_kwargs'])
def test_activity_kwargs(resource, activity, attr):
    cause = ActivityCause(
        memo=Memo(),
        logger=logging.getLogger('kopf.test.fake.logger'),
        indices=OperatorIndexers().indices,
        activity=activity,
        settings=OperatorSettings(),
    )
    kwargs = getattr(cause, attr)  # cause.kwargs / cause.sync_kwargs / cause.async_kwargs
    assert set(kwargs) == {'memo', 'logger', 'activity', 'settings'}
    assert kwargs['logger'] is cause.logger
    assert kwargs['activity'] is activity
    assert kwargs['settings'] is cause.settings


@pytest.mark.parametrize('attr', ['kwargs', 'sync_kwargs', 'async_kwargs'])
def test_admission_kwargs(resource, attr):
    body = {'metadata': {'uid': 'uid1', 'name': 'name1', 'namespace': 'ns1',
                         'labels': {'l1': 'v1'}, 'annotations': {'a1': 'v1'}},
            'spec': {'field': 'value'},
            'status': {'info': 'payload'}}
    cause = WebhookCause(
        logger=logging.getLogger('kopf.test.fake.logger'),
        indices=OperatorIndexers().indices,
        resource=resource,
        patch=Patch(),
        memo=Memo(),
        body=Body(body),
        dryrun=False,
        headers={'k1': 'v1'},
        sslpeer={'k2': 'v2'},
        userinfo={'k3': 'v3'},
        warnings=['w1'],
        webhook=None,
        reason=None,
        operation=None,
        subresource=None,
        new=BodyEssence(body),
        old=None,
        diff=diffs.diff(BodyEssence(body), None),
    )
    kwargs = getattr(cause, attr)  # cause.kwargs / cause.sync_kwargs / cause.async_kwargs
    assert set(kwargs) == {'logger', 'resource',
                           'dryrun', 'headers', 'sslpeer', 'userinfo', 'warnings', 'subresource',
                           'patch', 'memo',
                           'body', 'spec', 'status', 'meta', 'uid', 'name', 'namespace',
                           'labels', 'annotations', 'old', 'new', 'diff', 'operation'}
    assert kwargs['resource'] is cause.resource
    assert kwargs['logger'] is cause.logger
    assert kwargs['dryrun'] is cause.dryrun
    assert kwargs['headers'] is cause.headers
    assert kwargs['sslpeer'] is cause.sslpeer
    assert kwargs['userinfo'] is cause.userinfo
    assert kwargs['warnings'] is cause.warnings
    assert kwargs['patch'] is cause.patch
    assert kwargs['memo'] is cause.memo
    assert kwargs['body'] is cause.body
    assert kwargs['spec'] is cause.body.spec
    assert kwargs['meta'] is cause.body.metadata
    assert kwargs['status'] is cause.body.status
    assert kwargs['labels'] is cause.body.metadata.labels
    assert kwargs['annotations'] is cause.body.metadata.annotations
    assert kwargs['uid'] == cause.body.metadata.uid
    assert kwargs['name'] == cause.body.metadata.name
    assert kwargs['namespace'] == cause.body.metadata.namespace
    assert kwargs['operation'] == cause.operation
    assert kwargs['new'] == cause.new
    assert kwargs['old'] == cause.old
    assert kwargs['diff'] == cause.diff


@pytest.mark.parametrize('attr', ['kwargs', 'sync_kwargs', 'async_kwargs'])
def test_watching_kwargs(resource, attr):
    body = {'metadata': {'uid': 'uid1', 'name': 'name1', 'namespace': 'ns1',
                         'labels': {'l1': 'v1'}, 'annotations': {'a1': 'v1'}},
            'spec': {'field': 'value'},
            'status': {'info': 'payload'}}
    cause = WatchingCause(
        logger=logging.getLogger('kopf.test.fake.logger'),
        indices=OperatorIndexers().indices,
        resource=resource,
        patch=Patch(),
        memo=Memo(),
        body=Body(body),
        type='ADDED',
        event={'type': 'ADDED', 'object': {}},
    )
    kwargs = getattr(cause, attr)  # cause.kwargs / cause.sync_kwargs / cause.async_kwargs
    assert set(kwargs) == {'logger', 'resource',
                           'patch', 'event', 'type', 'memo',
                           'body', 'spec', 'status', 'meta', 'uid', 'name', 'namespace',
                           'labels', 'annotations'}
    assert kwargs['resource'] is cause.resource
    assert kwargs['logger'] is cause.logger
    assert kwargs['patch'] is cause.patch
    assert kwargs['event'] is cause.event
    assert kwargs['memo'] is cause.memo
    assert kwargs['type'] is cause.type
    assert kwargs['body'] is cause.body
    assert kwargs['spec'] is cause.body.spec
    assert kwargs['meta'] is cause.body.metadata
    assert kwargs['status'] is cause.body.status
    assert kwargs['labels'] is cause.body.metadata.labels
    assert kwargs['annotations'] is cause.body.metadata.annotations
    assert kwargs['uid'] == cause.body.metadata.uid
    assert kwargs['name'] == cause.body.metadata.name
    assert kwargs['namespace'] == cause.body.metadata.namespace


@pytest.mark.parametrize('attr', ['kwargs', 'sync_kwargs', 'async_kwargs'])
def test_changing_kwargs(resource, attr):
    body = {'metadata': {'uid': 'uid1', 'name': 'name1', 'namespace': 'ns1',
                         'labels': {'l1': 'v1'}, 'annotations': {'a1': 'v1'}},
            'spec': {'field': 'value'},
            'status': {'info': 'payload'}}
    cause = ChangingCause(
        logger=logging.getLogger('kopf.test.fake.logger'),
        indices=OperatorIndexers().indices,
        resource=resource,
        patch=Patch(),
        initial=False,
        reason=Reason.NOOP,
        memo=Memo(),
        body=Body(body),
        diff=Diff([]),
        old=BodyEssence(),
        new=BodyEssence(),
    )
    kwargs = getattr(cause, attr)  # cause.kwargs / cause.sync_kwargs / cause.async_kwargs
    assert set(kwargs) == {'logger', 'resource',
                           'patch', 'reason', 'memo',
                           'body', 'spec', 'status', 'meta', 'uid', 'name', 'namespace',
                           'labels', 'annotations', 'diff', 'old', 'new'}
    assert kwargs['resource'] is cause.resource
    assert kwargs['reason'] is cause.reason
    assert kwargs['logger'] is cause.logger
    assert kwargs['patch'] is cause.patch
    assert kwargs['memo'] is cause.memo
    assert kwargs['diff'] is cause.diff
    assert kwargs['old'] is cause.old
    assert kwargs['new'] is cause.new
    assert kwargs['body'] is cause.body
    assert kwargs['spec'] is cause.body.spec
    assert kwargs['meta'] is cause.body.metadata
    assert kwargs['status'] is cause.body.status
    assert kwargs['labels'] is cause.body.metadata.labels
    assert kwargs['annotations'] is cause.body.metadata.annotations
    assert kwargs['uid'] == cause.body.metadata.uid
    assert kwargs['name'] == cause.body.metadata.name
    assert kwargs['namespace'] == cause.body.metadata.namespace


@pytest.mark.parametrize('attr', ['kwargs', 'sync_kwargs', 'async_kwargs'])
def test_spawning_kwargs(resource, attr):
    body = {'metadata': {'uid': 'uid1', 'name': 'name1', 'namespace': 'ns1',
                         'labels': {'l1': 'v1'}, 'annotations': {'a1': 'v1'}},
            'spec': {'field': 'value'},
            'status': {'info': 'payload'}}
    cause = SpawningCause(
        logger=logging.getLogger('kopf.test.fake.logger'),
        indices=OperatorIndexers().indices,
        resource=resource,
        patch=Patch(),
        memo=Memo(),
        body=Body(body),
        reset=False,
    )
    kwargs = getattr(cause, attr)  # cause.kwargs / cause.sync_kwargs / cause.async_kwargs
    assert set(kwargs) == {'logger', 'resource', 'patch', 'memo',
                           'body', 'spec', 'status', 'meta', 'uid', 'name', 'namespace',
                           'labels', 'annotations'}
    assert kwargs['resource'] is cause.resource
    assert kwargs['logger'] is cause.logger
    assert kwargs['patch'] is cause.patch
    assert kwargs['memo'] is cause.memo
    assert kwargs['body'] is cause.body
    assert kwargs['spec'] is cause.body.spec
    assert kwargs['meta'] is cause.body.metadata
    assert kwargs['status'] is cause.body.status
    assert kwargs['labels'] is cause.body.metadata.labels
    assert kwargs['annotations'] is cause.body.metadata.annotations
    assert kwargs['uid'] == cause.body.metadata.uid
    assert kwargs['name'] == cause.body.metadata.name
    assert kwargs['namespace'] == cause.body.metadata.namespace


@pytest.mark.parametrize('attr', ['kwargs'])
def test_daemon_kwargs(resource, attr):
    body = {'metadata': {'uid': 'uid1', 'name': 'name1', 'namespace': 'ns1',
                         'labels': {'l1': 'v1'}, 'annotations': {'a1': 'v1'}},
            'spec': {'field': 'value'},
            'status': {'info': 'payload'}}
    cause = DaemonCause(
        logger=logging.getLogger('kopf.test.fake.logger'),
        indices=OperatorIndexers().indices,
        resource=resource,
        patch=Patch(),
        memo=Memo(),
        body=Body(body),
        stopper=DaemonStopper(),
    )
    kwargs = getattr(cause, attr)  # cause.kwargs
    assert set(kwargs) == {'logger', 'resource', 'patch', 'memo',
                           'body', 'spec', 'status', 'meta', 'uid', 'name', 'namespace',
                           'labels', 'annotations'}
    assert kwargs['resource'] is cause.resource
    assert kwargs['logger'] is cause.logger
    assert kwargs['patch'] is cause.patch
    assert kwargs['memo'] is cause.memo
    assert kwargs['body'] is cause.body
    assert kwargs['spec'] is cause.body.spec
    assert kwargs['meta'] is cause.body.metadata
    assert kwargs['status'] is cause.body.status
    assert kwargs['labels'] is cause.body.metadata.labels
    assert kwargs['annotations'] is cause.body.metadata.annotations
    assert kwargs['uid'] == cause.body.metadata.uid
    assert kwargs['name'] == cause.body.metadata.name
    assert kwargs['namespace'] == cause.body.metadata.namespace
    assert 'stopper' not in kwargs
    assert 'stopped' not in kwargs


@pytest.mark.parametrize('attr', ['sync_kwargs'])
def test_daemon_sync_stopper(resource, attr):
    cause = DaemonCause(
        logger=logging.getLogger('kopf.test.fake.logger'),
        indices=OperatorIndexers().indices,
        resource=resource,
        patch=Patch(),
        memo=Memo(),
        body=Body({}),
        stopper=DaemonStopper(),
    )
    kwargs = getattr(cause, attr)  # cause.sync_kwargs
    assert 'stopper' not in kwargs
    assert kwargs['stopped'] is cause.stopper.sync_waiter


@pytest.mark.parametrize('attr', ['async_kwargs'])
def test_daemon_async_stopper(resource, attr):
    cause = DaemonCause(
        logger=logging.getLogger('kopf.test.fake.logger'),
        indices=OperatorIndexers().indices,
        resource=resource,
        patch=Patch(),
        memo=Memo(),
        body=Body({}),
        stopper=DaemonStopper(),
    )
    kwargs = getattr(cause, attr)  # cause.async_kwargs
    assert 'stopper' not in kwargs
    assert kwargs['stopped'] is cause.stopper.async_waiter



================================================
FILE: tests/cli/conftest.py
================================================
import functools
import sys

import click.testing
import pytest

from kopf.cli import main

SCRIPT1 = """
import kopf

@kopf.on.create('kopfexamples')
def create_fn(spec, **_):
    print('Hello from create_fn!')
    print(repr(spec))
"""

SCRIPT2 = """
import kopf

@kopf.on.update('kopfexamples')
def update_fn(spec, **_):
    print('Hello from create_fn!')
    print(repr(spec))
"""


@pytest.fixture(autouse=True)
def srcdir(tmpdir):
    tmpdir.join('handler1.py').write(SCRIPT1)
    tmpdir.join('handler2.py').write(SCRIPT2)
    pkgdir = tmpdir.mkdir('package')
    pkgdir.join('__init__.py').write('')
    pkgdir.join('module_1.py').write(SCRIPT1)
    pkgdir.join('module_2.py').write(SCRIPT2)

    sys.path.insert(0, str(tmpdir))
    try:
        with tmpdir.as_cwd():
            yield tmpdir
    finally:
        sys.path.remove(str(tmpdir))


@pytest.fixture(autouse=True)
def clean_modules_cache():
    # Otherwise, the first loaded test-modules remain there forever,
    # preventing 2nd and further tests from passing.
    for key in list(sys.modules.keys()):
        if key.startswith('package'):
            del sys.modules[key]


@pytest.fixture()
def runner():
    runner = click.testing.CliRunner()
    return runner


@pytest.fixture()
def invoke(runner):
    return functools.partial(runner.invoke, main)


@pytest.fixture()
def preload(mocker):
    return mocker.patch('kopf._cogs.helpers.loaders.preload')


@pytest.fixture()
def real_run(mocker):
    return mocker.patch('kopf._core.reactor.running.run')



================================================
FILE: tests/cli/test_help.py
================================================
def test_help_in_root(invoke, mocker):
    result = invoke(['--help'])

    assert result.exit_code == 0
    assert 'Usage: kopf [OPTIONS]' in result.output
    assert '  run ' in result.output
    assert '  freeze ' in result.output
    assert '  resume ' in result.output


def test_help_in_subcommand(invoke, mocker):
    preload = mocker.patch('kopf._cogs.helpers.loaders.preload')
    real_run = mocker.patch('kopf._core.reactor.running.run')

    result = invoke(['run', '--help'])

    assert result.exit_code == 0
    assert not preload.called
    assert not real_run.called

    # Enough to be sure this is not a root command help.
    assert 'Usage: kopf run [OPTIONS]' in result.output
    assert '  --standalone' in result.output
    assert '  -m, --module' in result.output



================================================
FILE: tests/cli/test_logging.py
================================================
import logging

import pytest


@pytest.mark.parametrize('expect_debug, expect_info, options, envvars', [
    (False, True, [], {}),
    (False, False, ['-q'], {}),
    (False, False, ['--quiet'], {}),
    (False, False, [], {'KOPF_RUN_QUIET': 'true'}),
    (False, True, [], {'KOPF_ENV_QUIET': ''}),
    (True, True, ['-d'], {}),
    (True, True, ['--debug'], {}),
    (True, True, [], {'KOPF_RUN_DEBUG': 'true'}),
    (False, True, [], {'KOPF_ENV_DEBUG': ''}),
    (True, True, ['-v'], {}),
    (True, True, ['--verbose'], {}),
    (True, True, [], {'KOPF_RUN_VERBOSE': 'true'}),
    (False, True, [], {'KOPF_ENV_VERBOSE': ''}),
], ids=[
    'default',
    'opt-short-q', 'opt-long-quiet', 'env-quiet-true', 'env-quiet-empty',
    'opt-short-d', 'opt-long-debug', 'env-debug-true', 'env-debug-empty',
    'opt-short-v', 'opt-long-verbose', 'env-verbose-true', 'env-verbose-empty',
])
def test_verbosity(invoke, caplog, options, envvars, expect_debug, expect_info, preload, real_run):
    result = invoke(['run'] + options, env=envvars)
    assert result.exit_code == 0

    logger = logging.getLogger()
    logger.debug('some debug')
    logger.info('some info')
    logger.warning('some warning')
    logger.error('some error')

    assert len(caplog.records) >= 2 + int(expect_info) + int(expect_debug)
    assert caplog.records[-1].message == 'some error'
    assert caplog.records[-2].message == 'some warning'
    if expect_info:
        assert caplog.records[-3].message == 'some info'
    if expect_debug:
        assert caplog.records[-4].message == 'some debug'


@pytest.mark.parametrize('options', [
    ([]),
    (['-q']),
    (['--quiet']),
    (['-v']),
    (['--verbose']),
], ids=['default', 'q', 'quiet', 'v', 'verbose'])
def test_no_lowlevel_dumps_in_nondebug(invoke, caplog, options, preload, real_run):
    result = invoke(['run'] + options)
    assert result.exit_code == 0

    # TODO: This also goes to the pytest's output. Try to suppress it there (how?).
    logging.getLogger('asyncio').error('boom!')

    alien_records = [m for m in caplog.records if not m.name.startswith('kopf')]
    assert len(alien_records) == 0


@pytest.mark.parametrize('options', [
    (['-d']),
    (['--debug']),
], ids=['d', 'debug'])
def test_lowlevel_dumps_in_debug_mode(invoke, caplog, options, preload, real_run):
    result = invoke(['run'] + options)
    assert result.exit_code == 0

    logging.getLogger('asyncio').debug('hello!')

    alien_records = [m for m in caplog.records if not m.name.startswith('kopf')]
    assert len(alien_records) >= 1



================================================
FILE: tests/cli/test_options.py
================================================
import pytest


@pytest.mark.parametrize('kwarg, value, options, envvars', [
    ('paths', (), [], {}),
    ('paths', ('path1', 'path2'), ['path1', 'path2'], {}),
    # ('paths', ('path1', 'path2'), [], {'KOPF_RUN_PATHS': 'path1 path2'}), FIXME: UNSUPPORTED

    ('modules', (), [], {}),
    ('modules', ('mod1', 'mod2'), ['-m', 'mod1', '-m', 'mod2'], {}),
    ('modules', ('mod1', 'mod2'), ['--module', 'mod1', '--module', 'mod2'], {}),
    ('modules', ('mod1', 'mod2'), [], {'KOPF_RUN_MODULES': 'mod1 mod2'}),
], ids=[
    'default-paths', 'arg-paths', #'env-paths',
    'default-modules', 'opt-short-m', 'opt-long-modules', 'env-modules',
])
def test_options_passed_to_preload(invoke, options, envvars, kwarg, value, preload, real_run):
    result = invoke(['run'] + options, env=envvars)
    assert result.exit_code == 0
    assert preload.called
    assert preload.call_args[1][kwarg] == value


@pytest.mark.parametrize('kwarg, value, options, envvars', [
    ('standalone', None, [], {}),
    ('standalone', True, ['--standalone'], {}),
    ('standalone', True, [], {'KOPF_RUN_STANDALONE': 'true'}),

    ('namespaces', (), [], {}),
    ('namespaces', ('ns',), ['-n', 'ns'], {}),
    ('namespaces', ('ns',), ['--namespace=ns'], {}),
    ('namespaces', ('ns',), [], {'KOPF_RUN_NAMESPACE': 'ns'}),
    ('namespaces', ('ns',), [], {'KOPF_RUN_NAMESPACES': 'ns'}),

    ('namespaces', ('ns1', 'ns2'), ['--namespace=ns1', '-n', 'ns2'], {}),
    ('namespaces', ('ns1', 'ns2'), [], {'KOPF_RUN_NAMESPACES': 'ns1 ns2'}),

    ('peering_name', None, [], {}),
    ('peering_name', 'peer', ['-P', 'peer'], {}),
    ('peering_name', 'peer', ['--peering=peer'], {}),
    ('peering_name', 'peer', [], {'KOPF_RUN_PEERING': 'peer'}),

    ('priority', None, [], {}),
    ('priority', 123, ['-p', '123'], {}),
    ('priority', 123, ['--priority=123'], {}),
    ('priority', 123, [], {'KOPF_RUN_PRIORITY': '123'}),
    ('priority', 666, ['--dev'], {}),
], ids=[
    'default-standalone', 'opt-long-standalone', 'env-standalone',
    'default-namespace', 'opt-short-n', 'opt-long-namespace', 'env1-namespace', 'env2-namespace',
    'opt-multi-namespaces', 'env-multi-namespaces',
    'default-peering', 'opt-short-P', 'opt-long-peering', 'env-peering',
    'default-priority', 'opt-short-p', 'opt-long-priority', 'env-priority', 'opt-long-dev',
])
def test_options_passed_to_realrun(invoke, options, envvars, kwarg, value, preload, real_run):
    result = invoke(['run'] + options, env=envvars)
    assert result.exit_code == 0
    assert real_run.called
    assert real_run.call_args[1][kwarg] == value



================================================
FILE: tests/cli/test_preloading.py
================================================
import kopf


def test_nothing(invoke, real_run):
    result = invoke(['run'])
    assert result.exit_code == 0

    registry = kopf.get_default_registry()
    handlers = registry._changing.get_all_handlers()
    assert len(handlers) == 0


def test_one_file(invoke, real_run):
    result = invoke(['run', 'handler1.py'])
    assert result.exit_code == 0

    registry = kopf.get_default_registry()
    handlers = registry._changing.get_all_handlers()
    assert len(handlers) == 1
    assert handlers[0].id == 'create_fn'


def test_two_files(invoke, real_run):
    result = invoke(['run', 'handler1.py', 'handler2.py'])
    assert result.exit_code == 0

    registry = kopf.get_default_registry()
    handlers = registry._changing.get_all_handlers()
    assert len(handlers) == 2
    assert handlers[0].id == 'create_fn'
    assert handlers[1].id == 'update_fn'


def test_one_module(invoke, real_run):
    result = invoke(['run', '-m', 'package.module_1'])
    assert result.exit_code == 0

    registry = kopf.get_default_registry()
    handlers = registry._changing.get_all_handlers()
    assert len(handlers) == 1
    assert handlers[0].id == 'create_fn'


def test_two_modules(invoke, real_run):
    result = invoke(['run', '-m', 'package.module_1', '-m', 'package.module_2'])
    assert result.exit_code == 0

    registry = kopf.get_default_registry()
    handlers = registry._changing.get_all_handlers()
    assert len(handlers) == 2
    assert handlers[0].id == 'create_fn'
    assert handlers[1].id == 'update_fn'


def test_mixed_sources(invoke, real_run):
    result = invoke(['run', 'handler1.py', '-m', 'package.module_2'])
    assert result.exit_code == 0

    registry = kopf.get_default_registry()
    handlers = registry._changing.get_all_handlers()
    assert len(handlers) == 2
    assert handlers[0].id == 'create_fn'
    assert handlers[1].id == 'update_fn'



================================================
FILE: tests/dicts/test_cherrypicking.py
================================================
import copy

import pytest

from kopf._cogs.structs.dicts import cherrypick


def test_overrides_existing_keys():
    src = {'ignored-key': 'src-val', 'tested-key': 'src-val'}
    dst = {'ignored-key': 'dst-val', 'tested-key': 'dst-val'}
    cherrypick(src=src, dst=dst, fields=['tested-key'])
    assert dst == {'ignored-key': 'dst-val', 'tested-key': 'src-val'}


def test_adds_absent_dst_keys():
    src = {'ignored-key': 'src-val', 'tested-key': 'src-val'}
    dst = {'ignored-key': 'dst-val'}
    cherrypick(src=src, dst=dst, fields=['tested-key'])
    assert dst == {'ignored-key': 'dst-val', 'tested-key': 'src-val'}


def test_skips_absent_src_keys():
    src = {'ignored-key': 'src-val'}
    dst = {'ignored-key': 'dst-val', 'tested-key': 'dst-val'}
    cherrypick(src=src, dst=dst, fields=['tested-key'])
    assert dst == {'ignored-key': 'dst-val', 'tested-key': 'dst-val'}


def test_overrides_existing_subkeys():
    src = {'sub': {'ignored-key': 'src-val', 'tested-key': 'src-val'}}
    dst = {'sub': {'ignored-key': 'dst-val', 'tested-key': 'dst-val'}}
    cherrypick(src=src, dst=dst, fields=['sub.tested-key'])
    assert dst == {'sub': {'ignored-key': 'dst-val', 'tested-key': 'src-val'}}


def test_adds_absent_dst_subkeys():
    src = {'sub': {'ignored-key': 'src-val', 'tested-key': 'src-val'}}
    dst = {'sub': {'ignored-key': 'dst-val'}}
    cherrypick(src=src, dst=dst, fields=['sub.tested-key'])
    assert dst == {'sub': {'ignored-key': 'dst-val', 'tested-key': 'src-val'}}


def test_skips_absent_src_subkeys():
    src = {'sub': {'ignored-key': 'src-val'}}
    dst = {'sub': {'ignored-key': 'dst-val', 'tested-key': 'dst-val'}}
    cherrypick(src=src, dst=dst, fields=['sub.tested-key'])
    assert dst == {'sub': {'ignored-key': 'dst-val', 'tested-key': 'dst-val'}}


def test_ensures_dst_subdicts():
    src = {'sub': {'ignored-key': 'src-val', 'tested-key': 'src-val'}}
    dst = {}
    cherrypick(src=src, dst=dst, fields=['sub.tested-key'])
    assert dst == {'sub': {'tested-key': 'src-val'}}


def test_fails_on_nonmapping_src_key():
    src = {'sub': 'scalar-value'}
    dst = {'sub': {'ignored-key': 'src-val', 'tested-key': 'src-val'}}
    with pytest.raises(TypeError):
        cherrypick(src=src, dst=dst, fields=['sub.tested-key'])


def test_fails_on_nonmapping_dst_key():
    src = {'sub': {'ignored-key': 'src-val', 'tested-key': 'src-val'}}
    dst = {'sub': 'scalar-value'}
    with pytest.raises(TypeError):
        cherrypick(src=src, dst=dst, fields=['sub.tested-key'])


def test_exact_object_picked_by_default():
    src = {'tested-key': {'key': 'val'}}
    dst = {}
    cherrypick(src=src, dst=dst, fields=['tested-key'])

    assert dst == {'tested-key': {'key': 'val'}}
    assert dst['tested-key'] == src['tested-key']
    assert dst['tested-key'] is src['tested-key']

    src['tested-key']['key'] = 'replaced-val'
    assert dst['tested-key']['key'] == 'replaced-val'


def test_copied_object_picked_on_request():
    src = {'tested-key': {'key': 'val'}}
    dst = {}
    cherrypick(src=src, dst=dst, fields=['tested-key'], picker=copy.copy)

    assert dst == {'tested-key': {'key': 'val'}}
    assert dst['tested-key'] == src['tested-key']
    assert dst['tested-key'] is not src['tested-key']

    src['tested-key']['key'] = 'another-val'
    assert dst['tested-key']['key'] == 'val'



================================================
FILE: tests/dicts/test_dictviews.py
================================================
import pytest

from kopf._cogs.structs.dicts import MappingView, MutableMappingView, ReplaceableMappingView

all_classes = pytest.mark.parametrize('cls', [
    MappingView, MutableMappingView, ReplaceableMappingView])


@all_classes
def test_creation_with_source(cls):
    cls({})


@all_classes
def test_creation_without_source(cls):
    with pytest.raises(TypeError):
        cls()


@all_classes
def test_access_for_root(cls):
    data = {'field': 'value'}
    view = cls(data)
    value = view['field']
    assert value == 'value'


@all_classes
def test_access_for_path(cls):
    data = {'spec': {'field': 'value'}}
    view = cls(data, 'spec')
    value = view['field']
    assert value == 'value'


@all_classes
def test_unexistent_key_for_root(cls):
    data = {}
    view = cls(data)
    with pytest.raises(KeyError):
        view['field']


@all_classes
def test_unexistent_key_for_path(cls):
    data = {}
    view = cls(data, 'spec')
    with pytest.raises(KeyError):
        view['field']


@all_classes
def test_live_access_for_root(cls):
    data = {}
    view = cls(data)
    data['field'] = 'value'
    value = view['field']
    assert value == 'value'


@all_classes
def test_live_access_for_path(cls):
    data = {}
    view = cls(data, 'spec')
    data['spec'] = {'field': 'value'}
    value = view['field']
    assert value == 'value'


@all_classes
def test_length_for_root(cls):
    data = {'field': 'value'}
    view = cls(data)
    assert len(view) == 1


@all_classes
def test_length_for_path(cls):
    data = {'spec': {'field': 'value'}, 'extra-key': None}
    view = cls(data, 'spec')
    assert len(view) == 1


@all_classes
def test_bool_true_for_root(cls):
    data = {'field': 'value'}
    view = cls(data)
    assert bool(view)


@all_classes
def test_bool_false_for_root(cls):
    data = {}
    view = cls(data)
    assert not bool(view)


@all_classes
def test_bool_true_for_path(cls):
    data = {'spec': {'field': 'value'}}
    view = cls(data, 'spec')
    assert bool(view)


@all_classes
def test_bool_false_for_path(cls):
    data = {}
    view = cls(data, 'spec')
    assert not bool(view)


@all_classes
def test_keys_for_root(cls):
    data = {'field': 'value'}
    view = cls(data)
    assert list(view.keys()) == ['field']


@all_classes
def test_keys_for_path(cls):
    data = {'spec': {'field': 'value'}}
    view = cls(data, 'spec')
    assert list(view.keys()) == ['field']


@all_classes
def test_values_for_root(cls):
    data = {'field': 'value'}
    view = cls(data)
    assert list(view.values()) == ['value']


@all_classes
def test_values_for_path(cls):
    data = {'spec': {'field': 'value'}}
    view = cls(data, 'spec')
    assert list(view.values()) == ['value']


@all_classes
def test_items_for_root(cls):
    data = {'field': 'value'}
    view = cls(data)
    assert list(view.items()) == [('field', 'value')]


@all_classes
def test_items_for_path(cls):
    data = {'spec': {'field': 'value'}}
    view = cls(data, 'spec')
    assert list(view.items()) == [('field', 'value')]


@all_classes
def test_dict_for_root(cls):
    data = {'field': 'value'}
    view = cls(data)
    assert dict(view) == {'field': 'value'}


@all_classes
def test_dict_for_path(cls):
    data = {'spec': {'field': 'value'}}
    view = cls(data, 'spec')
    assert dict(view) == {'field': 'value'}


def test_update_for_root():
    data = {'field': 'value'}
    view = MutableMappingView(data)
    view['field'] = 'new-value'
    assert data == {'field': 'new-value'}


def test_update_for_path():
    data = {'spec': {'field': 'value'}}
    view = MutableMappingView(data, 'spec')
    view['field'] = 'new-value'
    assert data == {'spec': {'field': 'new-value'}}


def test_delete_for_root():
    data = {'field': 'value'}
    view = MutableMappingView(data)
    del view['field']
    assert data == {}


def test_delete_for_path():
    data = {'spec': {'field': 'value'}}
    view = MutableMappingView(data, 'spec')
    del view['field']
    assert data == {'spec': {}}


def test_chain_creation():
    data = {}
    view = MutableMappingView(data, 'spec')
    view['field'] = 'new-value'
    assert data == {'spec': {'field': 'new-value'}}


def test_replacing_from_another_view():
    data1 = {'field1': 'value1'}
    data2 = {'field2': 'value2'}
    view1 = ReplaceableMappingView(data1)
    view2 = ReplaceableMappingView(data2)
    view1._replace_from(view2)
    assert view1._src is not view2
    assert view1._src is view2._src
    assert dict(view1) == {'field2': 'value2'}


def test_replacing_with_another_view():
    data1 = {'field1': 'value1'}
    data2 = {'field2': 'value2'}
    view1 = ReplaceableMappingView(data1)
    view2 = ReplaceableMappingView(data2)
    view1._replace_with(view2)
    assert view1._src is view2
    assert dict(view1) == {'field2': 'value2'}


def test_replacing_with_regular_dict():
    data1 = {'field1': 'value1'}
    data2 = {'field2': 'value2'}
    view1 = ReplaceableMappingView(data1)
    view1._replace_with(data2)
    assert view1._src is data2
    assert dict(view1) == {'field2': 'value2'}



================================================
FILE: tests/dicts/test_ensuring.py
================================================
import pytest

from kopf._cogs.structs.dicts import ensure


def test_existing_key():
    d = {'abc': {'def': {'hij': 'val'}}}
    ensure(d, ['abc', 'def', 'hij'], 'new')
    assert d == {'abc': {'def': {'hij': 'new'}}}


def test_unexisting_key_in_existing_dict():
    d = {'abc': {'def': {}}}
    ensure(d, ['abc', 'def', 'hij'], 'new')
    assert d == {'abc': {'def': {'hij': 'new'}}}


def test_unexisting_key_in_unexisting_dict():
    d = {}
    ensure(d, ['abc', 'def', 'hij'], 'new')
    assert d == {'abc': {'def': {'hij': 'new'}}}


def test_toplevel_key():
    d = {'key': 'val'}
    ensure(d, ['key'], 'new')
    assert d == {'key': 'new'}


def test_nonmapping_key():
    d = {'key': 'val'}
    with pytest.raises(TypeError):
        ensure(d, ['key', 'sub'], 'new')


def test_empty_path():
    d = {}
    with pytest.raises(ValueError) as e:
        ensure(d, [], 'new')
    assert "Setting a root of a dict is impossible" in str(e.value)



================================================
FILE: tests/dicts/test_parsing.py
================================================
import pytest

from kopf._cogs.structs.dicts import parse_field


def test_from_none():
    path = parse_field(None)
    assert isinstance(path, tuple)
    assert len(path) == 0


def test_from_string_one_level():
    path = parse_field('field')
    assert isinstance(path, tuple)
    assert path == ('field',)


def test_from_string_two_levels():
    path = parse_field('field.subfield')
    assert isinstance(path, tuple)
    assert path == ('field', 'subfield')


def test_from_list():
    path = parse_field(['field' , 'subfield'])
    assert isinstance(path, tuple)
    assert path == ('field', 'subfield')


def test_from_tuple():
    path = parse_field(('field' , 'subfield'))
    assert isinstance(path, tuple)
    assert path == ('field', 'subfield')


@pytest.mark.parametrize('val', [dict(), set(), frozenset()])
def test_from_others_fails(val):
    with pytest.raises(ValueError):
        parse_field(val)



================================================
FILE: tests/dicts/test_removing.py
================================================
import pytest

from kopf._cogs.structs.dicts import remove


def test_existing_key():
    d = {'abc': {'def': {'hij': 'val', 'hello': 'world'}}}
    remove(d, ['abc', 'def', 'hij'])
    assert d == {'abc': {'def': {'hello': 'world'}}}


def test_unexisting_key_in_existing_dict():
    d = {'abc': {'def': {'hello': 'world'}}}
    remove(d, ['abc', 'def', 'hij'])
    assert d == {'abc': {'def': {'hello': 'world'}}}


def test_unexisting_key_in_unexisting_dict():
    d = {}
    remove(d, ['abc', 'def', 'hij'])
    assert d == {}


def test_parent_cascaded_deletion_up_to_the_root():
    d = {'abc': {'def': {'hij': 'val'}}}
    remove(d, ['abc', 'def', 'hij'])
    assert d == {}


def test_parent_cascaded_deletion_up_to_a_middle():
    d = {'abc': {'def': {'hij': 'val'}, 'hello': 'world'}}
    remove(d, ['abc', 'def', 'hij'])
    assert d == {'abc': {'hello': 'world'}}


def test_nonmapping_key():
    d = {'key': 'val'}
    with pytest.raises(TypeError):
        remove(d, ['key', 'sub'])


def test_empty_path():
    d = {}
    with pytest.raises(ValueError) as e:
        remove(d, [])
    assert "Removing a root of a dict is impossible" in str(e.value)



================================================
FILE: tests/dicts/test_resolving.py
================================================
"""
The test design notes:

* The field "abc.def.hij" is existent.
* The field "rst.uvw.xyz" is inexistent.
* Its mixtures ("a.b.z", "a.y.z") are used to simulate partial inexistence.

* For the existent keys, kwargs should not matter.
* For the non-existent keys, the default is returned,
  or a ``KeyError`` raised -- as with regular mappings.

* For special cases with "wrong" values (``"value"["z"]``, ``None["z"]``, etc),
  either a ``TypeError`` should be raised normally. If "wrong" values are said
  to be ignored, then they are treated the same as inexistent values,
  and the default value is returned or a ``KeyError`` is raised.

"""
import types

import pytest

from kopf._cogs.structs.dicts import resolve, resolve_obj

default = object()


#
# Both resolve functions should behave exactly the same for dicts.
#
@pytest.mark.parametrize('resolve', [resolve, resolve_obj])
def test_dict_with_existent_key_with_no_default(resolve):
    d = {'abc': {'def': {'hij': 'val'}}}
    r = resolve(d, ['abc', 'def', 'hij'])
    assert r == 'val'


@pytest.mark.parametrize('resolve', [resolve, resolve_obj])
def test_dict_with_existent_key_with_default(resolve):
    d = {'abc': {'def': {'hij': 'val'}}}
    r = resolve(d, ['abc', 'def', 'hij'], default)
    assert r == 'val'


@pytest.mark.parametrize('key', [
    pytest.param(['rst', 'uvw', 'xyz'], id='1stlvl'),
    pytest.param(['abc', 'uvw', 'xyz'], id='2ndlvl'),
    pytest.param(['abc', 'def', 'xyz'], id='3rdlvl'),
])
@pytest.mark.parametrize('resolve', [resolve, resolve_obj])
def test_dict_with_inexistent_key_with_no_default(resolve, key):
    d = {'abc': {'def': {'hij': 'val'}}}
    with pytest.raises(KeyError):
        resolve(d, key)


@pytest.mark.parametrize('key', [
    pytest.param(['rst', 'uvw', 'xyz'], id='1stlvl'),
    pytest.param(['abc', 'uvw', 'xyz'], id='2ndlvl'),
    pytest.param(['abc', 'def', 'xyz'], id='3rdlvl'),
])
@pytest.mark.parametrize('resolve', [resolve, resolve_obj])
def test_dict_with_inexistent_key_with_default(resolve, key):
    d = {'abc': {'def': {'hij': 'val'}}}
    r = resolve(d, key, default)
    assert r is default


@pytest.mark.parametrize('resolve', [resolve, resolve_obj])
def test_dict_with_nonmapping_with_no_default(resolve):
    d = {'key': 'val'}
    with pytest.raises(TypeError):
        resolve(d, ['key', 'sub'])


@pytest.mark.parametrize('resolve', [resolve, resolve_obj])
def test_dict_with_nonmapping_with_default(resolve):
    d = {'key': 'val'}
    r = resolve(d, ['key', 'sub'], default)
    assert r is default


@pytest.mark.parametrize('resolve', [resolve, resolve_obj])
def test_dict_with_none_is_treated_as_a_regular_default_value(resolve):
    d = {'abc': {'def': {'hij': 'val'}}}
    r = resolve(d, ['abc', 'def', 'xyz'], None)
    assert r is None


@pytest.mark.parametrize('resolve', [resolve, resolve_obj])
def test_dict_with_empty_path(resolve):
    d = {'key': 'val'}
    r = resolve(d, [])
    assert r == d
    assert r is d


#
# Specialised drill-down for objects.
#
class FakeKubernetesModel:  # no bases!
    __module__ = 'kubernetes.client.models.fake-for-tests'

    @property
    def metadata(self):
        return None

    attribute_map = {
        'AbC': 'abc',
        'zzz': 'dez',
    }


@pytest.fixture(params=[FakeKubernetesModel, types.SimpleNamespace])
def obj(request):
    cls = request.param
    obj = cls()
    if cls is FakeKubernetesModel:
        # With attribute mapping in mind.
        obj.key = 'val'
        obj.AbC = cls()
        obj.AbC.zzz = cls()
        obj.AbC.zzz.hij = 'val'
    else:
        # Exactly as they will be requested.
        obj.key = 'val'
        obj.abc = cls()
        obj.abc.dez = cls()
        obj.abc.dez.hij = 'val'
    return obj


def test_object_with_existent_key_with_no_default(obj):
    r = resolve_obj(obj, ['abc', 'dez', 'hij'])
    assert r == 'val'


def test_object_with_existent_key_with_default(obj):
    r = resolve_obj(obj, ['abc', 'dez', 'hij'], default)
    assert r == 'val'


@pytest.mark.parametrize('key', [
    pytest.param(['rst', 'uvw', 'xyz'], id='1stlvl'),
    pytest.param(['abc', 'uvw', 'xyz'], id='2ndlvl'),
    pytest.param(['abc', 'dez', 'xyz'], id='3rdlvl'),
])
def test_object_with_inexistent_key_with_no_default(obj, key):
    with pytest.raises(AttributeError):
        resolve_obj(obj, key)


@pytest.mark.parametrize('key', [
    pytest.param(['rst', 'uvw', 'xyz'], id='1stlvl'),
    pytest.param(['abc', 'uvw', 'xyz'], id='2ndlvl'),
    pytest.param(['abc', 'dez', 'xyz'], id='3rdlvl'),
])
def test_object_with_inexistent_key_with_default(obj, key):
    r = resolve_obj(obj, key, default)
    assert r is default


def test_object_with_nonmapping_with_no_default(obj):
    with pytest.raises(TypeError):
        resolve_obj(obj, ['key', 'sub'])


def test_object_with_nonmapping_with_default(obj):
    r = resolve_obj(obj, ['key', 'sub'], default)
    assert r is default


def test_object_with_none_is_treated_as_a_regular_default_value(obj):
    r = resolve_obj(obj, ['abc', 'dez', 'xyz'], None)
    assert r is None


def test_object_with_empty_path(obj):
    r = resolve_obj(obj, [])
    assert r == obj
    assert r is obj


#
# Some special cases.
#
@pytest.mark.parametrize('cls', (tuple, list, set, frozenset, str, bytes))
def test_raises_for_builtins(cls):
    obj = cls()
    with pytest.raises(TypeError):
        resolve_obj(obj, ['__class__'])



================================================
FILE: tests/dicts/test_walking.py
================================================
from kopf._cogs.structs.dicts import walk


def test_over_a_none():
    result = list(walk(None))
    assert len(result) == 0


def test_over_a_dict():
    obj = {}
    result = list(walk(obj))
    assert len(result) == 1
    assert result[0] is obj


def test_over_a_list_of_dicts():
    obj1 = {}
    obj2 = {}
    result = list(walk([obj1, obj2]))
    assert len(result) == 2
    assert result[0] is obj1
    assert result[1] is obj2


def test_over_a_tuple_of_dicts():
    obj1 = {}
    obj2 = {}
    result = list(walk((obj1, obj2)))
    assert len(result) == 2
    assert result[0] is obj1
    assert result[1] is obj2


def test_none_is_ignored():
    obj1 = {}
    obj2 = {}
    result = list(walk([obj1, None, obj2]))
    assert len(result) == 2
    assert result[0] is obj1
    assert result[1] is obj2


def test_simple_nested():
    obj1 = {'field': {'subfield': 'val'}}
    obj2 = {'field': {}}
    result = list(walk([obj1, obj2], nested=['field.subfield']))
    assert len(result) == 3
    assert result[0] is obj1
    assert result[1] == 'val'
    assert result[2] is obj2


def test_double_nested():
    obj1 = {'field': {'subfield': 'val'}}
    obj2 = {'field': {}}
    result = list(walk([obj1, obj2], nested=['field.subfield', 'field']))
    assert len(result) == 5
    assert result[0] is obj1
    assert result[1] == 'val'
    assert result[2] == {'subfield': 'val'}
    assert result[3] is obj2
    assert result[4] == {}



================================================
FILE: tests/diffs/test_calculation.py
================================================
import collections.abc

import pytest

from kopf._cogs.structs.diffs import DiffOperation, DiffScope, diff


@pytest.mark.parametrize('scope', list(DiffScope))
def test_none_for_old(scope):
    a = None
    b = object()
    d = diff(a, b, scope=scope)
    assert d == (('add', (), None, b),)


@pytest.mark.parametrize('scope', list(DiffScope))
def test_none_for_new(scope):
    a = object()
    b = None
    d = diff(a, b, scope=scope)
    assert d == (('remove', (), a, None),)


@pytest.mark.parametrize('scope', list(DiffScope))
def test_nones_for_both(scope):
    a = None
    b = None
    d = diff(a, b, scope=scope)
    assert d == ()


@pytest.mark.parametrize('scope', list(DiffScope))
def test_scalars_equal(scope):
    a = 100
    b = 100
    d = diff(a, b, scope=scope)
    assert d == ()


@pytest.mark.parametrize('scope', list(DiffScope))
def test_scalars_unequal(scope):
    a = 100
    b = 200
    d = diff(a, b, scope=scope)
    assert d == (('change', (), 100, 200),)


@pytest.mark.parametrize('scope', list(DiffScope))
def test_strings_equal(scope):
    a = 'hello'
    b = 'hello'
    d = diff(a, b, scope=scope)
    assert d == ()


@pytest.mark.parametrize('scope', list(DiffScope))
def test_strings_unequal(scope):
    a = 'hello'
    b = 'world'
    d = diff(a, b, scope=scope)
    assert d == (('change', (), 'hello', 'world'),)


@pytest.mark.parametrize('scope', list(DiffScope))
def test_lists_equal(scope):
    a = [100, 200, 300]
    b = [100, 200, 300]
    d = diff(a, b, scope=scope)
    assert d == ()


@pytest.mark.parametrize('scope', list(DiffScope))
def test_lists_unequal(scope):
    a = [100, 200, 300]
    b = [100, 666, 300]
    d = diff(a, b, scope=scope)
    assert d == (('change', (), [100, 200, 300], [100, 666, 300]),)


@pytest.mark.parametrize('scope', list(DiffScope))
def test_dicts_equal(scope):
    a = {'hello': 'world', 'key': 'val'}
    b = {'key': 'val', 'hello': 'world'}
    d = diff(a, b, scope=scope)
    assert d == ()


@pytest.mark.parametrize('scope', [DiffScope.FULL, DiffScope.RIGHT])
def test_dicts_with_keys_added_and_noticed(scope):
    a = {'hello': 'world'}
    b = {'hello': 'world', 'key': 'val'}
    d = diff(a, b, scope=scope)
    assert d == (('add', ('key',), None, 'val'),)


@pytest.mark.parametrize('scope', [DiffScope.LEFT])
def test_dicts_with_keys_added_but_ignored(scope):
    a = {'hello': 'world'}
    b = {'hello': 'world', 'key': 'val'}
    d = diff(a, b, scope=scope)
    assert d == ()


@pytest.mark.parametrize('scope', [DiffScope.FULL, DiffScope.LEFT])
def test_dicts_with_keys_removed_and_noticed(scope):
    a = {'hello': 'world', 'key': 'val'}
    b = {'hello': 'world'}
    d = diff(a, b, scope=scope)
    assert d == (('remove', ('key',), 'val', None),)


@pytest.mark.parametrize('scope', [DiffScope.RIGHT])
def test_dicts_with_keys_removed_but_ignored(scope):
    a = {'hello': 'world', 'key': 'val'}
    b = {'hello': 'world'}
    d = diff(a, b, scope=scope)
    assert d == ()


@pytest.mark.parametrize('scope', list(DiffScope))
def test_dicts_with_keys_changed(scope):
    a = {'hello': 'world', 'key': 'old'}
    b = {'hello': 'world', 'key': 'new'}
    d = diff(a, b, scope=scope)
    assert d == (('change', ('key',), 'old', 'new'),)


@pytest.mark.parametrize('scope', list(DiffScope))
def test_dicts_with_subkeys_changed(scope):
    a = {'main': {'hello': 'world', 'key': 'old'}}
    b = {'main': {'hello': 'world', 'key': 'new'}}
    d = diff(a, b, scope=scope)
    assert d == (('change', ('main', 'key'), 'old', 'new'),)


def test_custom_mappings_are_recursed():

    class SampleMapping(collections.abc.Mapping):
        def __init__(self, data=(), **kwargs) -> None:
            super().__init__()
            self._items = dict(data) | kwargs
        def __len__(self) -> int: return len(self._items)
        def __iter__(self): return iter(self._items)
        def __getitem__(self, item: str) -> str: return self._items[item]

    class MappingA(SampleMapping): pass
    class MappingB(SampleMapping): pass

    a = MappingA(a=100, b=200)
    b = MappingB(b=300, c=400)
    d = diff(a, b)
    assert (DiffOperation.REMOVE, ('a',), 100, None) in d
    assert (DiffOperation.CHANGE, ('b',), 200, 300) in d
    assert (DiffOperation.ADD, ('c',), None, 400) in d
    assert (DiffOperation.CHANGE, (), a, b) not in d


# A few examples of slightly more realistic non-abstracted use-cases below:
def test_dicts_adding_label():
    body_before_labelling = {'metadata': {}}
    body_after_labelling  = {'metadata': {'labels': 'LABEL'}}

    d = diff(body_before_labelling, body_after_labelling)
    assert d == (('add', ('metadata', 'labels'), None, 'LABEL'),)


def test_dicts_updating_storage_size():
    body_before_storage_size_update = {'spec': {'size': '42G'}}
    body_after_storage_size_update  = {'spec': {'size': '76G'}}

    d = diff(body_before_storage_size_update, body_after_storage_size_update)
    assert d == (('change', ('spec', 'size'), '42G', '76G'),)


def test_dicts_different_items_handled():
    body_before_storage_size_update = {'spec': {'items': ['task1', 'task2']}}
    body_after_storage_size_update  = {'spec': {'items': ['task3', 'task4']}}

    d = diff(body_before_storage_size_update, body_after_storage_size_update)
    assert d == (('change', ('spec', 'items'), ['task1', 'task2'], ['task3', 'task4']),)



================================================
FILE: tests/diffs/test_protocols.py
================================================
import pytest

from kopf._cogs.structs.diffs import Diff, DiffItem, DiffOperation


@pytest.mark.parametrize('operation', list(DiffOperation))
def test_operation_enum_behaves_as_string(operation: DiffOperation):
    assert isinstance(operation, str)
    assert operation == operation.value
    assert str(operation) == str(operation.value)
    assert repr(operation) == repr(operation.value)


@pytest.mark.parametrize('operation', list(DiffOperation))
def test_item_has_all_expected_properties(operation):
    item = DiffItem(operation, ('field',), 'a', 'b')
    assert item.operation is operation
    assert item.op is operation
    assert item.field == ('field',)
    assert item.old == 'a'
    assert item.new == 'b'


@pytest.mark.parametrize('operation', list(DiffOperation))
def test_item_comparison_to_tuple(operation):
    item = DiffItem(operation.value, (), 'a', 'b')
    assert item == (operation.value, (), 'a', 'b')


@pytest.mark.parametrize('operation', list(DiffOperation))
def test_item_comparison_to_list(operation):
    item = DiffItem(operation.value, (), 'a', 'b')
    assert item == [operation.value, (), 'a', 'b']


@pytest.mark.parametrize('operation', list(DiffOperation))
def test_item_comparison_to_another_item(operation):
    item1 = DiffItem(operation.value, (), 'a', 'b')
    item2 = DiffItem(operation.value, (), 'a', 'b')
    assert item1 == item2


# TODO: later implement it so that the order of items is irrelevant.
def test_diff_comparison_to_the_same():
    d1 = Diff([
        DiffItem(DiffOperation.ADD   , ('key1',), None, 'new1'),
        DiffItem(DiffOperation.CHANGE, ('key2',), 'old2', 'new2'),
        DiffItem(DiffOperation.REMOVE, ('key3',), 'old3', None),
    ])
    d2 = Diff([
        DiffItem(DiffOperation.ADD   , ('key1',), None, 'new1'),
        DiffItem(DiffOperation.CHANGE, ('key2',), 'old2', 'new2'),
        DiffItem(DiffOperation.REMOVE, ('key3',), 'old3', None),
    ])
    assert d1 == d2
    assert hash(d1) == hash(d2)
    assert d1 is not d2



================================================
FILE: tests/diffs/test_reduction.py
================================================
import pytest

from kopf._cogs.structs.diffs import Diff, DiffItem, DiffOperation, reduce

DIFF = Diff([
    DiffItem(DiffOperation.ADD   , ('key1',), None, 'new1'),
    DiffItem(DiffOperation.CHANGE, ('key2',), 'old2', 'new2'),
    DiffItem(DiffOperation.ADD   , ('key2', 'suba'), 'olda', 'newa'),
    DiffItem(DiffOperation.REMOVE, ('key2', 'subb'), 'oldb', 'newb'),
    DiffItem(DiffOperation.REMOVE, ('key3',), 'old3', None),
    DiffItem(DiffOperation.CHANGE, ('key4',),
             {'suba': 'olda', 'subc': 'oldc'},
             {'subb': 'newb', 'subc': 'newc'}),
])


@pytest.mark.parametrize('diff', [
    [['op', ['key', 'sub'], 'old', 'new']],
    [['op', ('key', 'sub'), 'old', 'new']],
    [('op', ['key', 'sub'], 'old', 'new')],
    [('op', ('key', 'sub'), 'old', 'new')],
    (['op', ['key', 'sub'], 'old', 'new'],),
    (['op', ('key', 'sub'), 'old', 'new'],),
    (('op', ['key', 'sub'], 'old', 'new'),),
    (('op', ('key', 'sub'), 'old', 'new'),),
], ids=[
    'lll-diff', 'llt-diff', 'ltl-diff', 'ltt-diff',
    'tll-diff', 'tlt-diff', 'ttl-diff', 'ttt-diff',
])
@pytest.mark.parametrize('path', [
    ['key', 'sub'],
    ('key', 'sub'),
], ids=['list-path', 'tuple-path'])
def test_type_ignored_for_inputs_but_is_tuple_for_output(diff, path):
    result = reduce(diff, path)
    assert result == (('op', (), 'old', 'new'),)


def test_empty_path_selects_all_ops():
    result = reduce(DIFF, [])
    assert result == DIFF


def test_existent_path_selects_relevant_ops():
    result = reduce(DIFF, ['key2'])
    assert result == (
        ('change', (), 'old2', 'new2'),
        ('add'   , ('suba',), 'olda', 'newa'),
        ('remove', ('subb',), 'oldb', 'newb'),
    )


@pytest.mark.parametrize('path', [
    ['nonexistent-key'],
    ['key1', 'nonexistent-key'],
    ['key2', 'nonexistent-key'],
    ['key3', 'nonexistent-key'],
    ['key4', 'nonexistent-key'],
    ['key4', 'suba', 'nonexistent-key'],
    ['key4', 'subb', 'nonexistent-key'],
    ['key4', 'subc', 'nonexistent-key'],
    ['key4', 'nonexistent-dict', 'nonexistent-key'],
])
def test_nonexistent_path_selects_nothing(path):
    result = reduce(DIFF, path)
    assert result == ()


def test_overly_specific_path_dives_into_dicts_for_addition():
    result = reduce(DIFF, ['key4', 'subb'])
    assert result == (
        ('add', (), None, 'newb'),
    )


def test_overly_specific_path_dives_into_dicts_for_removal():
    result = reduce(DIFF, ['key4', 'suba'])
    assert result == (
        ('remove', (), 'olda', None),
    )


def test_overly_specific_path_dives_into_dicts_for_change():
    result = reduce(DIFF, ['key4', 'subc'])
    assert result == (
        ('change', (), 'oldc', 'newc'),
    )



================================================
FILE: tests/e2e/conftest.py
================================================
import glob
import os.path
import pathlib
import subprocess

import pytest

from kopf._core.intents.registries import SmartOperatorRegistry

root_dir = os.path.relpath(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
examples = sorted(glob.glob(os.path.join(root_dir, 'examples/*/')))
assert examples  # if empty, it is just the detection failed
examples = [path for path in examples if not glob.glob(os.path.join(path, 'test*.py'))]


@pytest.fixture
def registry_factory():
    # Authentication is needed for the real e2e tests.
    return SmartOperatorRegistry


@pytest.fixture(params=examples, ids=[os.path.basename(path.rstrip('/')) for path in examples])
def exampledir(request):
    return pathlib.Path(request.param)


@pytest.fixture()
def with_crd():
    # Our best guess on which Kubernetes version we are running on.
    subprocess.run(f"kubectl apply -f examples/crd.yaml",
                   shell=True, check=True, timeout=10, capture_output=True)


@pytest.fixture()
def with_peering():
    subprocess.run(f"kubectl apply -f peering.yaml",
                   shell=True, check=True, timeout=10, capture_output=True)


@pytest.fixture()
def no_crd():
    subprocess.run("kubectl delete customresourcedefinition kopfexamples.kopf.dev",
                   shell=True, check=True, timeout=10, capture_output=True)


@pytest.fixture()
def no_peering():
    subprocess.run("kubectl delete customresourcedefinition kopfpeerings.kopf.dev",
                   shell=True, check=True, timeout=10, capture_output=True)



================================================
FILE: tests/e2e/test_examples.py
================================================
import ast
import collections
import re
import subprocess
import time
from collections.abc import Sequence
from typing import Any, Optional

import astpath
import pytest
from lxml import etree

from kopf.testing import KopfRunner


def test_all_examples_are_runnable(mocker, settings, with_crd, exampledir, caplog):

    # If the example has its own opinion on the timing, try to respect it.
    # See e.g. /examples/99-all-at-once/example.py.
    example_py = exampledir / 'example.py'
    e2e = E2EParser(str(example_py))

    # Skip the e2e test if the framework-optional but test-required library is missing.
    if e2e.imports_kubernetes:
        pytest.importorskip('kubernetes')

    # To prevent lengthy sleeps on the simulated retries.
    mocker.patch('kopf._core.actions.execution.DEFAULT_RETRY_DELAY', 1)

    # To prevent lengthy threads in the loop executor when the process exits.
    settings.watching.server_timeout = 10

    # Run an operator and simulate some activity with the operated resource.
    with KopfRunner(
        ['run', '--all-namespaces', '--standalone', '--verbose', str(example_py)],
        timeout=60,
    ) as runner:

        # Give it some time to start.
        _sleep_till_stopword(caplog=caplog,
                             delay=e2e.startup_time_limit,
                             patterns=e2e.startup_stop_words or ['Client is configured'])

        # Trigger the reaction. Give it some time to react and to sleep and to retry.
        subprocess.run("kubectl apply -f examples/obj.yaml",
                       shell=True, check=True, timeout=10, capture_output=True)
        _sleep_till_stopword(caplog=caplog,
                             delay=e2e.creation_time_limit,
                             patterns=e2e.creation_stop_words)

        # Trigger the reaction. Give it some time to react.
        subprocess.run("kubectl delete -f examples/obj.yaml",
                       shell=True, check=True, timeout=10, capture_output=True)
        _sleep_till_stopword(caplog=caplog,
                             delay=e2e.deletion_time_limit,
                             patterns=e2e.deletion_stop_words)

    # Give it some time to finish.
    _sleep_till_stopword(caplog=caplog,
                         delay=e2e.cleanup_time_limit,
                         patterns=e2e.cleanup_stop_words or ['Hung tasks', 'Root tasks'])

    # Verify that the operator did not die on start, or during the operation.
    assert runner.exception is None
    assert runner.exit_code == 0

    # There are usually more than these messages, but we only check for the certain ones.
    # This just shows us that the operator is doing something, it is alive.
    if e2e.has_mandatory_on_delete:
        assert '[default/kopf-example-1] Adding the finalizer' in runner.output
    if e2e.has_on_create:
        assert '[default/kopf-example-1] Creation is in progress:' in runner.output
    if e2e.has_mandatory_on_delete:
        assert '[default/kopf-example-1] Deletion is in progress:' in runner.output
    if e2e.has_changing_handlers:
        assert '[default/kopf-example-1] Deleted, really deleted' in runner.output
    if not e2e.allow_tracebacks:
        assert 'Traceback (most recent call last):' not in runner.output

    # Verify that once a handler succeeds, it is never re-executed again.
    handler_names = re.findall(r"'(.+?)' succeeded", runner.output)
    if e2e.success_counts is not None:
        checked_names = [name for name in handler_names if name in e2e.success_counts]
        name_counts = collections.Counter(checked_names)
        assert name_counts == e2e.success_counts
    else:
        name_counts = collections.Counter(handler_names)
        assert set(name_counts.values()) == {1}

    # Verify that once a handler fails, it is never re-executed again.
    handler_names = re.findall(r"'(.+?)' failed (?:permanently|with an exception. Will stop.)", runner.output)
    if e2e.failure_counts is not None:
        checked_names = [name for name in handler_names if name in e2e.failure_counts]
        name_counts = collections.Counter(checked_names)
        assert name_counts == e2e.failure_counts
    else:
        name_counts = collections.Counter(handler_names)
        assert not name_counts


def _sleep_till_stopword(
        caplog,
        delay: Optional[float] = None,
        patterns: Optional[Sequence[str]] = None,
        *,
        interval: Optional[float] = None,
) -> bool:
    patterns = list(patterns or [])
    delay = delay or (10.0 if patterns else 1.0)
    interval = interval or min(1.0, max(0.1, delay / 10.))
    started = time.perf_counter()
    found = False
    while not found and time.perf_counter() - started < delay:
        for message in list(caplog.messages):
            if any(re.search(pattern, message) for pattern in patterns or []):
                found = True
                break
        else:
            time.sleep(interval)
    return found


class E2EParser:
    """
    An AST-based parser of examples' codebase.

    The parser retrieves the information about the example without executing
    the whole example (which can have side-effects). Some snippets are still
    executed: e.g. values of E2E configs or values of some decorators' kwargs.
    """
    configs: dict[str, Any]
    xml2ast: dict[etree._Element, ast.AST]
    xtree: etree._Element

    def __init__(self, path: str) -> None:
        super().__init__()

        with open(path, encoding='utf-8') as f:
            self.path = path
            self.text = f.read()

        self.xml2ast = {}
        self.xtree = astpath.file_contents_to_xml_ast(self.text, node_mappings=self.xml2ast)

        self.configs = {
            name.attrib['id']: ast.literal_eval(self.xml2ast[assign].value)
            for name in self.xtree.xpath('''
                //Assign/targets/Name[starts-with(@id, "E2E_")] |
                //AnnAssign/target/Name[starts-with(@id, "E2E_")]
            ''')
            for assign in name.xpath('''ancestor::AnnAssign | ancestor::Assign''')  # strictly one!
        }

    @property
    def startup_time_limit(self) -> Optional[float]:
        return self.configs.get('E2E_STARTUP_TIME_LIMIT')

    @property
    def startup_stop_words(self) -> Optional[Sequence[str]]:
        return self.configs.get('E2E_STARTUP_STOP_WORDS')

    @property
    def cleanup_time_limit(self) -> Optional[float]:
        return self.configs.get('E2E_CLEANUP_TIME_LIMIT')

    @property
    def cleanup_stop_words(self) -> Optional[Sequence[str]]:
        return self.configs.get('E2E_CLEANUP_STOP_WORDS')

    @property
    def creation_time_limit(self) -> Optional[float]:
        return self.configs.get('E2E_CREATION_TIME_LIMIT')

    @property
    def creation_stop_words(self) -> Optional[Sequence[str]]:
        return self.configs.get('E2E_CREATION_STOP_WORDS')

    @property
    def deletion_time_limit(self) -> Optional[float]:
        return self.configs.get('E2E_DELETION_TIME_LIMIT')

    @property
    def deletion_stop_words(self) -> Optional[Sequence[str]]:
        return self.configs.get('E2E_DELETION_STOP_WORDS')

    @property
    def allow_tracebacks(self) -> Optional[bool]:
        return self.configs.get('E2E_ALLOW_TRACEBACKS')

    @property
    def success_counts(self) -> Optional[dict[str, int]]:
        return self.configs.get('E2E_SUCCESS_COUNTS')

    @property
    def failure_counts(self) -> Optional[dict[str, int]]:
        return self.configs.get('E2E_FAILURE_COUNTS')

    @property
    def imports_kubernetes(self) -> bool:
        # In English: all forms of `import kubernetes[.blah]`, `import kubernetes[.blah] as x`,
        # so as `from kubernetes[.blah] import x as y`.
        return bool(self.xtree.xpath('''
            //Import/names/alias[@name="kubernetes" or starts-with(@name, "kubernetes.")] |
            //ImportFrom[@module="kubernetes" or starts-with(@module, "kubernetes.")]
        '''))

    def has_handler(self, name: str) -> bool:
        # In English: any decorators that look like `@kopf.on.{name}(...)` or `@kopf.{name}(...)`.
        return bool(self.xtree.xpath(f'''
            (//FunctionDef | //AsyncFunctionDef)/decorator_list/Call[
                (
                    func/Attribute/value/Attribute/value/Name/@id="kopf" and
                    func/Attribute/value/Attribute/@attr="on" and
                    func/Attribute/@attr={name!r}
                ) or (
                    func/Attribute/value/Name/@id="kopf" and
                    func/Attribute/@attr={name!r}
                )
            ]
        '''))

    @property
    def has_on_create(self) -> bool:
        return self.has_handler('create')

    @property
    def has_on_update(self) -> bool:
        return self.has_handler('update')

    @property
    def has_on_delete(self) -> bool:
        return self.has_handler('delete')

    @property
    def has_changing_handlers(self) -> bool:
        return any(self.has_handler(name) for name in ['create', 'update', 'delete'])

    @property
    def has_mandatory_on_delete(self) -> bool:
        # In English: `optional=...` kwargs of `@kopf.on.delete(...)` decorators, if any.
        optional_kwargs = self.xtree.xpath('''
            (//FunctionDef | //AsyncFunctionDef)/decorator_list/Call[
                func/Attribute/value/Attribute/value/Name/@id="kopf" and
                func/Attribute/value/Attribute/@attr="on" and
                func/Attribute/@attr="delete"
            ]/keywords/keyword[@arg="optional"]
        ''')
        return (self.has_on_delete and
                not any(ast.literal_eval(self.xml2ast[kwarg].value) for kwarg in optional_kwargs))



================================================
FILE: tests/handling/conftest.py
================================================
"""
Testing the handling of events on the top level.

As input:

* Mocked cause detection, with the cause artificially simulated for each test.
  The proper cause detection is tested elsewhere (see ``test_detection.py``).
* Registered handlers in a global registry. Each handler is a normal function,
  which calls a mock -- to ease the assertions.

As output, we check mocked calls on the following:

* ``asyncio.sleep()`` -- for delays.
* ``kopf.clients.patching.patch_obj()`` -- for patch content.
* ``kopf.clients.events.post_event()`` -- for events posted.
* Handler mocks -- whether they were or were not called with specific arguments.
* Captured logs.

The above inputs & outputs represent the expected user scenario
rather than the specific implementation of it.
Therefore, we do not mock/spy/intercept anything within the handling routines
(except for cause detection), leaving it as the implementation details.
Specifically, this internal chain of calls happens on every event:

* ``causation.detect_*_cause()`` -- tested separately in ``/tests/causation/``.
* ``handle_cause()``
* ``execute()``
* ``_execute()``
* ``_call_handler()``
* ``invocation.invoke()`` -- tested separately in ``/tests/invocations/``.

Some of these aspects are tested separately to be sure they indeed execute
all possible cases properly. In the top-level event handling, we assume they do,
and only check for the upper-level behaviour, not all of the input combinations.
"""
import dataclasses
from typing import Callable
from unittest.mock import Mock

import pytest

import kopf
from kopf._core.intents.causes import ChangingCause


@pytest.fixture(autouse=True)
def _auto_mocked(k8s_mocked):
    pass


@dataclasses.dataclass(frozen=True, eq=False, order=False)
class HandlersContainer:
    index_mock: Mock
    event_mock: Mock
    create_mock: Mock
    update_mock: Mock
    delete_mock: Mock
    resume_mock: Mock
    event_fn: Callable
    create_fn: Callable
    update_fn: Callable
    delete_fn: Callable
    resume_fn: Callable


@pytest.fixture()
def handlers(registry):
    index_mock = Mock(return_value=None)
    event_mock = Mock(return_value=None)
    create_mock = Mock(return_value=None)
    update_mock = Mock(return_value=None)
    delete_mock = Mock(return_value=None)
    resume_mock = Mock(return_value=None)

    @kopf.index('kopfexamples', id='index_fn')
    async def index_fn(**kwargs):
        return index_mock(**kwargs)

    @kopf.on.event('kopfexamples', id='event_fn')
    async def event_fn(**kwargs):
        return event_mock(**kwargs)

    # Keep on-resume on top, to catch any issues with the test design (where it could be skipped).
    @kopf.on.resume('kopfexamples', id='resume_fn', timeout=600, retries=100,
                    deleted=True)  # only for resuming handles, to cover the resource being deleted.
    async def resume_fn(**kwargs):
        return resume_mock(**kwargs)

    @kopf.on.create('kopfexamples', id='create_fn', timeout=600, retries=100)
    async def create_fn(**kwargs):
        return create_mock(**kwargs)

    @kopf.on.update('kopfexamples', id='update_fn', timeout=600, retries=100)
    async def update_fn(**kwargs):
        return update_mock(**kwargs)

    @kopf.on.delete('kopfexamples', id='delete_fn', timeout=600, retries=100)
    async def delete_fn(**kwargs):
        return delete_mock(**kwargs)

    return HandlersContainer(
        index_mock=index_mock,
        event_mock=event_mock,
        create_mock=create_mock,
        update_mock=update_mock,
        delete_mock=delete_mock,
        resume_mock=resume_mock,
        event_fn=event_fn,
        create_fn=create_fn,
        update_fn=update_fn,
        delete_fn=delete_fn,
        resume_fn=resume_fn,
    )


@pytest.fixture()
def extrahandlers(registry, handlers):
    index_mock = Mock(return_value=None)
    event_mock = Mock(return_value=None)
    create_mock = Mock(return_value=None)
    update_mock = Mock(return_value=None)
    delete_mock = Mock(return_value=None)
    resume_mock = Mock(return_value=None)

    @kopf.index('kopfexamples', id='index_fn2')
    async def index_fn2(**kwargs):
        return index_mock(**kwargs)

    @kopf.on.event('kopfexamples', id='event_fn2')
    async def event_fn2(**kwargs):
        return event_mock(**kwargs)

    # Keep on-resume on top, to catch any issues with the test design (where it could be skipped).
    # Note: deleted=True -- only for resuming handles, to cover the resource being deleted.
    @kopf.on.resume('kopfexamples', id='resume_fn2', deleted=True)
    async def resume_fn2(**kwargs):
        return resume_mock(**kwargs)

    @kopf.on.create('kopfexamples', id='create_fn2')
    async def create_fn2(**kwargs):
        return create_mock(**kwargs)

    @kopf.on.update('kopfexamples', id='update_fn2')
    async def update_fn2(**kwargs):
        return update_mock(**kwargs)

    @kopf.on.delete('kopfexamples', id='delete_fn2')
    async def delete_fn2(**kwargs):
        return delete_mock(**kwargs)

    return HandlersContainer(
        index_mock=index_mock,
        event_mock=event_mock,
        create_mock=create_mock,
        update_mock=update_mock,
        delete_mock=delete_mock,
        resume_mock=resume_mock,
        event_fn=event_fn2,
        create_fn=create_fn2,
        update_fn=update_fn2,
        delete_fn=delete_fn2,
        resume_fn=resume_fn2,
    )


@pytest.fixture()
def cause_mock(mocker, settings):
    """
    Mock the resulting _cause_ of the resource change detection logic.

    The change detection is complex, depends on many fields and values, and it
    is difficult to simulate by artificial event bodies, especially its reason.

    Instead, we patch a method which detects the resource changing causes, and
    return a cause with the mocked reason (also, diff, and some other fields).

    The a value of this fixture, a mock is provided with a few fields to mock.
    The default is to no mock anything, unless defined in the test, and to use
    the original arguments to the detection method.
    """

    # Use everything from a mock, but use the passed `patch` dict as is.
    # The event handler passes its own accumulator, and checks/applies it later.
    def new_detect_fn(*, finalizer, diff, new, old, **kwargs):

        # For change detection, we ensure that there is no extra cycle of adding a finalizer.
        raw_event = kwargs.pop('raw_event', None)
        raw_body = raw_event['object']
        raw_body.setdefault('metadata', {}).setdefault('finalizers', [finalizer])

        # Pass through kwargs: resource, logger, patch, diff, old, new.
        # I.e. everything except what we mock -- for them, use the mocked values (if not None).
        return ChangingCause(
            reason=mock.reason,
            diff=mock.diff if mock.diff is not None else diff,
            new=mock.new if mock.new is not None else new,
            old=mock.old if mock.old is not None else old,
            **kwargs)

    # Substitute the real cause detector with out own mock-based one.
    mocker.patch('kopf._core.intents.causes.detect_changing_cause', new=new_detect_fn)

    # The mock object stores some values later used by the factory substitute.
    # Note: ONLY those fields we mock in the tests. Other kwargs should be passed through.
    mock = mocker.Mock(spec_set=['reason', 'diff', 'new', 'old'])
    mock.reason = None
    mock.diff = None
    mock.new = None
    mock.old = None
    return mock



================================================
FILE: tests/handling/test_activity_triggering.py
================================================
from collections.abc import Mapping

import freezegun
import pytest

from kopf._cogs.structs.ephemera import Memo
from kopf._cogs.structs.ids import HandlerId
from kopf._core.actions.execution import Outcome, PermanentError, TemporaryError
from kopf._core.actions.lifecycles import all_at_once
from kopf._core.engines.activities import ActivityError, run_activity
from kopf._core.engines.indexing import OperatorIndexers
from kopf._core.intents.causes import Activity
from kopf._core.intents.handlers import ActivityHandler
from kopf._core.intents.registries import OperatorRegistry


def test_activity_error_exception():
    outcome = Outcome(final=True)
    outcomes: Mapping[HandlerId, Outcome]
    outcomes = {HandlerId('id'): outcome}
    error = ActivityError("message", outcomes=outcomes)
    assert str(error) == "message"
    assert error.outcomes == outcomes


@pytest.mark.parametrize('activity', list(Activity))
async def test_results_are_returned_on_success(settings, activity):

    def sample_fn1(**_):
        return 123

    def sample_fn2(**_):
        return 456

    registry = OperatorRegistry()
    registry._activities.append(ActivityHandler(
        fn=sample_fn1, id='id1', activity=activity,
        param=None, errors=None, timeout=None, retries=None, backoff=None,
    ))
    registry._activities.append(ActivityHandler(
        fn=sample_fn2, id='id2', activity=activity,
        param=None, errors=None, timeout=None, retries=None, backoff=None,
    ))

    results = await run_activity(
        registry=registry,
        settings=settings,
        activity=activity,
        lifecycle=all_at_once,
        indices=OperatorIndexers().indices,
        memo=Memo(),
    )

    assert set(results.keys()) == {'id1', 'id2'}
    assert results['id1'] == 123
    assert results['id2'] == 456


@pytest.mark.parametrize('activity', list(Activity))
async def test_errors_are_raised_aggregated(settings, activity):

    def sample_fn1(**_):
        raise PermanentError("boo!123")

    def sample_fn2(**_):
        raise PermanentError("boo!456")

    registry = OperatorRegistry()
    registry._activities.append(ActivityHandler(
        fn=sample_fn1, id='id1', activity=activity,
        param=None, errors=None, timeout=None, retries=None, backoff=None,
    ))
    registry._activities.append(ActivityHandler(
        fn=sample_fn2, id='id2', activity=activity,
        param=None, errors=None, timeout=None, retries=None, backoff=None,
    ))

    with pytest.raises(ActivityError) as e:
        await run_activity(
            registry=registry,
            settings=settings,
            activity=activity,
            lifecycle=all_at_once,
            indices=OperatorIndexers().indices,
            memo=Memo(),
        )

    assert set(e.value.outcomes.keys()) == {'id1', 'id2'}
    assert e.value.outcomes['id1'].final
    assert e.value.outcomes['id1'].delay is None
    assert e.value.outcomes['id1'].result is None
    assert e.value.outcomes['id1'].exception is not None
    assert e.value.outcomes['id2'].final
    assert e.value.outcomes['id2'].delay is None
    assert e.value.outcomes['id2'].result is None
    assert e.value.outcomes['id2'].exception is not None
    assert str(e.value.outcomes['id1'].exception) == "boo!123"
    assert str(e.value.outcomes['id2'].exception) == "boo!456"


@pytest.mark.parametrize('activity', list(Activity))
async def test_errors_are_cascaded_from_one_of_the_originals(settings, activity):

    def sample_fn(**_):
        raise PermanentError("boo!")

    registry = OperatorRegistry()
    registry._activities.append(ActivityHandler(
        fn=sample_fn, id='id', activity=activity,
        param=None, errors=None, timeout=None, retries=None, backoff=None,
    ))

    with pytest.raises(ActivityError) as e:
        await run_activity(
            registry=registry,
            settings=settings,
            activity=activity,
            lifecycle=all_at_once,
            indices=OperatorIndexers().indices,
            memo=Memo(),
        )

    assert e.value.__cause__
    assert type(e.value.__cause__) is PermanentError
    assert str(e.value.__cause__) == "boo!"


@pytest.mark.parametrize('activity', list(Activity))
async def test_retries_are_simulated(settings, activity, mocker):
    mock = mocker.MagicMock()

    def sample_fn(**_):
        mock()
        raise TemporaryError('to be retried', delay=0)

    registry = OperatorRegistry()
    registry._activities.append(ActivityHandler(
        fn=sample_fn, id='id', activity=activity,
        param=None, errors=None, timeout=None, retries=3, backoff=None,
    ))

    with pytest.raises(ActivityError) as e:
        await run_activity(
            registry=registry,
            settings=settings,
            activity=activity,
            lifecycle=all_at_once,
            indices=OperatorIndexers().indices,
            memo=Memo(),
        )

    assert isinstance(e.value.outcomes['id'].exception, PermanentError)
    assert mock.call_count == 3


@pytest.mark.parametrize('activity', list(Activity))
async def test_delays_are_simulated(settings, activity, mocker):

    def sample_fn(**_):
        raise TemporaryError('to be retried', delay=123)

    registry = OperatorRegistry()
    registry._activities.append(ActivityHandler(
        fn=sample_fn, id='id', activity=activity,
        param=None, errors=None, timeout=None, retries=3, backoff=None,
    ))

    with freezegun.freeze_time() as frozen:

        async def sleep_substitute(*_, **__):
            frozen.tick(123)

        sleep = mocker.patch('kopf._cogs.aiokits.aiotime.sleep', wraps=sleep_substitute)

        with pytest.raises(ActivityError) as e:
            await run_activity(
                registry=registry,
                settings=settings,
                activity=activity,
                lifecycle=all_at_once,
                indices=OperatorIndexers().indices,
                memo=Memo(),
            )

    assert sleep.call_count >= 3  # 3 retries, 1 sleep each
    assert sleep.call_count <= 4  # 3 retries, 1 final success (delay=None), not more
    if sleep.call_count > 3:
        sleep.call_args_list[-1][0][0] is None



================================================
FILE: tests/handling/test_cause_handling.py
================================================
import asyncio
import logging

import pytest

import kopf
from kopf._cogs.structs.ephemera import Memo
from kopf._core.engines.indexing import OperatorIndexers
from kopf._core.intents.causes import Reason
from kopf._core.reactor.inventory import ResourceMemories
from kopf._core.reactor.processing import process_resource_event

LAST_SEEN_ANNOTATION = 'kopf.zalando.org/last-handled-configuration'

EVENT_TYPES = [None, 'ADDED', 'MODIFIED', 'DELETED']
EVENT_TYPES_WHEN_EXISTS = [None, 'ADDED', 'MODIFIED']


@pytest.mark.parametrize('event_type', EVENT_TYPES_WHEN_EXISTS)
async def test_create(registry, settings, handlers, resource, cause_mock, event_type,
                      caplog, assert_logs, k8s_mocked):
    caplog.set_level(logging.DEBUG)
    cause_mock.reason = Reason.CREATE

    event_queue = asyncio.Queue()
    await process_resource_event(
        lifecycle=kopf.lifecycles.all_at_once,
        registry=registry,
        settings=settings,
        resource=resource,
        indexers=OperatorIndexers(),
        memories=ResourceMemories(),
        memobase=Memo(),
        raw_event={'type': event_type, 'object': {}},
        event_queue=event_queue,
    )

    assert handlers.create_mock.call_count == 1
    assert not handlers.update_mock.called
    assert not handlers.delete_mock.called

    assert k8s_mocked.sleep.call_count == 0
    assert k8s_mocked.patch.call_count == 1
    assert not event_queue.empty()

    patch = k8s_mocked.patch.call_args_list[0][1]['payload']
    assert 'metadata' in patch
    assert 'annotations' in patch['metadata']
    assert LAST_SEEN_ANNOTATION in patch['metadata']['annotations']

    assert_logs([
        "Creation is in progress:",
        "Handler 'create_fn' is invoked",
        "Handler 'create_fn' succeeded",
        "Creation is processed:",
        "Patching with",
    ])


@pytest.mark.parametrize('event_type', EVENT_TYPES_WHEN_EXISTS)
async def test_update(registry, settings, handlers, resource, cause_mock, event_type,
                      caplog, assert_logs, k8s_mocked):
    caplog.set_level(logging.DEBUG)
    cause_mock.reason = Reason.UPDATE

    event_queue = asyncio.Queue()
    await process_resource_event(
        lifecycle=kopf.lifecycles.all_at_once,
        registry=registry,
        settings=settings,
        resource=resource,
        indexers=OperatorIndexers(),
        memories=ResourceMemories(),
        memobase=Memo(),
        raw_event={'type': event_type, 'object': {}},
        event_queue=event_queue,
    )

    assert not handlers.create_mock.called
    assert handlers.update_mock.call_count == 1
    assert not handlers.delete_mock.called

    assert k8s_mocked.sleep.call_count == 0
    assert k8s_mocked.patch.call_count == 1
    assert not event_queue.empty()

    patch = k8s_mocked.patch.call_args_list[0][1]['payload']
    assert 'metadata' in patch
    assert 'annotations' in patch['metadata']
    assert LAST_SEEN_ANNOTATION in patch['metadata']['annotations']

    assert_logs([
        "Updating is in progress:",
        "Handler 'update_fn' is invoked",
        "Handler 'update_fn' succeeded",
        "Updating is processed:",
        "Patching with",
    ])


@pytest.mark.parametrize('event_type', EVENT_TYPES_WHEN_EXISTS)
async def test_delete(registry, settings, handlers, resource, cause_mock, event_type,
                      caplog, assert_logs, k8s_mocked):
    caplog.set_level(logging.DEBUG)
    cause_mock.reason = Reason.DELETE
    finalizer = settings.persistence.finalizer
    event_body = {'metadata': {'deletionTimestamp': '...', 'finalizers': [finalizer]}}

    event_queue = asyncio.Queue()
    await process_resource_event(
        lifecycle=kopf.lifecycles.all_at_once,
        registry=registry,
        settings=settings,
        resource=resource,
        indexers=OperatorIndexers(),
        memories=ResourceMemories(),
        memobase=Memo(),
        raw_event={'type': event_type, 'object': event_body},
        event_queue=event_queue,
    )

    assert not handlers.create_mock.called
    assert not handlers.update_mock.called
    assert handlers.delete_mock.call_count == 1

    assert k8s_mocked.sleep.call_count == 0
    assert k8s_mocked.patch.call_count == 1
    assert not event_queue.empty()

    assert_logs([
        "Deletion is in progress:",
        "Handler 'delete_fn' is invoked",
        "Handler 'delete_fn' succeeded",
        "Deletion is processed:",
        "Removing the finalizer",
        "Patching with",
    ])


#
# Informational causes: just log, and do nothing else.
#

@pytest.mark.parametrize('event_type', EVENT_TYPES)
async def test_gone(registry, settings, handlers, resource, cause_mock, event_type,
                    caplog, assert_logs, k8s_mocked):
    caplog.set_level(logging.DEBUG)
    cause_mock.reason = Reason.GONE

    event_queue = asyncio.Queue()
    await process_resource_event(
        lifecycle=kopf.lifecycles.all_at_once,
        registry=registry,
        settings=settings,
        resource=resource,
        indexers=OperatorIndexers(),
        memories=ResourceMemories(),
        memobase=Memo(),
        raw_event={'type': event_type, 'object': {}},
        event_queue=event_queue,
    )

    assert not handlers.create_mock.called
    assert not handlers.update_mock.called
    assert not handlers.delete_mock.called

    assert not k8s_mocked.patch.called
    assert event_queue.empty()

    assert_logs([
        "Deleted, really deleted",
    ])


@pytest.mark.parametrize('event_type', EVENT_TYPES)
async def test_free(registry, settings, handlers, resource, cause_mock, event_type,
                    caplog, assert_logs, k8s_mocked):
    caplog.set_level(logging.DEBUG)
    cause_mock.reason = Reason.FREE

    event_queue = asyncio.Queue()
    await process_resource_event(
        lifecycle=kopf.lifecycles.all_at_once,
        registry=registry,
        settings=settings,
        resource=resource,
        indexers=OperatorIndexers(),
        memories=ResourceMemories(),
        memobase=Memo(),
        raw_event={'type': event_type, 'object': {}},
        event_queue=event_queue,
    )

    assert not handlers.create_mock.called
    assert not handlers.update_mock.called
    assert not handlers.delete_mock.called

    assert not k8s_mocked.sleep.called
    assert not k8s_mocked.patch.called
    assert event_queue.empty()

    assert_logs([
        "Deletion, but we are done with it",
    ])


@pytest.mark.parametrize('event_type', EVENT_TYPES)
async def test_noop(registry, settings, handlers, resource, cause_mock, event_type,
                    caplog, assert_logs, k8s_mocked):
    caplog.set_level(logging.DEBUG)
    cause_mock.reason = Reason.NOOP

    event_queue = asyncio.Queue()
    await process_resource_event(
        lifecycle=kopf.lifecycles.all_at_once,
        registry=registry,
        settings=settings,
        resource=resource,
        indexers=OperatorIndexers(),
        memories=ResourceMemories(),
        memobase=Memo(),
        raw_event={'type': event_type, 'object': {}},
        event_queue=event_queue,
    )

    assert not handlers.create_mock.called
    assert not handlers.update_mock.called
    assert not handlers.delete_mock.called

    assert not k8s_mocked.sleep.called
    assert not k8s_mocked.patch.called
    assert event_queue.empty()

    assert_logs([
        "Something has changed, but we are not interested",
    ])



================================================
FILE: tests/handling/test_cause_logging.py
================================================
import asyncio
import logging

import freezegun
import iso8601
import pytest

import kopf
from kopf._cogs.configs.progress import StatusProgressStorage
from kopf._cogs.structs.ephemera import Memo
from kopf._core.engines.indexing import OperatorIndexers
from kopf._core.intents.causes import ALL_REASONS, HANDLER_REASONS, Reason
from kopf._core.reactor.inventory import ResourceMemories
from kopf._core.reactor.processing import process_resource_event


@pytest.mark.parametrize('cause_type', ALL_REASONS)
async def test_all_logs_are_prefixed(registry, settings, resource, namespace, handlers,
                                     logstream, cause_type, cause_mock):
    event_type = None if cause_type == Reason.RESUME else 'irrelevant'
    event_body = {'metadata': {'namespace': namespace, 'name': 'name1'}}
    cause_mock.reason = cause_type

    await process_resource_event(
        lifecycle=kopf.lifecycles.all_at_once,
        registry=registry,
        settings=settings,
        resource=resource,
        indexers=OperatorIndexers(),
        memories=ResourceMemories(),
        memobase=Memo(),
        raw_event={'type': event_type, 'object': event_body},
        event_queue=asyncio.Queue(),
    )

    lines = logstream.getvalue().splitlines()
    assert lines  # no messages means that we cannot test it
    if namespace:
        assert all(line.startswith(f'prefix [{namespace}/name1] ') for line in lines)
    else:
        assert all(line.startswith('prefix [name1] ') for line in lines)


@pytest.mark.parametrize('diff', [
    pytest.param((('op', ('field',), 'old', 'new'),), id='realistic-diff'),
])
@pytest.mark.parametrize('cause_type', HANDLER_REASONS)
async def test_diffs_logged_if_present(registry, settings, resource, handlers,
                                       cause_type, cause_mock, caplog, assert_logs, diff):
    caplog.set_level(logging.DEBUG)

    event_type = None if cause_type == Reason.RESUME else 'irrelevant'
    cause_mock.reason = cause_type
    cause_mock.diff = diff
    cause_mock.new = {'field': 'old'}  # checked for `not None`, and JSON-serialised
    cause_mock.old = {'field': 'new'}  # checked for `not None`, and JSON-serialised

    await process_resource_event(
        lifecycle=kopf.lifecycles.all_at_once,
        registry=registry,
        settings=settings,
        resource=resource,
        indexers=OperatorIndexers(),
        memories=ResourceMemories(),
        memobase=Memo(),
        raw_event={'type': event_type, 'object': {}},
        event_queue=asyncio.Queue(),
    )
    assert_logs([
        "(Creation|Updating|Resuming|Deletion) is in progress: ",
        "(Creation|Updating|Resuming|Deletion) diff: "
    ])


@pytest.mark.parametrize('cause_type', HANDLER_REASONS)
@pytest.mark.parametrize('diff', [
    pytest.param(None, id='none-diff'),  # same as the default, but for clarity
    pytest.param([], id='empty-list-diff'),
    pytest.param((), id='empty-tuple-diff'),
])
async def test_diffs_not_logged_if_absent(registry, settings, resource, handlers, cause_type, cause_mock,
                                          caplog, assert_logs, diff):
    caplog.set_level(logging.DEBUG)

    event_type = None if cause_type == Reason.RESUME else 'irrelevant'
    cause_mock.reason = cause_type
    cause_mock.diff = diff

    await process_resource_event(
        lifecycle=kopf.lifecycles.all_at_once,
        registry=registry,
        settings=settings,
        resource=resource,
        indexers=OperatorIndexers(),
        memories=ResourceMemories(),
        memobase=Memo(),
        raw_event={'type': event_type, 'object': {}},
        event_queue=asyncio.Queue(),
    )
    assert_logs([
        "(Creation|Updating|Resuming|Deletion) is in progress: ",
    ], prohibited=[
        " diff: "
    ])



# Timestamps: time zero (0), before (B), after (A), and time zero+1s (1).
TS0 = iso8601.parse_date('2020-12-31T23:59:59.123456')
TS1_ISO = '2021-01-01T00:00:00.123456'


@pytest.mark.parametrize('cause_types', [
    # All combinations except for same-to-same (it is not an "extra" then).
    (a, b) for a in HANDLER_REASONS for b in HANDLER_REASONS if a != b
])
@freezegun.freeze_time(TS0)
async def test_supersession_is_logged(
        registry, settings, resource, handlers, cause_types, cause_mock, caplog, assert_logs):
    caplog.set_level(logging.DEBUG)

    settings.persistence.progress_storage = StatusProgressStorage()
    body = {'status': {'kopf': {'progress': {
        'create_fn': {'purpose': cause_types[0]},
        'update_fn': {'purpose': cause_types[0]},
        'resume_fn': {'purpose': cause_types[0]},
        'delete_fn': {'purpose': cause_types[0]},
    }}}}

    cause_mock.reason = cause_types[1]
    event_type = None if cause_types[1] == Reason.RESUME else 'irrelevant'

    await process_resource_event(
        lifecycle=kopf.lifecycles.all_at_once,
        registry=registry,
        settings=settings,
        resource=resource,
        indexers=OperatorIndexers(),
        memories=ResourceMemories(),
        memobase=Memo(),
        raw_event={'type': event_type, 'object': body},
        event_queue=asyncio.Queue(),
    )
    assert_logs([
        "(Creation|Updating|Resuming|Deletion) is superseded by (creation|updating|resuming|deletion): ",
        "(Creation|Updating|Resuming|Deletion) is in progress: ",
        "(Creation|Updating|Resuming|Deletion) is processed: ",
    ])



================================================
FILE: tests/handling/test_delays.py
================================================
import asyncio
import logging

import freezegun
import iso8601
import pytest

import kopf
from kopf._cogs.structs.ephemera import Memo
from kopf._core.actions.application import WAITING_KEEPALIVE_INTERVAL
from kopf._core.actions.execution import TemporaryError
from kopf._core.actions.progression import HandlerState
from kopf._core.engines.indexing import OperatorIndexers
from kopf._core.intents.causes import HANDLER_REASONS, Reason
from kopf._core.reactor.inventory import ResourceMemories
from kopf._core.reactor.processing import process_resource_event


@pytest.mark.parametrize('cause_reason', HANDLER_REASONS)
@pytest.mark.parametrize('now, delayed_iso, delay', [
    ['2020-01-01T00:00:00', '2020-01-01T00:04:56.789000+00:00', 4 * 60 + 56.789],
], ids=['fast'])
async def test_delayed_handlers_progress(
        registry, settings, handlers, resource, cause_mock, cause_reason,
        caplog, assert_logs, k8s_mocked, now, delayed_iso, delay):
    caplog.set_level(logging.DEBUG)

    handlers.create_mock.side_effect = TemporaryError("oops", delay=delay)
    handlers.update_mock.side_effect = TemporaryError("oops", delay=delay)
    handlers.delete_mock.side_effect = TemporaryError("oops", delay=delay)
    handlers.resume_mock.side_effect = TemporaryError("oops", delay=delay)

    event_type = None if cause_reason == Reason.RESUME else 'irrelevant'
    cause_mock.reason = cause_reason

    with freezegun.freeze_time(now):
        await process_resource_event(
            lifecycle=kopf.lifecycles.all_at_once,
            registry=registry,
            settings=settings,
            resource=resource,
            indexers=OperatorIndexers(),
            memories=ResourceMemories(),
            memobase=Memo(),
            raw_event={'type': event_type, 'object': {}},
            event_queue=asyncio.Queue(),
        )

    assert handlers.create_mock.call_count == (1 if cause_reason == Reason.CREATE else 0)
    assert handlers.update_mock.call_count == (1 if cause_reason == Reason.UPDATE else 0)
    assert handlers.delete_mock.call_count == (1 if cause_reason == Reason.DELETE else 0)
    assert handlers.resume_mock.call_count == (1 if cause_reason == Reason.RESUME else 0)

    assert not k8s_mocked.sleep.called
    assert k8s_mocked.patch.called

    fname = f'{cause_reason}_fn'
    patch = k8s_mocked.patch.call_args_list[0][1]['payload']
    assert patch['status']['kopf']['progress'][fname]['delayed'] == delayed_iso

    assert_logs([
        "Handler .+ is invoked",
        "Handler .+ failed temporarily: oops",
    ])


@pytest.mark.parametrize('cause_reason', HANDLER_REASONS)
@pytest.mark.parametrize('now, delayed_iso, delay', [
    ['2020-01-01T00:00:00', '2020-01-01T00:04:56.789000+00:00', 4 * 60 + 56.789],
    ['2020-01-01T00:00:00', '2099-12-31T23:59:59.000000+00:00', WAITING_KEEPALIVE_INTERVAL],
], ids=['fast', 'slow'])
async def test_delayed_handlers_sleep(
        registry, settings, handlers, resource, cause_mock, cause_reason,
        caplog, assert_logs, k8s_mocked, now, delayed_iso, delay):
    caplog.set_level(logging.DEBUG)

    # Simulate the original persisted state of the resource.
    # Make sure the finalizer is added since there are mandatory deletion handlers.
    started_dt = iso8601.parse_date('2000-01-01T00:00:00')  # long time ago is fine.
    delayed_dt = iso8601.parse_date(delayed_iso)
    event_type = None if cause_reason == Reason.RESUME else 'irrelevant'
    event_body = {
        'metadata': {'finalizers': [settings.persistence.finalizer]},
        'status': {'kopf': {'progress': {
            'create_fn': HandlerState(started=started_dt, delayed=delayed_dt).as_in_storage(),
            'update_fn': HandlerState(started=started_dt, delayed=delayed_dt).as_in_storage(),
            'delete_fn': HandlerState(started=started_dt, delayed=delayed_dt).as_in_storage(),
            'resume_fn': HandlerState(started=started_dt, delayed=delayed_dt).as_in_storage(),
        }}}
    }
    cause_mock.reason = cause_reason

    with freezegun.freeze_time(now):
        await process_resource_event(
            lifecycle=kopf.lifecycles.all_at_once,
            registry=registry,
            settings=settings,
            resource=resource,
            indexers=OperatorIndexers(),
            memories=ResourceMemories(),
            memobase=Memo(),
            raw_event={'type': event_type, 'object': event_body},
            event_queue=asyncio.Queue(),
        )

    assert not handlers.create_mock.called
    assert not handlers.update_mock.called
    assert not handlers.delete_mock.called
    assert not handlers.resume_mock.called

    # The dummy patch is needed to trigger the further changes. The value is irrelevant.
    assert k8s_mocked.patch.called
    assert 'dummy' in k8s_mocked.patch.call_args_list[-1][1]['payload']['status']['kopf']

    # The duration of sleep should be as expected.
    assert k8s_mocked.sleep.called
    assert k8s_mocked.sleep.call_args_list[0][0][0] == delay

    assert_logs([
        r"Sleeping for ([\d\.]+|[\d\.]+ \(capped [\d\.]+\)) seconds",
    ])



================================================
FILE: tests/handling/test_error_handling.py
================================================
import asyncio
import logging

import pytest

import kopf
from kopf._cogs.structs.ephemera import Memo
from kopf._core.actions.execution import PermanentError, TemporaryError
from kopf._core.engines.indexing import OperatorIndexers
from kopf._core.intents.causes import HANDLER_REASONS, Reason
from kopf._core.reactor.inventory import ResourceMemories
from kopf._core.reactor.processing import process_resource_event


# The extrahandlers are needed to prevent the cycle ending and status purging.
@pytest.mark.parametrize('cause_type', HANDLER_REASONS)
async def test_fatal_error_stops_handler(
        registry, settings, handlers, extrahandlers, resource, cause_mock, cause_type,
        caplog, assert_logs, k8s_mocked):
    caplog.set_level(logging.DEBUG)
    name1 = f'{cause_type}_fn'

    event_type = None if cause_type == Reason.RESUME else 'irrelevant'
    cause_mock.reason = cause_type
    handlers.create_mock.side_effect = PermanentError("oops")
    handlers.update_mock.side_effect = PermanentError("oops")
    handlers.delete_mock.side_effect = PermanentError("oops")
    handlers.resume_mock.side_effect = PermanentError("oops")

    await process_resource_event(
        lifecycle=kopf.lifecycles.one_by_one,
        registry=registry,
        settings=settings,
        resource=resource,
        indexers=OperatorIndexers(),
        memories=ResourceMemories(),
        memobase=Memo(),
        raw_event={'type': event_type, 'object': {}},
        event_queue=asyncio.Queue(),
    )

    assert handlers.create_mock.call_count == (1 if cause_type == Reason.CREATE else 0)
    assert handlers.update_mock.call_count == (1 if cause_type == Reason.UPDATE else 0)
    assert handlers.delete_mock.call_count == (1 if cause_type == Reason.DELETE else 0)
    assert handlers.resume_mock.call_count == (1 if cause_type == Reason.RESUME else 0)

    assert not k8s_mocked.sleep.called
    assert k8s_mocked.patch.called

    patch = k8s_mocked.patch.call_args_list[0][1]['payload']
    assert patch['status']['kopf']['progress'] is not None
    assert patch['status']['kopf']['progress'][name1]['failure'] is True
    assert patch['status']['kopf']['progress'][name1]['message'] == 'oops'

    assert_logs([
        "Handler .+ failed permanently: oops",
    ])


# The extrahandlers are needed to prevent the cycle ending and status purging.
@pytest.mark.parametrize('cause_type', HANDLER_REASONS)
async def test_retry_error_delays_handler(
        registry, settings, handlers, extrahandlers, resource, cause_mock, cause_type,
        caplog, assert_logs, k8s_mocked):
    caplog.set_level(logging.DEBUG)
    name1 = f'{cause_type}_fn'

    event_type = None if cause_type == Reason.RESUME else 'irrelevant'
    cause_mock.reason = cause_type
    handlers.create_mock.side_effect = TemporaryError("oops")
    handlers.update_mock.side_effect = TemporaryError("oops")
    handlers.delete_mock.side_effect = TemporaryError("oops")
    handlers.resume_mock.side_effect = TemporaryError("oops")

    await process_resource_event(
        lifecycle=kopf.lifecycles.one_by_one,
        registry=registry,
        settings=settings,
        resource=resource,
        indexers=OperatorIndexers(),
        memories=ResourceMemories(),
        memobase=Memo(),
        raw_event={'type': event_type, 'object': {}},
        event_queue=asyncio.Queue(),
    )

    assert handlers.create_mock.call_count == (1 if cause_type == Reason.CREATE else 0)
    assert handlers.update_mock.call_count == (1 if cause_type == Reason.UPDATE else 0)
    assert handlers.delete_mock.call_count == (1 if cause_type == Reason.DELETE else 0)
    assert handlers.resume_mock.call_count == (1 if cause_type == Reason.RESUME else 0)

    assert not k8s_mocked.sleep.called
    assert k8s_mocked.patch.called

    patch = k8s_mocked.patch.call_args_list[0][1]['payload']
    assert patch['status']['kopf']['progress'] is not None
    assert patch['status']['kopf']['progress'][name1]['failure'] is False
    assert patch['status']['kopf']['progress'][name1]['success'] is False
    assert patch['status']['kopf']['progress'][name1]['delayed']

    assert_logs([
        "Handler .+ failed temporarily: oops",
    ])


# The extrahandlers are needed to prevent the cycle ending and status purging.
@pytest.mark.parametrize('cause_type', HANDLER_REASONS)
async def test_arbitrary_error_delays_handler(
        registry, settings, handlers, extrahandlers, resource, cause_mock, cause_type,
        caplog, assert_logs, k8s_mocked):
    caplog.set_level(logging.DEBUG)
    name1 = f'{cause_type}_fn'

    event_type = None if cause_type == Reason.RESUME else 'irrelevant'
    cause_mock.reason = cause_type
    handlers.create_mock.side_effect = Exception("oops")
    handlers.update_mock.side_effect = Exception("oops")
    handlers.delete_mock.side_effect = Exception("oops")
    handlers.resume_mock.side_effect = Exception("oops")

    await process_resource_event(
        lifecycle=kopf.lifecycles.one_by_one,
        registry=registry,
        settings=settings,
        resource=resource,
        indexers=OperatorIndexers(),
        memories=ResourceMemories(),
        memobase=Memo(),
        raw_event={'type': event_type, 'object': {}},
        event_queue=asyncio.Queue(),
    )

    assert handlers.create_mock.call_count == (1 if cause_type == Reason.CREATE else 0)
    assert handlers.update_mock.call_count == (1 if cause_type == Reason.UPDATE else 0)
    assert handlers.delete_mock.call_count == (1 if cause_type == Reason.DELETE else 0)
    assert handlers.resume_mock.call_count == (1 if cause_type == Reason.RESUME else 0)

    assert not k8s_mocked.sleep.called
    assert k8s_mocked.patch.called

    patch = k8s_mocked.patch.call_args_list[0][1]['payload']
    assert patch['status']['kopf']['progress'] is not None
    assert patch['status']['kopf']['progress'][name1]['failure'] is False
    assert patch['status']['kopf']['progress'][name1]['success'] is False
    assert patch['status']['kopf']['progress'][name1]['delayed']

    assert_logs([
        "Handler .+ failed with an exception. Will retry.",
    ])



================================================
FILE: tests/handling/test_event_handling.py
================================================
import asyncio
import logging

import pytest

import kopf
from kopf._cogs.structs.ephemera import Memo
from kopf._core.engines.indexing import OperatorIndexers
from kopf._core.intents.causes import ALL_REASONS
from kopf._core.reactor.inventory import ResourceMemories
from kopf._core.reactor.processing import process_resource_event


@pytest.mark.parametrize('cause_type', ALL_REASONS)
async def test_handlers_called_always(
        registry, settings, handlers, extrahandlers, resource, cause_mock, cause_type,
        caplog, assert_logs, k8s_mocked):
    caplog.set_level(logging.DEBUG)
    cause_mock.reason = cause_type

    await process_resource_event(
        lifecycle=kopf.lifecycles.all_at_once,
        registry=registry,
        settings=settings,
        resource=resource,
        indexers=OperatorIndexers(),
        memories=ResourceMemories(),
        memobase=Memo(),
        raw_event={'type': 'ev-type', 'object': {'field': 'value'}},
        event_queue=asyncio.Queue(),
    )

    assert handlers.event_mock.call_count == 1
    assert extrahandlers.event_mock.call_count == 1

    event = handlers.event_mock.call_args_list[0][1]['event']
    assert 'field' in event['object']
    assert event['object']['field'] == 'value'
    assert event['type'] == 'ev-type'

    assert_logs([
        "Handler 'event_fn' is invoked.",
        "Handler 'event_fn' succeeded.",
        "Handler 'event_fn2' is invoked.",
        "Handler 'event_fn2' succeeded.",
    ])


@pytest.mark.parametrize('cause_type', ALL_REASONS)
async def test_errors_are_ignored(
        registry, settings, handlers, extrahandlers, resource, cause_mock, cause_type,
        caplog, assert_logs, k8s_mocked):
    caplog.set_level(logging.DEBUG)
    cause_mock.reason = cause_type
    handlers.event_mock.side_effect = Exception("oops")

    await process_resource_event(
        lifecycle=kopf.lifecycles.all_at_once,
        registry=registry,
        settings=settings,
        resource=resource,
        indexers=OperatorIndexers(),
        memories=ResourceMemories(),
        memobase=Memo(),
        raw_event={'type': 'ev-type', 'object': {}},
        event_queue=asyncio.Queue(),
    )

    assert handlers.event_mock.called
    assert extrahandlers.event_mock.called

    assert_logs([
        "Handler 'event_fn' is invoked.",
        "Handler 'event_fn' failed with an exception. Will ignore.",
        "Handler 'event_fn2' is invoked.",
        "Handler 'event_fn2' succeeded.",
    ])



================================================
FILE: tests/handling/test_multistep.py
================================================
import asyncio

import pytest

import kopf
from kopf._cogs.structs.ephemera import Memo
from kopf._core.engines.indexing import OperatorIndexers
from kopf._core.intents.causes import HANDLER_REASONS, Reason
from kopf._core.reactor.inventory import ResourceMemories
from kopf._core.reactor.processing import process_resource_event


@pytest.mark.parametrize('deletion_ts', [
    pytest.param({}, id='no-deletion-ts'),
    pytest.param({'deletionTimestamp': None}, id='empty-deletion-ts'),
    pytest.param({'deletionTimestamp': 'some'}, id='real-deletion-ts'),
])
@pytest.mark.parametrize('cause_type', HANDLER_REASONS)
async def test_1st_step_stores_progress_by_patching(
        registry, settings, handlers, extrahandlers,
        resource, cause_mock, cause_type, k8s_mocked, deletion_ts):
    name1 = f'{cause_type}_fn'
    name2 = f'{cause_type}_fn2'

    event_type = None if cause_type == Reason.RESUME else 'irrelevant'
    event_body = {
        'metadata': {'finalizers': [settings.persistence.finalizer]},
    }
    event_body['metadata'] |= deletion_ts
    cause_mock.reason = cause_type

    await process_resource_event(
        lifecycle=kopf.lifecycles.asap,
        registry=registry,
        settings=settings,
        resource=resource,
        indexers=OperatorIndexers(),
        memories=ResourceMemories(),
        memobase=Memo(),
        raw_event={'type': event_type, 'object': event_body},
        event_queue=asyncio.Queue(),
    )

    assert handlers.create_mock.call_count == (1 if cause_type == Reason.CREATE else 0)
    assert handlers.update_mock.call_count == (1 if cause_type == Reason.UPDATE else 0)
    assert handlers.delete_mock.call_count == (1 if cause_type == Reason.DELETE else 0)
    assert handlers.resume_mock.call_count == (1 if cause_type == Reason.RESUME else 0)

    assert not k8s_mocked.sleep.called
    assert k8s_mocked.patch.called

    patch = k8s_mocked.patch.call_args_list[0][1]['payload']
    assert patch['status']['kopf']['progress'] is not None

    assert patch['status']['kopf']['progress'][name1]['retries'] == 1
    assert patch['status']['kopf']['progress'][name1]['success'] is True

    assert patch['status']['kopf']['progress'][name2]['retries'] == 0
    assert patch['status']['kopf']['progress'][name2]['success'] is False

    assert patch['status']['kopf']['progress'][name1]['started']
    assert patch['status']['kopf']['progress'][name2]['started']

    # Premature removal of finalizers can prevent the 2nd step for deletion handlers.
    # So, the finalizers must never be removed on the 1st step.
    assert 'finalizers' not in patch['metadata']


@pytest.mark.parametrize('deletion_ts', [
    pytest.param({}, id='no-deletion-ts'),
    pytest.param({'deletionTimestamp': None}, id='empty-deletion-ts'),
    pytest.param({'deletionTimestamp': 'some'}, id='real-deletion-ts'),
])
@pytest.mark.parametrize('cause_type', HANDLER_REASONS)
async def test_2nd_step_finishes_the_handlers(caplog,
        registry, settings, handlers, extrahandlers,
        resource, cause_mock, cause_type, k8s_mocked, deletion_ts):
    name1 = f'{cause_type}_fn'
    name2 = f'{cause_type}_fn2'

    event_type = None if cause_type == Reason.RESUME else 'irrelevant'
    event_body = {
        'metadata': {'finalizers': [settings.persistence.finalizer]},
        'status': {'kopf': {'progress': {
            name1: {'started': '1979-01-01T00:00:00', 'success': True},
            name2: {'started': '1979-01-01T00:00:00'},
        }}}
    }
    event_body['metadata'] |= deletion_ts
    cause_mock.reason = cause_type

    await process_resource_event(
        lifecycle=kopf.lifecycles.one_by_one,
        registry=registry,
        settings=settings,
        resource=resource,
        indexers=OperatorIndexers(),
        memories=ResourceMemories(),
        memobase=Memo(),
        raw_event={'type': event_type, 'object': event_body},
        event_queue=asyncio.Queue(),
    )

    assert extrahandlers.create_mock.call_count == (1 if cause_type == Reason.CREATE else 0)
    assert extrahandlers.update_mock.call_count == (1 if cause_type == Reason.UPDATE else 0)
    assert extrahandlers.delete_mock.call_count == (1 if cause_type == Reason.DELETE else 0)
    assert extrahandlers.resume_mock.call_count == (1 if cause_type == Reason.RESUME else 0)

    assert not k8s_mocked.sleep.called
    assert k8s_mocked.patch.called

    patch = k8s_mocked.patch.call_args_list[0][1]['payload']
    assert patch['status']['kopf']['progress'] == {name1: None, name2: None}

    # Finalizers could be removed for resources being deleted on the 2nd step.
    # The logic can vary though: either by deletionTimestamp, or by reason==DELETE.
    if deletion_ts and deletion_ts['deletionTimestamp']:
        assert patch['metadata']['finalizers'] == []



================================================
FILE: tests/handling/test_no_handlers.py
================================================
import asyncio
import logging

import pytest

import kopf
from kopf._cogs.structs.ephemera import Memo
from kopf._core.engines.indexing import OperatorIndexers
from kopf._core.intents.causes import HANDLER_REASONS
from kopf._core.intents.handlers import ChangingHandler
from kopf._core.reactor.inventory import ResourceMemories
from kopf._core.reactor.processing import process_resource_event

LAST_SEEN_ANNOTATION = 'kopf.zalando.org/last-handled-configuration'


@pytest.mark.parametrize('cause_type', HANDLER_REASONS)
async def test_skipped_with_no_handlers(
        registry, settings, selector, resource, cause_mock, cause_type,
        caplog, assert_logs, k8s_mocked):
    caplog.set_level(logging.DEBUG)

    event_type = None
    event_body = {'metadata': {'finalizers': []}}
    cause_mock.reason = cause_type

    assert not registry._changing.has_handlers(resource=resource)  # prerequisite
    registry._changing.append(ChangingHandler(
        reason='a-non-existent-cause-type',
        fn=lambda **_: None, id='id', param=None,
        errors=None, timeout=None, retries=None, backoff=None,
        selector=selector, annotations=None, labels=None, when=None,
        field=None, value=None, old=None, new=None, field_needs_change=None,
        deleted=None, initial=None, requires_finalizer=None,
    ))

    await process_resource_event(
        lifecycle=kopf.lifecycles.all_at_once,
        registry=registry,
        settings=settings,
        resource=resource,
        indexers=OperatorIndexers(),
        memories=ResourceMemories(),
        memobase=Memo(),
        raw_event={'type': event_type, 'object': event_body},
        event_queue=asyncio.Queue(),
    )

    assert not k8s_mocked.sleep.called
    assert k8s_mocked.patch.called

    # The patch must contain ONLY the last-seen update, and nothing else.
    patch = k8s_mocked.patch.call_args_list[0][1]['payload']
    assert set(patch.keys()) == {'metadata'}
    assert set(patch['metadata'].keys()) == {'annotations'}
    assert set(patch['metadata']['annotations'].keys()) == {LAST_SEEN_ANNOTATION}

    assert_logs([
        "(Creation|Updating|Resuming|Deletion) is in progress:",
        "Patching with:",
    ], prohibited=[
        "(Creation|Updating|Resuming|Deletion) is processed:",
    ])


@pytest.mark.parametrize('cause_type', HANDLER_REASONS)
@pytest.mark.parametrize('initial', [True, False, None])
@pytest.mark.parametrize('deleted', [True, False, None])
@pytest.mark.parametrize('annotations, labels, when', [
    (None, {'some-label': '...'}, None),
    ({'some-annotation': '...'}, None, None),
    (None, None, lambda **_: False),
])
async def test_stealth_mode_with_mismatching_handlers(
        registry, settings, selector, resource, cause_mock, cause_type,
        caplog, assert_logs, k8s_mocked, annotations, labels, when, deleted, initial):
    caplog.set_level(logging.DEBUG)

    event_type = None
    event_body = {'metadata': {'finalizers': []}}
    cause_mock.reason = cause_type

    assert not registry._changing.has_handlers(resource=resource)  # prerequisite
    registry._changing.append(ChangingHandler(
        reason=None,
        fn=lambda **_: None, id='id', param=None,
        errors=None, timeout=None, retries=None, backoff=None,
        selector=selector, annotations=annotations, labels=labels, when=when,
        field=None, value=None, old=None, new=None, field_needs_change=None,
        deleted=deleted, initial=initial, requires_finalizer=None,
    ))

    await process_resource_event(
        lifecycle=kopf.lifecycles.all_at_once,
        registry=registry,
        settings=settings,
        resource=resource,
        indexers=OperatorIndexers(),
        memories=ResourceMemories(),
        memobase=Memo(),
        raw_event={'type': event_type, 'object': event_body},
        event_queue=asyncio.Queue(),
    )

    assert not k8s_mocked.sleep.called
    assert not k8s_mocked.patch.called
    assert not caplog.messages  # total stealth mode!



================================================
FILE: tests/handling/test_parametrization.py
================================================
import asyncio
from unittest.mock import Mock

import kopf
from kopf._cogs.structs.ephemera import Memo
from kopf._core.engines.indexing import OperatorIndexers
from kopf._core.reactor.inventory import ResourceMemories
from kopf._core.reactor.processing import process_resource_event


async def test_parameter_is_passed_when_specified(resource, cause_mock, registry, settings):
    mock = Mock()

    # If it works for this handler, we assume it works for all of them.
    # Otherwise, it is too difficult to trigger the actual invocation.
    @kopf.on.event(*resource, param=123)
    def fn(**kwargs):
        mock(**kwargs)

    event_queue = asyncio.Queue()
    await process_resource_event(
        lifecycle=kopf.lifecycles.all_at_once,
        registry=registry,
        settings=settings,
        resource=resource,
        indexers=OperatorIndexers(),
        memories=ResourceMemories(),
        memobase=Memo(),
        raw_event={'type': None, 'object': {}},
        event_queue=event_queue,
    )

    assert mock.called
    assert mock.call_args_list[0][1]['param'] == 123


async def test_parameter_is_passed_even_if_not_specified(resource, cause_mock, registry, settings):
    mock = Mock()

    # If it works for this handler, we assume it works for all of them.
    # Otherwise, it is too difficult to trigger the actual invocation.
    @kopf.on.event(*resource)
    def fn(**kwargs):
        mock(**kwargs)

    event_queue = asyncio.Queue()
    await process_resource_event(
        lifecycle=kopf.lifecycles.all_at_once,
        registry=registry,
        settings=settings,
        resource=resource,
        indexers=OperatorIndexers(),
        memories=ResourceMemories(),
        memobase=Memo(),
        raw_event={'type': None, 'object': {}},
        event_queue=event_queue,
    )

    assert mock.called
    assert mock.call_args_list[0][1]['param'] is None



================================================
FILE: tests/handling/test_retrying_limits.py
================================================
import asyncio
import logging

import freezegun
import pytest

import kopf
from kopf._cogs.structs.ephemera import Memo
from kopf._core.engines.indexing import OperatorIndexers
from kopf._core.intents.causes import HANDLER_REASONS, Reason
from kopf._core.reactor.inventory import ResourceMemories
from kopf._core.reactor.processing import process_resource_event


# The timeout is hard-coded in conftest.py:handlers().
# The extrahandlers are needed to prevent the cycle ending and status purging.
@pytest.mark.parametrize('cause_type', HANDLER_REASONS)
@pytest.mark.parametrize('now, ts', [
    ['2099-12-31T23:59:59', '2020-01-01T00:00:00'],
], ids=['slow'])
async def test_timed_out_handler_fails(
        registry, settings, handlers, extrahandlers, resource, cause_mock, cause_type,
        caplog, assert_logs, k8s_mocked, now, ts):
    caplog.set_level(logging.DEBUG)
    name1 = f'{cause_type}_fn'

    event_type = None if cause_type == Reason.RESUME else 'irrelevant'
    event_body = {
        'status': {'kopf': {'progress': {
            'create_fn': {'started': ts},
            'update_fn': {'started': ts},
            'delete_fn': {'started': ts},
            'resume_fn': {'started': ts},
        }}}
    }
    cause_mock.reason = cause_type

    with freezegun.freeze_time(now):
        await process_resource_event(
            lifecycle=kopf.lifecycles.one_by_one,
            registry=registry,
            settings=settings,
            resource=resource,
            indexers=OperatorIndexers(),
            memories=ResourceMemories(),
            memobase=Memo(),
            raw_event={'type': event_type, 'object': event_body},
            event_queue=asyncio.Queue(),
        )

    assert not handlers.create_mock.called
    assert not handlers.update_mock.called
    assert not handlers.delete_mock.called
    assert not handlers.resume_mock.called

    # Progress is reset, as the handler is not going to retry.
    assert not k8s_mocked.sleep.called
    assert k8s_mocked.patch.called

    patch = k8s_mocked.patch.call_args_list[0][1]['payload']
    assert patch['status']['kopf']['progress'] is not None
    assert patch['status']['kopf']['progress'][name1]['failure'] is True

    assert_logs([
        "Handler .+ has timed out after",
    ])


# The limits are hard-coded in conftest.py:handlers().
# The extrahandlers are needed to prevent the cycle ending and status purging.
@pytest.mark.parametrize('cause_type', HANDLER_REASONS)
async def test_retries_limited_handler_fails(
        registry, settings, handlers, extrahandlers, resource, cause_mock, cause_type,
        caplog, assert_logs, k8s_mocked):
    caplog.set_level(logging.DEBUG)
    name1 = f'{cause_type}_fn'

    event_type = None if cause_type == Reason.RESUME else 'irrelevant'
    event_body = {
        'status': {'kopf': {'progress': {
            'create_fn': {'retries': 100},
            'update_fn': {'retries': 100},
            'delete_fn': {'retries': 100},
            'resume_fn': {'retries': 100},
        }}}
    }
    cause_mock.reason = cause_type

    await process_resource_event(
        lifecycle=kopf.lifecycles.one_by_one,
        registry=registry,
        settings=settings,
        resource=resource,
        indexers=OperatorIndexers(),
        memories=ResourceMemories(),
        memobase=Memo(),
        raw_event={'type': event_type, 'object': event_body},
        event_queue=asyncio.Queue(),
    )

    assert not handlers.create_mock.called
    assert not handlers.update_mock.called
    assert not handlers.delete_mock.called
    assert not handlers.resume_mock.called

    # Progress is reset, as the handler is not going to retry.
    assert not k8s_mocked.sleep.called
    assert k8s_mocked.patch.called

    patch = k8s_mocked.patch.call_args_list[0][1]['payload']
    assert patch['status']['kopf']['progress'] is not None
    assert patch['status']['kopf']['progress'][name1]['failure'] is True

    assert_logs([
        r"Handler .+ has exceeded \d+ retries",
    ])



================================================
FILE: tests/handling/test_timing_consistency.py
================================================
import asyncio
import datetime

import freezegun
import iso8601

import kopf
from kopf._cogs.structs.ephemera import Memo
from kopf._core.engines.indexing import OperatorIndexers
from kopf._core.reactor.inventory import ResourceMemories
from kopf._core.reactor.processing import process_resource_event


async def test_consistent_awakening(registry, settings, resource, k8s_mocked, mocker):
    """
    A special case to ensure that "now" is consistent during the handling.

    Previously, "now" of `handler.awakened` and "now" of `state.delay` were
    different (maybe for less than 1 ms). If the scheduled awakening time was
    unlucky to be between these two points in time, the operator stopped
    reacting on this object until any other events or changes arrive.

    Implementation-wise, the operator neither selected the handlers (because
    it was "1ms too early", as per `handler.awakened`),
    nor did it sleep (because it was "1ms too late", as per `state.delay`),
    nor did it produce even a dummy patch (because zero-sleep meant "no sleep").

    After the fix, zero-sleep produces a dummy patch to trigger the reaction
    cycle after the sleep is over (as if it was an actual zero-time sleep).

    In the test, the time granularity is intentionally that low -- 1 µs.
    The time is anyway frozen and does not progress unless explicitly ticked.

    See also: #284
    """

    # Simulate that the object is scheduled to be awakened between the watch-event and sleep.
    ts0 = iso8601.parse_date('2019-12-30T10:56:43')
    tsA_triggered = "2019-12-30T10:56:42.999999"
    ts0_scheduled = "2019-12-30T10:56:43.000000"
    tsB_delivered = "2019-12-30T10:56:43.000001"

    # A dummy handler: it will not be selected for execution anyway, we just need to have it.
    @kopf.on.create(*resource, id='some-id')
    def handler_fn(**_):
        pass

    # Simulate the ticking of time, so that it goes beyond the scheduled awakening time.
    # Any hook point between handler selection and delay calculation is fine,
    # but State.store() also prevents other status-fields from being added and the patch populated.
    def move_to_tsB(*_, **__):
        frozen_dt.move_to(tsB_delivered)

    state_store = mocker.patch('kopf._core.actions.progression.State.store', side_effect=move_to_tsB)
    body = {'status': {'kopf': {'progress': {'some-id': {'delayed': ts0_scheduled}}}}}

    # Simulate the call as if the event has just arrived on the watch-stream.
    # Another way (the same effect): process_changing_cause() and its result.
    with freezegun.freeze_time(tsA_triggered) as frozen_dt:
        assert datetime.datetime.now(datetime.timezone.utc) < ts0  # extra precaution
        await process_resource_event(
            lifecycle=kopf.lifecycles.all_at_once,
            registry=registry,
            settings=settings,
            resource=resource,
            indexers=OperatorIndexers(),
            memories=ResourceMemories(),
            memobase=Memo(),
            raw_event={'type': 'ADDED', 'object': body},
            event_queue=asyncio.Queue(),
        )
        assert datetime.datetime.now(datetime.timezone.utc) > ts0  # extra precaution

    assert state_store.called

    # Without "now"-time consistency, neither sleep() would be called, nor a patch applied.
    # Verify that the patch was actually applied, so that the reaction cycle continues.
    assert k8s_mocked.patch.called
    assert 'dummy' in k8s_mocked.patch.call_args_list[-1][1]['payload']['status']['kopf']



================================================
FILE: tests/handling/daemons/conftest.py
================================================
import asyncio
import contextlib
import time
from unittest.mock import MagicMock, patch

import freezegun
import pytest

import kopf
from kopf._cogs.aiokits.aiotoggles import ToggleSet
from kopf._cogs.structs.bodies import RawBody
from kopf._cogs.structs.ephemera import Memo
from kopf._core.engines.daemons import daemon_killer
from kopf._core.engines.indexing import OperatorIndexers
from kopf._core.reactor.processing import process_resource_event


class DaemonDummy:

    def __init__(self):
        super().__init__()
        self.mock = MagicMock()
        self.kwargs = {}
        self.steps = {
            'called': asyncio.Event(),
            'finish': asyncio.Event(),
            'error': asyncio.Event(),
        }

    async def wait_for_daemon_done(self):
        stopped = self.kwargs['stopped']
        await stopped.wait()
        while not stopped.reason & stopped.reason.DONE:
            await asyncio.sleep(0)  # give control back to asyncio event loop


@pytest.fixture()
def dummy():
    return DaemonDummy()


@pytest.fixture()
def simulate_cycle(k8s_mocked, registry, settings, resource, memories, mocker):
    """
    Simulate K8s behaviour locally in memory (some meaningful approximation).
    """

    def _merge_dicts(src, dst):
        for key, val in src.items():
            if isinstance(val, dict) and key in dst:
                _merge_dicts(src[key], dst[key])
            else:
                dst[key] = val

    async def _simulate_cycle(event_object: RawBody):
        mocker.resetall()

        await process_resource_event(
            lifecycle=kopf.lifecycles.all_at_once,
            registry=registry,
            settings=settings,
            resource=resource,
            memories=memories,
            memobase=Memo(),
            indexers=OperatorIndexers(),
            raw_event={'type': 'irrelevant', 'object': event_object},
            event_queue=asyncio.Queue(),
        )

        # Do the same as k8s does: merge the patches into the object.
        for call in k8s_mocked.patch.call_args_list:
            _merge_dicts(call[1]['payload'], event_object)

    return _simulate_cycle


@pytest.fixture()
async def operator_paused():
    return ToggleSet(any)


@pytest.fixture()
async def conflicts_found(operator_paused: ToggleSet):
    return await operator_paused.make_toggle(name="conflicts_found fixture")


@pytest.fixture()
async def background_daemon_killer(settings, memories, operator_paused):
    """
    Run the daemon killer in the background.
    """
    task = asyncio.create_task(daemon_killer(
        settings=settings, memories=memories, operator_paused=operator_paused))
    yield task

    with contextlib.suppress(asyncio.CancelledError):
        task.cancel()
        await task


@pytest.fixture()
def frozen_time():
    """
    A helper to simulate time movements to step over long sleeps/timeouts.
    """
    # TODO LATER: Either freezegun should support the system clock, or find something else.
    with freezegun.freeze_time("2020-01-01T00:00:00") as frozen:
        # Use freezegun-supported time instead of system clocks -- for testing purposes only.
        # NB: Patch strictly after the time is frozen -- to use fake_time(), not real time().
        with patch('time.monotonic', time.time), patch('time.perf_counter', time.time):
            yield frozen


# The time-driven tests mock the sleeps, and shift the time as much as it was requested to sleep.
# This makes the sleep realistic for the app code, though executed instantly for the tests.
@pytest.fixture()
def manual_time(k8s_mocked, frozen_time):
    async def sleep_substitute(delay, *_, **__):
        if delay is None:
            pass
        elif isinstance(delay, float):
            frozen_time.tick(delay)
        else:
            frozen_time.tick(min(delay))

    k8s_mocked.sleep.side_effect = sleep_substitute
    yield frozen_time



================================================
FILE: tests/handling/daemons/test_daemon_errors.py
================================================
import logging

import kopf
from kopf._core.actions.execution import ErrorsMode, PermanentError, TemporaryError


async def test_daemon_stopped_on_permanent_error(
        settings, resource, dummy, manual_time, caplog, assert_logs, k8s_mocked, simulate_cycle):
    caplog.set_level(logging.DEBUG)

    @kopf.daemon(*resource, id='fn', backoff=0.01)
    async def fn(**kwargs):
        dummy.mock()
        dummy.kwargs = kwargs
        dummy.steps['called'].set()
        raise PermanentError("boo!")

    finalizer = settings.persistence.finalizer
    event_object = {'metadata': {'finalizers': [finalizer]}}
    await simulate_cycle(event_object)

    await dummy.steps['called'].wait()
    await dummy.wait_for_daemon_done()

    assert dummy.mock.call_count == 1
    assert k8s_mocked.patch.call_count == 0
    assert k8s_mocked.sleep.call_count == 0

    assert_logs([
        "Daemon 'fn' failed permanently: boo!",
        "Daemon 'fn' has exited on its own",
    ], prohibited=[
        "Daemon 'fn' succeeded.",
    ])


async def test_daemon_stopped_on_arbitrary_errors_with_mode_permanent(
        settings, resource, dummy, manual_time, caplog, assert_logs, k8s_mocked, simulate_cycle):
    caplog.set_level(logging.DEBUG)

    @kopf.daemon(*resource, id='fn', errors=ErrorsMode.PERMANENT, backoff=0.01)
    async def fn(**kwargs):
        dummy.mock()
        dummy.kwargs = kwargs
        dummy.steps['called'].set()
        raise Exception("boo!")

    finalizer = settings.persistence.finalizer
    event_object = {'metadata': {'finalizers': [finalizer]}}
    await simulate_cycle(event_object)

    await dummy.steps['called'].wait()
    await dummy.wait_for_daemon_done()

    assert dummy.mock.call_count == 1
    assert k8s_mocked.sleep.call_count == 0

    assert_logs([
        "Daemon 'fn' failed with an exception. Will stop.",
        "Daemon 'fn' has exited on its own",
    ], prohibited=[
        "Daemon 'fn' succeeded.",
    ])


async def test_daemon_retried_on_temporary_error(
        registry, settings, resource, dummy, manual_time,
        caplog, assert_logs, k8s_mocked, simulate_cycle):
    caplog.set_level(logging.DEBUG)

    @kopf.daemon(*resource, id='fn', backoff=1.0)
    async def fn(retry, **kwargs):
        dummy.mock()
        dummy.kwargs = kwargs
        dummy.steps['called'].set()
        if not retry:
            raise TemporaryError("boo!", delay=1.0)
        else:
            dummy.steps['finish'].set()

    finalizer = settings.persistence.finalizer
    event_object = {'metadata': {'finalizers': [finalizer]}}
    await simulate_cycle(event_object)

    await dummy.steps['called'].wait()
    await dummy.steps['finish'].wait()
    await dummy.wait_for_daemon_done()

    assert k8s_mocked.sleep.call_count == 1  # one for each retry
    assert k8s_mocked.sleep.call_args_list[0][0][0] == 1.0  # [call#][args/kwargs][arg#]

    assert_logs([
        "Daemon 'fn' failed temporarily: boo!",
        "Daemon 'fn' succeeded.",
        "Daemon 'fn' has exited on its own",
    ])


async def test_daemon_retried_on_arbitrary_error_with_mode_temporary(
        settings, resource, dummy, caplog, assert_logs, k8s_mocked, simulate_cycle, manual_time):
    caplog.set_level(logging.DEBUG)

    @kopf.daemon(*resource, id='fn', errors=ErrorsMode.TEMPORARY, backoff=1.0)
    async def fn(retry, **kwargs):
        dummy.mock()
        dummy.kwargs = kwargs
        dummy.steps['called'].set()
        if not retry:
            raise Exception("boo!")
        else:
            dummy.steps['finish'].set()

    finalizer = settings.persistence.finalizer
    event_object = {'metadata': {'finalizers': [finalizer]}}
    await simulate_cycle(event_object)

    await dummy.steps['called'].wait()
    await dummy.steps['finish'].wait()
    await dummy.wait_for_daemon_done()

    assert k8s_mocked.sleep.call_count == 1  # one for each retry
    assert k8s_mocked.sleep.call_args_list[0][0][0] == 1.0  # [call#][args/kwargs][arg#]

    assert_logs([
        "Daemon 'fn' failed with an exception. Will retry.",
        "Daemon 'fn' succeeded.",
        "Daemon 'fn' has exited on its own",
    ])


async def test_daemon_retried_until_retries_limit(
        resource, dummy, caplog, assert_logs, k8s_mocked, simulate_cycle, manual_time):
    caplog.set_level(logging.DEBUG)

    @kopf.daemon(*resource, id='fn', retries=3)
    async def fn(**kwargs):
        dummy.kwargs = kwargs
        dummy.steps['called'].set()
        raise TemporaryError("boo!", delay=1.0)

    await simulate_cycle({})
    await dummy.steps['called'].wait()
    await dummy.wait_for_daemon_done()

    assert k8s_mocked.sleep.call_count == 3  # one for each retry
    assert k8s_mocked.sleep.call_args_list[0][0][0] == 1.0  # [call#][args/kwargs][arg#]
    assert k8s_mocked.sleep.call_args_list[1][0][0] == 1.0  # [call#][args/kwargs][arg#]
    assert k8s_mocked.sleep.call_args_list[2][0][0] == 1.0  # [call#][args/kwargs][arg#]


async def test_daemon_retried_until_timeout(
        resource, dummy, caplog, assert_logs, k8s_mocked, simulate_cycle, manual_time):
    caplog.set_level(logging.DEBUG)

    @kopf.daemon(*resource, id='fn', timeout=3.0)
    async def fn(**kwargs):
        dummy.kwargs = kwargs
        dummy.steps['called'].set()
        raise TemporaryError("boo!", delay=1.0)

    await simulate_cycle({})
    await dummy.steps['called'].wait()
    await dummy.wait_for_daemon_done()

    assert k8s_mocked.sleep.call_count == 3  # one for each retry
    assert k8s_mocked.sleep.call_args_list[0][0][0] == 1.0  # [call#][args/kwargs][arg#]
    assert k8s_mocked.sleep.call_args_list[1][0][0] == 1.0  # [call#][args/kwargs][arg#]
    assert k8s_mocked.sleep.call_args_list[2][0][0] == 1.0  # [call#][args/kwargs][arg#]



================================================
FILE: tests/handling/daemons/test_daemon_filtration.py
================================================
import logging

import pytest

import kopf

# We assume that the handler filtering is tested in details elsewhere (for all handlers).
# Here, we only test if it is applied or not applied.


async def test_daemon_filtration_satisfied(
        settings, resource, dummy, caplog, assert_logs, k8s_mocked, simulate_cycle):
    caplog.set_level(logging.DEBUG)

    @kopf.daemon(*resource, id='fn',
                 labels={'a': 'value', 'b': kopf.PRESENT, 'c': kopf.ABSENT},
                 annotations={'x': 'value', 'y': kopf.PRESENT, 'z': kopf.ABSENT})
    async def fn(**kwargs):
        dummy.kwargs = kwargs
        dummy.steps['called'].set()

    finalizer = settings.persistence.finalizer
    event_body = {'metadata': {'labels': {'a': 'value', 'b': '...'},
                               'annotations': {'x': 'value', 'y': '...'},
                               'finalizers': [finalizer]}}
    await simulate_cycle(event_body)

    await dummy.steps['called'].wait()
    await dummy.wait_for_daemon_done()


@pytest.mark.parametrize('labels, annotations', [
    # Annotations mismatching (but labels are matching):
    ({'a': 'value', 'b': '...'}, {'x': 'mismatching-value', 'b': '...'}, ),  # x must be "value".
    ({'a': 'value', 'b': '...'}, {'x': 'value', 'y': '...', 'z': '...'}),  # z must be absent
    ({'a': 'value', 'b': '...'}, {'x': 'value'}),  # y must be present
    # labels mismatching (but annotations are matching):
    ({'a': 'mismatching-value', 'b': '...'}, {'x': 'value', 'y': '...'}),
    ({'a': 'value', 'b': '...', 'c': '...'}, {'x': 'value', 'y': '...'}),
    ({'a': 'value'}, {'x': 'value', 'y': '...'}),
])
async def test_daemon_filtration_mismatched(
        settings, resource, mocker, labels, annotations,
        caplog, assert_logs, k8s_mocked, simulate_cycle):
    caplog.set_level(logging.DEBUG)
    spawn_daemons = mocker.patch('kopf._core.engines.daemons.spawn_daemons')

    @kopf.daemon(*resource, id='fn',
                 labels={'a': 'value', 'b': kopf.PRESENT, 'c': kopf.ABSENT},
                 annotations={'x': 'value', 'y': kopf.PRESENT, 'z': kopf.ABSENT})
    async def fn(**kwargs):
        pass

    finalizer = settings.persistence.finalizer
    event_body = {'metadata': {'labels': labels,
                               'annotations': annotations,
                               'finalizers': [finalizer]}}
    await simulate_cycle(event_body)

    assert spawn_daemons.called
    assert spawn_daemons.call_args_list[0][1]['handlers'] == []



================================================
FILE: tests/handling/daemons/test_daemon_rematching.py
================================================
import logging

import kopf
from kopf._core.intents.stoppers import DaemonStoppingReason


async def test_running_daemon_is_stopped_when_mismatches(
        resource, dummy, timer, mocker, caplog, assert_logs, k8s_mocked, simulate_cycle):
    caplog.set_level(logging.DEBUG)

    @kopf.daemon(*resource, id='fn', when=lambda **_: is_matching)
    async def fn(**kwargs):
        dummy.mock()
        dummy.kwargs = kwargs
        dummy.steps['called'].set()
        await kwargs['stopped'].wait()

    # Ensure it is spawned while it is matching. (The same as the spawning tests.)
    mocker.resetall()
    is_matching = True
    await simulate_cycle({})
    await dummy.steps['called'].wait()
    assert dummy.mock.call_count == 1

    # Ensure it is stopped once it stops matching. (The same as the termination tests.)
    mocker.resetall()
    is_matching = False
    await simulate_cycle({})
    with timer:
        await dummy.wait_for_daemon_done()

    assert timer.seconds < 0.01  # near-instantly
    stopped = dummy.kwargs['stopped']
    assert DaemonStoppingReason.FILTERS_MISMATCH in stopped.reason



================================================
FILE: tests/handling/daemons/test_daemon_spawning.py
================================================
import logging

import kopf


async def test_daemon_is_spawned_at_least_once(
        resource, dummy, caplog, assert_logs, k8s_mocked, simulate_cycle):
    caplog.set_level(logging.DEBUG)

    @kopf.daemon(*resource, id='fn')
    async def fn(**kwargs):
        dummy.mock()
        dummy.kwargs = kwargs
        dummy.steps['called'].set()

    await simulate_cycle({})

    await dummy.steps['called'].wait()
    await dummy.wait_for_daemon_done()

    assert dummy.mock.call_count == 1  # not restarted


async def test_daemon_initial_delay_obeyed(
        resource, dummy, caplog, assert_logs, k8s_mocked, simulate_cycle):
    caplog.set_level(logging.DEBUG)

    @kopf.daemon(*resource, id='fn', initial_delay=1.0)
    async def fn(**kwargs):
        dummy.mock()
        dummy.kwargs = kwargs
        dummy.steps['called'].set()

    await simulate_cycle({})

    await dummy.steps['called'].wait()
    await dummy.wait_for_daemon_done()

    assert k8s_mocked.sleep.call_count >= 1
    assert k8s_mocked.sleep.call_count <= 2  # one optional extra call for sleep(None)
    assert k8s_mocked.sleep.call_args_list[0][0][0] == 1.0  # [call#][args/kwargs][arg#]



================================================
FILE: tests/handling/daemons/test_daemon_termination.py
================================================
import asyncio
import contextlib
import logging

import pytest

import kopf


async def test_daemon_exits_gracefully_and_instantly_on_resource_deletion(
        settings, resource, dummy, simulate_cycle,
        caplog, assert_logs, k8s_mocked, frozen_time, mocker, timer):
    caplog.set_level(logging.DEBUG)

    # A daemon-under-test.
    @kopf.daemon(*resource, id='fn')
    async def fn(**kwargs):
        dummy.kwargs = kwargs
        dummy.steps['called'].set()
        await kwargs['stopped'].wait()

    # 0th cycle: trigger spawning and wait until ready. Assume the finalizers are already added.
    finalizer = settings.persistence.finalizer
    event_object = {'metadata': {'finalizers': [finalizer]}}
    await simulate_cycle(event_object)
    await dummy.steps['called'].wait()

    # 1st stage: trigger termination due to resource deletion.
    mocker.resetall()
    event_object.setdefault('metadata', {})
    event_object['metadata'] |= {'deletionTimestamp': '...'}
    await simulate_cycle(event_object)

    # Check that the daemon has exited near-instantly, with no delays.
    with timer:
        await dummy.wait_for_daemon_done()

    assert timer.seconds < 0.01  # near-instantly
    assert k8s_mocked.sleep.call_count == 0
    assert k8s_mocked.patch.call_count == 1
    assert k8s_mocked.patch.call_args_list[0][1]['payload']['metadata']['finalizers'] == []


async def test_daemon_exits_gracefully_and_instantly_on_operator_exiting(
        settings, resource, dummy, simulate_cycle, background_daemon_killer,
        caplog, assert_logs, k8s_mocked, frozen_time, mocker, timer):
    caplog.set_level(logging.DEBUG)

    # A daemon-under-test.
    @kopf.daemon(*resource, id='fn')
    async def fn(**kwargs):
        dummy.kwargs = kwargs
        dummy.steps['called'].set()
        await kwargs['stopped'].wait()

    # 0th cycle: trigger spawning and wait until ready. Assume the finalizers are already added.
    finalizer = settings.persistence.finalizer
    event_object = {'metadata': {'finalizers': [finalizer]}}
    await simulate_cycle(event_object)
    await dummy.steps['called'].wait()

    # 1st stage: trigger termination due to operator exiting.
    mocker.resetall()
    background_daemon_killer.cancel()

    # Check that the daemon has exited near-instantly, with no delays.
    with timer:
        await dummy.wait_for_daemon_done()

    assert timer.seconds < 0.01  # near-instantly
    assert k8s_mocked.sleep.call_count == 0
    assert k8s_mocked.patch.call_count == 0

    # To prevent double-cancelling of the scheduler's system tasks in the fixture, let them finish:
    with contextlib.suppress(asyncio.CancelledError):
        await background_daemon_killer


@pytest.mark.usefixtures('background_daemon_killer')
async def test_daemon_exits_gracefully_and_instantly_on_operator_pausing(
        settings, memories, resource, dummy, simulate_cycle, conflicts_found,
        caplog, assert_logs, k8s_mocked, frozen_time, mocker, timer):
    caplog.set_level(logging.DEBUG)

    # A daemon-under-test.
    @kopf.daemon(*resource, id='fn')
    async def fn(**kwargs):
        dummy.kwargs = kwargs
        dummy.steps['called'].set()
        await kwargs['stopped'].wait()

    # 0th cycle: trigger spawning and wait until ready. Assume the finalizers are already added.
    finalizer = settings.persistence.finalizer
    event_object = {'metadata': {'finalizers': [finalizer]}}
    await simulate_cycle(event_object)
    await dummy.steps['called'].wait()

    # 1st stage: trigger termination due to the operator's pause.
    mocker.resetall()
    await conflicts_found.turn_to(True)

    # Check that the daemon has exited near-instantly, with no delays.
    with timer:
        await dummy.wait_for_daemon_done()
    assert timer.seconds < 0.01  # near-instantly

    # There is no way to test for re-spawning here: it is done by watch-events,
    # which are tested by the paused operators elsewhere (test_daemon_spawning.py).
    # We only test that it is capable for respawning (not forever-stopped):
    memory = await memories.recall(event_object)
    assert not memory.daemons_memory.forever_stopped


async def test_daemon_exits_instantly_via_cancellation_with_backoff(
        settings, resource, dummy, simulate_cycle,
        caplog, assert_logs, k8s_mocked, frozen_time, mocker):
    caplog.set_level(logging.DEBUG)
    dummy.steps['finish'].set()

    # A daemon-under-test.
    @kopf.daemon(*resource, id='fn', cancellation_backoff=5, cancellation_timeout=10)
    async def fn(**kwargs):
        dummy.kwargs = kwargs
        dummy.steps['called'].set()
        try:
            await asyncio.Event().wait()  # this one is cancelled.
        except asyncio.CancelledError:
            await dummy.steps['finish'].wait()  # simulated slow (non-instant) exiting.

    # Trigger spawning and wait until ready. Assume the finalizers are already added.
    finalizer = settings.persistence.finalizer
    event_object = {'metadata': {'finalizers': [finalizer]}}
    await simulate_cycle(event_object)
    await dummy.steps['called'].wait()

    # 1st stage: trigger termination due to resource deletion. Wait for backoff.
    mocker.resetall()
    event_object.setdefault('metadata', {})
    event_object['metadata'] |= {'deletionTimestamp': '...'}
    await simulate_cycle(event_object)

    assert k8s_mocked.sleep.call_count == 1
    assert k8s_mocked.sleep.call_args_list[0][0][0] == 5.0
    assert k8s_mocked.patch.call_count == 1
    assert k8s_mocked.patch.call_args_list[0][1]['payload']['status']['kopf']['dummy']

    # 2nd cycle: cancelling after the backoff is reached. Wait for cancellation timeout.
    mocker.resetall()
    frozen_time.tick(5)  # backoff time or slightly above it
    await simulate_cycle(event_object)

    assert k8s_mocked.sleep.call_count == 0
    assert k8s_mocked.patch.call_count == 1
    assert k8s_mocked.patch.call_args_list[0][1]['payload']['metadata']['finalizers'] == []

    # Cleanup.
    await dummy.wait_for_daemon_done()


async def test_daemon_exits_slowly_via_cancellation_with_backoff(
        settings, resource, dummy, simulate_cycle,
        caplog, assert_logs, k8s_mocked, frozen_time, mocker):
    caplog.set_level(logging.DEBUG)

    # A daemon-under-test.
    @kopf.daemon(*resource, id='fn', cancellation_backoff=5, cancellation_timeout=10)
    async def fn(**kwargs):
        dummy.kwargs = kwargs
        dummy.steps['called'].set()
        try:
            await asyncio.Event().wait()  # this one is cancelled.
        except asyncio.CancelledError:
            await dummy.steps['finish'].wait()  # simulated slow (non-instant) exiting.

    # Trigger spawning and wait until ready. Assume the finalizers are already added.
    finalizer = settings.persistence.finalizer
    event_object = {'metadata': {'finalizers': [finalizer]}}
    await simulate_cycle(event_object)
    await dummy.steps['called'].wait()

    # 1st stage: trigger termination due to resource deletion. Wait for backoff.
    mocker.resetall()
    event_object.setdefault('metadata', {})
    event_object['metadata'] |= {'deletionTimestamp': '...'}
    await simulate_cycle(event_object)

    assert k8s_mocked.sleep.call_count == 1
    assert k8s_mocked.sleep.call_args_list[0][0][0] == 5.0
    assert k8s_mocked.patch.call_count == 1
    assert k8s_mocked.patch.call_args_list[0][1]['payload']['status']['kopf']['dummy']

    # 2nd cycle: cancelling after the backoff is reached. Wait for cancellation timeout.
    mocker.resetall()
    frozen_time.tick(5)  # backoff time or slightly above it
    await simulate_cycle(event_object)

    assert k8s_mocked.sleep.call_count == 1
    assert k8s_mocked.sleep.call_args_list[0][0][0] == 10.0
    assert k8s_mocked.patch.call_count == 1
    assert k8s_mocked.patch.call_args_list[0][1]['payload']['status']['kopf']['dummy']

    # 3rd cycle: the daemon has exited, the resource should be unblocked from actual deletion.
    mocker.resetall()
    frozen_time.tick(1)  # any time below timeout
    dummy.steps['finish'].set()
    await asyncio.sleep(0)
    await simulate_cycle(event_object)
    await dummy.wait_for_daemon_done()

    assert k8s_mocked.sleep.call_count == 0
    assert k8s_mocked.patch.call_count == 1
    assert k8s_mocked.patch.call_args_list[0][1]['payload']['metadata']['finalizers'] == []


async def test_daemon_is_abandoned_due_to_cancellation_timeout_reached(
        settings, resource, dummy, simulate_cycle,
        caplog, assert_logs, k8s_mocked, frozen_time, mocker):
    caplog.set_level(logging.DEBUG)

    # A daemon-under-test.
    @kopf.daemon(*resource, id='fn', cancellation_timeout=10)
    async def fn(**kwargs):
        dummy.kwargs = kwargs
        dummy.steps['called'].set()
        try:
            await dummy.steps['finish'].wait()  # this one is cancelled.
        except asyncio.CancelledError:
            await dummy.steps['finish'].wait()  # simulated disobedience to be cancelled.

    # 0th cycle:tTrigger spawning and wait until ready. Assume the finalizers are already added.
    finalizer = settings.persistence.finalizer
    event_object = {'metadata': {'finalizers': [finalizer]}}
    await simulate_cycle(event_object)
    await dummy.steps['called'].wait()

    # 1st stage: trigger termination due to resource deletion. Wait for backoff.
    mocker.resetall()
    event_object.setdefault('metadata', {})
    event_object['metadata'] |= {'deletionTimestamp': '...'}
    await simulate_cycle(event_object)

    assert k8s_mocked.sleep.call_count == 1
    assert k8s_mocked.sleep.call_args_list[0][0][0] == 10.0
    assert k8s_mocked.patch.call_count == 1
    assert k8s_mocked.patch.call_args_list[0][1]['payload']['status']['kopf']['dummy']

    # 2rd cycle: the daemon has exited, the resource should be unblocked from actual deletion.
    mocker.resetall()
    frozen_time.tick(50)
    with pytest.warns(ResourceWarning, match=r"Daemon .+ did not exit in time"):
        await simulate_cycle(event_object)

    assert k8s_mocked.sleep.call_count == 0
    assert k8s_mocked.patch.call_count == 1
    assert k8s_mocked.patch.call_args_list[0][1]['payload']['metadata']['finalizers'] == []
    assert_logs(["Daemon 'fn' did not exit in time. Leaving it orphaned."])

    # Cleanup.
    dummy.steps['finish'].set()
    await dummy.wait_for_daemon_done()



================================================
FILE: tests/handling/daemons/test_timer_errors.py
================================================
import logging

import kopf
from kopf._core.actions.execution import ErrorsMode, PermanentError, TemporaryError


async def test_timer_stopped_on_permanent_error(
        settings, resource, dummy, caplog, assert_logs, k8s_mocked, simulate_cycle):
    caplog.set_level(logging.DEBUG)

    @kopf.timer(*resource, id='fn', backoff=0.01, interval=1.0)
    async def fn(**kwargs):
        dummy.mock()
        dummy.kwargs = kwargs
        dummy.steps['called'].set()
        kwargs['stopped']._setter.set()  # to exit the cycle
        raise PermanentError("boo!")

    event_object = {'metadata': {'finalizers': [settings.persistence.finalizer]}}
    await simulate_cycle(event_object)

    await dummy.steps['called'].wait()
    await dummy.wait_for_daemon_done()

    assert dummy.mock.call_count == 1
    assert k8s_mocked.sleep.call_count == 1  # one for each retry
    assert k8s_mocked.sleep.call_args_list[0][0][0] == 1.0

    assert_logs([
        "Timer 'fn' failed permanently: boo!",
    ], prohibited=[
        "Timer 'fn' succeeded.",
    ])


async def test_timer_stopped_on_arbitrary_errors_with_mode_permanent(
        settings, resource, dummy, caplog, assert_logs, k8s_mocked, simulate_cycle):
    caplog.set_level(logging.DEBUG)

    @kopf.timer(*resource, id='fn', errors=ErrorsMode.PERMANENT, backoff=0.01, interval=1.0)
    async def fn(**kwargs):
        dummy.mock()
        dummy.kwargs = kwargs
        dummy.steps['called'].set()
        kwargs['stopped']._setter.set()  # to exit the cycle
        raise Exception("boo!")

    event_object = {'metadata': {'finalizers': [settings.persistence.finalizer]}}
    await simulate_cycle(event_object)

    await dummy.steps['called'].wait()
    await dummy.wait_for_daemon_done()

    assert dummy.mock.call_count == 1
    assert k8s_mocked.sleep.call_count == 1  # one for each retry
    assert k8s_mocked.sleep.call_args_list[0][0][0] == 1.0

    assert_logs([
        "Timer 'fn' failed with an exception. Will stop.",
    ], prohibited=[
        "Timer 'fn' succeeded.",
    ])


async def test_timer_retried_on_temporary_error(
        settings, resource, dummy, manual_time,
        caplog, assert_logs, k8s_mocked, simulate_cycle):
    caplog.set_level(logging.DEBUG)

    @kopf.timer(*resource, id='fn', backoff=1.0, interval=1.0)
    async def fn(retry, **kwargs):
        dummy.mock()
        dummy.kwargs = kwargs
        dummy.steps['called'].set()
        if not retry:
            raise TemporaryError("boo!", delay=1.0)
        else:
            kwargs['stopped']._setter.set()  # to exit the cycle
            dummy.steps['finish'].set()

    event_object = {'metadata': {'finalizers': [settings.persistence.finalizer]}}
    await simulate_cycle(event_object)

    await dummy.steps['called'].wait()
    await dummy.steps['finish'].wait()
    await dummy.wait_for_daemon_done()

    assert k8s_mocked.sleep.call_count == 2  # one for each retry
    assert k8s_mocked.sleep.call_args_list[0][0][0] == [1.0]  # delays
    assert k8s_mocked.sleep.call_args_list[1][0][0] == 1.0  # interval

    assert_logs([
        "Timer 'fn' failed temporarily: boo!",
        "Timer 'fn' succeeded.",
    ])


async def test_timer_retried_on_arbitrary_error_with_mode_temporary(
        settings, resource, dummy, caplog, assert_logs, k8s_mocked, simulate_cycle, manual_time):
    caplog.set_level(logging.DEBUG)

    @kopf.timer(*resource, id='fn', errors=ErrorsMode.TEMPORARY, backoff=1.0, interval=1.0)
    async def fn(retry, **kwargs):
        dummy.mock()
        dummy.kwargs = kwargs
        dummy.steps['called'].set()
        if not retry:
            raise Exception("boo!")
        else:
            kwargs['stopped']._setter.set()  # to exit the cycle
            dummy.steps['finish'].set()

    event_object = {'metadata': {'finalizers': [settings.persistence.finalizer]}}
    await simulate_cycle(event_object)

    await dummy.steps['called'].wait()
    await dummy.steps['finish'].wait()
    await dummy.wait_for_daemon_done()

    assert k8s_mocked.sleep.call_count == 2  # one for each retry
    assert k8s_mocked.sleep.call_args_list[0][0][0] == [1.0]  # delays
    assert k8s_mocked.sleep.call_args_list[1][0][0] == 1.0  # interval

    assert_logs([
        "Timer 'fn' failed with an exception. Will retry.",
        "Timer 'fn' succeeded.",
    ])


async def test_timer_retried_until_retries_limit(
        resource, dummy, caplog, assert_logs, k8s_mocked, simulate_cycle, manual_time):
    caplog.set_level(logging.DEBUG)

    @kopf.timer(*resource, id='fn', retries=3, interval=1.0)
    async def fn(**kwargs):
        dummy.mock()
        dummy.kwargs = kwargs
        dummy.steps['called'].set()
        if dummy.mock.call_count >= 5:
            kwargs['stopped']._setter.set()  # to exit the cycle
        raise TemporaryError("boo!", delay=1.0)

    await simulate_cycle({})
    await dummy.steps['called'].wait()
    await dummy.wait_for_daemon_done()

    assert k8s_mocked.sleep.call_count >= 4  # one for each retry
    assert k8s_mocked.sleep.call_args_list[0][0][0] == [1.0]  # delays
    assert k8s_mocked.sleep.call_args_list[1][0][0] == [1.0]  # delays
    assert k8s_mocked.sleep.call_args_list[2][0][0] == [1.0]  # delays
    assert k8s_mocked.sleep.call_args_list[3][0][0] == 1.0  # interval


async def test_timer_retried_until_timeout(
        resource, dummy, caplog, assert_logs, k8s_mocked, simulate_cycle, manual_time):
    caplog.set_level(logging.DEBUG)

    @kopf.timer(*resource, id='fn', timeout=3.0, interval=1.0)
    async def fn(**kwargs):
        dummy.mock()
        dummy.kwargs = kwargs
        dummy.steps['called'].set()
        if dummy.mock.call_count >= 5:
            kwargs['stopped']._setter.set()  # to exit the cycle
        raise TemporaryError("boo!", delay=1.0)

    await simulate_cycle({})
    await dummy.steps['called'].wait()
    await dummy.wait_for_daemon_done()

    assert k8s_mocked.sleep.call_count >= 4  # one for each retry
    assert k8s_mocked.sleep.call_args_list[0][0][0] == [1.0]  # delays
    assert k8s_mocked.sleep.call_args_list[1][0][0] == [1.0]  # delays
    assert k8s_mocked.sleep.call_args_list[2][0][0] == [1.0]  # delays
    assert k8s_mocked.sleep.call_args_list[3][0][0] == 1.0  # interval



================================================
FILE: tests/handling/daemons/test_timer_filtration.py
================================================
import logging

import pytest

import kopf

# We assume that the handler filtering is tested in details elsewhere (for all handlers).
# Here, we only test if it is applied or not applied.


async def test_timer_filtration_satisfied(
        settings, resource, dummy, caplog, assert_logs, k8s_mocked, simulate_cycle):
    caplog.set_level(logging.DEBUG)

    @kopf.timer(*resource, id='fn',
                labels={'a': 'value', 'b': kopf.PRESENT, 'c': kopf.ABSENT},
                annotations={'x': 'value', 'y': kopf.PRESENT, 'z': kopf.ABSENT})
    async def fn(**kwargs):
        dummy.kwargs = kwargs
        dummy.steps['called'].set()

    event_body = {'metadata': {'labels': {'a': 'value', 'b': '...'},
                               'annotations': {'x': 'value', 'y': '...'},
                               'finalizers': [settings.persistence.finalizer]}}
    await simulate_cycle(event_body)

    await dummy.steps['called'].wait()
    await dummy.wait_for_daemon_done()


@pytest.mark.parametrize('labels, annotations', [
    # Annotations mismatching (but labels are matching):
    ({'a': 'value', 'b': '...'}, {'x': 'mismatching-value', 'b': '...'}, ),  # x must be "value".
    ({'a': 'value', 'b': '...'}, {'x': 'value', 'y': '...', 'z': '...'}),  # z must be absent
    ({'a': 'value', 'b': '...'}, {'x': 'value'}),  # y must be present
    # labels mismatching (but annotations are matching):
    ({'a': 'mismatching-value', 'b': '...'}, {'x': 'value', 'y': '...'}),
    ({'a': 'value', 'b': '...', 'c': '...'}, {'x': 'value', 'y': '...'}),
    ({'a': 'value'}, {'x': 'value', 'y': '...'}),
])
async def test_timer_filtration_mismatched(
        settings, resource, mocker, labels, annotations,
        caplog, assert_logs, k8s_mocked, simulate_cycle):
    caplog.set_level(logging.DEBUG)
    spawn_daemons = mocker.patch('kopf._core.engines.daemons.spawn_daemons')

    @kopf.timer(*resource, id='fn',
                labels={'a': 'value', 'b': kopf.PRESENT, 'c': kopf.ABSENT},
                annotations={'x': 'value', 'y': kopf.PRESENT, 'z': kopf.ABSENT})
    async def fn(**kwargs):
        pass

    event_body = {'metadata': {'labels': labels,
                               'annotations': annotations,
                               'finalizers': [settings.persistence.finalizer]}}
    await simulate_cycle(event_body)

    assert spawn_daemons.called
    assert spawn_daemons.call_args_list[0][1]['handlers'] == []



================================================
FILE: tests/handling/daemons/test_timer_intervals.py
================================================
import logging

import kopf

# TODO: tests for idle=  (more complicated)


async def test_timer_regular_interval(
        resource, dummy, caplog, assert_logs, k8s_mocked, simulate_cycle, frozen_time):
    caplog.set_level(logging.DEBUG)

    @kopf.timer(*resource, id='fn', interval=1.0, sharp=False)
    async def fn(**kwargs):
        dummy.mock()
        dummy.kwargs = kwargs
        dummy.steps['called'].set()
        frozen_time.tick(0.3)
        if dummy.mock.call_count >= 2:
            dummy.steps['finish'].set()
            kwargs['stopped']._setter.set()  # to exit the cycle

    await simulate_cycle({})
    await dummy.steps['called'].wait()
    await dummy.wait_for_daemon_done()

    assert dummy.mock.call_count == 2
    assert k8s_mocked.sleep.call_count == 2
    assert k8s_mocked.sleep.call_args_list[0][0][0] == 1.0
    assert k8s_mocked.sleep.call_args_list[1][0][0] == 1.0


async def test_timer_sharp_interval(
        resource, dummy, caplog, assert_logs, k8s_mocked, simulate_cycle, frozen_time):
    caplog.set_level(logging.DEBUG)

    @kopf.timer(*resource, id='fn', interval=1.0, sharp=True)
    async def fn(**kwargs):
        dummy.mock()
        dummy.kwargs = kwargs
        dummy.steps['called'].set()
        frozen_time.tick(0.3)
        if dummy.mock.call_count >= 2:
            dummy.steps['finish'].set()
            kwargs['stopped']._setter.set()  # to exit the cycle

    await simulate_cycle({})
    await dummy.steps['called'].wait()
    await dummy.steps['finish'].wait()
    await dummy.wait_for_daemon_done()

    assert dummy.mock.call_count == 2
    assert k8s_mocked.sleep.call_count == 2
    assert 0.7 <= k8s_mocked.sleep.call_args_list[0][0][0] < 0.71
    assert 0.7 <= k8s_mocked.sleep.call_args_list[1][0][0] < 0.71



================================================
FILE: tests/handling/daemons/test_timer_triggering.py
================================================
import logging

import kopf


async def test_timer_is_spawned_at_least_once(
        resource, dummy, caplog, assert_logs, k8s_mocked, simulate_cycle):
    caplog.set_level(logging.DEBUG)

    @kopf.timer(*resource, id='fn', interval=1.0)
    async def fn(**kwargs):
        dummy.mock()
        dummy.kwargs = kwargs
        dummy.steps['called'].set()
        kwargs['stopped']._setter.set()  # to exit the cycle

    await simulate_cycle({})
    await dummy.steps['called'].wait()

    assert dummy.mock.call_count == 1
    assert dummy.kwargs['retry'] == 0
    assert k8s_mocked.sleep.call_count == 1
    assert k8s_mocked.sleep.call_args_list[0][0][0] == 1.0

    await dummy.wait_for_daemon_done()


async def test_timer_initial_delay_obeyed(
        resource, dummy, caplog, assert_logs, k8s_mocked, simulate_cycle):
    caplog.set_level(logging.DEBUG)

    @kopf.timer(*resource, id='fn', initial_delay=5.0, interval=1.0)
    async def fn(**kwargs):
        dummy.mock()
        dummy.kwargs = kwargs
        dummy.steps['called'].set()
        kwargs['stopped']._setter.set()  # to exit the cycle

    await simulate_cycle({})
    await dummy.steps['called'].wait()

    assert dummy.mock.call_count == 1
    assert dummy.kwargs['retry'] == 0
    assert k8s_mocked.sleep.call_count == 2
    assert k8s_mocked.sleep.call_args_list[0][0][0] == 5.0
    assert k8s_mocked.sleep.call_args_list[1][0][0] == 1.0

    await dummy.wait_for_daemon_done()



================================================
FILE: tests/handling/indexing/conftest.py
================================================
import pytest

from kopf._cogs.structs.bodies import Body
from kopf._core.engines.indexing import OperatorIndexer, OperatorIndexers


@pytest.fixture()
def indexers():
    return OperatorIndexers()


@pytest.fixture()
def index(indexers):
    indexer = OperatorIndexer()
    indexers['index_fn'] = indexer
    return indexer.index


@pytest.fixture()
async def indexed_123(indexers, index, namespace):
    body = {'metadata': {'namespace': namespace, 'name': 'name1'}}
    key = indexers.make_key(Body(body))
    indexers['index_fn'].replace(key, 123)
    assert set(index) == {None}
    assert set(index[None]) == {123}



================================================
FILE: tests/handling/indexing/test_blocking_until_indexed.py
================================================
import asyncio
import logging

import pytest

from kopf._cogs.aiokits.aiotoggles import ToggleSet
from kopf._cogs.structs.ephemera import Memo
from kopf._core.actions.lifecycles import all_at_once
from kopf._core.reactor.inventory import ResourceMemories
from kopf._core.reactor.processing import process_resource_event

EVENT_TYPES_WHEN_EXISTS = [None, 'ADDED', 'MODIFIED']
EVENT_TYPES_WHEN_GONE = ['DELETED']
EVENT_TYPES = EVENT_TYPES_WHEN_EXISTS + EVENT_TYPES_WHEN_GONE


@pytest.mark.parametrize('event_type', EVENT_TYPES_WHEN_EXISTS)
async def test_reporting_on_resource_readiness(
        resource, settings, registry, indexers, caplog, event_type, handlers, timer):
    caplog.set_level(logging.DEBUG)

    operator_indexed = ToggleSet(all)
    resource_indexed = await operator_indexed.make_toggle()
    with timer:
        await process_resource_event(
            lifecycle=all_at_once,
            registry=registry,
            settings=settings,
            resource=resource,
            indexers=indexers,
            memories=ResourceMemories(),
            memobase=Memo(),
            raw_event={'type': event_type, 'object': {}},
            event_queue=asyncio.Queue(),
            operator_indexed=operator_indexed,
            resource_indexed=resource_indexed,
        )
    assert timer.seconds < 0.2  # asap, nowait
    assert operator_indexed.is_on()
    assert set(operator_indexed) == set()  # save RAM
    assert handlers.event_mock.called


@pytest.mark.parametrize('event_type', EVENT_TYPES_WHEN_EXISTS)
async def test_blocking_when_operator_is_not_ready(
        resource, settings, registry, indexers, caplog, event_type, handlers, timer):
    caplog.set_level(logging.DEBUG)

    operator_indexed = ToggleSet(all)
    resource_listed = await operator_indexed.make_toggle()
    resource_indexed = await operator_indexed.make_toggle()
    with pytest.raises(asyncio.TimeoutError), timer:
        await asyncio.wait_for(process_resource_event(
            lifecycle=all_at_once,
            registry=registry,
            settings=settings,
            resource=resource,
            indexers=indexers,
            memories=ResourceMemories(),
            memobase=Memo(),
            raw_event={'type': event_type, 'object': {}},
            event_queue=asyncio.Queue(),
            operator_indexed=operator_indexed,
            resource_indexed=resource_indexed,
        ), timeout=0.2)
    assert 0.2 < timer.seconds < 0.4
    assert operator_indexed.is_off()
    assert set(operator_indexed) == {resource_listed}
    assert not handlers.event_mock.called


@pytest.mark.parametrize('event_type', EVENT_TYPES_WHEN_EXISTS)
async def test_unblocking_once_operator_is_ready(
        resource, settings, registry, indexers, caplog, event_type, handlers, timer):
    caplog.set_level(logging.DEBUG)

    async def delayed_readiness(delay: float):
        await asyncio.sleep(delay)
        await resource_listed.turn_to(True)

    operator_indexed = ToggleSet(all)
    resource_listed = await operator_indexed.make_toggle()
    resource_indexed = await operator_indexed.make_toggle()
    with timer:
        asyncio.create_task(delayed_readiness(0.2))
        await process_resource_event(
            lifecycle=all_at_once,
            registry=registry,
            settings=settings,
            resource=resource,
            indexers=indexers,
            memories=ResourceMemories(),
            memobase=Memo(),
            raw_event={'type': event_type, 'object': {}},
            event_queue=asyncio.Queue(),
            operator_indexed=operator_indexed,
            resource_indexed=resource_indexed,
        )
    assert 0.2 < timer.seconds < 0.4
    assert operator_indexed.is_on()
    assert set(operator_indexed) == {resource_listed}
    assert handlers.event_mock.called



================================================
FILE: tests/handling/indexing/test_index_exclusion.py
================================================
import asyncio
import logging

import freezegun
import iso8601
import pytest

from kopf._cogs.aiokits.aiotoggles import Toggle
from kopf._cogs.structs.ephemera import Memo
from kopf._core.actions.execution import PermanentError, TemporaryError
from kopf._core.actions.lifecycles import all_at_once
from kopf._core.actions.progression import HandlerState, State
from kopf._core.reactor.processing import process_resource_event

EVENT_TYPES_WHEN_EXISTS = [None, 'ADDED', 'MODIFIED']
EVENT_TYPES_WHEN_GONE = ['DELETED']
EVENT_TYPES = EVENT_TYPES_WHEN_EXISTS + EVENT_TYPES_WHEN_GONE


#
# PART 1/2:
# First, test that the initial indexing state is interpreted properly
# and that it affects the indexing decision (to do or not to do).
#


@pytest.mark.parametrize('event_type', EVENT_TYPES_WHEN_EXISTS)
async def test_successes_are_removed_from_the_indexing_state(
        resource, namespace, settings, registry, memories, indexers, caplog, event_type, handlers):
    caplog.set_level(logging.DEBUG)
    body = {'metadata': {'namespace': namespace, 'name': 'name1'}}
    memory = await memories.recall(raw_body=body)
    memory.indexing_memory.indexing_state = State({'unrelated': HandlerState(success=True)})
    handlers.index_mock.side_effect = 123
    await process_resource_event(
        lifecycle=all_at_once,
        registry=registry,
        settings=settings,
        resource=resource,
        indexers=indexers,
        memories=memories,
        memobase=Memo(),
        raw_event={'type': event_type, 'object': body},
        event_queue=asyncio.Queue(),
        resource_indexed=Toggle(),  # used! only to enable indexing.
    )
    assert handlers.index_mock.call_count == 1
    assert memory.indexing_memory.indexing_state is None


@pytest.mark.parametrize('event_type', EVENT_TYPES_WHEN_EXISTS)
async def test_temporary_failures_with_no_delays_are_reindexed(
        resource, namespace, settings, registry, memories, indexers, index, caplog, event_type, handlers):
    caplog.set_level(logging.DEBUG)
    body = {'metadata': {'namespace': namespace, 'name': 'name1'}}
    memory = await memories.recall(raw_body=body)
    memory.indexing_memory.indexing_state = State({'index_fn': HandlerState(delayed=None)})
    await process_resource_event(
        lifecycle=all_at_once,
        registry=registry,
        settings=settings,
        resource=resource,
        indexers=indexers,
        memories=memories,
        memobase=Memo(),
        raw_event={'type': event_type, 'object': body},
        event_queue=asyncio.Queue(),
        resource_indexed=Toggle(),  # used! only to enable indexing.
    )
    assert handlers.index_mock.call_count == 1


@freezegun.freeze_time('2020-12-31T23:59:59.123456')
@pytest.mark.parametrize('event_type', EVENT_TYPES_WHEN_EXISTS)
async def test_temporary_failures_with_expired_delays_are_reindexed(
        resource, namespace, settings, registry, memories, indexers, index, caplog, event_type, handlers):
    caplog.set_level(logging.DEBUG)
    body = {'metadata': {'namespace': namespace, 'name': 'name1'}}
    delayed = iso8601.parse_date('2020-12-31T23:59:59')
    memory = await memories.recall(raw_body=body)
    memory.indexing_memory.indexing_state = State({'index_fn': HandlerState(delayed=delayed)})
    await process_resource_event(
        lifecycle=all_at_once,
        registry=registry,
        settings=settings,
        resource=resource,
        indexers=indexers,
        memories=memories,
        memobase=Memo(),
        raw_event={'type': event_type, 'object': body},
        event_queue=asyncio.Queue(),
        resource_indexed=Toggle(),  # used! only to enable indexing.
    )
    assert handlers.index_mock.call_count == 1


@pytest.mark.parametrize('event_type', EVENT_TYPES_WHEN_EXISTS)
async def test_permanent_failures_are_not_reindexed(
        resource, namespace, settings, registry, memories, indexers, index, caplog, event_type, handlers):
    caplog.set_level(logging.DEBUG)
    body = {'metadata': {'namespace': namespace, 'name': 'name1'}}
    memory = await memories.recall(raw_body=body)
    memory.indexing_memory.indexing_state = State({'index_fn': HandlerState(failure=True)})
    await process_resource_event(
        lifecycle=all_at_once,
        registry=registry,
        settings=settings,
        resource=resource,
        indexers=indexers,
        memories=memories,
        memobase=Memo(),
        raw_event={'type': event_type, 'object': body},
        event_queue=asyncio.Queue(),
        resource_indexed=Toggle(),  # used! only to enable indexing.
    )
    assert handlers.index_mock.call_count == 0


#
# PART 2/2:
# Once the resulting indexing state's and its effect on reindexing is tested,
# we can assert that some specific state is reached without actually reindexing.
#


@pytest.mark.usefixtures('indexed_123')
@pytest.mark.parametrize('event_type', EVENT_TYPES_WHEN_EXISTS)
async def test_removed_and_remembered_on_permanent_errors(
        resource, namespace, settings, registry, memories, indexers, index, caplog, event_type, handlers):
    caplog.set_level(logging.DEBUG)
    body = {'metadata': {'namespace': namespace, 'name': 'name1'}}
    memory = await memories.recall(raw_body=body)
    handlers.index_mock.side_effect = PermanentError("boo!")
    await process_resource_event(
        lifecycle=all_at_once,
        registry=registry,
        settings=settings,
        resource=resource,
        indexers=indexers,
        memories=memories,
        memobase=Memo(),
        raw_event={'type': event_type, 'object': body},
        event_queue=asyncio.Queue(),
        resource_indexed=Toggle(),  # used! only to enable indexing.
    )
    assert set(index) == set()
    assert memory.indexing_memory.indexing_state['index_fn'].finished == True
    assert memory.indexing_memory.indexing_state['index_fn'].failure == True
    assert memory.indexing_memory.indexing_state['index_fn'].success == False
    assert memory.indexing_memory.indexing_state['index_fn'].message == 'boo!'
    assert memory.indexing_memory.indexing_state['index_fn'].delayed == None


@freezegun.freeze_time('2020-12-31T00:00:00')
@pytest.mark.parametrize('delay_kwargs, expected_delayed', [
    (dict(), iso8601.parse_date('2020-12-31T00:01:00')),
    (dict(delay=0), iso8601.parse_date('2020-12-31T00:00:00')),
    (dict(delay=9), iso8601.parse_date('2020-12-31T00:00:09')),
    (dict(delay=None), None),
])
@pytest.mark.usefixtures('indexed_123')
@pytest.mark.parametrize('event_type', EVENT_TYPES_WHEN_EXISTS)
async def test_removed_and_remembered_on_temporary_errors(
        resource, namespace, settings, registry, memories, indexers, index, handlers,
        caplog, event_type, delay_kwargs, expected_delayed):
    caplog.set_level(logging.DEBUG)
    body = {'metadata': {'namespace': namespace, 'name': 'name1'}}
    memory = await memories.recall(raw_body=body)
    handlers.index_mock.side_effect = TemporaryError("boo!", **delay_kwargs)
    await process_resource_event(
        lifecycle=all_at_once,
        registry=registry,
        settings=settings,
        resource=resource,
        indexers=indexers,
        memories=memories,
        memobase=Memo(),
        raw_event={'type': event_type, 'object': body},
        event_queue=asyncio.Queue(),
        resource_indexed=Toggle(),  # used! only to enable indexing.
    )
    assert set(index) == set()
    assert memory.indexing_memory.indexing_state['index_fn'].finished == False
    assert memory.indexing_memory.indexing_state['index_fn'].failure == False
    assert memory.indexing_memory.indexing_state['index_fn'].success == False
    assert memory.indexing_memory.indexing_state['index_fn'].message == 'boo!'
    assert memory.indexing_memory.indexing_state['index_fn'].delayed == expected_delayed


@pytest.mark.usefixtures('indexed_123')
@pytest.mark.parametrize('event_type', EVENT_TYPES_WHEN_EXISTS)
async def test_preserved_on_ignored_errors(
        resource, namespace, settings, registry, memories, indexers, index, caplog, event_type, handlers):
    caplog.set_level(logging.DEBUG)
    body = {'metadata': {'namespace': namespace, 'name': 'name1'}}
    memory = await memories.recall(raw_body=body)
    handlers.index_mock.side_effect = Exception("boo!")
    await process_resource_event(
        lifecycle=all_at_once,
        registry=registry,
        settings=settings,
        resource=resource,
        indexers=indexers,
        memories=memories,
        memobase=Memo(),
        raw_event={'type': event_type, 'object': body},
        event_queue=asyncio.Queue(),
        resource_indexed=Toggle(),  # used! only to enable indexing.
    )
    assert set(index) == {None}
    assert set(index[None]) == {123}
    assert memory.indexing_memory.indexing_state is None



================================================
FILE: tests/handling/indexing/test_index_population.py
================================================
import asyncio
import logging

import pytest

from kopf._cogs.aiokits.aiotoggles import Toggle
from kopf._cogs.structs.ephemera import Memo
from kopf._core.actions.lifecycles import all_at_once
from kopf._core.reactor.inventory import ResourceMemories
from kopf._core.reactor.processing import process_resource_event

EVENT_TYPES_WHEN_EXISTS = [None, 'ADDED', 'MODIFIED']
EVENT_TYPES_WHEN_GONE = ['DELETED']
EVENT_TYPES = EVENT_TYPES_WHEN_EXISTS + EVENT_TYPES_WHEN_GONE


@pytest.mark.parametrize('event_type', EVENT_TYPES_WHEN_EXISTS)
async def test_initially_stored(
        resource, namespace, settings, registry, indexers, index, caplog, event_type, handlers):
    caplog.set_level(logging.DEBUG)
    body = {'metadata': {'namespace': namespace, 'name': 'name1'}}
    handlers.index_mock.return_value = 123
    await process_resource_event(
        lifecycle=all_at_once,
        registry=registry,
        settings=settings,
        resource=resource,
        indexers=indexers,
        memories=ResourceMemories(),
        memobase=Memo(),
        raw_event={'type': event_type, 'object': body},
        event_queue=asyncio.Queue(),
        resource_indexed=Toggle(),  # used! only to enable indexing.
    )
    assert set(index) == {None}
    assert set(index[None]) == {123}


@pytest.mark.usefixtures('indexed_123')
@pytest.mark.parametrize('event_type', EVENT_TYPES_WHEN_EXISTS)
async def test_overwritten(
        resource, namespace, settings, registry, indexers, index, caplog, event_type, handlers):
    caplog.set_level(logging.DEBUG)
    body = {'metadata': {'namespace': namespace, 'name': 'name1'}}
    handlers.index_mock.return_value = 456
    await process_resource_event(
        lifecycle=all_at_once,
        registry=registry,
        settings=settings,
        resource=resource,
        indexers=indexers,
        memories=ResourceMemories(),
        memobase=Memo(),
        raw_event={'type': event_type, 'object': body},
        event_queue=asyncio.Queue(),
        resource_indexed=Toggle(),  # used! only to enable indexing.
    )
    assert set(index) == {None}
    assert set(index[None]) == {456}


@pytest.mark.usefixtures('indexed_123')
@pytest.mark.parametrize('event_type', EVENT_TYPES_WHEN_EXISTS)
async def test_preserved_on_logical_deletion(
        resource, namespace, settings, registry, indexers, index, caplog, event_type, handlers):
    caplog.set_level(logging.DEBUG)
    body = {'metadata': {'namespace': namespace, 'name': 'name1',
                         'deletionTimestamp': '...'}}
    handlers.index_mock.return_value = 456
    await process_resource_event(
        lifecycle=all_at_once,
        registry=registry,
        settings=settings,
        resource=resource,
        indexers=indexers,
        memories=ResourceMemories(),
        memobase=Memo(),
        raw_event={'type': event_type, 'object': body},
        event_queue=asyncio.Queue(),
        resource_indexed=Toggle(),  # used! only to enable indexing.
    )
    assert set(index) == {None}
    assert set(index[None]) == {456}


@pytest.mark.usefixtures('indexed_123')
@pytest.mark.parametrize('event_type', EVENT_TYPES_WHEN_GONE)
async def test_removed_on_physical_deletion(
        resource, namespace, settings, registry, indexers, index, caplog, event_type, handlers):
    caplog.set_level(logging.DEBUG)
    body = {'metadata': {'namespace': namespace, 'name': 'name1'}}
    handlers.index_mock.return_value = 456
    await process_resource_event(
        lifecycle=all_at_once,
        registry=registry,
        settings=settings,
        resource=resource,
        indexers=indexers,
        memories=ResourceMemories(),
        memobase=Memo(),
        raw_event={'type': event_type, 'object': body},
        event_queue=asyncio.Queue(),
        resource_indexed=Toggle(),  # used! only to enable indexing.
    )
    assert set(index) == set()


@pytest.mark.usefixtures('indexed_123')
@pytest.mark.parametrize('event_type', EVENT_TYPES_WHEN_EXISTS)
async def test_removed_on_filters_mismatch(
        resource, namespace, settings, registry, indexers, index,
        caplog, event_type, handlers, mocker):

    # Simulate the indexing handler is gone out of scope (this is only one of the ways to do it):
    mocker.patch.object(registry._indexing, 'get_handlers', return_value=[])

    caplog.set_level(logging.DEBUG)
    body = {'metadata': {'namespace': namespace, 'name': 'name1'}}
    handlers.index_mock.return_value = 123
    await process_resource_event(
        lifecycle=all_at_once,
        registry=registry,
        settings=settings,
        resource=resource,
        indexers=indexers,
        memories=ResourceMemories(),
        memobase=Memo(),
        raw_event={'type': event_type, 'object': body},
        event_queue=asyncio.Queue(),
        resource_indexed=Toggle(),  # used! only to enable indexing.
    )
    assert set(index) == set()



================================================
FILE: tests/handling/subhandling/test_subhandling.py
================================================
import asyncio
import logging
from unittest.mock import Mock

import pytest

import kopf
from kopf._cogs.structs.ephemera import Memo
from kopf._core.engines.indexing import OperatorIndexers
from kopf._core.intents.causes import Reason
from kopf._core.reactor.inventory import ResourceMemories
from kopf._core.reactor.processing import process_resource_event

EVENT_TYPES_WHEN_EXISTS = [None, 'ADDED', 'MODIFIED']


@pytest.mark.parametrize('event_type', EVENT_TYPES_WHEN_EXISTS)
async def test_1st_level(registry, settings, resource, cause_mock, event_type,
                         caplog, assert_logs, k8s_mocked):
    caplog.set_level(logging.DEBUG)
    cause_mock.reason = Reason.CREATE

    fn_mock = Mock(return_value=None)
    sub1a_mock = Mock(return_value=None)
    sub1b_mock = Mock(return_value=None)

    # Only to justify the finalizer. See: cause_mock, which adds finalizers always.
    # TODO: get rid of mocks, test it normally.
    @kopf.on.delete('kopfexamples', id='del')
    async def _del(**_):
        pass

    @kopf.on.create('kopfexamples', id='fn')
    async def fn(**kwargs):
        fn_mock(**kwargs)
        @kopf.subhandler(id='sub1a')
        async def sub1a(**kwargs):
            sub1a_mock(**kwargs)
        @kopf.subhandler(id='sub1b')
        async def sub1b(**_):
            sub1b_mock(**kwargs)

    event_queue = asyncio.Queue()
    await process_resource_event(
        lifecycle=kopf.lifecycles.all_at_once,
        registry=registry,
        settings=settings,
        resource=resource,
        indexers=OperatorIndexers(),
        memories=ResourceMemories(),
        memobase=Memo(),
        raw_event={'type': event_type, 'object': {}},
        event_queue=event_queue,
    )

    assert fn_mock.call_count == 1
    assert sub1a_mock.call_count == 1
    assert sub1b_mock.call_count == 1

    assert k8s_mocked.sleep.call_count == 0
    assert k8s_mocked.patch.call_count == 1
    assert not event_queue.empty()

    assert_logs([
        "Creation is in progress:",
        "Handler 'fn' is invoked",
        "Handler 'fn/sub1a' is invoked",
        "Handler 'fn/sub1a' succeeded",
        "Handler 'fn/sub1b' is invoked",
        "Handler 'fn/sub1b' succeeded",
        "Handler 'fn' succeeded",
        "Creation is processed",
        "Patching with",
    ])


@pytest.mark.parametrize('event_type', EVENT_TYPES_WHEN_EXISTS)
async def test_2nd_level(registry, settings, resource, cause_mock, event_type,
                         caplog, assert_logs, k8s_mocked):
    caplog.set_level(logging.DEBUG)
    cause_mock.reason = Reason.CREATE

    fn_mock = Mock(return_value=None)
    sub1a_mock = Mock(return_value=None)
    sub1b_mock = Mock(return_value=None)
    sub1a2a_mock = Mock(return_value=None)
    sub1a2b_mock = Mock(return_value=None)
    sub1b2a_mock = Mock(return_value=None)
    sub1b2b_mock = Mock(return_value=None)

    # Only to justify the finalizer. See: cause_mock, which adds finalizers always.
    # TODO: get rid of mocks, test it normally.
    @kopf.on.delete('kopfexamples', id='del')
    async def _del(**_):
        pass

    @kopf.on.create(*resource, id='fn')
    def fn(**kwargs):
        fn_mock(**kwargs)
        @kopf.subhandler(id='sub1a')
        def sub1a(**kwargs):
            sub1a_mock(**kwargs)
            @kopf.subhandler(id='sub1a2a')
            def sub1a2a(**kwargs):
                sub1a2a_mock(**kwargs)
            @kopf.subhandler(id='sub1a2b')
            def sub1a2b(**kwargs):
                sub1a2b_mock(**kwargs)
        @kopf.subhandler(id='sub1b')
        def sub1b(**kwargs):
            sub1b_mock(**kwargs)
            @kopf.subhandler(id='sub1b2a')
            def sub1b2a(**kwargs):
                sub1b2a_mock(**kwargs)
            @kopf.subhandler(id='sub1b2b')
            def sub1b2b(**kwargs):
                sub1b2b_mock(**kwargs)

    event_queue = asyncio.Queue()
    await process_resource_event(
        lifecycle=kopf.lifecycles.all_at_once,
        registry=registry,
        settings=settings,
        resource=resource,
        indexers=OperatorIndexers(),
        memories=ResourceMemories(),
        memobase=Memo(),
        raw_event={'type': event_type, 'object': {}},
        event_queue=event_queue,
    )

    assert fn_mock.call_count == 1
    assert sub1a_mock.call_count == 1
    assert sub1b_mock.call_count == 1
    assert sub1a2a_mock.call_count == 1
    assert sub1a2b_mock.call_count == 1
    assert sub1b2a_mock.call_count == 1
    assert sub1b2b_mock.call_count == 1

    assert k8s_mocked.sleep.call_count == 0
    assert k8s_mocked.patch.call_count == 1
    assert not event_queue.empty()

    assert_logs([
        "Creation is in progress:",
        "Handler 'fn' is invoked",
        "Handler 'fn/sub1a' is invoked",
        "Handler 'fn/sub1a/sub1a2a' is invoked",
        "Handler 'fn/sub1a/sub1a2a' succeeded",
        "Handler 'fn/sub1a/sub1a2b' is invoked",
        "Handler 'fn/sub1a/sub1a2b' succeeded",
        "Handler 'fn/sub1a' succeeded",
        "Handler 'fn/sub1b' is invoked",
        "Handler 'fn/sub1b/sub1b2a' is invoked",
        "Handler 'fn/sub1b/sub1b2a' succeeded",
        "Handler 'fn/sub1b/sub1b2b' is invoked",
        "Handler 'fn/sub1b/sub1b2b' succeeded",
        "Handler 'fn/sub1b' succeeded",
        "Handler 'fn' succeeded",
        "Creation is processed",
        "Patching with",
    ])



================================================
FILE: tests/hierarchies/conftest.py
================================================
from unittest.mock import Mock

import pytest


class CustomIterable:
    def __init__(self, objs):
        self._objs = objs

    def __iter__(self):
        yield from self._objs


@pytest.fixture(params=[list, tuple, CustomIterable],
                ids=['list', 'tuple', 'custom'])
def multicls(request):
    return request.param


@pytest.fixture()
def pykube_object(pykube):
    obj = pykube.objects.CronJob(Mock(), {
        'metadata': {},
        'spec': {
            'jobTemplate': {},
        },
    })
    return obj


@pytest.fixture()
def kubernetes_model(kubernetes):
    # The most tricky class -- with attribute-to-key mapping (jobTemplate).
    obj = kubernetes.client.V1CronJob(
        metadata=kubernetes.client.V1ObjectMeta(),
        spec=kubernetes.client.V1CronJobSpec(
            schedule='* * * * *',
            job_template=kubernetes.client.V1JobTemplateSpec(),
        ),
    )
    return obj



================================================
FILE: tests/hierarchies/test_contextual_owner.py
================================================
import copy
import logging

import pytest

import kopf
from kopf._cogs.structs.bodies import Body, RawBody, RawEvent, RawMeta
from kopf._cogs.structs.ephemera import Memo
from kopf._cogs.structs.patches import Patch
from kopf._core.actions.execution import cause_var
from kopf._core.actions.invocation import context
from kopf._core.engines.indexing import OperatorIndexers
from kopf._core.intents.causes import ChangingCause, Reason, WatchingCause

OWNER_API_VERSION = 'owner-api-version'
OWNER_NAMESPACE = 'owner-namespace'
OWNER_KIND = 'OwnerKind'
OWNER_NAME = 'owner-name'
OWNER_UID = 'owner-uid'
OWNER_LABELS = {'label-1': 'value-1', 'label-2': 'value-2'}
OWNER = RawBody(
    apiVersion=OWNER_API_VERSION,
    kind=OWNER_KIND,
    metadata=RawMeta(
        namespace=OWNER_NAMESPACE,
        name=OWNER_NAME,
        uid=OWNER_UID,
        labels=OWNER_LABELS,
    ),
)


@pytest.fixture(params=['state-changing-cause', 'event-watching-cause'])
def owner(request, resource):
    body = Body(copy.deepcopy(OWNER))
    if request.param == 'state-changing-cause':
        cause = ChangingCause(
            logger=logging.getLogger('kopf.test.fake.logger'),
            indices=OperatorIndexers().indices,
            resource=resource,
            patch=Patch(),
            memo=Memo(),
            body=body,
            initial=False,
            reason=Reason.NOOP,
        )
        with context([(cause_var, cause)]):
            yield body
    elif request.param == 'event-watching-cause':
        cause = WatchingCause(
            logger=logging.getLogger('kopf.test.fake.logger'),
            indices=OperatorIndexers().indices,
            resource=resource,
            patch=Patch(),
            memo=Memo(),
            body=body,
            type='irrelevant',
            event=RawEvent(type='irrelevant', object=OWNER),
        )
        with context([(cause_var, cause)]):
            yield body
    else:
        raise RuntimeError(f"Wrong param for `owner` fixture: {request.param!r}")


def test_when_unset_for_owner_references_appending():
    with pytest.raises(LookupError) as e:
        kopf.append_owner_reference([])
    assert 'Owner must be set explicitly' in str(e.value)


def test_when_unset_for_owner_references_removal():
    with pytest.raises(LookupError) as e:
        kopf.remove_owner_reference([])
    assert 'Owner must be set explicitly' in str(e.value)


def test_when_unset_for_name_harmonization():
    with pytest.raises(LookupError) as e:
        kopf.harmonize_naming([])
    assert 'Owner must be set explicitly' in str(e.value)


def test_when_unset_for_namespace_adjustment():
    with pytest.raises(LookupError) as e:
        kopf.adjust_namespace([])
    assert 'Owner must be set explicitly' in str(e.value)


def test_when_unset_for_labelling():
    with pytest.raises(LookupError) as e:
        kopf.label([])
    assert 'Owner must be set explicitly' in str(e.value)


def test_when_unset_for_adopting():
    with pytest.raises(LookupError) as e:
        kopf.adopt([])
    assert 'Owner must be set explicitly' in str(e.value)


def test_when_empty_for_name_harmonization(owner):
    owner._replace_with({})
    with pytest.raises(LookupError) as e:
        kopf.harmonize_naming([])
    assert 'Name must be set explicitly' in str(e.value)


def test_when_empty_for_namespace_adjustment(owner):
    # An absent namespace means a cluster-scoped resource -- a valid case.
    obj = {}
    owner._replace_with({})
    kopf.adjust_namespace(obj)
    assert obj['metadata']['namespace'] is None


def test_when_empty_for_adopting(owner):
    owner._replace_with({})
    with pytest.raises(LookupError):
        kopf.adopt([])
    # any error message: the order of functions is not specific.


def test_when_set_for_name_harmonization(owner):
    obj = {}
    kopf.harmonize_naming(obj)
    assert obj['metadata']['generateName'].startswith(OWNER_NAME)


def test_when_set_for_namespace_adjustment(owner):
    obj = {}
    kopf.adjust_namespace(obj)
    assert obj['metadata']['namespace'] == OWNER_NAMESPACE


def test_when_set_for_owner_references_appending(owner):
    obj = {}
    kopf.append_owner_reference(obj)
    assert obj['metadata']['ownerReferences']
    assert obj['metadata']['ownerReferences'][0]['uid'] == OWNER_UID


def test_when_set_for_owner_references_removal(owner):
    obj = {}
    kopf.append_owner_reference(obj)  # assumed to work, tested above
    kopf.remove_owner_reference(obj)  # this one is being tested here
    assert not obj['metadata']['ownerReferences']


def test_when_set_for_labelling(owner):
    obj = {}
    kopf.label(obj)
    assert obj['metadata']['labels'] == {'label-1': 'value-1', 'label-2': 'value-2'}



================================================
FILE: tests/hierarchies/test_labelling.py
================================================
import pytest

import kopf


def test_adding_to_dict():
    obj = {}
    kopf.label(obj, {'label-1': 'value-1', 'label-2': 'value-2'})
    assert 'metadata' in obj
    assert 'labels' in obj['metadata']
    assert isinstance(obj['metadata']['labels'], dict)
    assert len(obj['metadata']['labels']) == 2
    assert 'label-1' in obj['metadata']['labels']
    assert 'label-2' in obj['metadata']['labels']
    assert obj['metadata']['labels']['label-1'] == 'value-1'
    assert obj['metadata']['labels']['label-2'] == 'value-2'


def test_adding_to_dicts(multicls):
    obj1 = {}
    obj2 = {}
    objs = multicls([obj1, obj2])
    kopf.label(objs, {'label-1': 'value-1', 'label-2': 'value-2'})
    assert isinstance(obj1['metadata']['labels'], dict)
    assert len(obj1['metadata']['labels']) == 2
    assert 'label-1' in obj1['metadata']['labels']
    assert 'label-2' in obj1['metadata']['labels']
    assert obj1['metadata']['labels']['label-1'] == 'value-1'
    assert obj1['metadata']['labels']['label-2'] == 'value-2'

    assert isinstance(obj2['metadata']['labels'], dict)
    assert len(obj2['metadata']['labels']) == 2
    assert 'label-1' in obj2['metadata']['labels']
    assert 'label-2' in obj2['metadata']['labels']
    assert obj2['metadata']['labels']['label-1'] == 'value-1'
    assert obj2['metadata']['labels']['label-2'] == 'value-2'


def test_adding_to_pykube_object(pykube_object):
    del pykube_object.obj['metadata']
    kopf.label(pykube_object, {'label-1': 'value-1', 'label-2': 'value-2'})
    assert len(pykube_object.labels) == 2
    assert 'label-1' in pykube_object.labels
    assert 'label-2' in pykube_object.labels
    assert pykube_object.labels['label-1'] == 'value-1'
    assert pykube_object.labels['label-2'] == 'value-2'


def test_adding_to_kubernetes_model(kubernetes_model):
    kubernetes_model.metadata = None
    kopf.label(kubernetes_model, {'label-1': 'value-1', 'label-2': 'value-2'})
    assert len(kubernetes_model.metadata.labels) == 2
    assert 'label-1' in kubernetes_model.metadata.labels
    assert 'label-2' in kubernetes_model.metadata.labels
    assert kubernetes_model.metadata.labels['label-1'] == 'value-1'
    assert kubernetes_model.metadata.labels['label-2'] == 'value-2'


def test_forcing_true_warns_on_deprecated_option():
    obj = {'metadata': {'labels': {'label': 'old-value'}}}
    with pytest.deprecated_call(match=r"use forced="):
        kopf.label(obj, {'label': 'new-value'}, force=True)
    assert obj['metadata']['labels']['label'] == 'new-value'


def test_forcing_false_warns_on_deprecated_option():
    obj = {'metadata': {'labels': {'label': 'old-value'}}}
    with pytest.deprecated_call(match=r"use forced="):
        kopf.label(obj, {'label': 'new-value'}, force=False)
    assert obj['metadata']['labels']['label'] == 'old-value'


def test_forcing_true_to_dict():
    obj = {'metadata': {'labels': {'label': 'old-value'}}}
    kopf.label(obj, {'label': 'new-value'}, forced=True)
    assert obj['metadata']['labels']['label'] == 'new-value'


def test_forcing_false_to_dict():
    obj = {'metadata': {'labels': {'label': 'old-value'}}}
    kopf.label(obj, {'label': 'new-value'}, forced=False)
    assert obj['metadata']['labels']['label'] == 'old-value'


def test_forcing_default_to_dict():
    obj = {'metadata': {'labels': {'label': 'old-value'}}}
    kopf.label(obj, {'label': 'new-value'})
    assert obj['metadata']['labels']['label'] == 'old-value'


def test_forcing_true_to_pykube_object(pykube_object):
    pykube_object.labels['label'] = 'old-value'
    kopf.label(pykube_object, {'label': 'new-value'}, forced=True)
    assert pykube_object.labels['label'] == 'new-value'


def test_forcing_false_to_pykube_object(pykube_object):
    pykube_object.labels['label'] = 'old-value'
    kopf.label(pykube_object, {'label': 'new-value'}, forced=False)
    assert pykube_object.labels['label'] == 'old-value'


def test_forcing_default_to_pykube_object(pykube_object):
    pykube_object.labels['label'] = 'old-value'
    kopf.label(pykube_object, {'label': 'new-value'})
    assert pykube_object.labels['label'] == 'old-value'


def test_forcing_true_to_kubernetes_model(kubernetes_model):
    kubernetes_model.metadata.labels = {'label': 'old-value'}
    kopf.label(kubernetes_model, {'label': 'new-value'}, forced=True)
    assert kubernetes_model.metadata.labels['label'] == 'new-value'


def test_forcing_false_to_kubernetes_model(kubernetes_model):
    kubernetes_model.metadata.labels = {'label': 'old-value'}
    kopf.label(kubernetes_model, {'label': 'new-value'}, forced=False)
    assert kubernetes_model.metadata.labels['label'] == 'old-value'


def test_forcing_default_to_kubernetes_model(kubernetes_model):
    kubernetes_model.metadata.labels = {'label': 'old-value'}
    kopf.label(kubernetes_model, {'label': 'new-value'})
    assert kubernetes_model.metadata.labels['label'] == 'old-value'


@pytest.mark.parametrize('nested', [
    pytest.param(('spec.jobTemplate', 'spec.unexistent'), id='tuple'),
    pytest.param(['spec.jobTemplate', 'spec.unexistent'], id='list'),
    pytest.param({'spec.jobTemplate', 'spec.unexistent'}, id='set'),
    pytest.param('spec.jobTemplate', id='string'),
])
def test_nested_with_forced_true_to_dict(nested):
    obj = {'metadata': {'labels': {'label': 'old-value'}},
           'spec': {'jobTemplate': {}}}
    kopf.label(obj, {'label': 'new-value'}, nested=nested, forced=True)
    assert obj['metadata']['labels']['label'] == 'new-value'
    assert obj['spec']['jobTemplate']['metadata']['labels']['label'] == 'new-value'
    assert 'unexistent' not in obj['spec']


@pytest.mark.parametrize('nested', [
    pytest.param(('spec.jobTemplate', 'spec.unexistent'), id='tuple'),
    pytest.param(['spec.jobTemplate', 'spec.unexistent'], id='list'),
    pytest.param({'spec.jobTemplate', 'spec.unexistent'}, id='set'),
    pytest.param('spec.jobTemplate', id='string'),
])
def test_nested_with_forced_true_to_pykube_object(nested, pykube_object):
    pykube_object.labels.update({'label': 'old-value'})
    pykube_object.obj.update({'spec': {'jobTemplate': {}}})
    kopf.label(pykube_object, {'label': 'new-value'}, nested=nested, forced=True)
    assert pykube_object.labels['label'] == 'new-value'
    assert pykube_object.obj['spec']['jobTemplate']['metadata']['labels']['label'] == 'new-value'
    assert 'unexistent' not in pykube_object.obj['spec']


@pytest.mark.parametrize('nested', [
    pytest.param(('spec.jobTemplate', 'spec.unexistent'), id='tuple'),
    pytest.param(['spec.jobTemplate', 'spec.unexistent'], id='list'),
    pytest.param({'spec.jobTemplate', 'spec.unexistent'}, id='set'),
    pytest.param('spec.jobTemplate', id='string'),
])
def test_nested_with_forced_true_to_kubernetes_model(nested, kubernetes_model):
    kubernetes_model.metadata.labels = {'label': 'old-value'}
    kopf.label(kubernetes_model, {'label': 'new-value'}, nested=nested, forced=True)
    assert kubernetes_model.metadata.labels['label'] == 'new-value'
    assert kubernetes_model.spec.job_template.metadata.labels['label'] == 'new-value'
    assert not hasattr(kubernetes_model.spec, 'unexistent')


@pytest.mark.parametrize('nested', [
    pytest.param(('spec.jobTemplate', 'spec.unexistent'), id='tuple'),
    pytest.param(['spec.jobTemplate', 'spec.unexistent'], id='list'),
    pytest.param({'spec.jobTemplate', 'spec.unexistent'}, id='set'),
    pytest.param('spec.jobTemplate', id='string'),
])
def test_nested_with_forced_false_to_dict(nested):
    obj = {'metadata': {'labels': {'label': 'old-value'}},
           'spec': {'jobTemplate': {}}}
    kopf.label(obj, {'label': 'new-value'}, nested=nested, forced=False)
    assert obj['metadata']['labels']['label'] == 'old-value'
    assert obj['spec']['jobTemplate']['metadata']['labels']['label'] == 'new-value'
    assert 'unexistent' not in obj['spec']


@pytest.mark.parametrize('nested', [
    pytest.param(('spec.jobTemplate', 'spec.unexistent'), id='tuple'),
    pytest.param(['spec.jobTemplate', 'spec.unexistent'], id='list'),
    pytest.param({'spec.jobTemplate', 'spec.unexistent'}, id='set'),
    pytest.param('spec.jobTemplate', id='string'),
])
def test_nested_with_forced_false_to_pykube_object(nested, pykube_object):
    pykube_object.labels.update({'label': 'old-value'})
    pykube_object.obj.update({'spec': {'jobTemplate': {}}})
    kopf.label(pykube_object, {'label': 'new-value'}, nested=nested, forced=False)
    assert pykube_object.labels['label'] == 'old-value'
    assert pykube_object.obj['spec']['jobTemplate']['metadata']['labels']['label'] == 'new-value'
    assert 'unexistent' not in pykube_object.obj['spec']


@pytest.mark.parametrize('nested', [
    pytest.param(('spec.jobTemplate', 'spec.unexistent'), id='tuple'),
    pytest.param(['spec.jobTemplate', 'spec.unexistent'], id='list'),
    pytest.param({'spec.jobTemplate', 'spec.unexistent'}, id='set'),
    pytest.param('spec.jobTemplate', id='string'),
])
def test_nested_with_forced_false_to_kubernetes_model(nested, kubernetes_model):
    kubernetes_model.metadata.labels = {'label': 'old-value'}
    kopf.label(kubernetes_model, {'label': 'new-value'}, nested=nested, forced=False)
    assert kubernetes_model.metadata.labels['label'] == 'old-value'
    assert kubernetes_model.spec.job_template.metadata.labels['label'] == 'new-value'
    assert not hasattr(kubernetes_model.spec, 'unexistent')



================================================
FILE: tests/hierarchies/test_name_harmonizing.py
================================================
import copy

import pytest

import kopf

forced_mode = pytest.mark.parametrize('forcedness', [
    pytest.param(dict(forced=True), id='forcedTrue'),
])
non_forced_mode = pytest.mark.parametrize('forcedness', [
    pytest.param(dict(forced=False), id='forcedFalse'),
    pytest.param(dict(), id='forcedAbsent'),
])
any_forced_mode = pytest.mark.parametrize('forcedness', [
    pytest.param(dict(forced=True), id='forcedTrue'),
    pytest.param(dict(forced=False), id='forcedFalse'),
    pytest.param(dict(), id='forcedAbsent'),
])

strict_mode = pytest.mark.parametrize('strictness', [
    pytest.param(dict(strict=True), id='strictTrue'),
])
non_strict_mode = pytest.mark.parametrize('strictness', [
    pytest.param(dict(strict=False), id='strictFalse'),
    pytest.param(dict(), id='strictAbsent'),
])
any_strict_mode = pytest.mark.parametrize('strictness', [
    pytest.param(dict(strict=True), id='strictTrue'),
    pytest.param(dict(strict=False), id='strictFalse'),
    pytest.param(dict(), id='strictAbsent'),
])

obj1_with_names = pytest.mark.parametrize('obj1', [
    pytest.param({'metadata': {'name': 'a'}}, id='regularname'),
    pytest.param({'metadata': {'generateName': 'b'}}, id='generatename'),
    pytest.param({'metadata': {'name': 'c', 'generateName': 'd'}}, id='bothnames'),
])
obj2_with_names = pytest.mark.parametrize('obj2', [
    pytest.param({'metadata': {'name': 'a'}}, id='regularname'),
    pytest.param({'metadata': {'generateName': 'b'}}, id='generatename'),
    pytest.param({'metadata': {'name': 'c', 'generateName': 'd'}}, id='bothnames'),
])
obj1_without_names = pytest.mark.parametrize('obj1', [
    pytest.param({}, id='withoutmeta'),
    pytest.param({'metadata': {}}, id='withmeta'),
])
obj2_without_names = pytest.mark.parametrize('obj2', [
    pytest.param({}, id='withoutmeta'),
    pytest.param({'metadata': {}}, id='withmeta'),
])


# In the NON-FORCED mode, the EXISTING names are preserved.
# The strictness is not involved due to this (no new names added).
@obj1_with_names
@any_strict_mode
@non_forced_mode
def test_preserved_name_of_dict(forcedness, strictness, obj1):
    obj1 = copy.deepcopy(obj1)
    kopf.harmonize_naming(obj1, name='provided-name', **forcedness, **strictness)
    assert obj1['metadata'].get('name') != 'provided-name'
    assert obj1['metadata'].get('generateName') != 'provided-name'


@obj2_with_names
@obj1_with_names
@any_strict_mode
@non_forced_mode
def test_preserved_names_of_dicts(forcedness, strictness, multicls, obj1, obj2):
    obj1, obj2 = copy.deepcopy(obj1), copy.deepcopy(obj2)
    objs = multicls([obj1, obj2])
    kopf.harmonize_naming(objs, name='provided-name', **forcedness, **strictness)
    assert obj1['metadata'].get('name') != 'provided-name'
    assert obj2['metadata'].get('name') != 'provided-name'
    assert obj1['metadata'].get('generateName') != 'provided-name'
    assert obj2['metadata'].get('generateName') != 'provided-name'


@obj1_with_names
@any_strict_mode
@non_forced_mode
def test_preserved_names_of_pykube_object(forcedness, strictness, pykube_object, obj1):
    pykube_object.obj = copy.deepcopy(obj1)
    kopf.harmonize_naming(pykube_object, name='provided-name', **forcedness, **strictness)
    assert pykube_object.obj['metadata'].get('name') != 'provided-name'
    assert pykube_object.obj['metadata'].get('generateName') != 'provided-name'


@obj1_with_names
@any_strict_mode
@non_forced_mode
def test_preserved_names_of_kubernetes_model(forcedness, strictness, kubernetes_model, obj1):
    kubernetes_model.metadata.name = obj1.get('metadata', {}).get('name')
    kubernetes_model.metadata.generate_name = obj1.get('metadata', {}).get('generateName')
    kopf.harmonize_naming(kubernetes_model, name='provided-name', **forcedness, **strictness)
    assert kubernetes_model.metadata.name != 'provided-name'
    assert kubernetes_model.metadata.generate_name != 'provided-name'


# In the FORCED mode, the EXISTING names are overwritten.
# It only depends which of the names -- regular or generated -- is left.
@obj1_with_names
@strict_mode
@forced_mode
def test_overwriting_of_strict_name_of_dict(forcedness, strictness, obj1):
    obj1 = copy.deepcopy(obj1)
    kopf.harmonize_naming(obj1, name='provided-name', **forcedness, **strictness)
    assert 'name' in obj1['metadata']
    assert 'generateName' not in obj1['metadata']
    assert obj1['metadata']['name'] == 'provided-name'


@obj2_with_names
@obj1_with_names
@strict_mode
@forced_mode
def test_overwriting_of_strict_names_of_dicts(forcedness, strictness, multicls, obj1, obj2):
    obj1, obj2 = copy.deepcopy(obj1), copy.deepcopy(obj2)
    objs = multicls([obj1, obj2])
    kopf.harmonize_naming(objs, name='provided-name', **forcedness, **strictness)
    assert 'name' in obj1['metadata']
    assert 'name' in obj2['metadata']
    assert 'generateName' not in obj1['metadata']
    assert 'generateName' not in obj2['metadata']
    assert obj2['metadata']['name'] == 'provided-name'
    assert obj1['metadata']['name'] == 'provided-name'


@obj1_with_names
@strict_mode
@forced_mode
def test_overwriting_of_strict_name_of_pykube_object(forcedness, strictness, pykube_object, obj1):
    pykube_object.obj = copy.deepcopy(obj1)
    kopf.harmonize_naming(pykube_object, name='provided-name', **forcedness, **strictness)
    assert pykube_object.obj['metadata'].get('name') == 'provided-name'
    assert pykube_object.obj['metadata'].get('generateName') is None


@obj1_with_names
@strict_mode
@forced_mode
def test_overwriting_of_strict_name_of_kubernetes_model(forcedness, strictness, kubernetes_model, obj1):
    kubernetes_model.metadata.name = obj1.get('metadata', {}).get('name')
    kubernetes_model.metadata.generate_name = obj1.get('metadata', {}).get('generateName')
    kopf.harmonize_naming(kubernetes_model, name='provided-name', **forcedness, **strictness)
    assert kubernetes_model.metadata.name == 'provided-name'
    assert kubernetes_model.metadata.generate_name is None


@obj1_with_names
@non_strict_mode
@forced_mode
def test_overwriting_of_relaxed_name_of_dict(forcedness, strictness, obj1):
    obj1 = copy.deepcopy(obj1)
    kopf.harmonize_naming(obj1, name='provided-name', **forcedness, **strictness)
    assert 'name' not in obj1['metadata']
    assert 'generateName' in obj1['metadata']
    assert obj1['metadata']['generateName'] == 'provided-name-'


@obj2_with_names
@obj1_with_names
@non_strict_mode
@forced_mode
def test_overwriting_of_relaxed_names_of_dicts(forcedness, strictness, multicls, obj1, obj2):
    obj1, obj2 = copy.deepcopy(obj1), copy.deepcopy(obj2)
    objs = multicls([obj1, obj2])
    kopf.harmonize_naming(objs, name='provided-name', **forcedness, **strictness)
    assert 'name' not in obj1['metadata']
    assert 'name' not in obj2['metadata']
    assert 'generateName' in obj1['metadata']
    assert 'generateName' in obj2['metadata']
    assert obj1['metadata']['generateName'] == 'provided-name-'
    assert obj2['metadata']['generateName'] == 'provided-name-'


@obj1_with_names
@non_strict_mode
@forced_mode
def test_overwriting_of_relaxed_name_of_pykube_object(forcedness, strictness, pykube_object, obj1):
    pykube_object.obj = copy.deepcopy(obj1)
    kopf.harmonize_naming(pykube_object, name='provided-name', **forcedness, **strictness)
    assert pykube_object.obj['metadata'].get('name') is None
    assert pykube_object.obj['metadata'].get('generateName') == 'provided-name-'


@obj1_with_names
@non_strict_mode
@forced_mode
def test_overwriting_of_relaxed_name_of_kubernetes_model(forcedness, strictness, kubernetes_model, obj1):
    kubernetes_model.metadata.name = obj1.get('metadata', {}).get('name')
    kubernetes_model.metadata.generate_name = obj1.get('metadata', {}).get('generateName')
    kopf.harmonize_naming(kubernetes_model, name='provided-name', **forcedness, **strictness)
    assert kubernetes_model.metadata.name is None
    assert kubernetes_model.metadata.generate_name == 'provided-name-'


# When names are ABSENT, they are added regardless of the forced mode.
# The only varying part is which name is added: regular or generated.
@obj1_without_names
@strict_mode
@any_forced_mode
def test_assignment_of_strict_name_of_dict(forcedness, strictness, obj1):
    obj1 = copy.deepcopy(obj1)
    kopf.harmonize_naming(obj1, name='provided-name', **forcedness, **strictness)
    assert 'name' in obj1['metadata']
    assert 'generateName' not in obj1['metadata']
    assert obj1['metadata']['name'] == 'provided-name'


@obj2_without_names
@obj1_without_names
@strict_mode
@any_forced_mode
def test_assignment_of_strict_names_of_dicts(forcedness, strictness, multicls, obj1, obj2):
    obj1, obj2 = copy.deepcopy(obj1), copy.deepcopy(obj2)
    objs = multicls([obj1, obj2])
    kopf.harmonize_naming(objs, name='provided-name', **forcedness, **strictness)
    assert 'name' in obj1['metadata']
    assert 'name' in obj2['metadata']
    assert 'generateName' not in obj1['metadata']
    assert 'generateName' not in obj2['metadata']
    assert obj1['metadata']['name'] == 'provided-name'
    assert obj2['metadata']['name'] == 'provided-name'


@obj1_without_names
@strict_mode
@any_forced_mode
def test_assignment_of_strict_name_of_pykube_object(forcedness, strictness, pykube_object, obj1):
    pykube_object.obj = copy.deepcopy(obj1)
    kopf.harmonize_naming(pykube_object, name='provided-name', **forcedness, **strictness)
    assert pykube_object.obj['metadata'].get('name') == 'provided-name'
    assert pykube_object.obj['metadata'].get('generateName') is None


@strict_mode
@any_forced_mode
def test_assignment_of_strict_name_of_kubernetes_model(forcedness, strictness, kubernetes_model):
    kubernetes_model.metadata = None
    kopf.harmonize_naming(kubernetes_model, name='provided-name', **forcedness, **strictness)
    assert kubernetes_model.metadata.name == 'provided-name'
    assert kubernetes_model.metadata.generate_name is None


@obj1_without_names
@non_strict_mode
@any_forced_mode
def test_assignment_of_nonstrict_name_of_dict(forcedness, strictness, obj1):
    obj1 = copy.deepcopy(obj1)
    kopf.harmonize_naming(obj1, name='provided-name', **forcedness, **strictness)
    assert 'name' not in obj1['metadata']
    assert 'generateName' in obj1['metadata']
    assert obj1['metadata']['generateName'] == 'provided-name-'


@obj2_without_names
@obj1_without_names
@non_strict_mode
@any_forced_mode
def test_assignment_of_nonstrict_names_of_dicts(forcedness, strictness, multicls, obj1, obj2):
    obj1, obj2 = copy.deepcopy(obj1), copy.deepcopy(obj2)
    objs = multicls([obj1, obj2])
    kopf.harmonize_naming(objs, name='provided-name', **forcedness, **strictness)
    assert 'name' not in obj1['metadata']
    assert 'name' not in obj2['metadata']
    assert 'generateName' in obj1['metadata']
    assert 'generateName' in obj2['metadata']
    assert obj1['metadata']['generateName'] == 'provided-name-'
    assert obj2['metadata']['generateName'] == 'provided-name-'


@obj1_without_names
@non_strict_mode
@any_forced_mode
def test_assignment_of_nonstrict_name_of_pykube_object(forcedness, strictness, pykube_object, obj1):
    pykube_object.obj = copy.deepcopy(obj1)
    kopf.harmonize_naming(pykube_object, name='provided-name', **forcedness, **strictness)
    assert pykube_object.obj['metadata'].get('name') is None
    assert pykube_object.obj['metadata'].get('generateName') == 'provided-name-'


@non_strict_mode
@any_forced_mode
def test_assignment_of_nonstrict_name_of_kubernetes_model(forcedness, strictness, kubernetes_model):
    kubernetes_model.metadata = None
    kopf.harmonize_naming(kubernetes_model, name='provided-name', **forcedness, **strictness)
    assert kubernetes_model.metadata.name is None
    assert kubernetes_model.metadata.generate_name == 'provided-name-'



================================================
FILE: tests/hierarchies/test_namespace_adjusting.py
================================================
import copy

import pytest

import kopf

forced_mode = pytest.mark.parametrize('forcedness', [
    pytest.param(dict(forced=True), id='forcedTrue'),
])
non_forced_mode = pytest.mark.parametrize('forcedness', [
    pytest.param(dict(forced=False), id='forcedFalse'),
    pytest.param(dict(), id='forcedAbsent'),
])
any_forced_mode = pytest.mark.parametrize('forcedness', [
    pytest.param(dict(forced=True), id='forcedTrue'),
    pytest.param(dict(forced=False), id='forcedFalse'),
    pytest.param(dict(), id='forcedAbsent'),
])

obj1_with_namespace = pytest.mark.parametrize('obj1', [
    pytest.param({'metadata': {'namespace': 'a'}}, id='withnamespace'),
])
obj2_with_namespace = pytest.mark.parametrize('obj2', [
    pytest.param({'metadata': {'namespace': 'a'}}, id='withnamespace'),
])
obj1_without_namespace = pytest.mark.parametrize('obj1', [
    pytest.param({}, id='withoutmetadata'),
    pytest.param({'metadata': {}}, id='withoutnamespace'),
])
obj2_without_namespace = pytest.mark.parametrize('obj2', [
    pytest.param({}, id='withoutmetadata'),
    pytest.param({'metadata': {}}, id='withoutnamespace'),
])


# In the NON-FORCED mode, the EXISTING namespaces are preserved.
@obj1_with_namespace
@non_forced_mode
def test_preserved_namespace_of_dict(forcedness, obj1):
    obj1 = copy.deepcopy(obj1)
    kopf.adjust_namespace(obj1, namespace='provided-namespace', **forcedness)
    assert 'namespace' in obj1['metadata']
    assert obj1['metadata']['namespace'] != 'provided-namespace'


@obj2_with_namespace
@obj1_with_namespace
@non_forced_mode
def test_preserved_namespaces_of_dicts(forcedness, multicls, obj1, obj2):
    obj1, obj2 = copy.deepcopy(obj1), copy.deepcopy(obj2)
    objs = multicls([obj1, obj2])
    kopf.adjust_namespace(objs, namespace='provided-namespace', **forcedness)
    assert 'namespace' in obj1['metadata']
    assert 'namespace' in obj2['metadata']
    assert obj1['metadata']['namespace'] != 'provided-namespace'
    assert obj2['metadata']['namespace'] != 'provided-namespace'


@obj1_with_namespace
@non_forced_mode
def test_preserved_namespace_of_pykube_object(forcedness, pykube_object, obj1):
    pykube_object.obj = copy.deepcopy(obj1)
    kopf.adjust_namespace(pykube_object, namespace='provided-namespace', **forcedness)
    assert pykube_object.obj['metadata'].get('namespace') != 'provided-namespace'


@obj1_with_namespace
@non_forced_mode
def test_preserved_namespace_of_kubernetes_model(forcedness, kubernetes_model, obj1):
    kubernetes_model.metadata.namespace = obj1.get('metadata', {}).get('namespace')
    kopf.adjust_namespace(kubernetes_model, namespace='provided-namespace', **forcedness)
    assert kubernetes_model.metadata.namespace != 'provided-namespace'


#
# In the FORCED mode, the EXISTING namespaces are overwritten.
#
@obj1_with_namespace
@forced_mode
def test_overwriting_of_namespace_of_dict(forcedness, obj1):
    obj1 = copy.deepcopy(obj1)
    kopf.adjust_namespace(obj1, namespace='provided-namespace', **forcedness)
    assert 'namespace' in obj1['metadata']
    assert obj1['metadata']['namespace'] == 'provided-namespace'


@obj2_with_namespace
@obj1_with_namespace
@forced_mode
def test_overwriting_of_namespaces_of_dicts(forcedness, multicls, obj1, obj2):
    obj1, obj2 = copy.deepcopy(obj1), copy.deepcopy(obj2)
    objs = multicls([obj1, obj2])
    kopf.adjust_namespace(objs, namespace='provided-namespace', **forcedness)
    assert 'namespace' in obj1['metadata']
    assert 'namespace' in obj2['metadata']
    assert obj1['metadata']['namespace'] == 'provided-namespace'
    assert obj2['metadata']['namespace'] == 'provided-namespace'


@obj1_with_namespace
@forced_mode
def test_overwriting_namespace_of_pykube_object(forcedness, pykube_object, obj1):
    pykube_object.obj = copy.deepcopy(obj1)
    kopf.adjust_namespace(pykube_object, namespace='provided-namespace', **forcedness)
    assert pykube_object.obj['metadata'].get('namespace') == 'provided-namespace'


@obj1_with_namespace
@forced_mode
def test_overwriting_namespace_of_kubernetes_model(forcedness, kubernetes_model, obj1):
    kubernetes_model.metadata.namespace = obj1.get('metadata', {}).get('namespace')
    kopf.adjust_namespace(kubernetes_model, namespace='provided-namespace', **forcedness)
    assert kubernetes_model.metadata.namespace == 'provided-namespace'


#
# When namespaces are ABSENT, they are added regardless of the forced mode.
#
@obj1_without_namespace
@any_forced_mode
def test_assignment_of_namespace_of_dict(forcedness, obj1):
    obj1 = copy.deepcopy(obj1)
    kopf.adjust_namespace(obj1, namespace='provided-namespace', **forcedness)
    assert 'namespace' in obj1['metadata']
    assert obj1['metadata']['namespace'] == 'provided-namespace'


@obj2_without_namespace
@obj1_without_namespace
@any_forced_mode
def test_assignment_of_namespaces_of_dicts(forcedness, multicls, obj1, obj2):
    obj1, obj2 = copy.deepcopy(obj1), copy.deepcopy(obj2)
    objs = multicls([obj1, obj2])
    kopf.adjust_namespace(objs, namespace='provided-namespace', **forcedness)
    assert 'namespace' in obj1['metadata']
    assert 'namespace' in obj2['metadata']
    assert obj1['metadata']['namespace'] == 'provided-namespace'
    assert obj2['metadata']['namespace'] == 'provided-namespace'


@obj1_without_namespace
@any_forced_mode
def test_assignment_namespace_of_pykube_object(forcedness, pykube_object, obj1):
    pykube_object.obj = copy.deepcopy(obj1)
    kopf.adjust_namespace(pykube_object, namespace='provided-namespace', **forcedness)
    assert pykube_object.obj['metadata'].get('namespace') == 'provided-namespace'


@any_forced_mode
def test_assignment_namespace_of_kubernetes_model(forcedness, kubernetes_model):
    kubernetes_model.metadata = None
    kopf.adjust_namespace(kubernetes_model, namespace='provided-namespace', **forcedness)
    assert kubernetes_model.metadata.namespace == 'provided-namespace'



================================================
FILE: tests/hierarchies/test_owner_referencing.py
================================================
import copy
from unittest.mock import call

import pytest

import kopf
from kopf._cogs.structs.bodies import Body, RawBody, RawMeta

OWNER_API_VERSION = 'owner-api-version'
OWNER_NAMESPACE = 'owner-namespace'
OWNER_KIND = 'OwnerKind'
OWNER_NAME = 'owner-name'
OWNER_UID = 'owner-uid'
OWNER_LABELS = {'label-1': 'value-1', 'label-2': 'value-2'}
OWNER = RawBody(
    apiVersion=OWNER_API_VERSION,
    kind=OWNER_KIND,
    metadata=RawMeta(
        namespace=OWNER_NAMESPACE,
        name=OWNER_NAME,
        uid=OWNER_UID,
        labels=OWNER_LABELS,
    ),
)


def test_appending_to_dict():
    obj = {}

    kopf.append_owner_reference(
        obj, owner=Body(OWNER), controller=False, block_owner_deletion=False
    )

    assert 'metadata' in obj
    assert 'ownerReferences' in obj['metadata']
    assert isinstance(obj['metadata']['ownerReferences'], list)
    assert len(obj['metadata']['ownerReferences']) == 1
    assert isinstance(obj['metadata']['ownerReferences'][0], dict)
    assert obj['metadata']['ownerReferences'][0]['apiVersion'] == OWNER_API_VERSION
    assert obj['metadata']['ownerReferences'][0]['kind'] == OWNER_KIND
    assert obj['metadata']['ownerReferences'][0]['name'] == OWNER_NAME
    assert obj['metadata']['ownerReferences'][0]['uid'] == OWNER_UID
    assert obj['metadata']['ownerReferences'][0]['controller'] == False
    assert obj['metadata']['ownerReferences'][0]['blockOwnerDeletion'] == False


def test_appending_to_dicts(multicls):
    obj1 = {}
    obj2 = {}
    objs = multicls([obj1, obj2])

    kopf.append_owner_reference(objs, owner=Body(OWNER))

    assert isinstance(obj1['metadata']['ownerReferences'], list)
    assert len(obj1['metadata']['ownerReferences']) == 1
    assert obj1['metadata']['ownerReferences'][0]['apiVersion'] == OWNER_API_VERSION
    assert obj1['metadata']['ownerReferences'][0]['kind'] == OWNER_KIND
    assert obj1['metadata']['ownerReferences'][0]['name'] == OWNER_NAME
    assert obj1['metadata']['ownerReferences'][0]['uid'] == OWNER_UID

    assert isinstance(obj2['metadata']['ownerReferences'], list)
    assert len(obj2['metadata']['ownerReferences']) == 1
    assert obj2['metadata']['ownerReferences'][0]['apiVersion'] == OWNER_API_VERSION
    assert obj2['metadata']['ownerReferences'][0]['kind'] == OWNER_KIND
    assert obj2['metadata']['ownerReferences'][0]['name'] == OWNER_NAME
    assert obj2['metadata']['ownerReferences'][0]['uid'] == OWNER_UID


def test_appending_to_pykube_object(pykube_object):
    del pykube_object.obj['metadata']
    kopf.append_owner_reference(pykube_object, owner=Body(OWNER))
    assert 'metadata' in pykube_object.obj
    assert 'ownerReferences' in pykube_object.obj['metadata']
    assert isinstance(pykube_object.obj['metadata']['ownerReferences'], list)
    assert len(pykube_object.obj['metadata']['ownerReferences']) == 1
    assert isinstance(pykube_object.obj['metadata']['ownerReferences'][0], dict)
    assert pykube_object.obj['metadata']['ownerReferences'][0]['apiVersion'] == OWNER_API_VERSION
    assert pykube_object.obj['metadata']['ownerReferences'][0]['kind'] == OWNER_KIND
    assert pykube_object.obj['metadata']['ownerReferences'][0]['name'] == OWNER_NAME
    assert pykube_object.obj['metadata']['ownerReferences'][0]['uid'] == OWNER_UID


def test_appending_to_kubernetes_model(kubernetes_model):
    kubernetes_model.metadata = None
    kopf.append_owner_reference(kubernetes_model, owner=Body(OWNER))
    assert kubernetes_model.metadata is not None
    assert kubernetes_model.metadata.owner_references is not None
    assert isinstance(kubernetes_model.metadata.owner_references, list)
    assert len(kubernetes_model.metadata.owner_references) == 1
    assert kubernetes_model.metadata.owner_references[0].api_version == OWNER_API_VERSION
    assert kubernetes_model.metadata.owner_references[0].kind == OWNER_KIND
    assert kubernetes_model.metadata.owner_references[0].name == OWNER_NAME
    assert kubernetes_model.metadata.owner_references[0].uid == OWNER_UID


def test_appending_deduplicates_by_uid():
    """
    The uid is the only necessary criterion to identify same objects.
    No matter how we change the irrelevant non-id fields, they must be ignored.
    """
    owner1 = copy.deepcopy(OWNER)
    owner2 = copy.deepcopy(OWNER)
    owner3 = copy.deepcopy(OWNER)
    owner1['kind'] = 'KindA'
    owner2['kind'] = 'KindA'
    owner3['kind'] = 'KindB'
    owner1['metadata']['name'] = 'name-a'
    owner2['metadata']['name'] = 'name-b'
    owner3['metadata']['name'] = 'name-b'
    owner1['metadata']['uid'] = 'uid-0'
    owner2['metadata']['uid'] = 'uid-0'
    owner3['metadata']['uid'] = 'uid-0'
    obj = {}

    kopf.append_owner_reference(obj, owner=Body(owner1))
    kopf.append_owner_reference(obj, owner=Body(owner2))
    kopf.append_owner_reference(obj, owner=Body(owner3))

    assert len(obj['metadata']['ownerReferences']) == 1
    assert obj['metadata']['ownerReferences'][0]['uid'] == 'uid-0'


def test_appending_distinguishes_by_uid():
    """
    Changing only the uid should be sufficient to consider a new owner.
    Here, all other non-id fields are the same, and must be ignored.
    """
    owner1 = copy.deepcopy(OWNER)
    owner2 = copy.deepcopy(OWNER)
    owner1['metadata']['uid'] = 'uid-a'
    owner2['metadata']['uid'] = 'uid-b'
    obj = {}

    kopf.append_owner_reference(obj, owner=Body(owner1))
    kopf.append_owner_reference(obj, owner=Body(owner2))

    uids = {ref['uid'] for ref in obj['metadata']['ownerReferences']}
    assert uids == {'uid-a', 'uid-b'}


def test_removal_from_dict():
    obj = {}

    kopf.append_owner_reference(obj, owner=Body(OWNER))  # assumed to work, tested above
    kopf.remove_owner_reference(obj, owner=Body(OWNER))  # this one is being tested here

    assert 'metadata' in obj
    assert 'ownerReferences' in obj['metadata']
    assert isinstance(obj['metadata']['ownerReferences'], list)
    assert len(obj['metadata']['ownerReferences']) == 0


def test_removal_from_dicts(multicls):
    obj1 = {}
    obj2 = {}
    objs = multicls([obj1, obj2])

    kopf.append_owner_reference(objs, owner=Body(OWNER))  # assumed to work, tested above
    kopf.remove_owner_reference(objs, owner=Body(OWNER))  # this one is being tested here

    assert 'metadata' in obj1
    assert 'ownerReferences' in obj1['metadata']
    assert isinstance(obj1['metadata']['ownerReferences'], list)
    assert len(obj1['metadata']['ownerReferences']) == 0

    assert 'metadata' in obj2
    assert 'ownerReferences' in obj2['metadata']
    assert isinstance(obj2['metadata']['ownerReferences'], list)
    assert len(obj2['metadata']['ownerReferences']) == 0


def test_removal_from_pykube_object(pykube_object):
    del pykube_object.obj['metadata']
    kopf.append_owner_reference(pykube_object, owner=Body(OWNER))
    kopf.remove_owner_reference(pykube_object, owner=Body(OWNER))
    assert 'metadata' in pykube_object.obj
    assert 'ownerReferences' in pykube_object.obj['metadata']
    assert isinstance(pykube_object.obj['metadata']['ownerReferences'], list)
    assert len(pykube_object.obj['metadata']['ownerReferences']) == 0


def test_removal_from_kubernetes_model(kubernetes_model):
    kubernetes_model.metadata = None
    kopf.append_owner_reference(kubernetes_model, owner=Body(OWNER))
    kopf.remove_owner_reference(kubernetes_model, owner=Body(OWNER))
    assert kubernetes_model.metadata is not None
    assert kubernetes_model.metadata.owner_references is not None
    assert isinstance(kubernetes_model.metadata.owner_references, list)
    assert len(kubernetes_model.metadata.owner_references) == 0


def test_removal_identifies_by_uid():
    owner1 = copy.deepcopy(OWNER)
    owner2 = copy.deepcopy(OWNER)
    owner3 = copy.deepcopy(OWNER)
    owner1['kind'] = 'KindA'
    owner2['kind'] = 'KindA'
    owner3['kind'] = 'KindB'
    owner1['metadata']['name'] = 'name-a'
    owner2['metadata']['name'] = 'name-b'
    owner3['metadata']['name'] = 'name-b'
    owner1['metadata']['uid'] = 'uid-0'
    owner2['metadata']['uid'] = 'uid-0'
    owner3['metadata']['uid'] = 'uid-0'
    obj = {}

    # Three different owners added, but all have the same uid.
    # One is removed and only once, all must be gone (due to same uids).
    kopf.append_owner_reference(obj, owner=Body(owner1))  # assumed to work, tested above
    kopf.append_owner_reference(obj, owner=Body(owner2))  # assumed to work, tested above
    kopf.append_owner_reference(obj, owner=Body(owner3))  # assumed to work, tested above
    kopf.remove_owner_reference(obj, owner=Body(owner1))  # this one is being tested here

    assert len(obj['metadata']['ownerReferences']) == 0


def test_removal_distinguishes_by_uid():
    owner1 = copy.deepcopy(OWNER)
    owner2 = copy.deepcopy(OWNER)
    owner3 = copy.deepcopy(OWNER)
    owner1['metadata']['uid'] = 'uid-a'
    owner2['metadata']['uid'] = 'uid-b'
    owner3['metadata']['uid'] = 'uid-c'
    obj = {}

    # Three very similar owners added, different only by uid.
    # One is removed, others must stay (even if kinds/names are the same).
    kopf.append_owner_reference(obj, owner=Body(owner1))  # assumed to work, tested above
    kopf.append_owner_reference(obj, owner=Body(owner2))  # assumed to work, tested above
    kopf.append_owner_reference(obj, owner=Body(owner3))  # assumed to work, tested above
    kopf.remove_owner_reference(obj, owner=Body(owner1))  # this one is being tested here

    uids = {ref['uid'] for ref in obj['metadata']['ownerReferences']}
    assert uids == {'uid-b', 'uid-c'}


# Not related to owner references only, but uses the OWNER constants.
@pytest.mark.parametrize('nested', [
    pytest.param(('spec.template',), id='tuple'),
    pytest.param(['spec.template'], id='list'),
    pytest.param({'spec.template'}, id='set'),
    pytest.param('spec.template', id='string'),
])
@pytest.mark.parametrize('strict', [True, False])
@pytest.mark.parametrize('forced', [True, False])
def test_adopting(mocker, forced, strict, nested):
    # These methods are tested in their own tests.
    # We just check that they are called at all.
    append_owner_ref = mocker.patch('kopf._kits.hierarchies.append_owner_reference')
    harmonize_naming = mocker.patch('kopf._kits.hierarchies.harmonize_naming')
    adjust_namespace = mocker.patch('kopf._kits.hierarchies.adjust_namespace')
    label = mocker.patch('kopf._kits.hierarchies.label')

    obj = {}
    kopf.adopt(obj, owner=Body(OWNER), forced=forced, strict=strict, nested=nested)

    assert append_owner_ref.called
    assert harmonize_naming.called
    assert adjust_namespace.called
    assert label.called

    assert append_owner_ref.call_args == call(obj, owner=Body(OWNER))
    assert harmonize_naming.call_args == call(obj, name=OWNER_NAME, forced=forced, strict=strict)
    assert adjust_namespace.call_args == call(obj, namespace=OWNER_NAMESPACE, forced=forced)
    assert label.call_args == call(obj, labels=OWNER_LABELS, nested=nested, forced=forced)



================================================
FILE: tests/hierarchies/test_type_validation.py
================================================
import pytest

import kopf
from kopf._cogs.structs.bodies import Body


def test_in_owner_reference_appending():
    with pytest.raises(TypeError) as e:
        kopf.append_owner_reference(object(), Body({}))
    assert "K8s object class is not supported" in str(e.value)


def test_in_owner_reference_removal():
    with pytest.raises(TypeError) as e:
        kopf.remove_owner_reference(object(), Body({}))
    assert "K8s object class is not supported" in str(e.value)


def test_in_name_harmonization():
    with pytest.raises(TypeError) as e:
        kopf.harmonize_naming(object(), 'x')
    assert "K8s object class is not supported" in str(e.value)


def test_in_namepace_adjustment():
    with pytest.raises(TypeError) as e:
        kopf.adjust_namespace(object(), 'x')
    assert "K8s object class is not supported" in str(e.value)


def test_in_labelling():
    with pytest.raises(TypeError) as e:
        kopf.label(object(), {})
    assert "K8s object class is not supported" in str(e.value)


def test_in_adopting():
    with pytest.raises(TypeError) as e:
        kopf.adopt(object(), Body({}))
    assert "K8s object class is not supported" in str(e.value)



================================================
FILE: tests/invocations/test_callbacks.py
================================================
import functools
import logging
import traceback
from unittest.mock import Mock

import pytest

from kopf._cogs.structs.bodies import Body
from kopf._cogs.structs.patches import Patch
from kopf._core.actions.invocation import invoke, is_async_fn
from kopf._core.engines.indexing import OperatorIndexers
from kopf._core.intents.causes import ChangingCause, Reason

STACK_TRACE_MARKER = object()


def _find_marker():
    marker_repr = repr(STACK_TRACE_MARKER)
    stack = traceback.StackSummary.extract(traceback.walk_stack(None), capture_locals=True)
    for frame in stack:
        if 'stack_trace_marker' in frame.locals:
            if frame.locals['stack_trace_marker'] == marker_repr:
                return True
    return False


def sync_fn(*args, **kwargs):
    return _find_marker()


async def async_fn(*args, **kwargs):
    return _find_marker()


def sync_mock_fn(mock, *args, **kwargs):
    return mock(*args, **kwargs)


async def async_mock_fn(mock, *args, **kwargs):
    return mock(*args, **kwargs)


def partials(fn, n):
    partial = fn
    for _ in range(n):
        partial = functools.partial(partial)
    return partial


def wrappers(fn, n):
    wrapper = fn
    for _ in range(n):
        @functools.wraps(wrapper)
        def wrapper(*args, wrapper=wrapper, **kwargs):
            return wrapper(*args, **kwargs)
    return wrapper


def awaiters(fn, n):
    wrapper = fn
    for _ in range(n):
        @functools.wraps(wrapper)
        async def wrapper(*args, wrapper=wrapper, **kwargs):
            return await wrapper(*args, **kwargs)
    return wrapper


def partials_wrappers(fn, n):
    wrapper = fn
    for _ in range(n):
        wrapper = functools.partial(wrapper)
        @functools.wraps(wrapper)
        def wrapper(*args, wrapper=wrapper, **kwargs):
            return wrapper(*args, **kwargs)
    return wrapper


def partials_awaiters(fn, n):
    wrapper = fn
    for _ in range(n):
        wrapper = functools.partial(wrapper)
        @functools.wraps(wrapper)
        async def wrapper(*args, wrapper=wrapper, **kwargs):
            return await wrapper(*args, **kwargs)
    return wrapper


fns = pytest.mark.parametrize(
    'fn', [
        (sync_mock_fn),
        (async_mock_fn),
    ])

# Every combination of partials, sync & async wrappers possible.
syncasyncparams = pytest.mark.parametrize(
    'fn, expected', [
        (sync_fn, False),
        (async_fn, True),
        (partials(sync_fn, 1), False),
        (partials(async_fn, 1), True),
        (partials(sync_fn, 9), False),
        (partials(async_fn, 9), True),
        (wrappers(sync_fn, 1), False),
        (wrappers(async_fn, 1), True),
        (wrappers(sync_fn, 9), False),
        (wrappers(async_fn, 9), True),
        (awaiters(async_fn, 1), True),
        (awaiters(async_fn, 9), True),
        (partials_wrappers(sync_fn, 9), False),
        (partials_wrappers(async_fn, 9), True),
        (partials_awaiters(async_fn, 9), True),
    ], ids=[
        'sync-direct',
        'async-direct',
        'sync-partial-once',
        'async-partial-once',
        'sync-partial-many',
        'async-partial-many',
        'sync-wrapper-once',
        'async-wrapper-once',
        'sync-wrapper-many',
        'async-wrapper-many',
        'async-awaiter-once',
        'async-awaiter-many',
        'sync-mixed-partials-wrappers',
        'async-mixed-partials-wrappers',
        'async-mixed-partials-awaiters',
    ])


async def test_detection_for_none():
    is_async = is_async_fn(None)
    assert not is_async


@syncasyncparams
async def test_async_detection(fn, expected):
    is_async = is_async_fn(fn)
    assert is_async is expected


@syncasyncparams
async def test_stacktrace_visibility(fn, expected):
    stack_trace_marker = STACK_TRACE_MARKER  # searched by fn
    found = await invoke(fn)
    assert found is expected


@fns
async def test_result_returned(fn):
    mock = Mock(return_value=999)
    result = await invoke(fn, kwargs=dict(mock=mock))
    assert result == 999


@fns
async def test_explicit_args_passed_properly(fn):
    mock = Mock()
    await invoke(fn, kwargs=dict(mock=mock, kw1=300, kw2=400))

    assert mock.called
    assert mock.call_count == 1

    assert len(mock.call_args[0]) == 0
    assert len(mock.call_args[1]) >= 2  # also the magic kwargs
    assert mock.call_args[1]['kw1'] == 300
    assert mock.call_args[1]['kw2'] == 400


@fns
async def test_special_kwargs_added(fn, resource):
    body = {'metadata': {'uid': 'uid', 'name': 'name', 'namespace': 'ns'},
            'spec': {'field': 'value'},
            'status': {'info': 'payload'}}

    # Values can be any.
    cause = ChangingCause(
        logger=logging.getLogger('kopf.test.fake.logger'),
        indices=OperatorIndexers().indices,
        resource=resource,
        patch=Patch(),
        initial=False,
        reason=Reason.NOOP,
        memo=object(),
        body=Body(body),
        diff=object(),
        old=object(),
        new=object(),
    )

    mock = Mock()
    await invoke(fn, kwargs=dict(mock=mock), kwargsrc=cause)

    assert mock.called
    assert mock.call_count == 1

    # Only check that kwargs are passed at all. The exact kwargs per cause are tested separately.
    assert 'logger' in mock.call_args[1]
    assert 'resource' in mock.call_args[1]



================================================
FILE: tests/k8s/conftest.py
================================================
import pytest


@pytest.fixture(autouse=True)
def _autouse_resp_mocker(resp_mocker, version_api):
    pass


@pytest.fixture(autouse=True)
def _prevent_retries_in_api_tests(settings):
    settings.networking.error_backoffs = []



================================================
FILE: tests/k8s/test_creating.py
================================================
import aiohttp.web
import pytest

from kopf._cogs.clients.creating import create_obj
from kopf._cogs.clients.errors import APIError


async def test_simple_body_with_arguments(
        resp_mocker, aresponses, hostname, settings, resource, namespace, logger, caplog):

    post_mock = resp_mocker(return_value=aiohttp.web.json_response({}))
    aresponses.add(hostname, resource.get_url(namespace=namespace), 'post', post_mock)

    body = {'x': 'y'}
    await create_obj(
        logger=logger,
        settings=settings,
        resource=resource,
        namespace=namespace,
        name='name1',
        body=body,
    )

    assert post_mock.called
    assert post_mock.call_count == 1

    data = post_mock.call_args_list[0][0][0].data  # [callidx][args/kwargs][argidx]
    if resource.namespaced:
        assert data == {'x': 'y', 'metadata': {'name': 'name1', 'namespace': 'ns'}}
    else:
        assert data == {'x': 'y', 'metadata': {'name': 'name1'}}


async def test_full_body_with_identifiers(
        resp_mocker, aresponses, hostname, settings, resource, namespace, caplog, logger):

    post_mock = resp_mocker(return_value=aiohttp.web.json_response({}))
    aresponses.add(hostname, resource.get_url(namespace=namespace), 'post', post_mock)

    body = {'x': 'y', 'metadata': {'name': 'name1', 'namespace': namespace}}
    await create_obj(
        logger=logger,
        settings=settings,
        resource=resource,
        body=body,
    )

    assert post_mock.called
    assert post_mock.call_count == 1

    data = post_mock.call_args_list[0][0][0].data  # [callidx][args/kwargs][argidx]
    assert data == {'x': 'y', 'metadata': {'name': 'name1', 'namespace': namespace}}


# Note: 401 is wrapped into a LoginError and is tested elsewhere.
@pytest.mark.parametrize('status', [400, 403, 404, 409, 500, 666])
async def test_raises_api_errors(
        resp_mocker, aresponses, hostname, settings, status, resource, namespace, logger,
        cluster_resource, namespaced_resource):

    post_mock = resp_mocker(return_value=aresponses.Response(status=status, reason='oops'))
    cluster_url = cluster_resource.get_url(namespace=None)
    namespaced_url = namespaced_resource.get_url(namespace='ns')
    aresponses.add(hostname, cluster_url, 'post', post_mock)
    aresponses.add(hostname, namespaced_url, 'post', post_mock)

    body = {'x': 'y'}
    with pytest.raises(APIError) as e:
        await create_obj(
            logger=logger,
            settings=settings,
            resource=resource,
            namespace=namespace,
            name='name1',
            body=body,
        )
    assert e.value.status == status



================================================
FILE: tests/k8s/test_errors.py
================================================
import aiohttp
import pytest

from kopf._cogs.clients.auth import APIContext, authenticated
from kopf._cogs.clients.errors import APIClientError, APIConflictError, APIError, \
                                      APIForbiddenError, APINotFoundError, \
                                      APIServerError, check_response


@authenticated
async def get_it(url: str, *, context: APIContext) -> None:
    response = await context.session.get(url)
    await check_response(response)
    return await response.json()


def test_aiohttp_is_not_leaked_outside():
    assert not issubclass(APIError, aiohttp.ClientError)


def test_exception_without_payload():
    exc = APIError(None, status=456)
    assert exc.status == 456
    assert exc.code is None
    assert exc.message is None
    assert exc.details is None


def test_exception_with_payload():
    exc = APIError({"message": "msg", "code": 123, "details": {"a": "b"}}, status=456)
    assert exc.status == 456
    assert exc.code == 123
    assert exc.message == "msg"
    assert exc.details == {"a": "b"}


@pytest.mark.parametrize('status', [200, 202, 300, 304])
async def test_no_error_on_success(
        resp_mocker, aresponses, hostname, status):

    resp = aresponses.Response(
        status=status,
        reason='oops',
        headers={'Content-Type': 'application/json'},
        text='{"kind": "Status", "code": "xxx", "message": "msg"}',
    )
    aresponses.add(hostname, '/', 'get', resp_mocker(return_value=resp))

    await get_it(f"http://{hostname}/")


# Note: 401 is wrapped into a LoginError and is tested elsewhere.
@pytest.mark.parametrize('status, exctype', [
    (403, APIForbiddenError),
    (404, APINotFoundError),
    (409, APIConflictError),
    (400, APIClientError),
    (403, APIClientError),
    (404, APIClientError),
    (500, APIServerError),
    (503, APIServerError),
    (400, APIError),
    (500, APIError),
    (666, APIError),
])
async def test_error_with_payload(
        resp_mocker, aresponses, hostname, status, exctype):

    resp = aresponses.Response(
        status=status,
        reason='oops',
        headers={'Content-Type': 'application/json'},
        text='{"kind": "Status", "code": 123, "message": "msg", "details": {"a": "b"}}',
    )
    aresponses.add(hostname, '/', 'get', resp_mocker(return_value=resp))

    with pytest.raises(APIError) as err:
        await get_it(f"http://{hostname}/")

    assert not isinstance(err.value, aiohttp.ClientResponseError)
    assert isinstance(err.value, exctype)
    assert err.value.status == status
    assert err.value.code == 123
    assert err.value.message == 'msg'
    assert err.value.details == {'a': 'b'}


@pytest.mark.parametrize('status', [400, 500, 666])
async def test_error_with_nonjson_payload(
        resp_mocker, aresponses, hostname, status):

    resp = aresponses.Response(
        status=status,
        reason='oops',
        headers={'Content-Type': 'application/json'},
        text='unparsable json',
    )
    aresponses.add(hostname, '/', 'get', resp_mocker(return_value=resp))

    with pytest.raises(APIError) as err:
        await get_it(f"http://{hostname}/")

    assert err.value.status == status
    assert err.value.code is None
    assert err.value.message is None
    assert err.value.details is None


@pytest.mark.parametrize('status', [400, 500, 666])
async def test_error_with_parseable_nonk8s_payload(
        resp_mocker, aresponses, hostname, status):

    resp = aresponses.Response(
        status=status,
        reason='oops',
        headers={'Content-Type': 'application/json'},
        text='{"kind": "NonStatus", "code": "xxx", "message": "msg"}',
    )
    aresponses.add(hostname, '/', 'get', resp_mocker(return_value=resp))

    with pytest.raises(APIError) as err:
        await get_it(f"http://{hostname}/")

    assert err.value.status == status
    assert err.value.code is None
    assert err.value.message is None
    assert err.value.details is None



================================================
FILE: tests/k8s/test_events.py
================================================
import aiohttp.web
import pytest

from kopf._cogs.clients.events import post_event
from kopf._cogs.structs.bodies import build_object_reference
from kopf._cogs.structs.references import Resource

EVENTS = Resource('', 'v1', 'events', namespaced=True)


async def test_posting(
        resp_mocker, aresponses, hostname, settings, logger):

    post_mock = resp_mocker(return_value=aiohttp.web.json_response({}))
    aresponses.add(hostname, '/api/v1/namespaces/ns/events', 'post', post_mock)

    obj = {'apiVersion': 'group/version',
           'kind': 'kind',
           'metadata': {'namespace': 'ns',
                        'name': 'name',
                        'uid': 'uid'}}
    ref = build_object_reference(obj)
    await post_event(
        ref=ref,
        type='type',
        reason='reason',
        message='message',
        resource=EVENTS,
        settings=settings,
        logger=logger,
    )

    assert post_mock.called
    assert post_mock.call_count == 1

    req = post_mock.call_args_list[0][0][0]  # [callidx][args/kwargs][argidx]
    assert req.method == 'POST'

    data = req.data
    assert data['type'] == 'type'
    assert data['reason'] == 'reason'
    assert data['message'] == 'message'
    assert data['source']['component'] == 'kopf'
    assert data['involvedObject']['apiVersion'] == 'group/version'
    assert data['involvedObject']['kind'] == 'kind'
    assert data['involvedObject']['namespace'] == 'ns'
    assert data['involvedObject']['name'] == 'name'
    assert data['involvedObject']['uid'] == 'uid'


async def test_no_events_for_events(
        resp_mocker, aresponses, hostname, settings, logger):

    post_mock = resp_mocker(return_value=aiohttp.web.json_response({}))
    aresponses.add(hostname, '/api/v1/namespaces/ns/events', 'post', post_mock)

    obj = {'apiVersion': 'v1',
           'kind': 'Event',
           'metadata': {'namespace': 'ns',
                        'name': 'name',
                        'uid': 'uid'}}
    ref = build_object_reference(obj)
    await post_event(
        ref=ref,
        type='type',
        reason='reason',
        message='message',
        resource=EVENTS,
        settings=settings,
        logger=logger,
    )

    assert not post_mock.called


async def test_api_errors_logged_but_suppressed(
        resp_mocker, aresponses, hostname, settings, logger, assert_logs):

    post_mock = resp_mocker(return_value=aresponses.Response(status=555, reason='oops'))
    aresponses.add(hostname, '/api/v1/namespaces/ns/events', 'post', post_mock)

    obj = {'apiVersion': 'group/version',
           'kind': 'kind',
           'metadata': {'namespace': 'ns',
                        'name': 'name',
                        'uid': 'uid'}}
    ref = build_object_reference(obj)
    await post_event(
        ref=ref,
        type='type',
        reason='reason',
        message='message',
        resource=EVENTS,
        settings=settings,
        logger=logger,
    )

    assert post_mock.called
    assert_logs(["Failed to post an event."])


async def test_regular_errors_escalate(
        resp_mocker, enforced_session, mocker, settings, logger):

    error = Exception('boo!')
    enforced_session.request = mocker.Mock(side_effect=error)

    obj = {'apiVersion': 'group/version',
           'kind': 'kind',
           'metadata': {'namespace': 'ns',
                        'name': 'name',
                        'uid': 'uid'}}
    ref = build_object_reference(obj)

    with pytest.raises(Exception) as excinfo:
        await post_event(
            ref=ref,
            type='type',
            reason='reason',
            message='message',
            resource=EVENTS,
            settings=settings,
            logger=logger,
        )

    assert excinfo.value is error


async def test_message_is_cut_to_max_length(
        resp_mocker, aresponses, hostname, settings, logger):

    post_mock = resp_mocker(return_value=aiohttp.web.json_response({}))
    aresponses.add(hostname, '/api/v1/namespaces/ns/events', 'post', post_mock)

    obj = {'apiVersion': 'group/version',
           'kind': 'kind',
           'metadata': {'namespace': 'ns',
                        'name': 'name',
                        'uid': 'uid'}}
    ref = build_object_reference(obj)
    message = 'start' + ('x' * 2048) + 'end'
    await post_event(
        ref=ref,
        type='type',
        reason='reason',
        message=message,
        resource=EVENTS,
        settings=settings,
        logger=logger,
    )

    data = post_mock.call_args_list[0][0][0].data  # [callidx][args/kwargs][argidx]
    assert len(data['message']) <= 1024  # max supported API message length
    assert '...' in data['message']
    assert data['message'].startswith('start')
    assert data['message'].endswith('end')


# 401 causes LoginError from the vault, and this is out of scope of API testing.
@pytest.mark.parametrize('status', [555, 500, 404, 403])
async def test_headers_are_not_leaked(
        resp_mocker, aresponses, hostname, settings, logger, assert_logs, status):

    post_mock = resp_mocker(return_value=aresponses.Response(status=status, reason='oops'))
    aresponses.add(hostname, '/api/v1/namespaces/ns/events', 'post', post_mock)

    obj = {'apiVersion': 'group/version',
           'kind': 'kind',
           'metadata': {'namespace': 'ns',
                        'name': 'name',
                        'uid': 'uid'}}
    ref = build_object_reference(obj)
    await post_event(
        ref=ref,
        type='type',
        reason='reason',
        message='message',
        resource=EVENTS,
        settings=settings,
        logger=logger,
    )

    assert_logs([
        "Failed to post an event.",
    ], prohibited=[
        "ClientResponseError",
        "RequestInfo",
        "headers=",
    ])



================================================
FILE: tests/k8s/test_list_objs.py
================================================
import aiohttp.web
import pytest

from kopf._cogs.clients.errors import APIError
from kopf._cogs.clients.fetching import list_objs
from kopf._cogs.structs.credentials import LoginError


async def test_listing_works(
        resp_mocker, aresponses, hostname, settings, logger, resource, namespace,
        cluster_resource, namespaced_resource):

    result = {'items': [{}, {}]}
    list_mock = resp_mocker(return_value=aiohttp.web.json_response(result))
    cluster_url = cluster_resource.get_url(namespace=None)
    namespaced_url = namespaced_resource.get_url(namespace='ns')
    aresponses.add(hostname, cluster_url, 'get', list_mock)
    aresponses.add(hostname, namespaced_url, 'get', list_mock)

    items, resource_version = await list_objs(
        logger=logger,
        settings=settings,
        resource=resource,
        namespace=namespace,
    )
    assert items == result['items']

    assert list_mock.called
    assert list_mock.call_count == 1


# Note: 401 is wrapped into a LoginError and is tested elsewhere.
@pytest.mark.parametrize('status', [400, 403, 500, 666])
async def test_raises_direct_api_errors(
        resp_mocker, aresponses, hostname, settings, logger, status, resource, namespace,
        cluster_resource, namespaced_resource):

    list_mock = resp_mocker(return_value=aresponses.Response(status=status, reason='oops'))
    cluster_url = cluster_resource.get_url(namespace=None)
    namespaced_url = namespaced_resource.get_url(namespace='ns')
    aresponses.add(hostname, cluster_url, 'get', list_mock)
    aresponses.add(hostname, namespaced_url, 'get', list_mock)

    with pytest.raises(APIError) as e:
        await list_objs(
            logger=logger,
            settings=settings,
            resource=resource,
            namespace=namespace,
        )
    assert e.value.status == status



================================================
FILE: tests/k8s/test_patching.py
================================================
import dataclasses

import aiohttp.web
import pytest

from kopf._cogs.clients.errors import APIError
from kopf._cogs.clients.patching import patch_obj
from kopf._cogs.structs.patches import Patch


async def test_without_subresources(
        resp_mocker, aresponses, hostname, settings, resource, namespace, logger):

    patch_mock = resp_mocker(return_value=aiohttp.web.json_response({}))
    aresponses.add(hostname, resource.get_url(namespace=namespace, name='name1'), 'patch', patch_mock)

    patch = Patch({'x': 'y'})
    await patch_obj(
        logger=logger,
        settings=settings,
        resource=resource,
        namespace=namespace,
        name='name1',
        patch=patch,
    )

    assert patch_mock.called
    assert patch_mock.call_count == 1

    data = patch_mock.call_args_list[0][0][0].data  # [callidx][args/kwargs][argidx]
    assert data == {'x': 'y'}


async def test_status_as_subresource_with_combined_payload(
        resp_mocker, aresponses, hostname, settings, resource, namespace, logger):
    resource = dataclasses.replace(resource, subresources=['status'])

    # Simulate Kopf's initial state and intention.
    patch = Patch({'spec': {'x': 'y'}, 'status': {'s': 't'}})

    # Simulate K8s API's behaviour. Assume something extra is added remotely.
    object_response = {'metadata': {'namespace': 'ns1', 'name': 'name1', 'extra': '123'},
                       'spec': {'x': 'y', 'extra': '456'},
                       'status': '...'}
    status_response = {'status': {'s': 't', 'extra': '789'}}

    object_url = resource.get_url(namespace=namespace, name='name1')
    status_url = resource.get_url(namespace=namespace, name='name1', subresource='status')
    object_patch_mock = resp_mocker(return_value=aiohttp.web.json_response(object_response))
    status_patch_mock = resp_mocker(return_value=aiohttp.web.json_response(status_response))
    aresponses.add(hostname, object_url, 'patch', object_patch_mock)
    aresponses.add(hostname, status_url, 'patch', status_patch_mock)

    reconstructed = await patch_obj(
        logger=logger,
        settings=settings,
        resource=resource,
        namespace=namespace,
        name='name1',
        patch=patch,
    )

    assert object_patch_mock.called
    assert object_patch_mock.call_count == 1
    assert status_patch_mock.called
    assert status_patch_mock.call_count == 1

    data = object_patch_mock.call_args_list[0][0][0].data  # [callidx][args/kwargs][argidx]
    assert data == {'spec': {'x': 'y'}}
    data = status_patch_mock.call_args_list[0][0][0].data  # [callidx][args/kwargs][argidx]
    assert data == {'status': {'s': 't'}}

    assert reconstructed == {'metadata': {'namespace': 'ns1', 'name': 'name1', 'extra': '123'},
                             'spec': {'x': 'y', 'extra': '456'},
                             'status': {'s': 't', 'extra': '789'}}


async def test_status_as_subresource_with_object_fields_only(
        resp_mocker, aresponses, hostname, settings, resource, namespace, logger):
    resource = dataclasses.replace(resource, subresources=['status'])

    # Simulate Kopf's initial state and intention.
    patch = Patch({'spec': {'x': 'y'}})

    # Simulate K8s API's behaviour. Assume something extra is added remotely.
    object_response = {'metadata': {'namespace': 'ns1', 'name': 'name1', 'extra': '123'},
                       'spec': {'x': 'y', 'extra': '456'},
                       'status': '...'}
    status_response = {'status': {'s': 't', 'extra': '789'}}

    object_url = resource.get_url(namespace=namespace, name='name1')
    status_url = resource.get_url(namespace=namespace, name='name1', subresource='status')
    object_patch_mock = resp_mocker(return_value=aiohttp.web.json_response(object_response))
    status_patch_mock = resp_mocker(return_value=aiohttp.web.json_response(status_response))
    aresponses.add(hostname, object_url, 'patch', object_patch_mock)
    aresponses.add(hostname, status_url, 'patch', status_patch_mock)

    reconstructed = await patch_obj(
        logger=logger,
        settings=settings,
        resource=resource,
        namespace=namespace,
        name='name1',
        patch=patch,
    )

    assert object_patch_mock.called
    assert object_patch_mock.call_count == 1
    assert not status_patch_mock.called

    data = object_patch_mock.call_args_list[0][0][0].data  # [callidx][args/kwargs][argidx]
    assert data == {'spec': {'x': 'y'}}

    assert reconstructed == {'metadata': {'namespace': 'ns1', 'name': 'name1', 'extra': '123'},
                             'spec': {'x': 'y', 'extra': '456'},
                             'status': '...'}


async def test_status_as_subresource_with_status_fields_only(
        resp_mocker, aresponses, hostname, settings, resource, namespace, logger):
    resource = dataclasses.replace(resource, subresources=['status'])

    # Simulate Kopf's initial state and intention.
    patch = Patch({'status': {'s': 't'}})

    # Simulate K8s API's behaviour. Assume something extra is added remotely.
    object_response = {'metadata': {'namespace': 'ns1', 'name': 'name1', 'extra': '123'},
                       'spec': {'x': 'y', 'extra': '456'},
                       'status': '...'}
    status_response = {'status': {'s': 't', 'extra': '789'}}

    object_url = resource.get_url(namespace=namespace, name='name1')
    status_url = resource.get_url(namespace=namespace, name='name1', subresource='status')
    object_patch_mock = resp_mocker(return_value=aiohttp.web.json_response(object_response))
    status_patch_mock = resp_mocker(return_value=aiohttp.web.json_response(status_response))
    aresponses.add(hostname, object_url, 'patch', object_patch_mock)
    aresponses.add(hostname, status_url, 'patch', status_patch_mock)

    reconstructed = await patch_obj(
        logger=logger,
        settings=settings,
        resource=resource,
        namespace=namespace,
        name='name1',
        patch=patch,
    )

    assert not object_patch_mock.called
    assert status_patch_mock.called
    assert status_patch_mock.call_count == 1

    data = status_patch_mock.call_args_list[0][0][0].data  # [callidx][args/kwargs][argidx]
    assert data == {'status': {'s': 't'}}

    assert reconstructed == {'status': {'s': 't', 'extra': '789'}}


async def test_status_as_body_field_with_combined_payload(
        resp_mocker, aresponses, hostname, settings, resource, namespace, logger):

    # Simulate Kopf's initial state and intention.
    patch = Patch({'spec': {'x': 'y'}, 'status': {'s': 't'}})

    # Simulate K8s API's behaviour. Assume something extra is added remotely.
    object_response = {'metadata': {'namespace': 'ns1', 'name': 'name1', 'extra': '123'},
                       'spec': {'x': 'y', 'extra': '456'},
                       'status': '...'}
    status_response = {'s': 't', 'extra': '789'}

    object_url = resource.get_url(namespace=namespace, name='name1')
    status_url = resource.get_url(namespace=namespace, name='name1', subresource='status')
    object_patch_mock = resp_mocker(return_value=aiohttp.web.json_response(object_response))
    status_patch_mock = resp_mocker(return_value=aiohttp.web.json_response(status_response))
    aresponses.add(hostname, object_url, 'patch', object_patch_mock)
    aresponses.add(hostname, status_url, 'patch', status_patch_mock)

    reconstructed = await patch_obj(
        logger=logger,
        settings=settings,
        resource=resource,
        namespace=namespace,
        name='name1',
        patch=patch,
    )

    assert object_patch_mock.called
    assert object_patch_mock.call_count == 1
    assert not status_patch_mock.called

    data = object_patch_mock.call_args_list[0][0][0].data  # [callidx][args/kwargs][argidx]
    assert data == {'spec': {'x': 'y'}, 'status': {'s': 't'}}

    assert reconstructed == {'metadata': {'namespace': 'ns1', 'name': 'name1', 'extra': '123'},
                             'spec': {'x': 'y', 'extra': '456'},
                             'status': '...'}


@pytest.mark.parametrize('status', [404])
async def test_ignores_absent_objects(
        resp_mocker, aresponses, hostname, settings, status, resource, namespace, logger,
        cluster_resource, namespaced_resource):

    patch_mock = resp_mocker(return_value=aresponses.Response(status=status, reason='oops'))
    cluster_url = cluster_resource.get_url(namespace=None, name='name1')
    namespaced_url = namespaced_resource.get_url(namespace='ns', name='name1')
    aresponses.add(hostname, cluster_url, 'patch', patch_mock)
    aresponses.add(hostname, namespaced_url, 'patch', patch_mock)

    patch = {'x': 'y'}
    result = await patch_obj(
        logger=logger,
        settings=settings,
        resource=resource,
        namespace=namespace,
        name='name1',
        patch=patch,
    )

    assert result is None


# Note: 401 is wrapped into a LoginError and is tested elsewhere.
@pytest.mark.parametrize('status', [400, 403, 500, 666])
async def test_raises_api_errors(
        resp_mocker, aresponses, hostname, settings, status, resource, namespace, logger,
        cluster_resource, namespaced_resource):

    patch_mock = resp_mocker(return_value=aresponses.Response(status=status, reason='oops'))
    cluster_url = cluster_resource.get_url(namespace=None, name='name1')
    namespaced_url = namespaced_resource.get_url(namespace='ns', name='name1')
    aresponses.add(hostname, cluster_url, 'patch', patch_mock)
    aresponses.add(hostname, namespaced_url, 'patch', patch_mock)

    patch = {'x': 'y'}
    with pytest.raises(APIError) as e:
        await patch_obj(
            logger=logger,
            settings=settings,
            resource=resource,
            namespace=namespace,
            name='name1',
            patch=patch,
        )
    assert e.value.status == status



================================================
FILE: tests/k8s/test_scanning.py
================================================
import aiohttp.web
import pytest

from kopf._cogs.clients.errors import APIError
from kopf._cogs.clients.scanning import scan_resources


async def test_no_resources_in_empty_apis(
        resp_mocker, aresponses, hostname, settings, logger):

    core_mock = resp_mocker(return_value=aiohttp.web.json_response({'versions': []}))
    apis_mock = resp_mocker(return_value=aiohttp.web.json_response({'groups': []}))

    aresponses.add(hostname, '/api', 'get', core_mock)
    aresponses.add(hostname, '/apis', 'get', apis_mock)

    resources = await scan_resources(settings=settings, logger=logger)
    assert len(resources) == 0

    assert core_mock.call_count == 1
    assert apis_mock.call_count == 1


@pytest.mark.parametrize('namespaced', [True, False])
async def test_resources_in_old_apis(
        resp_mocker, aresponses, hostname, settings, logger, namespaced):


    core_mock = resp_mocker(return_value=aiohttp.web.json_response({'versions': ['v1']}))
    apis_mock = resp_mocker(return_value=aiohttp.web.json_response({'groups': []}))
    scan_mock = resp_mocker(return_value=aiohttp.web.json_response({'resources': [
        {
            'kind': 'kind1',
            'name': 'plural1',
            'singularName': 'singular1',
            'namespaced': namespaced,
            'categories': ['category1', 'category2'],
            'shortNames': ['shortname1', 'shortname2'],
            'verbs': ['verb1', 'verb2'],
        },
    ]}))
    aresponses.add(hostname, '/api', 'get', core_mock)
    aresponses.add(hostname, '/apis', 'get', apis_mock)
    aresponses.add(hostname, '/api/v1', 'get', scan_mock)

    resources = await scan_resources(settings=settings, logger=logger)
    assert len(resources) == 1

    resource1 = list(resources)[0]
    assert resource1.group == ''
    assert resource1.version == 'v1'
    assert resource1.kind == 'kind1'
    assert resource1.plural == 'plural1'
    assert resource1.singular == 'singular1'
    assert resource1.preferred == True
    assert resource1.namespaced == namespaced
    assert resource1.subresources == set()
    assert resource1.categories == {'category1', 'category2'}
    assert resource1.shortcuts == {'shortname1', 'shortname2'}
    assert resource1.verbs == {'verb1', 'verb2'}

    assert core_mock.call_count == 1
    assert apis_mock.call_count == 1
    assert scan_mock.call_count == 1


@pytest.mark.parametrize('namespaced', [True, False])
@pytest.mark.parametrize('preferred_version, expected_preferred', [
    ('version1', True),
    ('versionX', False),
])
async def test_resources_in_new_apis(
        resp_mocker, aresponses, hostname, settings, logger, namespaced,
        preferred_version, expected_preferred):

    core_mock = resp_mocker(return_value=aiohttp.web.json_response({'versions': []}))
    apis_mock = resp_mocker(return_value=aiohttp.web.json_response({'groups': [
        {
            'name': 'group1',
            'preferredVersion': {'version': preferred_version},
            'versions': [{'version': 'version1'}],
        },
    ]}))
    g1v1_mock = resp_mocker(return_value=aiohttp.web.json_response({'resources': [
        {
            'kind': 'kind1',
            'name': 'plural1',
            'singularName': 'singular1',
            'namespaced': namespaced,
            'categories': ['category1', 'category2'],
            'shortNames': ['shortname1', 'shortname2'],
            'verbs': ['verb1', 'verb2'],
        },
    ]}))
    aresponses.add(hostname, '/api', 'get', core_mock)
    aresponses.add(hostname, '/apis', 'get', apis_mock)
    aresponses.add(hostname, '/apis/group1/version1', 'get', g1v1_mock)

    resources = await scan_resources(settings=settings, logger=logger)
    assert len(resources) == 1

    resource1 = list(resources)[0]
    assert resource1.group == 'group1'
    assert resource1.version == 'version1'
    assert resource1.kind == 'kind1'
    assert resource1.plural == 'plural1'
    assert resource1.singular == 'singular1'
    assert resource1.preferred == expected_preferred
    assert resource1.namespaced == namespaced
    assert resource1.subresources == set()
    assert resource1.categories == {'category1', 'category2'}
    assert resource1.shortcuts == {'shortname1', 'shortname2'}
    assert resource1.verbs == {'verb1', 'verb2'}

    assert core_mock.call_count == 1
    assert apis_mock.call_count == 1
    assert g1v1_mock.call_count == 1


async def test_subresources_in_old_apis(
        resp_mocker, aresponses, hostname, settings, logger):

    core_mock = resp_mocker(return_value=aiohttp.web.json_response({'versions': ['v1']}))
    apis_mock = resp_mocker(return_value=aiohttp.web.json_response({'groups': []}))
    v1v1_mock = resp_mocker(return_value=aiohttp.web.json_response({'resources': [
        {
            'kind': 'kind1',
            'name': 'plural1',
            'singularName': 'singular1',
            'namespaced': True,
            'categories': [],
            'shortNames': [],
            'verbs': [],
        },
        {
            'name': 'plural1/sub1',
        },
        {
            'name': 'plural1/sub2',
        },
        {
            'name': 'pluralX/sub3',
        },
    ]}))
    aresponses.add(hostname, '/api', 'get', core_mock)
    aresponses.add(hostname, '/apis', 'get', apis_mock)
    aresponses.add(hostname, '/api/v1', 'get', v1v1_mock)

    resources = await scan_resources(settings=settings, logger=logger)
    assert len(resources) == 1
    resource1 = list(resources)[0]
    assert resource1.subresources == {'sub1', 'sub2'}


async def test_subresources_in_new_apis(
        resp_mocker, aresponses, hostname, settings, logger):

    core_mock = resp_mocker(return_value=aiohttp.web.json_response({'versions': []}))
    apis_mock = resp_mocker(return_value=aiohttp.web.json_response({'groups': [
        {
            'name': 'group1',
            'preferredVersion': {'version': 'version1'},
            'versions': [{'version': 'version1'}],
        },
    ]}))
    g1v1_mock = resp_mocker(return_value=aiohttp.web.json_response({'resources': [
        {
            'kind': 'kind1',
            'name': 'plural1',
            'singularName': 'singular1',
            'namespaced': True,
            'categories': [],
            'shortNames': [],
            'verbs': [],
        },
        {
            'name': 'plural1/sub1',
        },
        {
            'name': 'plural1/sub2',
        },
        {
            'name': 'pluralX/sub3',
        },
    ]}))
    aresponses.add(hostname, '/api', 'get', core_mock)
    aresponses.add(hostname, '/apis', 'get', apis_mock)
    aresponses.add(hostname, '/apis/group1/version1', 'get', g1v1_mock)

    resources = await scan_resources(settings=settings, logger=logger)
    assert len(resources) == 1
    resource1 = list(resources)[0]
    assert resource1.subresources == {'sub1', 'sub2'}



@pytest.mark.parametrize('group_filter, exp_core, exp_apis, exp_crv1, exp_g1v1, exp_g2v1', [
    pytest.param([''], 1, 0, 1, 0, 0, id='only-core'),
    pytest.param(['g1'], 0, 1, 0, 1, 0, id='only-g1'),
    pytest.param(['g2'], 0, 1, 0, 0, 1, id='only-g2'),
    pytest.param(['', 'g1'], 1, 1, 1, 1, 0, id='core-and-g1'),
    pytest.param(['', 'g2'], 1, 1, 1, 0, 1, id='core-and-g2'),
    pytest.param(['g1', 'g2'], 0, 1, 0, 1, 1, id='g1-and-g2'),
    pytest.param(['X'], 0, 1, 0, 0, 0, id='unexistent'),
    pytest.param([], 0, 0, 0, 0, 0, id='restrictive'),
    pytest.param(None, 1, 1, 1, 1, 1, id='unfiltered'),
])
async def test_group_filtering(
        resp_mocker, aresponses, hostname, settings, logger,
        group_filter, exp_core, exp_apis, exp_crv1, exp_g1v1, exp_g2v1):

    core_mock = resp_mocker(return_value=aiohttp.web.json_response({'versions': ['v1']}))
    apis_mock = resp_mocker(return_value=aiohttp.web.json_response({'groups': [
        {'name': 'g1', 'preferredVersion': {'version': ''}, 'versions': [{'version': 'g1v1'}]},
        {'name': 'g2', 'preferredVersion': {'version': ''}, 'versions': [{'version': 'g2v1'}]},
    ]}))
    crv1_mock = resp_mocker(return_value=aiohttp.web.json_response({'resources': []}))
    g1v1_mock = resp_mocker(return_value=aiohttp.web.json_response({'resources': []}))
    g2v1_mock = resp_mocker(return_value=aiohttp.web.json_response({'resources': []}))
    aresponses.add(hostname, '/api', 'get', core_mock)
    aresponses.add(hostname, '/api/v1', 'get', crv1_mock)
    aresponses.add(hostname, '/apis', 'get', apis_mock)
    aresponses.add(hostname, '/apis/g1/g1v1', 'get', g1v1_mock)
    aresponses.add(hostname, '/apis/g2/g2v1', 'get', g2v1_mock)

    await scan_resources(groups=group_filter, settings=settings, logger=logger)

    assert core_mock.call_count == exp_core
    assert apis_mock.call_count == exp_apis

    assert crv1_mock.call_count == exp_crv1
    assert g1v1_mock.call_count == exp_g1v1
    assert g2v1_mock.call_count == exp_g2v1



@pytest.mark.parametrize('status', [404])
async def test_http404_returns_no_resources_from_old_apis(
        resp_mocker, aresponses, hostname, settings, logger, status):

    core_mock = resp_mocker(return_value=aiohttp.web.json_response({'versions': ['v1']}))
    apis_mock = resp_mocker(return_value=aiohttp.web.json_response({'groups': []}))
    status_mock = resp_mocker(return_value=aresponses.Response(status=status, reason='oops'))
    aresponses.add(hostname, '/api', 'get', core_mock)
    aresponses.add(hostname, '/apis', 'get', apis_mock)
    aresponses.add(hostname, '/api/v1', 'get', status_mock)

    resources = await scan_resources(settings=settings, logger=logger)

    assert not resources
    assert status_mock.call_count == 1


@pytest.mark.parametrize('status', [404])
async def test_http404_returns_no_resources_from_new_apis(
        resp_mocker, aresponses, hostname, settings, logger, status):

    core_mock = resp_mocker(return_value=aiohttp.web.json_response({'versions': []}))
    apis_mock = resp_mocker(return_value=aiohttp.web.json_response({'groups': [
        {'name': 'g1', 'preferredVersion': {'version': ''}, 'versions': [{'version': 'g1v1'}]},
    ]}))
    status_mock = resp_mocker(return_value=aresponses.Response(status=status, reason='oops'))
    aresponses.add(hostname, '/api', 'get', core_mock)
    aresponses.add(hostname, '/apis', 'get', apis_mock)
    aresponses.add(hostname, '/apis/g1/g1v1', 'get', status_mock)

    resources = await scan_resources(settings=settings, logger=logger)

    assert not resources
    assert status_mock.call_count == 1


@pytest.mark.parametrize('status', [403, 500, 666])
async def test_unknown_api_statuses_escalate_from_old_apis(
        resp_mocker, aresponses, hostname, settings, logger, status):

    core_mock = resp_mocker(return_value=aiohttp.web.json_response({'versions': ['v1']}))
    apis_mock = resp_mocker(return_value=aiohttp.web.json_response({'groups': []}))
    status_mock = resp_mocker(return_value=aresponses.Response(status=status, reason='oops'))
    aresponses.add(hostname, '/api', 'get', core_mock)
    aresponses.add(hostname, '/apis', 'get', apis_mock)
    aresponses.add(hostname, '/api/v1', 'get', status_mock)

    with pytest.raises(APIError) as err:
        await scan_resources(settings=settings, logger=logger)

    assert err.value.status == status
    assert status_mock.call_count == 1


@pytest.mark.parametrize('status', [403, 500, 666])
async def test_unknown_api_statuses_escalate_from_new_apis(
        resp_mocker, aresponses, hostname, settings, logger, status):

    core_mock = resp_mocker(return_value=aiohttp.web.json_response({'versions': []}))
    apis_mock = resp_mocker(return_value=aiohttp.web.json_response({'groups': [
        {'name': 'g1', 'preferredVersion': {'version': ''}, 'versions': [{'version': 'g1v1'}]},
    ]}))
    status_mock = resp_mocker(return_value=aresponses.Response(status=status, reason='oops'))
    aresponses.add(hostname, '/api', 'get', core_mock)
    aresponses.add(hostname, '/apis', 'get', apis_mock)
    aresponses.add(hostname, '/apis/g1/g1v1', 'get', status_mock)

    with pytest.raises(APIError) as err:
        await scan_resources(settings=settings, logger=logger)

    assert err.value.status == status
    assert status_mock.call_count == 1


async def test_empty_singulars_fall_back_to_kinds(
        resp_mocker, aresponses, hostname, settings, logger):

    # Only one endpoint is enough, core v1 is easier to mock:
    core_mock = resp_mocker(return_value=aiohttp.web.json_response({'versions': ['v1']}))
    apis_mock = resp_mocker(return_value=aiohttp.web.json_response({'groups': []}))
    scan_mock = resp_mocker(return_value=aiohttp.web.json_response({'resources': [
        {
            'kind': 'MultiWordKind',
            'name': '...',
            'singularName': '',  # as in K3s
            'namespaced': True,
            'categories': [],
            'shortNames': [],
            'verbs': [],
        },
    ]}))
    aresponses.add(hostname, '/api', 'get', core_mock)
    aresponses.add(hostname, '/apis', 'get', apis_mock)
    aresponses.add(hostname, '/api/v1', 'get', scan_mock)

    resources = await scan_resources(groups=[''], settings=settings, logger=logger)
    assert len(resources) == 1

    resource1 = list(resources)[0]
    assert resource1.singular == 'multiwordkind'


# TODO: LATER: test that the requests are done in parallel, and the total timing is the best possible.



================================================
FILE: tests/k8s/test_watching_bookmarks.py
================================================
import asyncio
import json

import aiohttp.web

from kopf._cogs.clients.watching import Bookmark, continuous_watch


async def test_listed_is_inbetween(
        settings, resource, namespace, hostname, aresponses):

    # Resource version is used as a continutation for the watch-queries.
    list_data = {'metadata': {'resourceVersion': '123'}, 'items': [
        {'spec': 'a'},
        {'spec': 'b'},
    ]}
    list_resp = aiohttp.web.json_response(list_data)
    list_url = resource.get_url(namespace=namespace)

    # The same as in the `stream` fixture. But here, we also mock lists.
    stream_data = [
        {'type': 'ADDED', 'object': {'spec': 'c'}},  # stream.feed()
        {'type': 'ADDED', 'object': {'spec': 'd'}},  # stream.feed()
        {'type': 'ERROR', 'object': {'code': 410}},  # stream.close()
    ]
    stream_text = '\n'.join(json.dumps(event) for event in stream_data)
    stream_resp = aresponses.Response(text=stream_text)
    stream_query = {'watch': 'true', 'resourceVersion': '123'}
    stream_url = resource.get_url(namespace=namespace, params=stream_query)

    aresponses.add(hostname, list_url, 'get', list_resp, match_querystring=True)
    aresponses.add(hostname, stream_url, 'get', stream_resp, match_querystring=True)

    events = []
    async for event in continuous_watch(settings=settings,
                                        resource=resource,
                                        namespace=namespace,
                                        operator_pause_waiter=asyncio.Future()):
        events.append(event)

    assert len(events) == 5
    assert events[0]['object']['spec'] == 'a'
    assert events[1]['object']['spec'] == 'b'
    assert events[2] == Bookmark.LISTED
    assert events[3]['object']['spec'] == 'c'
    assert events[4]['object']['spec'] == 'd'



================================================
FILE: tests/k8s/test_watching_continuously.py
================================================
"""
Only the tests from the K8s client library (simulated) to the event stream.

Excluded: the watch-stream consumption and queue multiplexing routines
(see ``tests_queueing.py``).

Used for internal control that the event multiplexing works are intended.
If the intentions change, the tests should be rewritten.
They are NOT part of the public interface of the framework.
"""
import asyncio
import logging

import aiohttp
import pytest

from kopf._cogs.clients.watching import Bookmark, WatchingError, continuous_watch

STREAM_WITH_NORMAL_EVENTS = [
    {'type': 'ADDED', 'object': {'spec': 'a'}},
    {'type': 'ADDED', 'object': {'spec': 'b'}},
]
STREAM_WITH_UNKNOWN_EVENT = [
    {'type': 'ADDED', 'object': {'spec': 'a'}},
    {'type': 'UNKNOWN', 'object': {}},
    {'type': 'ADDED', 'object': {'spec': 'b'}},
]
STREAM_WITH_ERROR_410GONE = [
    {'type': 'ADDED', 'object': {'spec': 'a'}},
    {'type': 'ERROR', 'object': {'code': 410}},
    {'type': 'ADDED', 'object': {'spec': 'b'}},
]
STREAM_WITH_ERROR_CODE = [
    {'type': 'ADDED', 'object': {'spec': 'a'}},
    {'type': 'ERROR', 'object': {'code': 666}},
    {'type': 'ADDED', 'object': {'spec': 'b'}},
]


class SampleException(Exception):
    pass


async def test_empty_stream_yields_nothing(
        settings, resource, stream, namespace):

    stream.feed([], namespace=namespace)
    stream.close(namespace=namespace)

    events = []
    async for event in continuous_watch(settings=settings,
                                        resource=resource,
                                        namespace=namespace,
                                        operator_pause_waiter=asyncio.Future()):
        events.append(event)

    assert len(events) == 1
    assert events[0] == Bookmark.LISTED


async def test_event_stream_yields_everything(
        settings, resource, stream, namespace):

    stream.feed(STREAM_WITH_NORMAL_EVENTS, namespace=namespace)
    stream.close(namespace=namespace)

    events = []
    async for event in continuous_watch(settings=settings,
                                        resource=resource,
                                        namespace=namespace,
                                        operator_pause_waiter=asyncio.Future()):
        events.append(event)

    assert len(events) == 3
    assert events[0] == Bookmark.LISTED
    assert events[1]['object']['spec'] == 'a'
    assert events[2]['object']['spec'] == 'b'


async def test_unknown_event_type_ignored(
        settings, resource, stream, namespace, caplog):
    caplog.set_level(logging.DEBUG)

    stream.feed(STREAM_WITH_UNKNOWN_EVENT, namespace=namespace)
    stream.close(namespace=namespace)

    events = []
    async for event in continuous_watch(settings=settings,
                                        resource=resource,
                                        namespace=namespace,
                                        operator_pause_waiter=asyncio.Future()):
        events.append(event)

    assert len(events) == 3
    assert events[0] == Bookmark.LISTED
    assert events[1]['object']['spec'] == 'a'
    assert events[2]['object']['spec'] == 'b'
    assert "Ignoring an unsupported event type" in caplog.text
    assert "UNKNOWN" in caplog.text


async def test_error_410gone_exits_normally(
        settings, resource, stream, namespace, caplog):
    caplog.set_level(logging.DEBUG)

    stream.feed(STREAM_WITH_ERROR_410GONE, namespace=namespace)
    stream.close(namespace=namespace)

    events = []
    async for event in continuous_watch(settings=settings,
                                        resource=resource,
                                        namespace=namespace,
                                        operator_pause_waiter=asyncio.Future()):
        events.append(event)

    assert len(events) == 2
    assert events[0] == Bookmark.LISTED
    assert events[1]['object']['spec'] == 'a'
    assert "Restarting the watch-stream" in caplog.text


async def test_unknown_error_raises_exception(
        settings, resource, stream, namespace):

    stream.feed(STREAM_WITH_ERROR_CODE, namespace=namespace)
    stream.close(namespace=namespace)

    events = []
    with pytest.raises(WatchingError) as e:
        async for event in continuous_watch(settings=settings,
                                            resource=resource,
                                            namespace=namespace,
                                            operator_pause_waiter=asyncio.Future()):
            events.append(event)

    assert len(events) == 2
    assert events[0] == Bookmark.LISTED
    assert events[1]['object']['spec'] == 'a'
    assert '666' in str(e.value)


async def test_exception_escalates(
        settings, resource, stream, namespace, enforced_session, mocker):

    enforced_session.request = mocker.Mock(side_effect=SampleException())
    stream.feed([], namespace=namespace)
    stream.close(namespace=namespace)

    events = []
    with pytest.raises(SampleException):
        async for event in continuous_watch(settings=settings,
                                            resource=resource,
                                            namespace=namespace,
                                            operator_pause_waiter=asyncio.Future()):
            events.append(event)

    assert len(events) == 0


# See: See: https://github.com/zalando-incubator/kopf/issues/275
async def test_long_line_parsing(
        settings, resource, stream, namespace, aresponses):

    content = [
        {'type': 'ADDED', 'object': {'spec': {'field': 'x'}}},
        {'type': 'ADDED', 'object': {'spec': {'field': 'y' * (2 * 1024 * 1024)}}},
        {'type': 'ADDED', 'object': {'spec': {'field': 'z' * (4 * 1024 * 1024)}}},
    ]
    stream.feed(content, namespace=namespace)
    stream.close(namespace=namespace)

    events = []
    async for event in continuous_watch(settings=settings,
                                        resource=resource,
                                        namespace=namespace,
                                        operator_pause_waiter=asyncio.Future()):
        events.append(event)

    assert len(events) == 4
    assert events[0] == Bookmark.LISTED
    assert len(events[1]['object']['spec']['field']) == 1
    assert len(events[2]['object']['spec']['field']) == 2 * 1024 * 1024
    assert len(events[3]['object']['spec']['field']) == 4 * 1024 * 1024

@pytest.mark.parametrize("connection_error",
    [
        aiohttp.ClientConnectionError,
        aiohttp.ClientPayloadError,
        asyncio.TimeoutError
    ]
)
async def test_list_objs_connection_errors_are_caught(
        settings, resource, stream, namespace, enforced_session, mocker, connection_error):

    enforced_session.request = mocker.Mock(side_effect=connection_error())
    stream.feed([], namespace=namespace)
    stream.close(namespace=namespace)

    events = []
    async for event in continuous_watch(settings=settings,
                                            resource=resource,
                                            namespace=namespace,
                                            operator_pause_waiter=asyncio.Future()):
        events.append(event)

    assert len(events) == 0



================================================
FILE: tests/k8s/test_watching_infinitely.py
================================================
import pytest

from kopf._cogs.clients.errors import APIClientError, APIError
from kopf._cogs.clients.watching import Bookmark, infinite_watch

STREAM_WITH_UNKNOWN_EVENT = [
    {'type': 'ADDED', 'object': {'spec': 'a'}},
    {'type': 'UNKNOWN', 'object': {}},
    {'type': 'ADDED', 'object': {'spec': 'b'}},
]
STREAM_WITH_ERROR_410GONE = [
    {'type': 'ADDED', 'object': {'spec': 'a'}},
    {'type': 'ERROR', 'object': {'code': 410}},
    {'type': 'ADDED', 'object': {'spec': 'b'}},
]


class SampleException(Exception):
    pass


async def test_exception_escalates(
        settings, resource, stream, namespace, enforced_session, mocker):

    enforced_session.request = mocker.Mock(side_effect=SampleException())
    stream.feed([], namespace=namespace)
    stream.close(namespace=namespace)

    events = []
    with pytest.raises(SampleException):
        async for event in infinite_watch(settings=settings,
                                          resource=resource,
                                          namespace=namespace,
                                          _iterations=1):
            events.append(event)

    assert len(events) == 0


async def test_infinite_watch_never_exits_normally(
        settings, resource, stream, namespace, aresponses):
    error = aresponses.Response(status=555, reason='oops')
    stream.feed(
        STREAM_WITH_ERROR_410GONE,  # watching restarted
        STREAM_WITH_UNKNOWN_EVENT,  # event ignored
        error,  # to finally exit it somehow
        namespace=namespace,
    )
    stream.close(namespace=namespace)

    events = []
    with pytest.raises(APIError) as e:
        async for event in infinite_watch(settings=settings,
                                          resource=resource,
                                          namespace=namespace):
            events.append(event)

    assert e.value.status == 555

    assert len(events) == 5
    assert events[0] == Bookmark.LISTED
    assert events[1]['object']['spec'] == 'a'
    assert events[2] == Bookmark.LISTED
    assert events[3]['object']['spec'] == 'a'
    assert events[4]['object']['spec'] == 'b'


async def test_too_many_requests_exception(
        settings, resource, stream, namespace, enforced_session, mocker):

    exc = APIClientError({
        "apiVersion": "v1",
        "code": 429,
        "status": "Failure",
        "details": {
            "retryAfterSeconds": 1,
        }
    }, status=429)
    enforced_session.request = mocker.Mock(side_effect=exc)
    stream.feed([], namespace=namespace)
    stream.close(namespace=namespace)

    events = []
    async for event in infinite_watch(settings=settings,
                                      resource=resource,
                                      namespace=namespace,
                                      _iterations=1):
        events.append(event)

    assert len(events) == 0



================================================
FILE: tests/k8s/test_watching_with_freezes.py
================================================
import asyncio
import logging

import pytest

from kopf._cogs.aiokits.aiotoggles import ToggleSet
from kopf._cogs.clients.watching import streaming_block


async def test_pausing_is_ignored_if_turned_off(
        resource, namespace, timer, caplog, assert_logs):
    caplog.set_level(logging.DEBUG)

    operator_paused = ToggleSet(any)
    await operator_paused.make_toggle(False)

    with timer:
        async with streaming_block(
            resource=resource,
            namespace=namespace,
            operator_paused=operator_paused,
        ):
            pass

    assert timer.seconds < 0.2  # no waits, exits as soon as possible
    assert_logs([], prohibited=[
        r"Pausing the watch-stream for",
        r"Resuming the watch-stream for",
    ])


async def test_pausing_waits_forever_if_not_resumed(
        resource, namespace, timer, caplog, assert_logs):
    caplog.set_level(logging.DEBUG)

    operator_paused = ToggleSet(any)
    await operator_paused.make_toggle(True)

    async def do_it():
        async with streaming_block(
                resource=resource,
                namespace=namespace,
                operator_paused=operator_paused,
        ):
            pass

    with pytest.raises(asyncio.TimeoutError), timer:
        await asyncio.wait_for(do_it(), timeout=0.5)

    assert timer.seconds >= 0.5
    assert_logs([
        r"Pausing the watch-stream for",
    ], prohibited=[
        r"Resuming the watch-stream for",
    ])


async def test_pausing_waits_until_resumed(
        resource, namespace, timer, caplog, assert_logs):
    caplog.set_level(logging.DEBUG)

    operator_paused = ToggleSet(any)
    conflicts_found = await operator_paused.make_toggle(True)

    async def delayed_resuming(delay: float):
        await asyncio.sleep(delay)
        await conflicts_found.turn_to(False)

    with timer:
        asyncio.create_task(delayed_resuming(0.2))
        async with streaming_block(
            resource=resource,
            namespace=namespace,
            operator_paused=operator_paused,
        ):
            pass

    assert timer.seconds >= 0.2
    assert timer.seconds <= 0.5
    assert_logs([
        r"Pausing the watch-stream for",
        r"Resuming the watch-stream for",
    ])



================================================
FILE: tests/lifecycles/conftest.py
================================================
import pytest

import kopf


@pytest.fixture(autouse=True)
def clear_default_lifecycle():
    try:
        yield
    finally:
        kopf.set_default_lifecycle(None)



================================================
FILE: tests/lifecycles/test_global_defaults.py
================================================
import kopf


def test_getting_default_lifecycle():
    lifecycle = kopf.get_default_lifecycle()
    assert lifecycle is kopf.lifecycles.asap


def test_setting_default_lifecycle():
    lifecycle_expected = lambda handlers, *args, **kwargs: handlers
    kopf.set_default_lifecycle(lifecycle_expected)
    lifecycle_actual = kopf.get_default_lifecycle()
    assert lifecycle_actual is lifecycle_expected


def test_resetting_default_lifecycle():
    lifecycle_distracting = lambda handlers, *args, **kwargs: handlers
    kopf.set_default_lifecycle(lifecycle_distracting)
    kopf.set_default_lifecycle(None)
    lifecycle_actual = kopf.get_default_lifecycle()
    assert lifecycle_actual is kopf.lifecycles.asap



================================================
FILE: tests/lifecycles/test_handler_selection.py
================================================
import pytest

import kopf
from kopf._core.actions.execution import Outcome
from kopf._core.actions.progression import State


@pytest.mark.parametrize('lifecycle', [
    kopf.lifecycles.all_at_once,
    kopf.lifecycles.one_by_one,
    kopf.lifecycles.randomized,
    kopf.lifecycles.shuffled,
    kopf.lifecycles.asap,
])
def test_with_empty_input(lifecycle):
    state = State.from_scratch()
    handlers = []
    selected = lifecycle(handlers, state=state)
    assert isinstance(selected, (tuple, list))
    assert len(selected) == 0


def test_all_at_once_respects_order():
    handler1 = object()
    handler2 = object()
    handler3 = object()

    handlers = [handler1, handler2, handler3]
    selected = kopf.lifecycles.all_at_once(handlers)
    assert isinstance(selected, (tuple, list))
    assert len(selected) == 3
    assert list(selected) == handlers

    handlers = [handler3, handler2, handler1]
    selected = kopf.lifecycles.all_at_once(handlers)
    assert isinstance(selected, (tuple, list))
    assert len(selected) == 3
    assert list(selected) == handlers


def test_one_by_one_respects_order():
    handler1 = object()
    handler2 = object()
    handler3 = object()

    handlers = [handler1, handler2, handler3]
    selected = kopf.lifecycles.one_by_one(handlers)
    assert isinstance(selected, (tuple, list))
    assert len(selected) == 1
    assert selected[0] is handler1

    handlers = [handler3, handler2, handler1]
    selected = kopf.lifecycles.one_by_one(handlers)
    assert len(selected) == 1
    assert selected[0] is handler3


def test_randomized_takes_only_one():
    handler1 = object()
    handler2 = object()
    handler3 = object()

    handlers = [handler1, handler2, handler3]
    selected = kopf.lifecycles.randomized(handlers)
    assert isinstance(selected, (tuple, list))
    assert len(selected) == 1
    assert selected[0] in {handler1, handler2, handler3}


def test_shuffled_takes_them_all():
    handler1 = object()
    handler2 = object()
    handler3 = object()

    handlers = [handler1, handler2, handler3]
    selected = kopf.lifecycles.shuffled(handlers)
    assert isinstance(selected, (tuple, list))
    assert len(selected) == 3
    assert set(selected) == {handler1, handler2, handler3}


def test_asap_takes_the_first_one_when_no_retries(mocker):
    handler1 = mocker.Mock(id='id1', spec_set=['id'])
    handler2 = mocker.Mock(id='id2', spec_set=['id'])
    handler3 = mocker.Mock(id='id3', spec_set=['id'])

    state = State.from_scratch().with_handlers([handler1, handler2, handler3])
    handlers = [handler1, handler2, handler3]
    selected = kopf.lifecycles.asap(handlers, state=state)
    assert isinstance(selected, (tuple, list))
    assert len(selected) == 1
    assert selected[0] is handler1


def test_asap_takes_the_least_retried(mocker):
    handler1 = mocker.Mock(id='id1', spec_set=['id'])
    handler2 = mocker.Mock(id='id2', spec_set=['id'])
    handler3 = mocker.Mock(id='id3', spec_set=['id'])

    # Set the pre-existing state, and verify that it was set properly.
    state = State.from_scratch().with_handlers([handler1, handler2, handler3])
    state = state.with_outcomes({handler1.id: Outcome(final=False)})
    state = state.with_outcomes({handler1.id: Outcome(final=False)})
    state = state.with_outcomes({handler3.id: Outcome(final=False)})
    assert state[handler1.id].retries == 2
    assert state[handler2.id].retries == 0
    assert state[handler3.id].retries == 1

    handlers = [handler1, handler2, handler3]
    selected = kopf.lifecycles.asap(handlers, state=state)
    assert isinstance(selected, (tuple, list))
    assert len(selected) == 1
    assert selected[0] is handler2



================================================
FILE: tests/lifecycles/test_real_invocation.py
================================================
import logging

import pytest

import kopf
from kopf._cogs.structs.bodies import Body
from kopf._cogs.structs.ephemera import Memo
from kopf._cogs.structs.patches import Patch
from kopf._core.actions.progression import State
from kopf._core.engines.indexing import OperatorIndexers
from kopf._core.intents.causes import ChangingCause, Reason


@pytest.mark.parametrize('lifecycle', [
    kopf.lifecycles.all_at_once,
    kopf.lifecycles.one_by_one,
    kopf.lifecycles.randomized,
    kopf.lifecycles.shuffled,
    kopf.lifecycles.asap,
])
async def test_protocol_invocation(lifecycle, resource):
    """
    To be sure that all kwargs are accepted properly.
    Especially when the new kwargs are added or an invocation protocol changed.
    """
    # The values are irrelevant, they can be anything.
    state = State.from_scratch()
    cause = ChangingCause(
        logger=logging.getLogger('kopf.test.fake.logger'),
        indices=OperatorIndexers().indices,
        resource=resource,
        patch=Patch(),
        memo=Memo(),
        body=Body({}),
        initial=False,
        reason=Reason.NOOP,
    )
    handlers = []
    selected = lifecycle(handlers, state=state, **cause.kwargs)
    assert isinstance(selected, (tuple, list))
    assert len(selected) == 0



================================================
FILE: tests/logging/conftest.py
================================================
import asyncio

import pytest

from kopf._core.engines.posting import event_queue_loop_var, event_queue_var


@pytest.fixture(autouse=True)
def _caplog_all_levels(caplog):
    caplog.set_level(0)


@pytest.fixture(autouse=True)
def event_queue_loop(loop):  # must be sync-def
    token = event_queue_loop_var.set(loop)
    try:
        yield loop
    finally:
        event_queue_loop_var.reset(token)


@pytest.fixture(autouse=True)
def event_queue():
    queue = asyncio.Queue()
    token = event_queue_var.set(queue)
    try:
        yield queue
    finally:
        event_queue_var.reset(token)



================================================
FILE: tests/logging/test_configuration.py
================================================
import logging.handlers
from collections.abc import Collection

import pytest

from kopf._core.actions.loggers import LogFormat, ObjectFormatter, ObjectJsonFormatter, \
                                       ObjectPrefixingJsonFormatter, ObjectPrefixingTextFormatter, \
                                       ObjectTextFormatter, configure


@pytest.fixture(autouse=True)
def _clear_own_handlers():
    logger = logging.getLogger()
    logger.handlers[:] = [
        handler for handler in logger.handlers
        if not isinstance(handler, logging.StreamHandler) or
           not isinstance(handler.formatter, ObjectFormatter)
    ]
    original_handlers = logger.handlers[:]
    yield
    logger.handlers[:] = original_handlers


def _get_own_handlers(logger: logging.Logger) -> Collection[logging.Handler]:
    return [
        handler for handler in logger.handlers
        if isinstance(handler, logging.StreamHandler) and
           isinstance(handler.formatter, ObjectFormatter)
    ]


def test_own_formatter_is_used():
    configure()
    logger = logging.getLogger()
    own_handlers = _get_own_handlers(logger)
    assert len(own_handlers) == 1


@pytest.mark.parametrize('log_format', [LogFormat.FULL, LogFormat.PLAIN, '%(message)s'])
def test_formatter_nonprefixed_text(log_format):
    configure(log_format=log_format, log_prefix=False)
    logger = logging.getLogger()
    own_handlers = _get_own_handlers(logger)
    assert len(own_handlers) == 1
    assert type(own_handlers[0].formatter) is ObjectTextFormatter


@pytest.mark.parametrize('log_format', [LogFormat.FULL, LogFormat.PLAIN, '%(message)s'])
def test_formatter_prefixed_text(log_format):
    configure(log_format=log_format, log_prefix=True)
    logger = logging.getLogger()
    own_handlers = _get_own_handlers(logger)
    assert len(own_handlers) == 1
    assert type(own_handlers[0].formatter) is ObjectPrefixingTextFormatter


@pytest.mark.parametrize('log_format', [LogFormat.JSON])
def test_formatter_nonprefixed_json(log_format):
    configure(log_format=log_format, log_prefix=False)
    logger = logging.getLogger()
    own_handlers = _get_own_handlers(logger)
    assert len(own_handlers) == 1
    assert type(own_handlers[0].formatter) is ObjectJsonFormatter


@pytest.mark.parametrize('log_format', [LogFormat.JSON])
def test_formatter_prefixed_json(log_format):
    configure(log_format=log_format, log_prefix=True)
    logger = logging.getLogger()
    own_handlers = _get_own_handlers(logger)
    assert len(own_handlers) == 1
    assert type(own_handlers[0].formatter) is ObjectPrefixingJsonFormatter


@pytest.mark.parametrize('log_format', [LogFormat.JSON])
def test_json_has_no_prefix_by_default(log_format):
    configure(log_format=log_format, log_prefix=None)
    logger = logging.getLogger()
    own_handlers = _get_own_handlers(logger)
    assert len(own_handlers) == 1
    assert type(own_handlers[0].formatter) is ObjectJsonFormatter


def test_error_on_unknown_formatter():
    with pytest.raises(ValueError):
        configure(log_format=object())


@pytest.mark.parametrize('verbose, debug, quiet, expected_level', [
    (None, None, None, logging.INFO),
    (True, None, None, logging.DEBUG),
    (None, True, None, logging.DEBUG),
    (True, True, True, logging.DEBUG),
    (None, None, True, logging.WARNING),
])
def test_levels(verbose, debug, quiet, expected_level):
    configure(verbose=verbose, debug=debug, quiet=quiet)
    logger = logging.getLogger()
    assert logger.level == expected_level



================================================
FILE: tests/logging/test_formatters.py
================================================
import json
import logging.handlers

import pytest

from kopf._cogs.structs.bodies import Body
from kopf._core.actions.loggers import LocalObjectLogger, ObjectJsonFormatter, \
                                       ObjectPrefixingJsonFormatter, ObjectPrefixingTextFormatter, \
                                       ObjectTextFormatter


@pytest.fixture()
def ns_body():
    return Body({
        'kind': 'kind1',
        'apiVersion': 'api1/v1',
        'metadata': {'uid': 'uid1', 'name': 'name1', 'namespace': 'namespace1'},
    })


@pytest.fixture()
def cluster_body():
    return Body({
        'kind': 'kind1',
        'apiVersion': 'api1/v1',
        'metadata': {'uid': 'uid1', 'name': 'name1'},
    })


@pytest.fixture()
def ns_record(settings, ns_body):
    handler = logging.handlers.BufferingHandler(capacity=100)
    logger = LocalObjectLogger(body=ns_body, settings=settings)  # to avoid k8s-posting
    logger.logger.addHandler(handler)
    logger.info("hello")
    return handler.buffer[0]


@pytest.fixture()
def cluster_record(settings, cluster_body):
    handler = logging.handlers.BufferingHandler(capacity=100)
    logger = LocalObjectLogger(body=cluster_body, settings=settings)  # to avoid k8s-posting
    logger.logger.addHandler(handler)
    logger.info("hello")
    return handler.buffer[0]


def test_prefixing_text_formatter_adds_prefixes_when_namespaced(ns_record):
    formatter = ObjectPrefixingTextFormatter()
    formatted = formatter.format(ns_record)
    assert formatted == '[namespace1/name1] hello'


def test_prefixing_text_formatter_adds_prefixes_when_cluster(cluster_record):
    formatter = ObjectPrefixingTextFormatter()
    formatted = formatter.format(cluster_record)
    assert formatted == '[name1] hello'


def test_prefixing_json_formatter_adds_prefixes_when_namespaced(ns_record):
    formatter = ObjectPrefixingJsonFormatter()
    formatted = formatter.format(ns_record)
    decoded = json.loads(formatted)
    assert decoded['message'] == '[namespace1/name1] hello'


def test_prefixing_json_formatter_adds_prefixes_when_clustered(cluster_record):
    formatter = ObjectPrefixingJsonFormatter()
    formatted = formatter.format(cluster_record)
    decoded = json.loads(formatted)
    assert decoded['message'] == '[name1] hello'


def test_regular_text_formatter_omits_prefixes(ns_record):
    formatter = ObjectTextFormatter()
    formatted = formatter.format(ns_record)
    assert formatted == 'hello'


def test_regular_json_formatter_omits_prefixes(ns_record):
    formatter = ObjectJsonFormatter()
    formatted = formatter.format(ns_record)
    decoded = json.loads(formatted)
    assert decoded['message'] == 'hello'


@pytest.mark.parametrize('cls', [ObjectJsonFormatter, ObjectPrefixingJsonFormatter])
@pytest.mark.parametrize('levelno, expected_severity', [
    (0,  'debug'),
    (logging.DEBUG - 1, 'debug'),
    (logging.DEBUG, 'debug'),
    (logging.DEBUG + 1, 'info'),
    (logging.INFO - 1, 'info'),
    (logging.INFO, 'info'),
    (logging.INFO + 1, 'warn'),
    (logging.WARNING - 1, 'warn'),
    (logging.WARNING, 'warn'),
    (logging.WARNING + 1, 'error'),
    (logging.ERROR - 1, 'error'),
    (logging.ERROR, 'error'),
    (logging.ERROR + 1, 'fatal'),
    (logging.FATAL - 1, 'fatal'),
    (logging.FATAL, 'fatal'),
    (logging.FATAL + 1, 'fatal'),
    (999, 'fatal'),
])
def test_json_formatters_add_severity(ns_record, cls, levelno, expected_severity):
    ns_record.levelno = levelno
    ns_record.levelname = 'must-be-irrelevant'
    formatter = cls()
    formatted = formatter.format(ns_record)
    decoded = json.loads(formatted)
    assert 'severity' in decoded
    assert decoded['severity'] == expected_severity


@pytest.mark.parametrize('cls', [ObjectJsonFormatter, ObjectPrefixingJsonFormatter])
def test_json_formatters_add_refkey_with_default_key(ns_record, cls):
    formatter = cls()
    formatted = formatter.format(ns_record)
    decoded = json.loads(formatted)
    assert 'object' in decoded
    assert decoded['object'] == {
        'uid': 'uid1',
        'name': 'name1',
        'namespace': 'namespace1',
        'apiVersion': 'api1/v1',
        'kind': 'kind1',
    }


@pytest.mark.parametrize('cls', [ObjectJsonFormatter, ObjectPrefixingJsonFormatter])
def test_json_formatters_add_refkey_with_custom_key(ns_record, cls):
    formatter = cls(refkey='k8s-obj')
    formatted = formatter.format(ns_record)
    decoded = json.loads(formatted)
    assert 'k8s-obj' in decoded
    assert decoded['k8s-obj'] == {
        'uid': 'uid1',
        'name': 'name1',
        'namespace': 'namespace1',
        'apiVersion': 'api1/v1',
        'kind': 'kind1',
    }



================================================
FILE: tests/logging/test_loggers.py
================================================
import pytest

from kopf._cogs.structs.bodies import Body
from kopf._core.actions.loggers import LocalObjectLogger, ObjectLogger


# Async -- to make the log enqueueing loop running.
@pytest.mark.parametrize('cls', [ObjectLogger, LocalObjectLogger])
async def test_mandatory_body(cls, settings, caplog):
    with pytest.raises(TypeError):
        cls(settings=settings)


# Async -- to make the log enqueueing loop running.
@pytest.mark.parametrize('cls', [ObjectLogger, LocalObjectLogger])
async def test_mandatory_settings(cls, settings, caplog):
    with pytest.raises(TypeError):
        cls(body=Body({}))


# Async -- to make the log enqueueing loop running.
@pytest.mark.parametrize('cls', [ObjectLogger, LocalObjectLogger])
async def test_extras_from_metadata(cls, settings, caplog):
    body = Body({
        'kind': 'kind1',
        'apiVersion': 'api1/v1',
        'metadata': {'uid': 'uid1', 'name': 'name1', 'namespace': 'namespace1'},
    })

    logger = cls(body=body, settings=settings)
    logger.info("hello")

    assert len(caplog.records) == 1
    assert hasattr(caplog.records[0], 'k8s_ref')
    assert caplog.records[0].k8s_ref == {
        'uid': 'uid1',
        'name': 'name1',
        'namespace': 'namespace1',
        'apiVersion': 'api1/v1',
        'kind': 'kind1',
    }


# Async -- to make the log enqueueing loop running.
@pytest.mark.parametrize('cls', [ObjectLogger])
async def test_k8s_posting_enabled_in_a_regular_logger(cls, settings, caplog):
    body = Body({})

    logger = cls(body=body, settings=settings)
    logger.info("hello")

    assert len(caplog.records) == 1
    assert hasattr(caplog.records[0], 'k8s_skip')
    assert caplog.records[0].k8s_skip is False


# Async -- to make the log enqueueing loop running.
@pytest.mark.parametrize('cls', [LocalObjectLogger])
async def test_k8s_posting_disabled_in_a_local_logger(cls, settings, caplog):
    body = Body({})

    logger = cls(body=body, settings=settings)
    logger.info("hello")

    assert len(caplog.records) == 1
    assert hasattr(caplog.records[0], 'k8s_skip')
    assert caplog.records[0].k8s_skip is True



================================================
FILE: tests/observation/test_processing_of_namespaces.py
================================================
import asyncio

import pytest

from kopf._cogs.structs.bodies import RawBody, RawEvent
from kopf._cogs.structs.references import Insights
from kopf._core.reactor.observation import process_discovered_namespace_event


async def test_initial_listing_is_ignored():
    insights = Insights()
    e1 = RawEvent(type=None, object=RawBody(metadata={'name': 'ns1'}))

    async def delayed_injection(delay: float):
        await asyncio.sleep(delay)
        await process_discovered_namespace_event(
            insights=insights, raw_event=e1, namespaces=['ns*'])

    task = asyncio.create_task(delayed_injection(0))
    with pytest.raises(asyncio.TimeoutError):
        async with insights.revised:
            await asyncio.wait_for(insights.revised.wait(), timeout=0.1)
    await task
    assert not insights.namespaces


@pytest.mark.parametrize('etype', ['ADDED', 'MODIFIED'])
async def test_followups_for_addition(timer, etype):
    insights = Insights()
    e1 = RawEvent(type=etype, object=RawBody(metadata={'name': 'ns1'}))

    async def delayed_injection(delay: float):
        await asyncio.sleep(delay)
        await process_discovered_namespace_event(
            insights=insights, raw_event=e1, namespaces=['ns*'])

    task = asyncio.create_task(delayed_injection(0.1))
    with timer:
        async with insights.revised:
            await insights.revised.wait()
    await task
    assert 0.1 < timer.seconds < 0.11
    assert insights.namespaces == {'ns1'}


@pytest.mark.parametrize('etype', ['DELETED'])
async def test_followups_for_deletion(timer, etype):
    insights = Insights()
    insights.namespaces.add('ns1')
    e1 = RawEvent(type=etype, object=RawBody(metadata={'name': 'ns1'}))

    async def delayed_injection(delay: float):
        await asyncio.sleep(delay)
        await process_discovered_namespace_event(
            insights=insights, raw_event=e1, namespaces=['ns*'])

    task = asyncio.create_task(delayed_injection(0.1))
    with timer:
        async with insights.revised:
            await insights.revised.wait()
    await task
    assert 0.1 < timer.seconds < 0.11
    assert not insights.namespaces



================================================
FILE: tests/observation/test_processing_of_resources.py
================================================
import asyncio

import aiohttp.web
import pytest

import kopf
from kopf._cogs.structs.bodies import RawBody, RawEvent
from kopf._cogs.structs.references import NAMESPACES, Insights, Resource
from kopf._core.reactor.observation import process_discovered_resource_event

# Implementation awareness: the events only trigger the re-scan, so the fields can be reduced
# to only the group name which is being rescanned. Other fields are ignored in the events.
# The actual data is taken from the API. It is tested elsewhere, so we rely on its correctness here.
GROUP1_EVENT = RawBody(spec={'group': 'group1'})


@pytest.fixture()
def core_mock(resp_mocker, aresponses, hostname):
    mock = resp_mocker(return_value=aiohttp.web.json_response({'versions': ['v1']}))
    aresponses.add(hostname, '/api', 'get', mock)
    return mock


@pytest.fixture()
def apis_mock(resp_mocker, aresponses, hostname):
    mock = resp_mocker(return_value=aiohttp.web.json_response({'groups': [
        {
            'name': 'group1',
            'preferredVersion': {'version': 'version1'},
            'versions': [{'version': 'version1'}],
        },
    ]}))
    aresponses.add(hostname, '/apis', 'get', mock)
    return mock


@pytest.fixture()
def corev1_mock(resp_mocker, aresponses, hostname, core_mock):
    mock = resp_mocker(return_value=aiohttp.web.json_response({'resources': [
        {
            'kind': 'Namespace',
            'name': 'namespaces',
            'singularName': 'namespace',
            'namespaced': False,
            'categories': [],
            'shortNames': [],
            'verbs': ['list', 'watch', 'patch'],
        },
    ]}))
    aresponses.add(hostname, '/api/v1', 'get', mock)
    return mock


@pytest.fixture()
def group1_mock(resp_mocker, aresponses, hostname, apis_mock):
    mock = resp_mocker(return_value=aiohttp.web.json_response({'resources': [
        {
            'kind': 'kind1',
            'name': 'plural1',
            'singularName': 'singular1',
            'namespaced': True,
            'categories': ['category1a', 'category1b'],
            'shortNames': ['shortname1a', 'shortname1b'],
            'verbs': ['list', 'watch', 'patch'],
        },
    ]}))
    aresponses.add(hostname, '/apis/group1/version1', 'get', mock)
    return mock


@pytest.fixture()
def group1_empty_mock(resp_mocker, aresponses, hostname, apis_mock):
    mock = resp_mocker(return_value=aiohttp.web.json_response({'resources': []}))
    aresponses.add(hostname, '/apis/group1/version1', 'get', mock)
    return mock


@pytest.fixture()
def group1_404mock(resp_mocker, aresponses, hostname, apis_mock):
    mock = resp_mocker(return_value=aresponses.Response(status=404, reason='oops'))
    aresponses.add(hostname, '/apis/group1/version1', 'get', mock)
    return mock


@pytest.fixture()
async def insights():
    return Insights()


@pytest.fixture(params=[
    (kopf.on.event, 'watched_resources'),
    (kopf.daemon, 'watched_resources'),
    (kopf.timer, 'watched_resources'),
    (kopf.index, 'watched_resources'),
    (kopf.index, 'indexed_resources'),
    (kopf.on.resume, 'watched_resources'),
    (kopf.on.create, 'watched_resources'),
    (kopf.on.update, 'watched_resources'),
    (kopf.on.delete, 'watched_resources'),
    (kopf.on.validate, 'webhook_resources'),
    (kopf.on.mutate, 'webhook_resources'),
])
def insights_resources(request, registry, insights):
    decorator, insights_field = request.param

    @decorator('group1', 'version1', 'plural1')
    def fn(**_): pass

    return getattr(insights, insights_field)


@pytest.mark.parametrize('decorator', [kopf.on.validate, kopf.on.mutate])
@pytest.mark.parametrize('etype', ['ADDED', 'MODIFIED'])
async def test_nonwatchable_resources_are_ignored(
        settings, registry, apis_mock, group1_mock, timer, etype, decorator, insights):

    @decorator('group1', 'version1', 'plural1')
    def fn(**_): pass

    e1 = RawEvent(type=etype, object=RawBody(spec={'group': 'group1'}))

    async def delayed_injection(delay: float):
        await asyncio.sleep(delay)
        await process_discovered_resource_event(
            insights=insights, raw_event=e1, registry=registry, settings=settings)

    task = asyncio.create_task(delayed_injection(0.1))
    with timer:
        async with insights.revised:
            await insights.revised.wait()
    await task
    assert 0.1 < timer.seconds < 1.0
    assert not insights.watched_resources
    assert apis_mock.called
    assert group1_mock.called


async def test_initial_listing_is_ignored(
        settings, registry, apis_mock, group1_mock, insights):

    e1 = RawEvent(type=None, object=RawBody(spec={'group': 'group1'}))

    async def delayed_injection(delay: float):
        await asyncio.sleep(delay)
        await process_discovered_resource_event(
            insights=insights, raw_event=e1, registry=registry, settings=settings)

    task = asyncio.create_task(delayed_injection(0))
    with pytest.raises(asyncio.TimeoutError):
        async with insights.revised:
            await asyncio.wait_for(insights.revised.wait(), timeout=0.1)
    await task
    assert not insights.indexed_resources
    assert not insights.watched_resources
    assert not insights.webhook_resources
    assert not apis_mock.called
    assert not group1_mock.called


@pytest.mark.parametrize('etype', ['ADDED', 'MODIFIED'])
async def test_followups_for_addition(
        settings, registry, apis_mock, group1_mock, timer, etype, insights, insights_resources):

    e1 = RawEvent(type=etype, object=RawBody(spec={'group': 'group1'}))
    r1 = Resource(group='group1', version='version1', plural='plural1')

    async def delayed_injection(delay: float):
        await asyncio.sleep(delay)
        await process_discovered_resource_event(
            insights=insights, raw_event=e1, registry=registry, settings=settings)

    task = asyncio.create_task(delayed_injection(0.1))
    with timer:
        async with insights.revised:
            await insights.revised.wait()
    await task
    assert 0.1 < timer.seconds < 1.0
    assert insights_resources == {r1}
    assert apis_mock.called
    assert group1_mock.called


@pytest.mark.parametrize('etype', ['ADDED', 'MODIFIED', 'DELETED'])
async def test_followups_for_deletion_of_resource(
        settings, registry, apis_mock, group1_empty_mock, timer, etype,
        insights, insights_resources):

    e1 = RawEvent(type=etype, object=RawBody(spec={'group': 'group1'}))
    r1 = Resource(group='group1', version='version1', plural='plural1')
    insights_resources.add(r1)

    async def delayed_injection(delay: float):
        await asyncio.sleep(delay)
        await process_discovered_resource_event(
            insights=insights, raw_event=e1, registry=registry, settings=settings)

    task = asyncio.create_task(delayed_injection(0.1))
    with timer:
        async with insights.revised:
            await insights.revised.wait()
    await task
    assert 0.1 < timer.seconds < 1.0
    assert not insights_resources
    assert apis_mock.called
    assert group1_empty_mock.called


@pytest.mark.parametrize('etype', ['ADDED', 'MODIFIED', 'DELETED'])
async def test_followups_for_deletion_of_group(
        settings, registry, apis_mock, group1_404mock, timer, etype, insights, insights_resources):

    e1 = RawEvent(type=etype, object=RawBody(spec={'group': 'group1'}))
    r1 = Resource(group='group1', version='version1', plural='plural1')
    insights_resources.add(r1)

    async def delayed_injection(delay: float):
        await asyncio.sleep(delay)
        await process_discovered_resource_event(
            insights=insights, raw_event=e1, registry=registry, settings=settings)

    task = asyncio.create_task(delayed_injection(0.1))
    with timer:
        async with insights.revised:
            await insights.revised.wait()
    await task
    assert 0.1 < timer.seconds < 1.0
    assert not insights_resources
    assert apis_mock.called
    assert group1_404mock.called


@pytest.mark.parametrize('etype', ['DELETED'])
async def test_backbone_is_filled(
        settings, registry, core_mock, corev1_mock, timer, etype, insights):

    e1 = RawEvent(type=etype, object=RawBody(spec={'group': ''}))

    async def delayed_injection(delay: float):
        await asyncio.sleep(delay)
        await process_discovered_resource_event(
            insights=insights, raw_event=e1, registry=registry, settings=settings)

    task = asyncio.create_task(delayed_injection(0.1))
    with timer:
        await insights.backbone.wait_for(NAMESPACES)
    await task
    assert 0.1 < timer.seconds < 1.0
    assert NAMESPACES in insights.backbone
    assert core_mock.called
    assert corev1_mock.called



================================================
FILE: tests/observation/test_revision_of_namespaces.py
================================================
from kopf._cogs.structs.bodies import RawBody, RawEvent
from kopf._cogs.structs.references import Insights
from kopf._core.reactor.observation import revise_namespaces


def test_bodies_for_initial_population(registry):
    b1 = RawBody(metadata={'name': 'ns1'})
    insights = Insights()
    revise_namespaces(insights=insights, namespaces=['ns*'], raw_bodies=[b1])
    assert insights.namespaces == {'ns1'}


def test_bodies_for_additional_population(registry):
    b1 = RawBody(metadata={'name': 'ns1'})
    b2 = RawBody(metadata={'name': 'ns2'})
    insights = Insights()
    revise_namespaces(insights=insights, namespaces=['ns*'], raw_bodies=[b1])
    revise_namespaces(insights=insights, namespaces=['ns*'], raw_bodies=[b2])
    assert insights.namespaces == {'ns1', 'ns2'}


def test_bodies_for_deletion_via_timestamp(registry):
    b1 = RawBody(metadata={'name': 'ns1'})
    b2 = RawBody(metadata={'name': 'ns1', 'deletionTimestamp': '...'})
    insights = Insights()
    revise_namespaces(insights=insights, namespaces=['ns*'], raw_bodies=[b1])
    revise_namespaces(insights=insights, namespaces=['ns*'], raw_bodies=[b2])
    assert not insights.namespaces


def test_bodies_ignored_for_mismatching(registry):
    b1 = RawBody(metadata={'name': 'def1'})
    insights = Insights()
    revise_namespaces(insights=insights, namespaces=['ns*'], raw_bodies=[b1])
    assert not insights.namespaces


def test_events_for_initial_population(registry):
    e1 = RawEvent(type=None, object=RawBody(metadata={'name': 'ns1'}))
    insights = Insights()
    revise_namespaces(insights=insights, namespaces=['ns*'], raw_events=[e1])
    assert insights.namespaces == {'ns1'}


def test_events_for_additional_population(registry):
    e1 = RawEvent(type=None, object=RawBody(metadata={'name': 'ns1'}))
    e2 = RawEvent(type=None, object=RawBody(metadata={'name': 'ns2'}))
    insights = Insights()
    revise_namespaces(insights=insights, namespaces=['ns*'], raw_events=[e1])
    revise_namespaces(insights=insights, namespaces=['ns*'], raw_events=[e2])
    assert insights.namespaces == {'ns1', 'ns2'}


def test_events_for_deletion_via_timestamp(registry):
    e1 = RawEvent(type=None, object=RawBody(metadata={'name': 'ns1'}))
    e2 = RawEvent(type=None, object=RawBody(metadata={'name': 'ns1', 'deletionTimestamp': '...'}))
    insights = Insights()
    revise_namespaces(insights=insights, namespaces=['ns*'], raw_events=[e1])
    revise_namespaces(insights=insights, namespaces=['ns*'], raw_events=[e2])
    assert not insights.namespaces


def test_events_for_deletion_via_event_type(registry):
    e1 = RawEvent(type=None, object=RawBody(metadata={'name': 'ns1'}))
    e2 = RawEvent(type='DELETED', object=RawBody(metadata={'name': 'ns1'}))
    insights = Insights()
    revise_namespaces(insights=insights, namespaces=['ns*'], raw_events=[e1])
    revise_namespaces(insights=insights, namespaces=['ns*'], raw_events=[e2])
    assert not insights.namespaces


def test_events_ignored_for_mismatching(registry):
    e1 = RawEvent(type=None, object=RawBody(metadata={'name': 'def1'}))
    insights = Insights()
    revise_namespaces(insights=insights, namespaces=['ns*'], raw_events=[e1])
    assert not insights.namespaces



================================================
FILE: tests/observation/test_revision_of_resources.py
================================================
import pytest

import kopf
from kopf._cogs.structs.references import EVERYTHING, Insights, Resource
from kopf._core.reactor.observation import revise_resources

VERBS = ['list', 'watch', 'patch']


@pytest.fixture()
async def insights():
    return Insights()


@pytest.fixture(params=[
    (kopf.on.event, 'watched_resources'),
    (kopf.daemon, 'watched_resources'),
    (kopf.timer, 'watched_resources'),
    (kopf.index, 'watched_resources'),
    (kopf.index, 'indexed_resources'),
    (kopf.on.resume, 'watched_resources'),
    (kopf.on.create, 'watched_resources'),
    (kopf.on.update, 'watched_resources'),
    (kopf.on.delete, 'watched_resources'),
    (kopf.on.validate, 'webhook_resources'),
    (kopf.on.mutate, 'webhook_resources'),
])
def insights_resources(request, registry, insights):
    decorator, insights_field = request.param

    @decorator('group1', 'version1', 'plural1')
    @decorator('group2', 'version2', 'plural2')
    def fn(**_): pass

    return getattr(insights, insights_field)


def test_initial_population(registry, insights, insights_resources):
    r1 = Resource(group='group1', version='version1', plural='plural1', verbs=VERBS)
    revise_resources(registry=registry, insights=insights, group=None, resources=[r1])
    assert insights_resources == {r1}


def test_replacing_all_insights(registry, insights, insights_resources):
    r1 = Resource(group='group1', version='version1', plural='plural1', verbs=VERBS)
    r2 = Resource(group='group2', version='version2', plural='plural2', verbs=VERBS)
    revise_resources(registry=registry, insights=insights, group=None, resources=[r1])
    revise_resources(registry=registry, insights=insights, group=None, resources=[r2])
    assert insights_resources == {r2}


def test_replacing_an_existing_group(registry, insights, insights_resources):
    r1 = Resource(group='group1', version='version1', plural='plural1', verbs=VERBS)
    r2 = Resource(group='group2', version='version2', plural='plural2', verbs=VERBS)
    revise_resources(registry=registry, insights=insights, group=None, resources=[r1])
    revise_resources(registry=registry, insights=insights, group='group1', resources=[r2])
    assert insights_resources == {r2}


def test_replacing_a_new_group(registry, insights, insights_resources):
    r1 = Resource(group='group1', version='version1', plural='plural1', verbs=VERBS)
    r2 = Resource(group='group2', version='version2', plural='plural2', verbs=VERBS)
    revise_resources(registry=registry, insights=insights, group=None, resources=[r1])
    revise_resources(registry=registry, insights=insights, group='group2', resources=[r2])
    assert insights_resources == {r1, r2}


@pytest.mark.parametrize('decorator', [
    kopf.on.event, kopf.daemon, kopf.timer, kopf.index,
    kopf.on.resume, kopf.on.create, kopf.on.update, kopf.on.delete,
])
def test_ambiguity_in_specific_selectors(registry, decorator, caplog, assert_logs, insights):
    r1 = Resource(group='g1', version='v1', plural='plural', verbs=VERBS)
    r2 = Resource(group='g2', version='v2', plural='plural', verbs=VERBS)

    @decorator(plural='plural')
    def fn(**_): pass

    revise_resources(registry=registry, insights=insights, group=None, resources=[r1, r2])
    assert not insights.watched_resources
    assert not insights.webhook_resources
    assert_logs([r"Ambiguous resources will not be served"])


@pytest.mark.parametrize('decorator', [
    kopf.on.event, kopf.daemon, kopf.timer, kopf.index,
    kopf.on.resume, kopf.on.create, kopf.on.update, kopf.on.delete,
])
def test_corev1_overrides_ambuigity(registry, decorator, caplog, assert_logs, insights):
    r1 = Resource(group='', version='v1', plural='pods', verbs=VERBS)
    r2 = Resource(group='metrics.k8s.io', version='v1', plural='pods', verbs=VERBS)

    @decorator(plural='pods')
    def fn(**_): pass

    revise_resources(registry=registry, insights=insights, group=None, resources=[r1, r2])
    assert insights.watched_resources == {r1}
    assert_logs([], prohibited=[r"Ambiguous resources will not be served"])


@pytest.mark.parametrize('decorator', [
    kopf.on.event, kopf.daemon, kopf.timer, kopf.index,
    kopf.on.resume, kopf.on.create, kopf.on.update, kopf.on.delete,
])
def test_no_ambiguity_in_generic_selector(registry, decorator, caplog, assert_logs, insights):
    r1 = Resource(group='g1', version='v1', plural='plural', verbs=VERBS)
    r2 = Resource(group='g2', version='v2', plural='plural', verbs=VERBS)

    @decorator(EVERYTHING)
    def fn(**_): pass

    revise_resources(registry=registry, insights=insights, group=None, resources=[r1, r2])
    assert insights.watched_resources == {r1, r2}
    assert_logs([], prohibited=[r"Ambiguous resources will not be served"])


@pytest.mark.parametrize('decorator', [
    kopf.on.event, kopf.daemon, kopf.timer, kopf.index,
    kopf.on.resume, kopf.on.create, kopf.on.update, kopf.on.delete,
])
def test_selectors_with_no_resources(registry, decorator, caplog, assert_logs, insights):
    r1 = Resource(group='group1', version='version1', plural='plural1', verbs=VERBS)
    r2 = Resource(group='group2', version='version2', plural='plural2', verbs=VERBS)

    @decorator(plural='plural3')
    def fn(**_): pass

    revise_resources(registry=registry, insights=insights, group=None, resources=[r1, r2])
    assert not insights.watched_resources
    assert_logs([r"Unresolved resources cannot be served"])


@pytest.mark.parametrize('decorator', [
    kopf.daemon, kopf.timer,
    kopf.on.resume, kopf.on.create, kopf.on.update, kopf.on.delete,
])
def test_nonwatchable_excluded(registry, decorator, caplog, assert_logs, insights):
    r1 = Resource(group='group1', version='version1', plural='plural1', verbs=[])

    @decorator('group1', 'version1', 'plural1')
    def fn(**_): pass

    revise_resources(registry=registry, insights=insights, group=None, resources=[r1])
    assert not insights.watched_resources
    assert_logs([r"Non-watchable resources will not be served: {plural1.version1.group1}"])


@pytest.mark.parametrize('decorator', [
    kopf.daemon, kopf.timer,
    kopf.on.resume, kopf.on.create, kopf.on.update, kopf.on.delete,
])
def test_nonpatchable_excluded(registry, decorator, caplog, assert_logs, insights):
    r1 = Resource(group='group1', version='version1', plural='plural1', verbs=['watch', 'list'])

    @decorator('group1', 'version1', 'plural1')  # because it patches!
    def fn(**_): pass

    revise_resources(registry=registry, insights=insights, group=None, resources=[r1])
    assert not insights.watched_resources
    assert_logs([r"Non-patchable resources will not be served: {plural1.version1.group1}"])


@pytest.mark.parametrize('decorator', [
    kopf.daemon, kopf.timer,
    kopf.on.resume, kopf.on.create, kopf.on.update, kopf.on.delete,
])
def test_watchedonly_resources_are_excluded_from_other_sets(registry, decorator, insights):

    r1 = Resource(group='group1', version='version1', plural='plural1', verbs=VERBS)

    @decorator('group1', 'version1', 'plural1')
    def fn(**_): pass

    revise_resources(registry=registry, insights=insights, group=None, resources=[r1])
    assert insights.watched_resources
    assert not insights.indexed_resources
    assert not insights.webhook_resources


@pytest.mark.parametrize('decorator', [
    kopf.on.mutate, kopf.on.validate,
])
def test_webhookonly_resources_are_excluded_from_other_sets(registry, decorator, insights):

    r1 = Resource(group='group1', version='version1', plural='plural1', verbs=VERBS)

    @decorator('group1', 'version1', 'plural1')
    def fn(**_): pass

    revise_resources(registry=registry, insights=insights, group=None, resources=[r1])
    assert not insights.watched_resources
    assert not insights.indexed_resources
    assert insights.webhook_resources


@pytest.mark.parametrize('decorator', [
    kopf.index,
])
def test_indexed_resources_are_duplicated_in_watched_resources(registry, decorator, insights):

    r1 = Resource(group='group1', version='version1', plural='plural1', verbs=VERBS)

    @decorator('group1', 'version1', 'plural1')
    def fn(**_): pass

    revise_resources(registry=registry, insights=insights, group=None, resources=[r1])
    assert insights.watched_resources
    assert insights.indexed_resources
    assert not insights.webhook_resources



================================================
FILE: tests/orchestration/test_task_adjustments.py
================================================
import asyncio
from typing import Optional

import pytest

from kopf._cogs.aiokits import aiotasks, aiotoggles
from kopf._cogs.structs import bodies
from kopf._cogs.structs.references import Insights, Resource
from kopf._core.engines.peering import Identity
from kopf._core.reactor.orchestration import Ensemble, EnsembleKey, adjust_tasks


async def processor(*, raw_event: bodies.RawEvent, stream_pressure: Optional[asyncio.Event]) -> None:
    pass


@pytest.fixture(autouse=True)
def _auto_mocked(k8s_mocked):
    pass


@pytest.fixture()
async def insights(request, settings):
    insights = Insights()
    await insights.backbone.fill(resources=[
        request.getfixturevalue(name)
        for name in ['peering_resource', 'cluster_peering_resource', 'namespaced_peering_resource']
        if name in request.fixturenames
    ])
    settings.peering.mandatory = True
    return insights


@pytest.fixture()
async def ensemble(_no_asyncio_pending_tasks):
    operator_indexed = aiotoggles.ToggleSet(all)
    operator_paused = aiotoggles.ToggleSet(any)
    peering_missing = await operator_paused.make_toggle()
    ensemble = Ensemble(
        operator_indexed=operator_indexed,
        operator_paused=operator_paused,
        peering_missing=peering_missing,
    )

    try:
        yield ensemble
    finally:
        await aiotasks.stop(ensemble.get_tasks(ensemble.get_keys()), title='...')  # cleanup


async def test_empty_insights_cause_no_adjustments(
        settings, insights, ensemble):

    await adjust_tasks(
        processor=processor,
        identity=Identity('...'),
        settings=settings,
        insights=insights,
        ensemble=ensemble,
    )

    assert not ensemble.watcher_tasks
    assert not ensemble.peering_tasks
    assert not ensemble.pinging_tasks
    assert not ensemble.conflicts_found


async def test_new_resources_and_namespaces_spawn_new_tasks(
        settings, ensemble: Ensemble, insights: Insights, peering_resource):
    settings.peering.namespaced = peering_resource.namespaced

    r1 = Resource(group='group1', version='version1', plural='plural1', namespaced=True)
    r2 = Resource(group='group2', version='version2', plural='plural2', namespaced=True)
    insights.watched_resources.add(r1)
    insights.watched_resources.add(r2)
    insights.namespaces.add('ns1')
    insights.namespaces.add('ns2')
    r1ns1 = EnsembleKey(resource=r1, namespace='ns1')
    r1ns2 = EnsembleKey(resource=r1, namespace='ns2')
    r2ns1 = EnsembleKey(resource=r2, namespace='ns1')
    r2ns2 = EnsembleKey(resource=r2, namespace='ns2')
    peerns = peering_resource.namespaced
    peer1 = EnsembleKey(resource=peering_resource, namespace='ns1' if peerns else None)
    peer2 = EnsembleKey(resource=peering_resource, namespace='ns2' if peerns else None)

    await adjust_tasks(
        processor=processor,
        identity=Identity('...'),
        settings=settings,
        insights=insights,
        ensemble=ensemble,
    )

    assert set(ensemble.watcher_tasks) == {r1ns1, r1ns2, r2ns1, r2ns2}
    assert set(ensemble.peering_tasks) == {peer1, peer2}
    assert set(ensemble.pinging_tasks) == {peer1, peer2}
    assert set(ensemble.conflicts_found) == {peer1, peer2}


async def test_gone_resources_and_namespaces_stop_running_tasks(
        settings, ensemble: Ensemble, insights: Insights, peering_resource):
    settings.peering.namespaced = peering_resource.namespaced

    r1 = Resource(group='group1', version='version1', plural='plural1', namespaced=True)
    r2 = Resource(group='group2', version='version2', plural='plural2', namespaced=True)
    insights.watched_resources.add(r1)
    insights.watched_resources.add(r2)
    insights.namespaces.add('ns1')
    insights.namespaces.add('ns2')
    r1ns1 = EnsembleKey(resource=r1, namespace='ns1')
    r1ns2 = EnsembleKey(resource=r1, namespace='ns2')
    r2ns1 = EnsembleKey(resource=r2, namespace='ns1')
    r2ns2 = EnsembleKey(resource=r2, namespace='ns2')
    peerns = peering_resource.namespaced
    peer1 = EnsembleKey(resource=peering_resource, namespace='ns1' if peerns else None)

    await adjust_tasks(  # initialisation
        processor=processor,
        identity=Identity('...'),
        settings=settings,
        insights=insights,
        ensemble=ensemble
    )

    r1ns2_task = ensemble.watcher_tasks[r1ns2]
    r2ns1_task = ensemble.watcher_tasks[r2ns1]
    r2ns2_task = ensemble.watcher_tasks[r2ns2]

    insights.watched_resources.discard(r2)
    insights.namespaces.discard('ns2')

    await adjust_tasks(  # action-under-test
        processor=processor,
        identity=Identity('...'),
        settings=settings,
        insights=insights,
        ensemble=ensemble,
    )

    assert set(ensemble.watcher_tasks) == {r1ns1}
    assert set(ensemble.peering_tasks) == {peer1}
    assert set(ensemble.pinging_tasks) == {peer1}
    assert set(ensemble.conflicts_found) == {peer1}
    assert r1ns2_task.cancelled()
    assert r2ns1_task.cancelled()
    assert r2ns2_task.cancelled()


async def test_cluster_tasks_continue_running_on_namespace_deletion(
        settings, ensemble: Ensemble, insights: Insights, cluster_peering_resource):
    settings.peering.namespaced = cluster_peering_resource.namespaced

    r1 = Resource(group='group1', version='version1', plural='plural1', namespaced=True)
    r2 = Resource(group='group2', version='version2', plural='plural2', namespaced=True)
    insights.watched_resources.add(r1)
    insights.watched_resources.add(r2)
    insights.namespaces.add(None)
    r1nsN = EnsembleKey(resource=r1, namespace=None)
    r2nsN = EnsembleKey(resource=r2, namespace=None)
    peerN = EnsembleKey(resource=cluster_peering_resource, namespace=None)

    await adjust_tasks(  # initialisation
        processor=processor,
        identity=Identity('...'),
        settings=settings,
        insights=insights,
        ensemble=ensemble
    )

    r1nsN_task = ensemble.watcher_tasks[r1nsN]
    r2nsN_task = ensemble.watcher_tasks[r2nsN]

    insights.namespaces.discard(None)

    await adjust_tasks(  # action-under-test
        processor=processor,
        identity=Identity('...'),
        settings=settings,
        insights=insights,
        ensemble=ensemble,
    )

    assert set(ensemble.watcher_tasks) == {r1nsN, r2nsN}
    assert set(ensemble.peering_tasks) == {peerN}
    assert set(ensemble.pinging_tasks) == {peerN}
    assert set(ensemble.conflicts_found) == {peerN}
    assert not r1nsN_task.cancelled()
    assert not r2nsN_task.cancelled()
    assert not r1nsN_task.done()
    assert not r2nsN_task.done()


async def test_no_peering_tasks_with_no_peering_resources(
        settings, ensemble):

    settings.peering.mandatory = False
    insights = Insights()
    r1 = Resource(group='group1', version='version1', plural='plural1', namespaced=True)
    insights.watched_resources.add(r1)
    insights.namespaces.add('ns1')

    await adjust_tasks(
        processor=processor,
        identity=Identity('...'),
        settings=settings,
        insights=insights,
        ensemble=ensemble,
    )

    assert ensemble.watcher_tasks
    assert not ensemble.peering_tasks
    assert not ensemble.pinging_tasks
    assert not ensemble.conflicts_found


async def test_paused_with_mandatory_peering_but_absent_peering_resource(
        settings, ensemble: Ensemble):

    settings.peering.mandatory = True
    insights = Insights()

    await ensemble.peering_missing.turn_to(False)  # prerequisite
    assert ensemble.peering_missing.is_off()  # prerequisite
    assert ensemble.operator_paused.is_off()  # prerequisite

    await adjust_tasks(
        processor=processor,
        identity=Identity('...'),
        settings=settings,
        insights=insights,
        ensemble=ensemble,
    )

    assert ensemble.peering_missing.is_on()
    assert ensemble.operator_paused.is_on()


async def test_unpaused_with_mandatory_peering_and_existing_peering_resource(
        settings, ensemble: Ensemble, insights: Insights, peering_resource):
    settings.peering.namespaced = peering_resource.namespaced

    await ensemble.peering_missing.turn_to(True)  # prerequisite
    assert ensemble.peering_missing.is_on()  # prerequisite
    assert ensemble.operator_paused.is_on()  # prerequisite

    await adjust_tasks(
        processor=processor,
        identity=Identity('...'),
        settings=settings,
        insights=insights,
        ensemble=ensemble,
    )

    assert ensemble.peering_missing.is_off()
    assert ensemble.operator_paused.is_off()



================================================
FILE: tests/peering/conftest.py
================================================
import pytest


@pytest.fixture(autouse=True)
def _autouse_fake_vault(fake_vault):
    pass



================================================
FILE: tests/peering/test_freeze_mode.py
================================================
import freezegun
import pytest

from kopf._cogs.aiokits import aiotoggles
from kopf._cogs.structs import bodies
from kopf._core.engines.peering import process_peering_event


async def test_other_peering_objects_are_ignored(
        mocker, k8s_mocked, settings,
        peering_resource, peering_namespace):

    status = mocker.Mock()
    status.items.side_effect = Exception("This should not be called.")
    event = bodies.RawEvent(
        type='ADDED',  # irrelevant
        object={
            'metadata': {'name': 'their-name'},
            'status': status,
        })

    settings.peering.name = 'our-name'
    await process_peering_event(
        raw_event=event,
        autoclean=False,
        identity='id',
        settings=settings,
        namespace=peering_namespace,
        resource=peering_resource,
    )
    assert not status.items.called
    assert not k8s_mocked.patch.called
    assert k8s_mocked.sleep.call_count == 0


@freezegun.freeze_time('2020-12-31T23:59:59.123456')
async def test_toggled_on_for_higher_priority_peer_when_initially_off(
        k8s_mocked, caplog, assert_logs, settings,
        peering_resource, peering_namespace):

    event = bodies.RawEvent(
        type='ADDED',  # irrelevant
        object={
            'metadata': {'name': 'name', 'namespace': peering_namespace},  # for matching
            'status': {
                'higher-prio': {
                    'priority': 101,
                    'lifetime': 10,
                    'lastseen': '2020-12-31T23:59:59'
                },
            },
        })
    settings.peering.name = 'name'
    settings.peering.priority = 100

    conflicts_found = aiotoggles.Toggle(False)
    k8s_mocked.sleep.return_value = 1  # as if interrupted by stream pressure

    caplog.set_level(0)
    assert conflicts_found.is_off()
    await process_peering_event(
        raw_event=event,
        conflicts_found=conflicts_found,
        autoclean=False,
        namespace=peering_namespace,
        resource=peering_resource,
        identity='id',
        settings=settings,
    )
    assert conflicts_found.is_on()
    assert k8s_mocked.sleep.call_count == 1
    assert 9 < k8s_mocked.sleep.call_args[0][0][0] < 10
    assert not k8s_mocked.patch.called
    assert_logs(["Pausing operations in favour of"], prohibited=[
        "Possibly conflicting operators",
        "Pausing all operators, including self",
        "Resuming operations after the pause",
    ])


@freezegun.freeze_time('2020-12-31T23:59:59.123456')
async def test_ignored_for_higher_priority_peer_when_already_on(
        k8s_mocked, caplog, assert_logs, settings,
        peering_resource, peering_namespace):

    event = bodies.RawEvent(
        type='ADDED',  # irrelevant
        object={
            'metadata': {'name': 'name', 'namespace': peering_namespace},  # for matching
            'status': {
                'higher-prio': {
                    'priority': 101,
                    'lifetime': 10,
                    'lastseen': '2020-12-31T23:59:59'
                },
            },
        })
    settings.peering.name = 'name'
    settings.peering.priority = 100

    conflicts_found = aiotoggles.Toggle(True)
    k8s_mocked.sleep.return_value = 1  # as if interrupted by stream pressure

    caplog.set_level(0)
    assert conflicts_found.is_on()
    await process_peering_event(
        raw_event=event,
        conflicts_found=conflicts_found,
        autoclean=False,
        namespace=peering_namespace,
        resource=peering_resource,
        identity='id',
        settings=settings,
    )
    assert conflicts_found.is_on()
    assert k8s_mocked.sleep.call_count == 1
    assert 9 < k8s_mocked.sleep.call_args[0][0][0] < 10
    assert not k8s_mocked.patch.called
    assert_logs([], prohibited=[
        "Possibly conflicting operators",
        "Pausing all operators, including self",
        "Pausing operations in favour of",
        "Resuming operations after the pause",
    ])


@freezegun.freeze_time('2020-12-31T23:59:59.123456')
async def test_toggled_off_for_lower_priority_peer_when_initially_on(
        k8s_mocked, caplog, assert_logs, settings,
        peering_resource, peering_namespace):

    event = bodies.RawEvent(
        type='ADDED',  # irrelevant
        object={
            'metadata': {'name': 'name', 'namespace': peering_namespace},  # for matching
            'status': {
                'higher-prio': {
                    'priority': 99,
                    'lifetime': 10,
                    'lastseen': '2020-12-31T23:59:59'
                },
            },
        })
    settings.peering.name = 'name'
    settings.peering.priority = 100

    conflicts_found = aiotoggles.Toggle(True)
    k8s_mocked.sleep.return_value = 1  # as if interrupted by stream pressure

    caplog.set_level(0)
    assert conflicts_found.is_on()
    await process_peering_event(
        raw_event=event,
        conflicts_found=conflicts_found,
        autoclean=False,
        namespace=peering_namespace,
        resource=peering_resource,
        identity='id',
        settings=settings,
    )
    assert conflicts_found.is_off()
    assert k8s_mocked.sleep.call_count == 1
    assert k8s_mocked.sleep.call_args[0][0] == []
    assert not k8s_mocked.patch.called
    assert_logs(["Resuming operations after the pause"], prohibited=[
        "Possibly conflicting operators",
        "Pausing all operators, including self",
        "Pausing operations in favour of",
    ])


@freezegun.freeze_time('2020-12-31T23:59:59.123456')
async def test_ignored_for_lower_priority_peer_when_already_off(
        k8s_mocked, caplog, assert_logs, settings,
        peering_resource, peering_namespace):

    event = bodies.RawEvent(
        type='ADDED',  # irrelevant
        object={
            'metadata': {'name': 'name', 'namespace': peering_namespace},  # for matching
            'status': {
                'higher-prio': {
                    'priority': 99,
                    'lifetime': 10,
                    'lastseen': '2020-12-31T23:59:59'
                },
            },
        })
    settings.peering.name = 'name'
    settings.peering.priority = 100

    conflicts_found = aiotoggles.Toggle(False)
    k8s_mocked.sleep.return_value = 1  # as if interrupted by stream pressure

    caplog.set_level(0)
    assert conflicts_found.is_off()
    await process_peering_event(
        raw_event=event,
        conflicts_found=conflicts_found,
        autoclean=False,
        namespace=peering_namespace,
        resource=peering_resource,
        identity='id',
        settings=settings,
    )
    assert conflicts_found.is_off()
    assert k8s_mocked.sleep.call_count == 1
    assert k8s_mocked.sleep.call_args[0][0] == []
    assert not k8s_mocked.patch.called
    assert_logs([], prohibited=[
        "Possibly conflicting operators",
        "Pausing all operators, including self",
        "Pausing operations in favour of",
        "Resuming operations after the pause",
    ])


@freezegun.freeze_time('2020-12-31T23:59:59.123456')
async def test_toggled_on_for_same_priority_peer_when_initially_off(
        k8s_mocked, caplog, assert_logs, settings,
        peering_resource, peering_namespace):

    event = bodies.RawEvent(
        type='ADDED',  # irrelevant
        object={
            'metadata': {'name': 'name', 'namespace': peering_namespace},  # for matching
            'status': {
                'higher-prio': {
                    'priority': 100,
                    'lifetime': 10,
                    'lastseen': '2020-12-31T23:59:59'
                },
            },
        })
    settings.peering.name = 'name'
    settings.peering.priority = 100

    conflicts_found = aiotoggles.Toggle(False)
    k8s_mocked.sleep.return_value = 1  # as if interrupted by stream pressure

    caplog.set_level(0)
    assert conflicts_found.is_off()
    await process_peering_event(
        raw_event=event,
        conflicts_found=conflicts_found,
        autoclean=False,
        namespace=peering_namespace,
        resource=peering_resource,
        identity='id',
        settings=settings,
    )
    assert conflicts_found.is_on()
    assert k8s_mocked.sleep.call_count == 1
    assert 9 < k8s_mocked.sleep.call_args[0][0][0] < 10
    assert not k8s_mocked.patch.called
    assert_logs([
        "Possibly conflicting operators",
        "Pausing all operators, including self",
    ], prohibited=[
        "Pausing operations in favour of",
        "Resuming operations after the pause",
    ])


@freezegun.freeze_time('2020-12-31T23:59:59.123456')
async def test_ignored_for_same_priority_peer_when_already_on(
        k8s_mocked, caplog, assert_logs, settings,
        peering_resource, peering_namespace):

    event = bodies.RawEvent(
        type='ADDED',  # irrelevant
        object={
            'metadata': {'name': 'name', 'namespace': peering_namespace},  # for matching
            'status': {
                'higher-prio': {
                    'priority': 100,
                    'lifetime': 10,
                    'lastseen': '2020-12-31T23:59:59'
                },
            },
        })
    settings.peering.name = 'name'
    settings.peering.priority = 100

    conflicts_found = aiotoggles.Toggle(True)
    k8s_mocked.sleep.return_value = 1  # as if interrupted by stream pressure

    caplog.set_level(0)
    assert conflicts_found.is_on()
    await process_peering_event(
        raw_event=event,
        conflicts_found=conflicts_found,
        autoclean=False,
        namespace=peering_namespace,
        resource=peering_resource,
        identity='id',
        settings=settings,
    )
    assert conflicts_found.is_on()
    assert k8s_mocked.sleep.call_count == 1
    assert 9 < k8s_mocked.sleep.call_args[0][0][0] < 10
    assert not k8s_mocked.patch.called
    assert_logs([
        "Possibly conflicting operators",
    ], prohibited=[
        "Pausing all operators, including self",
        "Pausing operations in favour of",
        "Resuming operations after the pause",
    ])


@freezegun.freeze_time('2020-12-31T23:59:59.123456')
@pytest.mark.parametrize('priority', [100, 101])
async def test_resumes_immediately_on_expiration_of_blocking_peers(
        k8s_mocked, caplog, assert_logs, settings, priority,
        peering_resource, peering_namespace):

    event = bodies.RawEvent(
        type='ADDED',  # irrelevant
        object={
            'metadata': {'name': 'name', 'namespace': peering_namespace},  # for matching
            'status': {
                'higher-prio': {
                    'priority': priority,
                    'lifetime': 10,
                    'lastseen': '2020-12-31T23:59:59'
                },
            },
        })
    settings.peering.name = 'name'
    settings.peering.priority = 100

    conflicts_found = aiotoggles.Toggle(True)
    k8s_mocked.sleep.return_value = None  # as if finished sleeping uninterrupted

    caplog.set_level(0)
    assert conflicts_found.is_on()
    await process_peering_event(
        raw_event=event,
        conflicts_found=conflicts_found,
        autoclean=False,
        namespace=peering_namespace,
        resource=peering_resource,
        identity='id',
        settings=settings,
    )
    assert conflicts_found.is_on()
    assert k8s_mocked.sleep.call_count == 1
    assert 9 < k8s_mocked.sleep.call_args[0][0][0] < 10
    assert k8s_mocked.patch.called



================================================
FILE: tests/peering/test_id_generation.py
================================================
import os

import freezegun
import pytest

from kopf._core.engines.peering import detect_own_id

SAME_GOOD = [
    ('some-host.example.com', 'other-host.example.com'),
    ('some-host', 'other-host'),
]

# Priorities: A (left) should be selected over B (right).
GOOD_BAD = [
    ('some-host.example.com', 'some-host'),
    ('some-host.example.com', '1.2.3.4'),
    ('some-host.example.com', '::1'),
    ('some-host.example.com', '1.0...0.ip6.arpa'),
    ('some-host.example.com', '4.3.2.1.in-addr.arpa'),
    ('some-host.example.com', '1.2.3.4'),
    ('some-host', '1.2.3.4'),
    ('some-host', '::1'),
    ('some-host', '1.0...0.ip6.arpa'),
    ('some-host', '4.3.2.1.in-addr.arpa'),
    ('some-host', '1.2.3.4'),
]


@pytest.fixture(autouse=True)
def _intercept_os_calls(mocker):
    mocker.patch('getpass.getuser', return_value='some-user')
    mocker.patch('socket.gethostname')
    mocker.patch('socket.gethostbyaddr')


@pytest.mark.parametrize('manual', [True, False])
def test_from_a_pod_id(mocker, manual):
    mocker.patch('socket.gethostname', return_value='some-host')
    mocker.patch('socket.gethostbyaddr', side_effect=lambda fqdn: (fqdn, [], []))
    mocker.patch.dict(os.environ, POD_ID='some-pod-1')
    own_id = detect_own_id(manual=manual)
    assert own_id == 'some-pod-1'


def test_suffixes_appended(mocker):
    mocker.patch('random.choices', return_value='random-str')
    mocker.patch('socket.gethostname', return_value='some-host')
    mocker.patch('socket.gethostbyaddr', side_effect=lambda fqdn: (fqdn, [], []))
    with freezegun.freeze_time('2020-12-31T23:59:59.123456'):
        own_id = detect_own_id(manual=False)
    assert own_id == 'some-user@some-host/20201231235959/random-str'


def test_suffixes_ignored(mocker):
    mocker.patch('socket.gethostname', return_value='some-host')
    mocker.patch('socket.gethostbyaddr', side_effect=lambda fqdn: (fqdn, [], []))
    own_id = detect_own_id(manual=True)
    assert own_id == 'some-user@some-host'


@pytest.mark.parametrize('good1, good2', SAME_GOOD)
def test_good_hostnames_over_good_aliases__symmetric(mocker, good1, good2):
    mocker.patch('socket.gethostname', return_value=good1)
    mocker.patch('socket.gethostbyaddr', side_effect=lambda fqdn: (fqdn, [good2], []))
    own_id = detect_own_id(manual=True)
    assert own_id == f'some-user@{good1}'

    mocker.patch('socket.gethostname', return_value=good2)
    mocker.patch('socket.gethostbyaddr', side_effect=lambda fqdn: (fqdn, [good1], []))
    own_id = detect_own_id(manual=True)
    assert own_id == f'some-user@{good2}'


@pytest.mark.parametrize('good1, good2', SAME_GOOD)
def test_good_aliases_over_good_addresses__symmetric(mocker, good1, good2):
    mocker.patch('socket.gethostname', return_value='localhost')
    mocker.patch('socket.gethostbyaddr', side_effect=lambda fqdn: (fqdn, [good1], [good2]))
    own_id = detect_own_id(manual=True)
    assert own_id == f'some-user@{good1}'

    mocker.patch('socket.gethostname', return_value='localhost')
    mocker.patch('socket.gethostbyaddr', side_effect=lambda fqdn: (fqdn, [good2], [good1]))
    own_id = detect_own_id(manual=True)
    assert own_id == f'some-user@{good2}'


@pytest.mark.parametrize('good, bad', GOOD_BAD)
def test_good_aliases_over_bad_hostnames(mocker, good, bad):
    mocker.patch('socket.gethostname', return_value=bad)
    mocker.patch('socket.gethostbyaddr', side_effect=lambda fqdn: (fqdn, [good], []))
    own_id = detect_own_id(manual=True)
    assert own_id == f'some-user@{good}'


@pytest.mark.parametrize('good, bad', GOOD_BAD)
def test_good_addresses_over_bad_aliases(mocker, good, bad):
    mocker.patch('socket.gethostname', return_value='localhost')
    mocker.patch('socket.gethostbyaddr', side_effect=lambda fqdn: (fqdn, [bad], [good]))
    own_id = detect_own_id(manual=True)
    assert own_id == f'some-user@{good}'


@pytest.mark.parametrize('fqdn', [
    'my-host',
    'my-host.local',
    'my-host.localdomain',
    'my-host.local.localdomain',
    'my-host.localdomain.local',
])
def test_useless_suffixes_removed(mocker, fqdn):
    mocker.patch('socket.gethostname', return_value=fqdn)
    mocker.patch('socket.gethostbyaddr', side_effect=lambda fqdn: (fqdn, [], []))
    own_id = detect_own_id(manual=True)
    assert own_id == 'some-user@my-host'



================================================
FILE: tests/peering/test_keepalive.py
================================================
from itertools import chain, repeat
from unittest import mock

import pytest

from kopf._core.engines.peering import keepalive


class StopInfiniteCycleException(Exception):
    pass


async def test_background_task_runs(mocker, settings, namespaced_peering_resource):
    touch_mock = mocker.patch('kopf._core.engines.peering.touch')

    sleep_mock = mocker.patch('asyncio.sleep')
    # restore the default behavior after exhausting test values.
    # pytest-aiohttp calls asyncio.sleep during teardown, before the mock is removed.
    sleep_mock.side_effect = chain([None, None, StopInfiniteCycleException], repeat(mock.DEFAULT))

    randint_mock = mocker.patch('random.randint')
    randint_mock.side_effect = [7, 5, 9]

    settings.peering.lifetime = 33
    with pytest.raises(StopInfiniteCycleException):
        await keepalive(settings=settings, identity='id',
                        resource=namespaced_peering_resource, namespace='namespace')

    assert randint_mock.call_count == 3  # only to be sure that we test the right thing
    assert sleep_mock.call_count == 3
    assert sleep_mock.call_args_list[0][0][0] == 33 - 7
    assert sleep_mock.call_args_list[1][0][0] == 33 - 5
    assert sleep_mock.call_args_list[2][0][0] == 33 - 9

    assert touch_mock.call_count == 4  # 3 updates + 1 clean-up



================================================
FILE: tests/peering/test_peer_patching.py
================================================
import aiohttp.web
import freezegun
import pytest

from kopf._core.engines.peering import Peer, clean, touch


@pytest.mark.parametrize('lastseen', [
    pytest.param('2020-01-01T00:00:00', id='when-dead'),
    pytest.param('2020-12-31T23:59:59', id='when-alive'),
])
@freezegun.freeze_time('2020-12-31T23:59:59.123456')
async def test_cleaning_peers_purges_them(
        hostname, aresponses, resp_mocker, settings, lastseen,
        peering_resource, peering_namespace):

    settings.peering.name = 'name0'
    patch_mock = resp_mocker(return_value=aiohttp.web.json_response({}))
    url = peering_resource.get_url(name='name0', namespace=peering_namespace)
    aresponses.add(hostname, url, 'patch', patch_mock)

    peer = Peer(identity='id1', lastseen=lastseen)
    await clean(peers=[peer], resource=peering_resource, settings=settings,
                namespace=peering_namespace)

    assert patch_mock.called
    patch = await patch_mock.call_args_list[0][0][0].json()
    assert set(patch['status']) == {'id1'}
    assert patch['status']['id1'] is None


@freezegun.freeze_time('2020-12-31T23:59:59.123456')
async def test_touching_a_peer_stores_it(
        hostname, aresponses, resp_mocker, settings,
        peering_resource, peering_namespace):

    settings.peering.name = 'name0'
    patch_mock = resp_mocker(return_value=aiohttp.web.json_response({}))
    url = peering_resource.get_url(name='name0', namespace=peering_namespace)
    aresponses.add(hostname, url, 'patch', patch_mock)

    await touch(identity='id1', resource=peering_resource, settings=settings,
                namespace=peering_namespace)

    assert patch_mock.called
    patch = await patch_mock.call_args_list[0][0][0].json()
    assert set(patch['status']) == {'id1'}
    assert patch['status']['id1']['priority'] == 0
    assert patch['status']['id1']['lastseen'] == '2020-12-31T23:59:59.123456+00:00'
    assert patch['status']['id1']['lifetime'] == 60


@freezegun.freeze_time('2020-12-31T23:59:59.123456')
async def test_expiring_a_peer_purges_it(
        hostname, aresponses, resp_mocker, settings,
        peering_resource, peering_namespace):

    settings.peering.name = 'name0'
    patch_mock = resp_mocker(return_value=aiohttp.web.json_response({}))
    url = peering_resource.get_url(name='name0', namespace=peering_namespace)
    aresponses.add(hostname, url, 'patch', patch_mock)

    await touch(identity='id1', resource=peering_resource, settings=settings,
                namespace=peering_namespace, lifetime=0)

    assert patch_mock.called
    patch = await patch_mock.call_args_list[0][0][0].json()
    assert set(patch['status']) == {'id1'}
    assert patch['status']['id1'] is None


@freezegun.freeze_time('2020-12-31T23:59:59.123456')
async def test_logs_are_skipped_in_stealth_mode(
        hostname, aresponses, resp_mocker, settings, assert_logs, caplog,
        peering_resource, peering_namespace):

    caplog.set_level(0)
    settings.peering.stealth = True
    settings.peering.name = 'name0'
    patch_mock = resp_mocker(return_value=aiohttp.web.json_response({}))
    url = peering_resource.get_url(name='name0', namespace=peering_namespace)
    aresponses.add(hostname, url, 'patch', patch_mock)

    await touch(identity='id1', resource=peering_resource, settings=settings,
                namespace=peering_namespace)

    assert_logs([], prohibited=[
        "Keep-alive in",
    ])


async def test_logs_are_logged_in_exposed_mode(
        hostname, aresponses, resp_mocker, settings, assert_logs, caplog,
        peering_resource, peering_namespace):

    caplog.set_level(0)
    settings.peering.stealth = False
    settings.peering.name = 'name0'
    patch_mock = resp_mocker(return_value=aiohttp.web.json_response({}))
    url = peering_resource.get_url(name='name0', namespace=peering_namespace)
    aresponses.add(hostname, url, 'patch', patch_mock)

    await touch(identity='id1', resource=peering_resource, settings=settings,
                namespace=peering_namespace)

    assert_logs([
        r"Keep-alive in 'name0' (in 'ns'|cluster-wide): ok",
    ])


@pytest.mark.parametrize('stealth', [True, False], ids=['stealth', 'exposed'])
async def test_logs_are_logged_when_absent(
        hostname, aresponses, resp_mocker, stealth, settings, assert_logs, caplog,
        peering_resource, peering_namespace):

    caplog.set_level(0)
    settings.peering.stealth = stealth
    settings.peering.name = 'name0'
    patch_mock = resp_mocker(return_value=aresponses.Response(status=404, reason='oops'))
    url = peering_resource.get_url(name='name0', namespace=peering_namespace)
    aresponses.add(hostname, url, 'patch', patch_mock)

    await touch(identity='id1', resource=peering_resource, settings=settings,
                namespace=peering_namespace)

    assert_logs([
        r"Keep-alive in 'name0' (in 'ns'|cluster-wide): not found",
    ])



================================================
FILE: tests/peering/test_peers.py
================================================
import datetime

import freezegun
import iso8601

from kopf._core.engines.peering import Peer


@freezegun.freeze_time('2020-12-31T23:59:59.123456')
def test_defaults():
    peer = Peer(identity='id')
    assert peer.identity == 'id'
    assert peer.lifetime == datetime.timedelta(seconds=60)
    assert peer.lastseen == iso8601.parse_date('2020-12-31T23:59:59.123456')


@freezegun.freeze_time('2020-12-31T23:59:59.123456')
def test_repr():
    peer = Peer(identity='some-id')
    text = repr(peer)
    assert text == "<Peer some-id: priority=0, lifetime=60, lastseen='2020-12-31T23:59:59.123456+00:00'>"


@freezegun.freeze_time('2020-12-31T23:59:59.123456')
def test_priority_specified():
    peer = Peer(identity='id', priority=123)
    assert peer.priority == 123


@freezegun.freeze_time('2020-12-31T23:59:59.123456')
def test_priority_unspecified():
    peer = Peer(identity='id')
    assert peer.priority == 0


@freezegun.freeze_time('2020-12-31T23:59:59.123456')
def test_creation_with_lifetime_as_number():
    peer = Peer(identity='id', lifetime=123)
    assert peer.lifetime == datetime.timedelta(seconds=123)


@freezegun.freeze_time('2020-12-31T23:59:59.123456')
def test_creation_with_lifetime_unspecified():
    peer = Peer(identity='id')
    assert peer.lifetime == datetime.timedelta(seconds=60)


@freezegun.freeze_time('2020-12-31T23:59:59.123456')
def test_creation_with_lastseen_as_string():
    peer = Peer(identity='id', lastseen='2020-01-01T12:34:56.789123')
    assert peer.lastseen == iso8601.parse_date('2020-01-01T12:34:56.789123')


@freezegun.freeze_time('2020-12-31T23:59:59.123456')
def test_creation_with_lastseen_unspecified():
    peer = Peer(identity='id')
    assert peer.lastseen == iso8601.parse_date('2020-12-31T23:59:59.123456')


@freezegun.freeze_time('2020-12-31T23:59:59.123456')
def test_creation_as_alive():
    peer = Peer(
        identity='id',
        lifetime=10,
        lastseen='2020-12-31T23:59:50.123456',  # less than 10 seconds before "now"
    )
    assert peer.lifetime == datetime.timedelta(seconds=10)
    assert peer.lastseen == iso8601.parse_date('2020-12-31T23:59:50.123456')
    assert peer.deadline == iso8601.parse_date('2021-01-01T00:00:00.123456')
    assert peer.is_dead is False


@freezegun.freeze_time('2020-12-31T23:59:59.123456')
def test_creation_as_dead():
    peer = Peer(
        identity='id',
        lifetime=10,
        lastseen='2020-12-31T23:59:49.123456',  # 10 seconds before "now" sharp
    )
    assert peer.lifetime == datetime.timedelta(seconds=10)
    assert peer.lastseen == iso8601.parse_date('2020-12-31T23:59:49.123456')
    assert peer.deadline == iso8601.parse_date('2020-12-31T23:59:59.123456')
    assert peer.is_dead is True



================================================
FILE: tests/peering/test_resource_guessing.py
================================================
import pytest

from kopf._cogs.structs.references import CLUSTER_PEERINGS_K, CLUSTER_PEERINGS_Z, \
                                          NAMESPACED_PEERINGS_K, NAMESPACED_PEERINGS_Z
from kopf._core.engines.peering import guess_selectors


@pytest.mark.parametrize('namespaced, expected_selectors', [
    (False, [CLUSTER_PEERINGS_K, CLUSTER_PEERINGS_Z]),
    (True, [NAMESPACED_PEERINGS_K, NAMESPACED_PEERINGS_Z]),
])
@pytest.mark.parametrize('mandatory', [False, True])
def test_resource_when_not_standalone(settings, namespaced, mandatory, expected_selectors):
    settings.peering.standalone = False
    settings.peering.namespaced = namespaced
    settings.peering.mandatory = mandatory
    selectors = guess_selectors(settings=settings)
    assert selectors == expected_selectors


@pytest.mark.parametrize('namespaced', [False, True])
@pytest.mark.parametrize('mandatory', [False, True])
def test_resource_when_standalone(settings, namespaced, mandatory):
    settings.peering.standalone = True
    settings.peering.namespaced = namespaced
    settings.peering.mandatory = mandatory
    selectors = guess_selectors(settings=settings)
    assert not selectors



================================================
FILE: tests/persistence/test_annotations_hashing.py
================================================
import json

import pytest

from kopf._cogs.configs.conventions import StorageKeyFormingConvention
from kopf._cogs.configs.diffbase import AnnotationsDiffBaseStorage
from kopf._cogs.configs.progress import AnnotationsProgressStorage, \
                                        ProgressRecord, SmartProgressStorage
from kopf._cogs.structs.bodies import Body
from kopf._cogs.structs.ids import HandlerId
from kopf._cogs.structs.patches import Patch

ANNOTATIONS_POPULATING_STORAGES = [
    AnnotationsProgressStorage,
    SmartProgressStorage,
]
STORAGE_KEY_FORMING_CLASSES = [
    StorageKeyFormingConvention,
    AnnotationsProgressStorage,
    AnnotationsDiffBaseStorage,
]

CONTENT_DATA = ProgressRecord(
    started='2020-01-01T00:00:00',
    stopped='2020-12-31T23:59:59',
    delayed='3000-01-01T00:00:00',
    retries=0,
    success=False,
    failure=False,
    message=None,
)

CONTENT_JSON = json.dumps(CONTENT_DATA, separators=(',', ':'))

COMMON_KEYS = [
    # For character replacements (only those that happen in our own ids, not all of them).
    ['my-operator.example.com', 'a_b.c-d/e', 'my-operator.example.com/a_b.c-d.e'],
]

V1_KEYS = [
    # For V1 length cutting. Hint: the prefix length is 23, the remaining space is 63 - 23 - 1 = 39.
    # The suffix itself (if appended) takes 9, so it is 30 left. The same math for no prefix.
    ['my-operator.example.com', 'x', 'my-operator.example.com/x'],
    ['my-operator.example.com', 'x' * 39, 'my-operator.example.com/' + 'x' * 39],
    ['my-operator.example.com', 'x' * 40, 'my-operator.example.com/' + 'x' * 30 + 'xx-tEokcg'],
    ['my-operator.example.com', 'y' * 40, 'my-operator.example.com/' + 'y' * 30 + 'yy-VZlvhw'],
    ['my-operator.example.com', 'z' * 40, 'my-operator.example.com/' + 'z' * 30 + 'zz-LlPQyA'],

    # For special chars in base64 encoding ("+" and "/"), which are not compatible with K8s.
    # The numbers are found empirically so that both "/" and "+" are found in the base64'ed digest.
    ['my-operator.example.com', 'fn' * 323, 'my-operator.example.com/' + 'fn' * 15 + 'fn-Az-r.g'],
]

V2_KEYS = [
    # For V2 length cutting: 63 for the name part only, not the whole annotation.
    # The suffix itself (if appended) takes 9, so it is 63-9=54 left.
    ['my-operator.example.com', 'x', 'my-operator.example.com/x'],
    ['my-operator.example.com', 'x' * 63, 'my-operator.example.com/' + 'x' * 63],
    ['my-operator.example.com', 'x' * 64, 'my-operator.example.com/' + 'x' * 54 + 'xx-SItAqA'],
    ['my-operator.example.com', 'y' * 64, 'my-operator.example.com/' + 'y' * 54 + 'yy-0d251g'],
    ['my-operator.example.com', 'z' * 64, 'my-operator.example.com/' + 'z' * 54 + 'zz-E7wvIA'],

    # For special chars in base64 encoding ("+" and "/"), which are not compatible with K8s.
    # The numbers are found empirically so that both "/" and "+" are found in the base64'ed digest.
    ['my-operator.example.com', 'fn' * 323, 'my-operator.example.com/' + 'fn' * 27 + 'fn-Az-r.g'],
]


@pytest.mark.parametrize('cls', STORAGE_KEY_FORMING_CLASSES)
def test_keys_for_all_versions(cls):
    storage = cls(v1=True, prefix='kopf.zalando.org')
    v1_key = storage.make_v1_key('.' * 64)
    v2_key = storage.make_v2_key('.' * 64)
    assert v1_key != v2_key  # prerequisite
    keys = storage.make_keys('.' * 64)
    assert len(list(keys)) == 2
    assert v1_key in keys
    assert v2_key in keys


@pytest.mark.parametrize('cls', STORAGE_KEY_FORMING_CLASSES)
def test_keys_deduplication(cls):
    storage = cls(v1=True, prefix='kopf.zalando.org')
    v1_key = storage.make_v1_key('...')
    v2_key = storage.make_v2_key('...')
    assert v1_key == v2_key  # prerequisite
    keys = storage.make_keys('...')
    assert len(list(keys)) == 1
    assert v1_key in keys
    assert v2_key in keys


@pytest.mark.parametrize('kind, owners, expected', [
    pytest.param('ReplicaSet', [{'kind': 'Deployment'}], 'kopf.dev/xyz-ofDRS', id='DRS'),
    pytest.param('ReplicaSet', [{'kind': 'OtherOwner'}], 'kopf.dev/xyz', id='not-deployment'),
    pytest.param('OtherKind', [{'kind': 'Deployment'}], 'kopf.dev/xyz', id='not-replicaset'),
])
@pytest.mark.parametrize('cls', STORAGE_KEY_FORMING_CLASSES)
def test_keys_of_replicaset_owned_by_deployment(cls, kind, owners, expected):
    storage = cls(v1=True, prefix='kopf.dev')
    body = Body({'kind': kind, 'metadata': {'ownerReferences': owners}})
    keys = storage.make_keys('xyz', body=body)
    assert set(keys) == {expected}


@pytest.mark.parametrize('prefix, provided_key, expected_key', COMMON_KEYS + V1_KEYS)
@pytest.mark.parametrize('cls', STORAGE_KEY_FORMING_CLASSES)
def test_key_hashing_v1(cls, prefix, provided_key, expected_key):
    storage = cls(v1=True, prefix=prefix)
    returned_key = storage.make_v1_key(provided_key)
    assert returned_key == expected_key


@pytest.mark.parametrize('prefix, provided_key, expected_key', COMMON_KEYS + V2_KEYS)
@pytest.mark.parametrize('cls', STORAGE_KEY_FORMING_CLASSES)
def test_key_hashing_v2(cls, prefix, provided_key, expected_key):
    storage = cls(v1=True, prefix=prefix)
    returned_key = storage.make_v2_key(provided_key)
    assert returned_key == expected_key


@pytest.mark.parametrize('prefix, provided_key, expected_key', COMMON_KEYS + V1_KEYS + V2_KEYS)
@pytest.mark.parametrize('cls', ANNOTATIONS_POPULATING_STORAGES)
def test_keys_hashed_on_fetching(cls, prefix, provided_key, expected_key):
    storage = cls(prefix=prefix)
    body = Body({'metadata': {'annotations': {expected_key: CONTENT_JSON}}})
    record = storage.fetch(body=body, key=HandlerId(provided_key))
    assert record is not None
    assert record == CONTENT_DATA


@pytest.mark.parametrize('prefix, provided_key, expected_key', COMMON_KEYS + V1_KEYS + V2_KEYS)
@pytest.mark.parametrize('cls', ANNOTATIONS_POPULATING_STORAGES)
def test_keys_normalized_on_storing(cls, prefix, provided_key, expected_key):
    storage = cls(prefix=prefix)
    patch = Patch()
    body = Body({'metadata': {'annotations': {expected_key: 'null'}}})
    storage.store(body=body, patch=patch, key=HandlerId(provided_key), record=CONTENT_DATA)
    assert expected_key in patch.metadata.annotations


@pytest.mark.parametrize('prefix, provided_key, expected_key', COMMON_KEYS + V1_KEYS + V2_KEYS)
@pytest.mark.parametrize('cls', ANNOTATIONS_POPULATING_STORAGES)
def test_keys_normalized_on_purging(cls, prefix, provided_key, expected_key):
    storage = cls(prefix=prefix)
    patch = Patch()
    body = Body({'metadata': {'annotations': {expected_key: 'null'}}})
    storage.purge(body=body, patch=patch, key=HandlerId(provided_key))
    assert expected_key in patch.metadata.annotations


@pytest.mark.parametrize('prefix, provided_key, expected_key', COMMON_KEYS + V1_KEYS + V2_KEYS)
@pytest.mark.parametrize('cls', ANNOTATIONS_POPULATING_STORAGES)
def test_keys_normalized_on_touching(cls, prefix, provided_key, expected_key):
    storage = cls(prefix=prefix, touch_key=provided_key)
    patch = Patch()
    body = Body({})
    storage.touch(body=body, patch=patch, value='irrelevant')
    assert expected_key in patch.metadata.annotations


@pytest.mark.parametrize('cls', ANNOTATIONS_POPULATING_STORAGES + STORAGE_KEY_FORMING_CLASSES)
def test_warning_on_long_prefix(cls):
    with pytest.warns(UserWarning, match=r"The annotations prefix is too long"):
        cls(v1=True, prefix='x' * (253 - 63))



================================================
FILE: tests/persistence/test_essences.py
================================================
import pytest

from kopf._cogs.configs.diffbase import AnnotationsDiffBaseStorage, \
                                        DiffBaseStorage, StatusDiffBaseStorage
from kopf._cogs.structs.bodies import Body

ALL_STORAGES = [AnnotationsDiffBaseStorage, StatusDiffBaseStorage]


@pytest.mark.parametrize('cls', ALL_STORAGES)
def test_get_essence_removes_resource_references(
        cls: type[DiffBaseStorage],
):
    body = Body({'apiVersion': 'group/version', 'kind': 'Kind'})
    storage = cls()
    essence = storage.build(body=body)
    assert essence == {}


@pytest.mark.parametrize('field', [
    'uid',
    'name',
    'namespace',
    'selfLink',
    'generation',
    'finalizers',
    'resourceVersion',
    'creationTimestamp',
    'deletionTimestamp',
    'any-unexpected-field',
])
@pytest.mark.parametrize('cls', ALL_STORAGES)
def test_get_essence_removes_system_fields_and_cleans_parents(
        field: str,
        cls: type[DiffBaseStorage],
):
    body = Body({'metadata': {field: 'x'}})
    storage = cls()
    essence = storage.build(body=body)
    assert essence == {}


@pytest.mark.parametrize('field', [
    'uid',
    'name',
    'namespace',
    'selfLink',
    'generation',
    'finalizers',
    'resourceVersion',
    'creationTimestamp',
    'deletionTimestamp',
    'any-unexpected-field',
])
@pytest.mark.parametrize('cls', ALL_STORAGES)
def test_get_essence_removes_system_fields_but_keeps_extra_fields(
        field: str,
        cls: type[DiffBaseStorage],
):
    body = Body({'metadata': {field: 'x', 'other': 'y'}})
    storage = cls()
    essence = storage.build(body=body, extra_fields=['metadata.other'])
    assert essence == {'metadata': {'other': 'y'}}


@pytest.mark.parametrize('annotation', [
    pytest.param('kubectl.kubernetes.io/last-applied-configuration', id='kubectl'),
])
@pytest.mark.parametrize('cls', ALL_STORAGES)
def test_get_essence_removes_garbage_annotations_and_cleans_parents(
        annotation: str,
        cls: type[DiffBaseStorage],
):
    body = Body({'metadata': {'annotations': {annotation: 'x'}}})
    storage = cls()
    essence = storage.build(body=body)
    assert essence == {}


@pytest.mark.parametrize('annotation', [
    pytest.param('kubectl.kubernetes.io/last-applied-configuration', id='kubectl'),
])
@pytest.mark.parametrize('cls', ALL_STORAGES)
def test_get_essence_removes_garbage_annotations_but_keeps_others(
        annotation: str,
        cls: type[DiffBaseStorage],
):
    body = Body({'metadata': {'annotations': {annotation: 'x', 'other': 'y'}}})
    storage = cls()
    essence = storage.build(body=body)
    assert essence == {'metadata': {'annotations': {'other': 'y'}}}


@pytest.mark.parametrize('prefix', [
    'kopf-domain.tld',
    'kopf.domain.tld',
    'domain.tld.kopf',
    'domain.tld-kopf',
    'domain.kopf.tld',
    'domain.kopf-tld',
    'domain-kopf.tld',
    'domain-kopf-tld',
])
@pytest.mark.parametrize('cls', ALL_STORAGES)
def test_get_essence_keeps_annotations_mentioning_kopf_but_not_from_other_operators(
        prefix: str,
        cls: type[DiffBaseStorage],
):
    annotation = f'{prefix}/to-be-removed'
    body = Body({'metadata': {'annotations': {annotation: 'x'}}})
    storage = cls()
    essence = storage.build(body=body)
    assert essence == {'metadata': {'annotations': {annotation: 'x'}}}


@pytest.mark.parametrize('prefix', [
    'kopf.zalando.org',
    'sub.kopf.zalando.org',
    'sub.sub.kopf.zalando.org',
])
@pytest.mark.parametrize('cls', ALL_STORAGES)
def test_get_essence_removes_other_operators_annotations_by_domain(
        prefix: str,
        cls: type[DiffBaseStorage],
):
    annotation = f'{prefix}/to-be-removed'
    body = Body({'metadata': {'annotations': {annotation: 'x', 'other': 'y'}}})
    storage = cls()
    essence = storage.build(body=body)
    assert essence == {'metadata': {'annotations': {'other': 'y'}}}


@pytest.mark.parametrize('prefix', [
    'domain.tld',
])
@pytest.mark.parametrize('cls', ALL_STORAGES)
def test_get_essence_removes_other_operators_annotations_by_marker(
        prefix: str,
        cls: type[DiffBaseStorage],
):
    marker = f'{prefix}/kopf-managed'
    annotation = f'{prefix}/to-be-removed'
    body = Body({'metadata': {'annotations': {annotation: 'x', marker: 'yes', 'other': 'y'}}})
    storage = cls()
    essence = storage.build(body=body)
    assert essence == {'metadata': {'annotations': {'other': 'y'}}}


@pytest.mark.parametrize('cls', ALL_STORAGES)
def test_get_essence_removes_status_and_cleans_parents(
        cls: type[DiffBaseStorage],
):
    body = Body({'status': {'kopf': {'progress': 'x', 'anything': 'y'}, 'other': 'z'}})
    storage = cls()
    essence = storage.build(body=body)
    assert essence == {}


@pytest.mark.parametrize('cls', ALL_STORAGES)
def test_get_essence_removes_status_but_keeps_extra_fields(
        cls: type[DiffBaseStorage],
):
    body = Body({'status': {'kopf': {'progress': 'x', 'anything': 'y'}, 'other': 'z'}})
    storage = cls()
    essence = storage.build(body=body, extra_fields=['status.other'])
    assert essence == {'status': {'other': 'z'}}


@pytest.mark.parametrize('cls', ALL_STORAGES)
def test_get_essence_clones_body(
        cls: type[DiffBaseStorage],
):
    body = Body({'spec': {'depth': {'field': 'x'}}})
    storage = cls()
    essence = storage.build(body=body)
    body['spec']['depth']['field'] = 'y'
    assert essence is not body
    assert essence['spec'] is not body['spec']
    assert essence['spec']['depth'] is not body['spec']['depth']
    assert essence['spec']['depth']['field'] == 'x'



================================================
FILE: tests/persistence/test_outcomes.py
================================================
from kopf._core.actions.execution import Outcome, Result


def test_creation_for_ignored_handlers():
    outcome = Outcome(final=True)
    assert outcome.final
    assert outcome.delay is None
    assert outcome.result is None
    assert outcome.exception is None
    assert not outcome.subrefs


def test_creation_for_results():
    result = Result(object())
    outcome = Outcome(final=True, result=result)
    assert outcome.final
    assert outcome.delay is None
    assert outcome.result is result
    assert outcome.exception is None
    assert not outcome.subrefs


def test_creation_for_permanent_errors():
    error = Exception()
    outcome = Outcome(final=True, exception=error)
    assert outcome.final
    assert outcome.delay is None
    assert outcome.result is None
    assert outcome.exception is error
    assert not outcome.subrefs


def test_creation_for_temporary_errors():
    error = Exception()
    outcome = Outcome(final=False, exception=error, delay=123)
    assert not outcome.final
    assert outcome.delay == 123
    assert outcome.result is None
    assert outcome.exception is error
    assert not outcome.subrefs


def test_creation_with_subrefs():
    outcome = Outcome(final=True, subrefs=['sub1', 'sub2'])
    assert outcome.subrefs == ['sub1', 'sub2']



================================================
FILE: tests/persistence/test_states.py
================================================
import datetime
from unittest.mock import Mock

import freezegun
import iso8601
import pytest

from kopf._cogs.configs.progress import SmartProgressStorage, StatusProgressStorage
from kopf._cogs.structs.bodies import Body
from kopf._cogs.structs.patches import Patch
from kopf._core.actions.execution import Outcome
from kopf._core.actions.progression import State, StateCounters, deliver_results
from kopf._core.intents.causes import HANDLER_REASONS, Reason

# Timestamps: time zero (0), before (B), after (A), and time zero+1s (1).
TSB_ISO = '2020-12-31T23:59:59.000000+00:00'
TS0_ISO = '2020-12-31T23:59:59.123456+00:00'
TS1_ISO = '2021-01-01T00:00:00.123456+00:00'
TSA_ISO = '2020-12-31T23:59:59.999999+00:00'
TSB = iso8601.parse_date(TSB_ISO)
TS0 = iso8601.parse_date(TS0_ISO)
TS1 = iso8601.parse_date(TS1_ISO)
TSA = iso8601.parse_date(TSA_ISO)
ZERO_DELTA = datetime.timedelta(seconds=0)


# Use only the status-populating storages, to keep the tests with their original assertions.
# The goal is to test the states, not the storages. The storages are tested in test_storages.py.
@pytest.fixture(params=[StatusProgressStorage, SmartProgressStorage])
def storage(request):
    return request.param()


@pytest.fixture()
def handler():
    return Mock(id='some-id', spec_set=['id'])


#
# State creation from scratch and from storage; basic properties checks:
#


def test_created_empty_from_scratch():
    state = State.from_scratch()
    assert len(state) == 0
    assert state.purpose is None
    assert state.done == True
    assert state.delay is None
    assert state.delays == []
    assert state.counts == StateCounters(success=0, failure=0, running=0)
    assert state.extras == {}


@pytest.mark.parametrize('body', [
    ({}),
    ({'status': {}}),
    ({'status': {'kopf': {}}}),
    ({'status': {'kopf': {'progress': {}}}}),
])
def test_created_empty_from_empty_storage_with_handlers(storage, handler, body):
    state = State.from_storage(body=Body(body), handlers=[handler], storage=storage)
    assert len(state) == 0
    assert state.purpose is None
    assert state.done == True
    assert state.delay is None
    assert state.delays == []
    assert state.counts == StateCounters(success=0, failure=0, running=0)
    assert state.extras == {}


@pytest.mark.parametrize('body', [
    ({'status': {'kopf': {'progress': {'some-id': {}}}}}),
    ({'status': {'kopf': {'progress': {'some-id': {'success': True}}}}}),
    ({'status': {'kopf': {'progress': {'some-id': {'failure': True}}}}}),
])
def test_created_empty_from_filled_storage_without_handlers(storage, handler, body):
    state = State.from_storage(body=Body(body), handlers=[], storage=storage)
    assert len(state) == 0
    assert state.purpose is None
    assert state.done == True
    assert state.delay is None
    assert state.delays == []
    assert state.counts == StateCounters(success=0, failure=0, running=0)
    assert state.extras == {}


#
# Active/passive states.
#


def test_created_from_storage_as_passive(storage, handler):
    body = {'status': {'kopf': {'progress': {'some-id': {}}}}}
    state = State.from_storage(body=Body(body), handlers=[handler], storage=storage)
    assert len(state) == 1
    assert state['some-id'].active is False


def test_created_from_handlers_as_active(storage, handler):
    state = State.from_scratch()
    state = state.with_handlers([handler])
    assert len(state) == 1
    assert state['some-id'].active is True


def test_switched_from_passive_to_active(storage, handler):
    body = {'status': {'kopf': {'progress': {'some-id': {'purpose': None}}}}}
    state = State.from_storage(body=Body(body), handlers=[handler], storage=storage)
    state = state.with_handlers([handler])
    assert len(state) == 1
    assert state['some-id'].active is True


def test_passed_through_with_outcomes_when_passive(storage, handler):
    body = {'status': {'kopf': {'progress': {'some-id': {'purpose': None}}}}}
    state = State.from_storage(body=Body(body), handlers=[handler], storage=storage)
    state = state.with_outcomes({'some-id': Outcome(final=True)})
    assert len(state) == 1
    assert state['some-id'].active is False


def test_passed_through_with_outcomes_when_active(storage, handler):
    state = State.from_scratch()
    state = state.with_handlers([handler])
    state = state.with_outcomes({'some-id': Outcome(final=True)})
    assert len(state) == 1
    assert state['some-id'].active is True


def test_passive_states_are_not_used_in_done_calculation(storage, handler):
    body = {'status': {'kopf': {'progress': {'some-id': {}}}}}
    state = State.from_storage(body=Body(body), handlers=[handler], storage=storage)
    assert len(state) == 1
    assert state.done is True  # because the unfinished handler state is ignored


def test_active_states_are_used_in_done_calculation(storage, handler):
    body = {'status': {'kopf': {'progress': {'some-id': {}}}}}
    state = State.from_storage(body=Body(body), handlers=[handler], storage=storage)
    state = state.with_handlers([handler])
    assert len(state) == 1
    assert state.done is False


@freezegun.freeze_time(TS0)
def test_passive_states_are_not_used_in_delays_calculation(storage, handler):
    body = {'status': {'kopf': {'progress': {'some-id': {'delayed': TS1_ISO}}}}}
    state = State.from_storage(body=Body(body), handlers=[handler], storage=storage)
    assert len(state) == 1
    assert state.delays == []


@freezegun.freeze_time(TS0)
def test_active_states_are_used_in_delays_calculation(storage, handler):
    body = {'status': {'kopf': {'progress': {'some-id': {'delayed': TS1_ISO}}}}}
    state = State.from_storage(body=Body(body), handlers=[handler], storage=storage)
    state = state.with_handlers([handler])
    assert len(state) == 1
    assert state.delays == [1.0]


#
# Purpose propagation and re-purposing of the states (overall and per-handler):
#


def test_created_from_purposeless_storage(storage, handler):
    body = {'status': {'kopf': {'progress': {'some-id': {'purpose': None}}}}}
    state = State.from_storage(body=Body(body), handlers=[handler], storage=storage)
    assert len(state) == 1
    assert state.purpose is None
    assert state['some-id'].purpose is None


@pytest.mark.parametrize('reason', HANDLER_REASONS)
def test_created_from_purposeful_storage(storage, handler, reason):
    body = {'status': {'kopf': {'progress': {'some-id': {'purpose': reason.value}}}}}
    state = State.from_storage(body=Body(body), handlers=[handler], storage=storage)
    assert len(state) == 1
    assert state.purpose is None
    assert state['some-id'].purpose == reason


@pytest.mark.parametrize('reason', HANDLER_REASONS)
def test_enriched_with_handlers_keeps_the_original_purpose(handler, reason):
    state = State.from_scratch()
    state = state.with_purpose(reason)
    state = state.with_handlers([handler])
    assert state.purpose == reason


@pytest.mark.parametrize('reason', HANDLER_REASONS)
def test_enriched_with_outcomes_keeps_the_original_purpose(reason):
    state = State.from_scratch()
    state = state.with_purpose(reason)
    state = state.with_outcomes({})
    assert state.purpose == reason


@pytest.mark.parametrize('reason', HANDLER_REASONS)
def test_repurposed_before_handlers(handler, reason):
    state = State.from_scratch()
    state = state.with_purpose(reason).with_handlers([handler])
    assert len(state) == 1
    assert state.purpose == reason
    assert state['some-id'].purpose == reason


@pytest.mark.parametrize('reason', HANDLER_REASONS)
def test_repurposed_after_handlers(handler, reason):
    state = State.from_scratch()
    state = state.with_handlers([handler]).with_purpose(reason)
    assert len(state) == 1
    assert state.purpose == reason
    assert state['some-id'].purpose is None


@pytest.mark.parametrize('reason', HANDLER_REASONS)
def test_repurposed_with_handlers(handler, reason):
    state = State.from_scratch()
    state = state.with_handlers([handler]).with_purpose(reason, handlers=[handler])
    assert len(state) == 1
    assert state.purpose == reason
    assert state['some-id'].purpose == reason


@pytest.mark.parametrize('reason', HANDLER_REASONS)
def test_repurposed_not_affecting_the_existing_handlers_from_scratch(handler, reason):
    state = State.from_scratch()
    state = state.with_handlers([handler]).with_purpose(reason).with_handlers([handler])
    assert len(state) == 1
    assert state.purpose == reason
    assert state['some-id'].purpose is None


@pytest.mark.parametrize('reason', HANDLER_REASONS)
def test_repurposed_not_affecting_the_existing_handlers_from_storage(storage, handler, reason):
    body = {'status': {'kopf': {'progress': {'some-id': {'purpose': None}}}}}
    state = State.from_storage(body=Body(body), handlers=[handler], storage=storage)
    state = state.with_handlers([handler]).with_purpose(reason).with_handlers([handler])
    assert len(state) == 1
    assert state.purpose == reason
    assert state['some-id'].purpose is None


#
# Counts & extras calculation with different combinations of purposes:
#


@pytest.mark.parametrize('expected_extras, body', [
    # (success, failure, running)
    (StateCounters(0, 0, 1), {'status': {'kopf': {'progress': {'some-id': {}}}}}),
    (StateCounters(0, 1, 0), {'status': {'kopf': {'progress': {'some-id': {'failure': True}}}}}),
    (StateCounters(1, 0, 0), {'status': {'kopf': {'progress': {'some-id': {'success': True}}}}}),
])
@pytest.mark.parametrize('stored_reason, processed_reason', [
    # All combinations except for same-to-same (it is not an "extra" then).
    (a, b) for a in HANDLER_REASONS for b in HANDLER_REASONS if a != b
])
def test_with_handlers_irrelevant_to_the_purpose(
        storage, handler, body, expected_extras, stored_reason, processed_reason):
    body['status']['kopf']['progress']['some-id']['purpose'] = stored_reason.value
    state = State.from_storage(body=Body(body), handlers=[handler], storage=storage)
    state = state.with_purpose(processed_reason)
    assert len(state) == 1
    assert state.extras[stored_reason] == expected_extras
    assert state.counts == StateCounters(success=0, failure=0, running=0)
    assert state.done == True
    assert state.delays == []


@pytest.mark.parametrize('expected_counts, expected_done, expected_delays, body', [
    # (success, failure)
    (StateCounters(0, 0, 1), False, [0.0], {'status': {'kopf': {'progress': {'some-id': {}}}}}),
    (StateCounters(0, 1, 0), True, [], {'status': {'kopf': {'progress': {'some-id': {'failure': True}}}}}),
    (StateCounters(1, 0, 0), True, [], {'status': {'kopf': {'progress': {'some-id': {'success': True}}}}}),
])
@pytest.mark.parametrize('reason', HANDLER_REASONS)
def test_with_handlers_relevant_to_the_purpose(
        storage, handler, body, expected_counts, expected_done, expected_delays, reason):
    body['status']['kopf']['progress']['some-id']['purpose'] = reason.value
    state = State.from_storage(body=Body(body), handlers=[handler], storage=storage)
    state = state.with_purpose(reason).with_handlers([handler])
    assert len(state) == 1
    assert state.extras == {}
    assert state.counts == expected_counts
    assert state.done == expected_done
    assert state.delays == expected_delays


@pytest.mark.parametrize('expected_counts, expected_done, expected_delays, body', [
    (StateCounters(0, 0, 1), False, [1.0], {'status': {'kopf': {'progress': {'some-id': {}}}}}),
    (StateCounters(0, 1, 0), True, [], {'status': {'kopf': {'progress': {'some-id': {'failure': True}}}}}),
    (StateCounters(1, 0, 0), True, [], {'status': {'kopf': {'progress': {'some-id': {'success': True}}}}}),
])
@pytest.mark.parametrize('reason', HANDLER_REASONS)
@freezegun.freeze_time(TS0)
def test_with_handlers_relevant_to_the_purpose_and_delayed(
        storage, handler, body, expected_counts, expected_done, expected_delays, reason):
    body['status']['kopf']['progress']['some-id']['delayed'] = TS1_ISO
    body['status']['kopf']['progress']['some-id']['purpose'] = reason.value
    state = State.from_storage(body=Body(body), handlers=[handler], storage=storage)
    state = state.with_purpose(reason).with_handlers([handler])
    assert len(state) == 1
    assert state.extras == {}
    assert state.counts == expected_counts
    assert state.done == expected_done
    assert state.delays == expected_delays


@pytest.mark.parametrize('reason', [Reason.CREATE, Reason.UPDATE, Reason.RESUME])
@freezegun.freeze_time(TS0)
def test_issue_601_deletion_supersedes_other_processing(storage, reason):

    body = {'status': {'kopf': {'progress': {
        'fn1': {'purpose': reason.value, 'failure': True},
        'fn2': {'purpose': reason.value, 'success': True},
        'fn3': {'purpose': reason.value, 'delayed': TS1_ISO},
    }}}}
    create_handler1 = Mock(id='fn1', spec_set=['id'])
    create_handler2 = Mock(id='fn2', spec_set=['id'])
    create_handler3 = Mock(id='fn3', spec_set=['id'])
    delete_handler9 = Mock(id='delete_fn', spec_set=['id'])
    owned_handlers = [create_handler1, create_handler2, create_handler3, delete_handler9]
    cause_handlers = [delete_handler9]

    state = State.from_storage(body=Body(body), handlers=owned_handlers, storage=storage)
    state = state.with_purpose(Reason.DELETE)
    state = state.with_handlers(cause_handlers)

    assert len(state) == 4
    assert state.extras == {reason: StateCounters(success=1, failure=1, running=1)}
    assert state.counts == StateCounters(success=0, failure=0, running=1)
    assert state.done == False
    assert state.delays == [0.0]

    state = state.with_outcomes({'delete_fn': Outcome(final=True)})

    assert state.extras == {reason: StateCounters(success=1, failure=1, running=1)}
    assert state.counts == StateCounters(success=1, failure=0, running=0)
    assert state.done == True
    assert state.delays == []


#
# Handlers' time-based states: starting, running, retrying, finishing, etc.
#


@freezegun.freeze_time(TS0)
def test_started_from_scratch(storage, handler):
    patch = Patch()
    state = State.from_scratch()
    state = state.with_handlers([handler])
    state.store(body=Body({}), patch=patch, storage=storage)
    assert patch['status']['kopf']['progress']['some-id']['started'] == TS0_ISO


@pytest.mark.parametrize('expected, body', [
    (TS0_ISO, {'status': {'kopf': {'progress': {'some-id': {}}}}}),
    (TS0_ISO, {'status': {'kopf': {'progress': {'some-id': {'started': None}}}}}),
    (TS0_ISO, {'status': {'kopf': {'progress': {'some-id': {'started': TS0_ISO}}}}}),
    (TSB_ISO, {'status': {'kopf': {'progress': {'some-id': {'started': TSB_ISO}}}}}),
    (TSA_ISO, {'status': {'kopf': {'progress': {'some-id': {'started': TSA_ISO}}}}}),
])
@freezegun.freeze_time(TS0)
def test_started_from_storage(storage, handler, body, expected):
    patch = Patch()
    state = State.from_storage(body=Body(body), handlers=[handler], storage=storage)
    state.store(body=Body({}), patch=patch, storage=storage)
    assert patch['status']['kopf']['progress']['some-id']['started'] == expected


@pytest.mark.parametrize('expected, body', [
    (TS0_ISO, {'status': {'kopf': {'progress': {'some-id': {}}}}}),
    (TS0_ISO, {'status': {'kopf': {'progress': {'some-id': {'started': None}}}}}),
    (TS0_ISO, {'status': {'kopf': {'progress': {'some-id': {'started': TS0_ISO}}}}}),
    (TSB_ISO, {'status': {'kopf': {'progress': {'some-id': {'started': TSB_ISO}}}}}),
    (TSA_ISO, {'status': {'kopf': {'progress': {'some-id': {'started': TSA_ISO}}}}}),
])
def test_started_from_storage_is_preferred_over_from_scratch(storage, handler, body, expected):
    with freezegun.freeze_time(TS0):
        state = State.from_storage(body=Body(body), handlers=[handler], storage=storage)
    with freezegun.freeze_time(TS1):
        state = state.with_handlers([handler])
    patch = Patch()
    state.store(body=Body({}), patch=patch, storage=storage)
    assert patch['status']['kopf']['progress']['some-id']['started'] == expected


@pytest.mark.parametrize('expected, body', [
    (ZERO_DELTA, {}),
    (ZERO_DELTA, {'status': {}}),
    (ZERO_DELTA, {'status': {'kopf': {}}}),
    (ZERO_DELTA, {'status': {'kopf': {'progress': {}}}}),
    (ZERO_DELTA, {'status': {'kopf': {'progress': {'some-id': {}}}}}),
    (ZERO_DELTA, {'status': {'kopf': {'progress': {'some-id': {'started': None}}}}}),
    (ZERO_DELTA, {'status': {'kopf': {'progress': {'some-id': {'started': TS0_ISO}}}}}),
    (TS0 - TSB, {'status': {'kopf': {'progress': {'some-id': {'started': TSB_ISO}}}}}),
    (TS0 - TSA, {'status': {'kopf': {'progress': {'some-id': {'started': TSA_ISO}}}}}),
])
@freezegun.freeze_time(TS0)
def test_runtime(storage, handler, expected, body):
    state = State.from_storage(body=Body(body), handlers=[handler], storage=storage)
    state = state.with_handlers([handler])
    result = state[handler.id].runtime
    assert result == expected


@pytest.mark.parametrize('expected, body', [
    (False, {}),
    (False, {'status': {}}),
    (False, {'status': {'kopf': {}}}),
    (False, {'status': {'kopf': {'progress': {}}}}),
    (False, {'status': {'kopf': {'progress': {'some-id': {}}}}}),
    (False, {'status': {'kopf': {'progress': {'some-id': {'success': False}}}}}),
    (False, {'status': {'kopf': {'progress': {'some-id': {'failure': False}}}}}),
    (False, {'status': {'kopf': {'progress': {'some-id': {'success': None}}}}}),
    (False, {'status': {'kopf': {'progress': {'some-id': {'failure': None}}}}}),
    (True , {'status': {'kopf': {'progress': {'some-id': {'success': True}}}}}),
    (True , {'status': {'kopf': {'progress': {'some-id': {'failure': True}}}}}),
])
def test_finished_flag(storage, handler, expected, body):
    state = State.from_storage(body=Body(body), handlers=[handler], storage=storage)
    state = state.with_handlers([handler])
    result = state[handler.id].finished
    assert result == expected


@pytest.mark.parametrize('expected, body', [

    # Everything that is finished is not sleeping, no matter the sleep/awake field.
    (False, {'status': {'kopf': {'progress': {'some-id': {'success': True}}}}}),
    (False, {'status': {'kopf': {'progress': {'some-id': {'failure': True}}}}}),
    (False, {'status': {'kopf': {'progress': {'some-id': {'success': True, 'delayed': TS0_ISO}}}}}),
    (False, {'status': {'kopf': {'progress': {'some-id': {'failure': True, 'delayed': TS0_ISO}}}}}),

    # Everything with no sleep/awake field set is not sleeping either.
    (False, {'status': {'kopf': {'progress': {'some-id': {}}}}}),
    (False, {'status': {'kopf': {'progress': {'some-id': {'delayed': None}}}}}),
    (False, {'status': {'kopf': {'progress': {'some-id': {'success': None}}}}}),
    (False, {'status': {'kopf': {'progress': {'some-id': {'failure': None}}}}}),
    (False, {'status': {'kopf': {'progress': {'some-id': {'success': None, 'delayed': None}}}}}),
    (False, {'status': {'kopf': {'progress': {'some-id': {'failure': None, 'delayed': None}}}}}),

    # When not finished and has awake time, the output depends on the relation to "now".
    (False, {'status': {'kopf': {'progress': {'some-id': {'delayed': TS0_ISO}}}}}),
    (False, {'status': {'kopf': {'progress': {'some-id': {'delayed': TS0_ISO, 'success': None}}}}}),
    (False, {'status': {'kopf': {'progress': {'some-id': {'delayed': TS0_ISO, 'failure': None}}}}}),
    (False, {'status': {'kopf': {'progress': {'some-id': {'delayed': TSB_ISO}}}}}),
    (False, {'status': {'kopf': {'progress': {'some-id': {'delayed': TSB_ISO, 'success': None}}}}}),
    (False, {'status': {'kopf': {'progress': {'some-id': {'delayed': TSB_ISO, 'failure': None}}}}}),
    (True , {'status': {'kopf': {'progress': {'some-id': {'delayed': TSA_ISO}}}}}),
    (True , {'status': {'kopf': {'progress': {'some-id': {'delayed': TSA_ISO, 'success': None}}}}}),
    (True , {'status': {'kopf': {'progress': {'some-id': {'delayed': TSA_ISO, 'failure': None}}}}}),
])
@freezegun.freeze_time(TS0)
def test_sleeping_flag(storage, handler, expected, body):
    state = State.from_storage(body=Body(body), handlers=[handler], storage=storage)
    state = state.with_handlers([handler])
    result = state[handler.id].sleeping
    assert result == expected


@pytest.mark.parametrize('expected, body', [

    # Everything that is finished never awakens, no matter the sleep/awake field.
    (False, {'status': {'kopf': {'progress': {'some-id': {'success': True}}}}}),
    (False, {'status': {'kopf': {'progress': {'some-id': {'failure': True}}}}}),
    (False, {'status': {'kopf': {'progress': {'some-id': {'success': True, 'delayed': TS0_ISO}}}}}),
    (False, {'status': {'kopf': {'progress': {'some-id': {'failure': True, 'delayed': TS0_ISO}}}}}),

    # Everything with no sleep/awake field is not sleeping, thus by definition is awake.
    (True , {'status': {'kopf': {'progress': {'some-id': {}}}}}),
    (True , {'status': {'kopf': {'progress': {'some-id': {'delayed': None}}}}}),
    (True , {'status': {'kopf': {'progress': {'some-id': {'success': None}}}}}),
    (True , {'status': {'kopf': {'progress': {'some-id': {'failure': None}}}}}),
    (True , {'status': {'kopf': {'progress': {'some-id': {'success': None, 'delayed': None}}}}}),
    (True , {'status': {'kopf': {'progress': {'some-id': {'failure': None, 'delayed': None}}}}}),

    # When not finished and has awake time, the output depends on the relation to "now".
    (True , {'status': {'kopf': {'progress': {'some-id': {'delayed': TS0_ISO}}}}}),
    (True , {'status': {'kopf': {'progress': {'some-id': {'delayed': TS0_ISO, 'success': None}}}}}),
    (True , {'status': {'kopf': {'progress': {'some-id': {'delayed': TS0_ISO, 'failure': None}}}}}),
    (True , {'status': {'kopf': {'progress': {'some-id': {'delayed': TSB_ISO}}}}}),
    (True , {'status': {'kopf': {'progress': {'some-id': {'delayed': TSB_ISO, 'success': None}}}}}),
    (True , {'status': {'kopf': {'progress': {'some-id': {'delayed': TSB_ISO, 'failure': None}}}}}),
    (False, {'status': {'kopf': {'progress': {'some-id': {'delayed': TSA_ISO}}}}}),
    (False, {'status': {'kopf': {'progress': {'some-id': {'delayed': TSA_ISO, 'success': None}}}}}),
    (False, {'status': {'kopf': {'progress': {'some-id': {'delayed': TSA_ISO, 'failure': None}}}}}),
])
@freezegun.freeze_time(TS0)
def test_awakened_flag(storage, handler, expected, body):
    state = State.from_storage(body=Body(body), handlers=[handler], storage=storage)
    state = state.with_handlers([handler])
    result = state[handler.id].awakened
    assert result == expected


@pytest.mark.parametrize('expected, body', [
    (None, {}),
    (None, {'status': {}}),
    (None, {'status': {'kopf': {}}}),
    (None, {'status': {'kopf': {'progress': {}}}}),
    (None, {'status': {'kopf': {'progress': {'some-id': {}}}}}),
    (None, {'status': {'kopf': {'progress': {'some-id': {'delayed': None}}}}}),
    (TS0, {'status': {'kopf': {'progress': {'some-id': {'delayed': TS0_ISO}}}}}),
])
def test_awakening_time(storage, handler, expected, body):
    state = State.from_storage(body=Body(body), handlers=[handler], storage=storage)
    state = state.with_handlers([handler])
    result = state[handler.id].delayed
    assert result == expected


@pytest.mark.parametrize('expected, body', [
    (0, {}),
    (0, {'status': {}}),
    (0, {'status': {'kopf': {'progress': {}}}}),
    (0, {'status': {'kopf': {'progress': {'some-id': {}}}}}),
    (0, {'status': {'kopf': {'progress': {'some-id': {'retries': None}}}}}),
    (6, {'status': {'kopf': {'progress': {'some-id': {'retries': 6}}}}}),
])
def test_get_retry_count(storage, handler, expected, body):
    state = State.from_storage(body=Body(body), handlers=[handler], storage=storage)
    state = state.with_handlers([handler])
    result = state[handler.id].retries
    assert result == expected


@pytest.mark.parametrize('body, delay, expected', [
    ({}, None, None),
    ({}, 0, TS0_ISO),
    ({}, 1, TS1_ISO),
])
@freezegun.freeze_time(TS0)
def test_set_awake_time(storage, handler, expected, body, delay):
    patch = Patch()
    state = State.from_storage(body=Body(body), handlers=[handler], storage=storage)
    state = state.with_handlers([handler])
    state = state.with_outcomes(outcomes={handler.id: Outcome(final=False, delay=delay)})
    state.store(patch=patch, body=Body(body), storage=storage)
    assert patch['status']['kopf']['progress']['some-id'].get('delayed') == expected


@pytest.mark.parametrize('expected_retries, expected_delayed, delay, body', [
    (1, None, None, {}),
    (1, TS0_ISO, 0, {}),
    (1, TS1_ISO, 1, {}),

    (1, None, None, {'status': {'kopf': {'progress': {'some-id': {'retries': None}}}}}),
    (1, TS0_ISO, 0, {'status': {'kopf': {'progress': {'some-id': {'retries': None}}}}}),
    (1, TS1_ISO, 1, {'status': {'kopf': {'progress': {'some-id': {'retries': None}}}}}),

    (6, None, None, {'status': {'kopf': {'progress': {'some-id': {'retries': 5}}}}}),
    (6, TS0_ISO, 0, {'status': {'kopf': {'progress': {'some-id': {'retries': 5}}}}}),
    (6, TS1_ISO, 1, {'status': {'kopf': {'progress': {'some-id': {'retries': 5}}}}}),
])
@freezegun.freeze_time(TS0)
def test_set_retry_time(storage, handler, expected_retries, expected_delayed, body, delay):
    patch = Patch()
    state = State.from_storage(body=Body(body), handlers=[handler], storage=storage)
    state = state.with_handlers([handler])
    state = state.with_outcomes(outcomes={handler.id: Outcome(final=False, delay=delay)})
    state.store(patch=patch, body=Body(body), storage=storage)
    assert patch['status']['kopf']['progress']['some-id']['retries'] == expected_retries
    assert patch['status']['kopf']['progress']['some-id']['delayed'] == expected_delayed


#
# Sub-handlers ids persistence for later purging of them.
#


def test_subrefs_added_to_empty_state(storage, handler):
    body = {}
    patch = Patch()
    outcome_subrefs = ['sub2/b', 'sub2/a', 'sub2', 'sub1', 'sub3']
    expected_subrefs = ['sub1', 'sub2', 'sub2/a', 'sub2/b', 'sub3']
    outcome = Outcome(final=True, subrefs=outcome_subrefs)
    state = State.from_storage(body=Body(body), handlers=[handler], storage=storage)
    state = state.with_handlers([handler])
    state = state.with_outcomes(outcomes={handler.id: outcome})
    state.store(patch=patch, body=Body(body), storage=storage)
    assert patch['status']['kopf']['progress']['some-id']['subrefs'] == expected_subrefs


def test_subrefs_added_to_preexisting_subrefs(storage, handler):
    body = {'status': {'kopf': {'progress': {'some-id': {'subrefs': ['sub9/2', 'sub9/1']}}}}}
    patch = Patch()
    outcome_subrefs = ['sub2/b', 'sub2/a', 'sub2', 'sub1', 'sub3']
    expected_subrefs = ['sub1', 'sub2', 'sub2/a', 'sub2/b', 'sub3', 'sub9/1', 'sub9/2']
    outcome = Outcome(final=True, subrefs=outcome_subrefs)
    state = State.from_storage(body=Body(body), handlers=[handler], storage=storage)
    state = state.with_handlers([handler])
    state = state.with_outcomes(outcomes={handler.id: outcome})
    state.store(patch=patch, body=Body(body), storage=storage)
    assert patch['status']['kopf']['progress']['some-id']['subrefs'] == expected_subrefs


def test_subrefs_ignored_when_not_specified(storage, handler):
    body = {}
    patch = Patch()
    outcome = Outcome(final=True, subrefs=[])
    state = State.from_storage(body=Body(body), handlers=[handler], storage=storage)
    state = state.with_handlers([handler])
    state = state.with_outcomes(outcomes={handler.id: outcome})
    state.store(patch=patch, body=Body(body), storage=storage)
    assert patch['status']['kopf']['progress']['some-id']['subrefs'] is None


#
# Persisting outcomes: successes, failures, results, exceptions, etc.
#


@pytest.mark.parametrize('expected_retries, expected_stopped, body', [
    (1, TS0_ISO, {}),
    (6, TS0_ISO, {'status': {'kopf': {'progress': {'some-id': {'retries': 5}}}}}),
])
@freezegun.freeze_time(TS0)
def test_store_failure(storage, handler, expected_retries, expected_stopped, body):
    error = Exception('some-error')
    patch = Patch()
    state = State.from_storage(body=Body(body), handlers=[handler], storage=storage)
    state = state.with_handlers([handler])
    state = state.with_outcomes(outcomes={handler.id: Outcome(final=True, exception=error)})
    state.store(patch=patch, body=Body(body), storage=storage)
    assert patch['status']['kopf']['progress']['some-id']['success'] is False
    assert patch['status']['kopf']['progress']['some-id']['failure'] is True
    assert patch['status']['kopf']['progress']['some-id']['retries'] == expected_retries
    assert patch['status']['kopf']['progress']['some-id']['stopped'] == expected_stopped
    assert patch['status']['kopf']['progress']['some-id']['message'] == 'some-error'


@pytest.mark.parametrize('expected_retries, expected_stopped, body', [
    (1, TS0_ISO, {}),
    (6, TS0_ISO, {'status': {'kopf': {'progress': {'some-id': {'retries': 5}}}}}),
])
@freezegun.freeze_time(TS0)
def test_store_success(storage, handler, expected_retries, expected_stopped, body):
    patch = Patch()
    state = State.from_storage(body=Body(body), handlers=[handler], storage=storage)
    state = state.with_handlers([handler])
    state = state.with_outcomes(outcomes={handler.id: Outcome(final=True)})
    state.store(patch=patch, body=Body(body), storage=storage)
    assert patch['status']['kopf']['progress']['some-id']['success'] is True
    assert patch['status']['kopf']['progress']['some-id']['failure'] is False
    assert patch['status']['kopf']['progress']['some-id']['retries'] == expected_retries
    assert patch['status']['kopf']['progress']['some-id']['stopped'] == expected_stopped
    assert patch['status']['kopf']['progress']['some-id']['message'] is None


@pytest.mark.parametrize('result, expected_patch', [
    (None, {}),
    ('string', {'status': {'some-id': 'string'}}),
    ({'field': 'value'}, {'status': {'some-id': {'field': 'value'}}}),
])
def test_store_result(handler, expected_patch, result):
    patch = Patch()
    outcomes = {handler.id: Outcome(final=True, result=result)}
    deliver_results(outcomes=outcomes, patch=patch)
    assert patch == expected_patch


#
# Purging the state in the storage.
#


def test_purge_progress_when_exists_in_body(storage, handler):
    body = {'status': {'kopf': {'progress': {'some-id': {'retries': 5}}}}}
    patch = Patch()
    state = State.from_storage(body=Body(body), handlers=[handler], storage=storage)
    state.purge(patch=patch, body=Body(body), storage=storage, handlers=[handler])
    assert patch == {'status': {'kopf': {'progress': {'some-id': None}}}}


def test_purge_progress_when_already_empty_in_body_and_patch(storage, handler):
    body = {}
    patch = Patch()
    state = State.from_storage(body=Body(body), handlers=[handler], storage=storage)
    state.purge(patch=patch, body=Body(body), storage=storage, handlers=[handler])
    assert not patch


def test_purge_progress_when_already_empty_in_body_but_not_in_patch(storage, handler):
    body = {}
    patch = Patch({'status': {'kopf': {'progress': {'some-id': {'retries': 5}}}}})
    state = State.from_storage(body=Body(body), handlers=[handler], storage=storage)
    state.purge(patch=patch, body=Body(body), storage=storage, handlers=[handler])
    assert not patch


def test_purge_progress_when_known_at_restoration_only(storage, handler):
    body = {'status': {'kopf': {'progress': {'some-id': {'retries': 5}}}}}
    patch = Patch()
    state = State.from_storage(body=Body(body), handlers=[handler], storage=storage)
    state.purge(patch=patch, body=Body(body), storage=storage, handlers=[])
    assert patch == {'status': {'kopf': {'progress': {'some-id': None}}}}


def test_purge_progress_when_known_at_purge_only(storage, handler):
    body = {'status': {'kopf': {'progress': {'some-id': {'retries': 5}}}}}
    patch = Patch()
    state = State.from_storage(body=Body(body), handlers=[], storage=storage)
    state.purge(patch=patch, body=Body(body), storage=storage, handlers=[handler])
    assert patch == {'status': {'kopf': {'progress': {'some-id': None}}}}


def test_purge_progress_cascades_to_subrefs(storage, handler):
    body = {'status': {'kopf': {'progress': {
        'some-id': {'subrefs': ['sub1', 'sub2', 'sub3']},
        'sub1': {},
        'sub2': {},
        # 'sub3' is intentionally absent -- should not be purged as already non-existent.
        'sub-unrelated': {},  # should be ignored, as not related to the involved handlers.
    }}}}
    patch = Patch()
    state = State.from_storage(body=Body(body), handlers=[handler], storage=storage)
    state.purge(patch=patch, body=Body(body), storage=storage, handlers=[handler])
    assert patch == {'status': {'kopf': {'progress': {
        'some-id': None,
        'sub1': None,
        'sub2': None,
    }}}}


def test_original_body_is_not_modified_by_storing(storage, handler):
    body = Body({})
    patch = Patch()
    state = State.from_storage(body=body, handlers=[handler], storage=storage)
    state.store(body=body, patch=patch, storage=storage)
    assert dict(body) == {}



================================================
FILE: tests/persistence/test_storing_of_diffbase.py
================================================
import json

import pytest

from kopf._cogs.configs.diffbase import AnnotationsDiffBaseStorage, DiffBaseStorage, \
                                        MultiDiffBaseStorage, StatusDiffBaseStorage
from kopf._cogs.structs.bodies import Body, BodyEssence
from kopf._cogs.structs.dicts import FieldSpec
from kopf._cogs.structs.patches import Patch


class DualDiffBaseStorage(MultiDiffBaseStorage):
    def __init__(
            self,
            prefix: str = 'kopf.zalando.org',
            key: str = 'last-handled-configuration',
            field: FieldSpec = 'status.kopf.last-handled-configuration',
    ):
        super().__init__([
            AnnotationsDiffBaseStorage(prefix=prefix, key=key),
            StatusDiffBaseStorage(field=field),
        ])


ALL_STORAGES = [AnnotationsDiffBaseStorage, StatusDiffBaseStorage, DualDiffBaseStorage]
ANNOTATIONS_POPULATING_STORAGES = [AnnotationsDiffBaseStorage, DualDiffBaseStorage]
STATUS_POPULATING_STORAGES = [StatusDiffBaseStorage, DualDiffBaseStorage]

ESSENCE_DATA_1 = BodyEssence(
    spec={
        'string-field': 'value1',
        'integer-field': 123,
        'float-field': 123.456,
        'false-field': False,
        'true-field': True,
        # Nones/nulls are not stored by K8s, so we do not test them.
    },
)

ESSENCE_DATA_2 = BodyEssence(
    spec={
        'hello': 'world',
        'the-cake': False,
        # Nones/nulls are not stored by K8s, so we do not test them.
    },
)

ESSENCE_JSON_1 = json.dumps(ESSENCE_DATA_1, separators=(',', ':'))
ESSENCE_JSON_2 = json.dumps(ESSENCE_DATA_2, separators=(',', ':'))


#
# Creation and parametrization.
#


def test_annotations_store_with_defaults():
    storage = AnnotationsDiffBaseStorage()
    assert storage.prefix == 'kopf.zalando.org'
    assert storage.key == 'last-handled-configuration'


def test_annotations_storage_with_prefix_and_key():
    storage = AnnotationsDiffBaseStorage(prefix='my-operator.my-company.com', key='diff-base')
    assert storage.prefix == 'my-operator.my-company.com'
    assert storage.key == 'diff-base'


def test_status_storage_with_field():
    storage = StatusDiffBaseStorage(field='status.my-operator.diff-base')
    assert storage.field == ('status', 'my-operator', 'diff-base')


#
# Common behaviour.
#


@pytest.mark.parametrize('cls', ALL_STORAGES)
def test_fetching_from_empty_body_returns_none(
        cls: type[DiffBaseStorage]):
    storage = cls()
    body = Body({})
    data = storage.fetch(body=body)
    assert data is None


#
# Annotations-populating.
#


@pytest.mark.parametrize('suffix', [
    pytest.param('', id='no-suffix'),
    pytest.param('\n', id='newline-suffix'),
])
@pytest.mark.parametrize('prefix', [
    pytest.param('', id='no-prefix'),
    pytest.param('\n', id='newline-prefix'),
])
@pytest.mark.parametrize('cls', ANNOTATIONS_POPULATING_STORAGES)
def test_fetching_from_annotations_storage(cls, prefix, suffix):
    storage = cls(prefix='my-operator.example.com', key='diff-base')
    body = Body({'metadata': {'annotations': {
        'my-operator.example.com/diff-base': prefix + ESSENCE_JSON_1 + suffix,
    }}})
    content = storage.fetch(body=body)

    assert content == ESSENCE_DATA_1


@pytest.mark.parametrize('cls', ANNOTATIONS_POPULATING_STORAGES)
def test_storing_to_annotations_storage_populates_keys(cls):
    storage = cls(prefix='my-operator.example.com', key='diff-base')
    patch = Patch()
    body = Body({})
    storage.store(body=body, patch=patch, essence=ESSENCE_DATA_1)

    assert patch
    assert patch.meta.annotations['my-operator.example.com/diff-base'][0] != '\n'
    assert patch.meta.annotations['my-operator.example.com/diff-base'][-1] == '\n'
    assert patch.meta.annotations['my-operator.example.com/diff-base'].strip() == ESSENCE_JSON_1


@pytest.mark.parametrize('cls', ANNOTATIONS_POPULATING_STORAGES)
def test_storing_to_annotations_storage_overwrites_old_content(cls):
    storage = cls(prefix='my-operator.example.com', key='diff-base')
    patch = Patch()
    body = Body({})
    storage.store(body=body, patch=patch, essence=ESSENCE_DATA_1)
    storage.store(body=body, patch=patch, essence=ESSENCE_DATA_2)

    assert patch
    assert patch.meta.annotations['my-operator.example.com/diff-base'][0] != '\n'
    assert patch.meta.annotations['my-operator.example.com/diff-base'][-1] == '\n'
    assert patch.meta.annotations['my-operator.example.com/diff-base'].strip() == ESSENCE_JSON_2


#
# Status-populating.
#


@pytest.mark.parametrize('suffix', [
    pytest.param('', id='no-suffix'),
    pytest.param('\n', id='newline-suffix'),
])
@pytest.mark.parametrize('prefix', [
    pytest.param('', id='no-prefix'),
    pytest.param('\n', id='newline-prefix'),
])
@pytest.mark.parametrize('cls', STATUS_POPULATING_STORAGES)
def test_fetching_from_status_storage(cls, prefix, suffix):
    storage = cls(field='status.my-operator.diff-base')
    body = Body({'status': {'my-operator': {'diff-base': prefix + ESSENCE_JSON_1 + suffix}}})
    content = storage.fetch(body=body)

    assert content == ESSENCE_DATA_1


@pytest.mark.parametrize('cls', STATUS_POPULATING_STORAGES)
def test_storing_to_status_storage_populates_keys(cls):
    storage = cls(field='status.my-operator.diff-base')
    patch = Patch()
    body = Body({})
    storage.store(body=body, patch=patch, essence=ESSENCE_DATA_1)

    assert patch
    assert patch.status['my-operator']['diff-base'][0] != '\n'
    assert patch.status['my-operator']['diff-base'][-1] != '\n'
    assert patch.status['my-operator']['diff-base'] == ESSENCE_JSON_1


@pytest.mark.parametrize('cls', STATUS_POPULATING_STORAGES)
def test_storing_to_status_storage_overwrites_old_content(
        cls: type[DiffBaseStorage]):
    storage = cls(field='status.my-operator.diff-base')
    patch = Patch()
    body = Body({})
    storage.store(body=body, patch=patch, essence=ESSENCE_DATA_1)
    storage.store(body=body, patch=patch, essence=ESSENCE_DATA_2)

    assert patch
    assert patch.status['my-operator']['diff-base'][0] != '\n'
    assert patch.status['my-operator']['diff-base'][-1] != '\n'
    assert patch.status['my-operator']['diff-base'] == ESSENCE_JSON_2



================================================
FILE: tests/persistence/test_storing_of_progress.py
================================================
import json

import pytest

from kopf._cogs.configs.progress import AnnotationsProgressStorage, ProgressRecord, \
                                        ProgressStorage, SmartProgressStorage, \
                                        StatusProgressStorage
from kopf._cogs.structs.bodies import Body
from kopf._cogs.structs.ids import HandlerId
from kopf._cogs.structs.patches import Patch

ALL_STORAGES = [AnnotationsProgressStorage, StatusProgressStorage, SmartProgressStorage]
ANNOTATIONS_POPULATING_STORAGES = [AnnotationsProgressStorage, SmartProgressStorage]
STATUS_POPULATING_STORAGES = [StatusProgressStorage, SmartProgressStorage]

CONTENT_DATA_1 = ProgressRecord(
    started='2020-01-01T00:00:00',
    stopped='2020-12-31T23:59:59',
    delayed='3000-01-01T00:00:00',
    retries=123,
    success=False,
    failure=False,
    message=None,
    subrefs=None,
)

CONTENT_DATA_2 = ProgressRecord(
    started='2021-01-01T00:00:00',
    stopped='2021-12-31T23:59:59',
    delayed='3001-01-01T00:00:00',
    retries=456,
    success=False,
    failure=False,
    message="Some error.",
    subrefs=['sub1', 'sub2'],
)

CONTENT_JSON_1 = json.dumps(CONTENT_DATA_1, separators=(',', ':'))
CONTENT_JSON_2 = json.dumps(CONTENT_DATA_2, separators=(',', ':'))


#
# Storage creation.
#


def test_status_storage_with_defaults():
    storage = StatusProgressStorage()
    assert storage.field == ('status', 'kopf', 'progress')  # as before the change
    assert storage.touch_field == ('status', 'kopf', 'dummy')  # as before the change


def test_status_storage_with_name():
    storage = StatusProgressStorage(name='my-operator')
    assert storage.field == ('status', 'my-operator', 'progress')
    assert storage.touch_field == ('status', 'my-operator', 'dummy')


def test_status_storage_with_field():
    storage = StatusProgressStorage(field='status.my-operator', touch_field='status.my-dummy')
    assert storage.field == ('status', 'my-operator')
    assert storage.touch_field == ('status', 'my-dummy')


def test_annotations_storage_with_defaults():
    storage = AnnotationsProgressStorage()
    assert storage.prefix == 'kopf.zalando.org'


def test_annotations_storage_with_prefix():
    storage = AnnotationsProgressStorage(prefix='my-operator.my-company.com')
    assert storage.prefix == 'my-operator.my-company.com'


def test_smart_storage_with_defaults():
    storage = SmartProgressStorage()
    assert isinstance(storage.storages[0], AnnotationsProgressStorage)
    assert isinstance(storage.storages[1], StatusProgressStorage)
    assert storage.storages[0].prefix == 'kopf.zalando.org'
    assert storage.storages[1].field == ('status', 'kopf', 'progress')
    assert storage.storages[1].touch_field == ('status', 'kopf', 'dummy')


def test_smart_storage_with_name():
    storage = SmartProgressStorage(name='my-operator')
    assert isinstance(storage.storages[0], AnnotationsProgressStorage)
    assert isinstance(storage.storages[1], StatusProgressStorage)
    assert storage.storages[0].prefix == 'kopf.zalando.org'
    assert storage.storages[1].field == ('status', 'my-operator', 'progress')
    assert storage.storages[1].touch_field == ('status', 'my-operator', 'dummy')


def test_smart_storage_with_field():
    storage = SmartProgressStorage(field='status.my-operator', touch_field='status.my-dummy')
    assert isinstance(storage.storages[0], AnnotationsProgressStorage)
    assert isinstance(storage.storages[1], StatusProgressStorage)
    assert storage.storages[0].prefix == 'kopf.zalando.org'
    assert storage.storages[1].field == ('status', 'my-operator')
    assert storage.storages[1].touch_field == ('status', 'my-dummy')


def test_smart_storage_with_prefix():
    storage = SmartProgressStorage(prefix='my-operator.my-company.com')
    assert isinstance(storage.storages[0], AnnotationsProgressStorage)
    assert isinstance(storage.storages[1], StatusProgressStorage)
    assert storage.storages[0].prefix == 'my-operator.my-company.com'
    assert storage.storages[1].field == ('status', 'kopf', 'progress')
    assert storage.storages[1].touch_field == ('status', 'kopf', 'dummy')


#
# Common behaviour.
#


@pytest.mark.parametrize('cls', ALL_STORAGES)
def test_fetching_from_empty_body_returns_none(
        cls: type[ProgressStorage]):
    storage = cls()
    body = Body({})
    data = storage.fetch(body=body, key=HandlerId('id1'))
    assert data is None


@pytest.mark.parametrize('cls', ALL_STORAGES)
def test_purging_already_empty_body_does_nothing(
        cls: type[ProgressStorage]):
    storage = cls()
    patch = Patch()
    body = Body({})
    storage.purge(body=body, patch=patch, key=HandlerId('id1'))
    assert not patch


#
# Annotations-populating.
#


@pytest.mark.parametrize('cls', ANNOTATIONS_POPULATING_STORAGES)
def test_fetching_from_annotations_storage(cls):
    storage = cls(prefix='my-operator.example.com', verbose=True)
    body = Body({'metadata': {'annotations': {
        'my-operator.example.com/id1': CONTENT_JSON_1,
    }}})
    content = storage.fetch(body=body, key=HandlerId('id1'))

    assert content == CONTENT_DATA_1


@pytest.mark.parametrize('cls', ANNOTATIONS_POPULATING_STORAGES)
def test_storing_to_annotations_storage_populates_keys(cls):
    storage = cls(prefix='my-operator.example.com', verbose=True)
    patch = Patch()
    body = Body({})
    storage.store(body=body, patch=patch, key=HandlerId('id1'), record=CONTENT_DATA_1)

    assert patch
    assert patch['metadata']['annotations']['my-operator.example.com/id1'] == CONTENT_JSON_1


@pytest.mark.parametrize('cls', ANNOTATIONS_POPULATING_STORAGES)
def test_storing_to_annotations_storage_appends_keys(cls):
    storage = cls(prefix='my-operator.example.com', verbose=True)
    patch = Patch()
    body = Body({})
    storage.store(body=body, patch=patch, key=HandlerId('id1'), record=CONTENT_DATA_1)
    storage.store(body=body, patch=patch, key=HandlerId('id2'), record=CONTENT_DATA_2)

    assert patch
    assert patch['metadata']['annotations']['my-operator.example.com/id1'] == CONTENT_JSON_1
    assert patch['metadata']['annotations']['my-operator.example.com/id2'] == CONTENT_JSON_2


@pytest.mark.parametrize('cls', ANNOTATIONS_POPULATING_STORAGES)
def test_storing_to_annotations_storage_overwrites_old_content(cls):
    storage = cls(prefix='my-operator.example.com', verbose=True)
    patch = Patch()
    body = Body({})
    storage.store(body=body, patch=patch, key=HandlerId('id1'), record=CONTENT_DATA_1)
    storage.store(body=body, patch=patch, key=HandlerId('id1'), record=CONTENT_DATA_2)

    assert patch
    assert patch['metadata']['annotations']['my-operator.example.com/id1'] == CONTENT_JSON_2


@pytest.mark.parametrize('cls', ANNOTATIONS_POPULATING_STORAGES)
def test_storing_to_annotations_storage_cleans_content(cls):
    storage = cls(prefix='my-operator.example.com')  # no verbose=
    patch = Patch()
    body = Body({})
    content = ProgressRecord(
        started=None,
        stopped=None,
        delayed=None,
        retries=None,
        success=None,
        failure=None,
        message=None,
        subrefs=None,
    )
    storage.store(body=body, patch=patch, key=HandlerId('id1'), record=content)

    assert patch
    assert patch['metadata']['annotations']['my-operator.example.com/id1'] == json.dumps({})


@pytest.mark.parametrize('cls', ANNOTATIONS_POPULATING_STORAGES)
def test_purging_of_annotations_storage_nullifies_content(cls):
    storage = cls(prefix='my-operator.example.com', verbose=True)
    patch = Patch()
    body = Body({'metadata': {'annotations': {
        'my-operator.example.com/id1': CONTENT_JSON_1,
    }}})
    storage.purge(body=body, patch=patch, key=HandlerId('id1'))

    assert patch
    assert patch['metadata']['annotations']['my-operator.example.com/id1'] is None


@pytest.mark.parametrize('body_data', [
    pytest.param({}, id='without-data'),
    pytest.param({'metadata': {'annotations': {'my-operator.example.com/my-dummy': 'something'}}}, id='with-data'),
])
@pytest.mark.parametrize('cls', ANNOTATIONS_POPULATING_STORAGES)
def test_touching_via_annotations_storage_with_payload(cls, body_data):
    storage = cls(prefix='my-operator.example.com', touch_key='my-dummy')
    patch = Patch()
    body = Body(body_data)
    storage.touch(body=body, patch=patch, value='hello')

    assert patch
    assert patch['metadata']['annotations']['my-operator.example.com/my-dummy'] == 'hello'


@pytest.mark.parametrize('cls', ANNOTATIONS_POPULATING_STORAGES)
def test_touching_via_annotations_storage_with_none_when_absent(cls):
    storage = cls(prefix='my-operator.example.com', touch_key='my-dummy')
    patch = Patch()
    body = Body({})
    storage.touch(body=body, patch=patch, value=None)

    assert not patch


@pytest.mark.parametrize('cls', ANNOTATIONS_POPULATING_STORAGES)
def test_touching_via_annotations_storage_with_none_when_present(cls):
    storage = cls(prefix='my-operator.example.com', touch_key='my-dummy')
    patch = Patch()
    body = Body({'metadata': {'annotations': {'my-operator.example.com/my-dummy': 'something'}}})
    storage.touch(body=body, patch=patch, value=None)

    assert patch
    assert patch['metadata']['annotations']['my-operator.example.com/my-dummy'] is None


#
# Status-populating.
#


@pytest.mark.parametrize('cls', STATUS_POPULATING_STORAGES)
def test_fetching_from_status_storage(cls):
    storage = cls(field='status.my-operator')
    body = Body({'status': {'my-operator': {'id1': CONTENT_DATA_1, 'id2': CONTENT_DATA_2}}})
    content = storage.fetch(body=body, key=HandlerId('id1'))

    assert content == CONTENT_DATA_1


@pytest.mark.parametrize('cls', STATUS_POPULATING_STORAGES)
def test_storing_to_status_storage_populates_keys(cls):
    storage = cls(field='status.my-operator')
    patch = Patch()
    body = Body({})
    storage.store(body=body, patch=patch, key=HandlerId('id1'), record=CONTENT_DATA_1)

    assert patch
    assert patch['status']['my-operator']['id1'] == CONTENT_DATA_1


@pytest.mark.parametrize('cls', STATUS_POPULATING_STORAGES)
def test_storing_to_status_storage_appends_keys(cls):
    storage = cls(field='status.my-operator')
    patch = Patch()
    body = Body({})
    storage.store(body=body, patch=patch, key=HandlerId('id1'), record=CONTENT_DATA_1)
    storage.store(body=body, patch=patch, key=HandlerId('id2'), record=CONTENT_DATA_1)

    assert patch
    assert patch['status']['my-operator']['id1'] == CONTENT_DATA_1
    assert patch['status']['my-operator']['id2'] == CONTENT_DATA_1


@pytest.mark.parametrize('cls', STATUS_POPULATING_STORAGES)
def test_storing_to_status_storage_overwrites_old_content(cls):
    storage = cls(field='status.my-operator')
    patch = Patch()
    body = Body({})
    storage.store(body=body, patch=patch, key=HandlerId('id1'), record=CONTENT_DATA_1)
    storage.store(body=body, patch=patch, key=HandlerId('id1'), record=CONTENT_DATA_2)

    assert patch
    assert patch['status']['my-operator']['id1'] == CONTENT_DATA_2


@pytest.mark.parametrize('cls', STATUS_POPULATING_STORAGES)
def test_purging_of_status_storage_nullifies_content(cls):
    storage = cls(field='status.my-operator')
    patch = Patch()
    body = Body({'status': {'my-operator': {'id1': CONTENT_DATA_1}}})
    storage.purge(body=body, patch=patch, key=HandlerId('id1'))

    assert patch
    assert patch['status']['my-operator']['id1'] is None


@pytest.mark.parametrize('body_data', [
    pytest.param({}, id='without-data'),
    pytest.param({'status': {'my-dummy': 'something'}}, id='with-data'),
])
@pytest.mark.parametrize('cls', STATUS_POPULATING_STORAGES)
def test_touching_via_status_storage_with_payload(cls, body_data):
    storage = cls(field='status.my-operator', touch_field='status.my-dummy')
    patch = Patch()
    body = Body(body_data)
    storage.touch(body=body, patch=patch, value='hello')

    assert patch
    assert patch['status']['my-dummy'] == 'hello'


@pytest.mark.parametrize('cls', STATUS_POPULATING_STORAGES)
def test_touching_via_status_storage_with_none_when_absent(cls):
    storage = cls(touch_field='status.my-dummy')
    patch = Patch()
    body = Body({})
    storage.touch(body=body, patch=patch, value=None)

    assert not patch


@pytest.mark.parametrize('cls', STATUS_POPULATING_STORAGES)
def test_touching_via_status_storage_with_none_when_present(cls):
    storage = cls(touch_field='status.my-dummy')
    patch = Patch()
    body = Body({'status': {'my-dummy': 'something'}})
    storage.touch(body=body, patch=patch, value=None)

    assert patch
    assert patch['status']['my-dummy'] is None



================================================
FILE: tests/posting/conftest.py
================================================
import asyncio

import pytest

from kopf._core.engines.posting import event_queue_loop_var, event_queue_var


@pytest.fixture()
def event_queue_loop(loop):  # must be sync-def
    token = event_queue_loop_var.set(loop)
    try:
        yield loop
    finally:
        event_queue_loop_var.reset(token)


@pytest.fixture()
def event_queue():
    queue = asyncio.Queue()
    token = event_queue_var.set(queue)
    try:
        yield queue
    finally:
        event_queue_var.reset(token)



================================================
FILE: tests/posting/test_log2k8s.py
================================================
import logging

import pytest

from kopf._core.actions.loggers import LocalObjectLogger, ObjectLogger

OBJ1 = {'apiVersion': 'group1/version1', 'kind': 'Kind1',
        'metadata': {'uid': 'uid1', 'name': 'name1', 'namespace': 'ns1'}}
REF1 = {'apiVersion': 'group1/version1', 'kind': 'Kind1',
        'uid': 'uid1', 'name': 'name1', 'namespace': 'ns1'}


@pytest.mark.parametrize('logfn, event_type', [
    ['info', "Normal"],
    ['warning', "Warning"],
    ['error', "Error"],
    ['critical', "Fatal"],
])
async def test_posting_normal_levels(settings, caplog, logstream, logfn, event_type,
                                     event_queue, event_queue_loop):

    logger = ObjectLogger(body=OBJ1, settings=settings)
    logger_fn = getattr(logger, logfn)

    logger_fn("hello %s", "world")

    assert event_queue.qsize() == 1
    event1 = event_queue.get_nowait()
    assert event1.ref == REF1
    assert event1.type == event_type
    assert event1.reason == "Logging"
    assert event1.message == "hello world"
    assert caplog.messages == ["hello world"]


@pytest.mark.parametrize('logfn, event_type, min_levelno', [
    ['debug', "Debug", logging.DEBUG],
    ['info', "Normal", logging.INFO],
    ['warning', "Warning", logging.WARNING],
    ['error', "Error", logging.ERROR],
    ['critical', "Fatal", logging.CRITICAL],
])
async def test_posting_above_config(settings, caplog, logstream, logfn, event_type, min_levelno,
                                    event_queue, event_queue_loop, mocker):
    logger = ObjectLogger(body=OBJ1, settings=settings)
    logger_fn = getattr(logger, logfn)

    settings.posting.level = min_levelno
    logger_fn("hello %s", "world")
    settings.posting.level = min_levelno + 1
    logger_fn("must not be posted")

    assert event_queue.qsize() == 1
    event1 = event_queue.get_nowait()
    assert event1.ref == REF1
    assert event1.type == event_type
    assert event1.reason == "Logging"
    assert event1.message == "hello world"
    assert caplog.messages == ["hello world", "must not be posted"]


@pytest.mark.parametrize('logfn', [
    'debug',
])
async def test_skipping_hidden_levels(settings, caplog, logstream, logfn,
                                      event_queue, event_queue_loop):

    logger = ObjectLogger(body=OBJ1, settings=settings)
    logger_fn = getattr(logger, logfn)

    logger_fn("hello %s", "world")
    logger.info("must be here")

    assert event_queue.qsize() == 1  # not 2!
    assert caplog.messages == ["hello world", "must be here"]


@pytest.mark.parametrize('logfn', [
    'debug',
    'info',
    'warning',
    'error',
    'critical',
])
async def test_skipping_below_config(settings, caplog, logstream, logfn,
                                     event_queue, event_queue_loop, mocker):

    logger = ObjectLogger(body=OBJ1, settings=settings)
    logger_fn = getattr(logger, logfn)

    settings.posting.level = 666
    logger_fn("hello %s", "world")
    settings.posting.level = 0
    logger.info("must be here")

    assert event_queue.qsize() == 1  # not 2!
    assert caplog.messages == ["hello world", "must be here"]


@pytest.mark.parametrize('logfn', [
    'debug',
    'info',
    'warning',
    'error',
    'critical',
])
async def test_skipping_when_disabled(settings, caplog, logstream, logfn,
                                      event_queue, event_queue_loop):

    logger = LocalObjectLogger(body=OBJ1, settings=settings)
    logger_fn = getattr(logger, logfn)

    settings.posting.enabled = False
    settings.posting.level = 0
    logger_fn("hello %s", "world")

    assert event_queue.qsize() == 0
    assert caplog.messages == ["hello world"]


@pytest.mark.parametrize('logfn', [
    'debug',
    'info',
    'warning',
    'error',
    'critical',
])
async def test_skipping_when_local_with_all_levels(settings, caplog, logstream, logfn,
                                                   event_queue, event_queue_loop):

    logger = LocalObjectLogger(body=OBJ1, settings=settings)
    logger_fn = getattr(logger, logfn)

    logger_fn("hello %s", "world")

    assert event_queue.qsize() == 0
    assert caplog.messages == ["hello world"]



================================================
FILE: tests/posting/test_poster.py
================================================
import asyncio
import logging

import pytest

from kopf import event, exception, info, warn
from kopf._cogs.structs.references import Backbone, Resource
from kopf._core.engines.posting import K8sEvent, event_queue_loop_var, event_queue_var, poster

OBJ1 = {'apiVersion': 'group1/version1', 'kind': 'Kind1',
        'metadata': {'uid': 'uid1', 'name': 'name1', 'namespace': 'ns1'}}
REF1 = {'apiVersion': 'group1/version1', 'kind': 'Kind1',
        'uid': 'uid1', 'name': 'name1', 'namespace': 'ns1'}
OBJ2 = {'apiVersion': 'group2/version2', 'kind': 'Kind2',
        'metadata': {'uid': 'uid2', 'name': 'name2', 'namespace': 'ns2'}}
REF2 = {'apiVersion': 'group2/version2', 'kind': 'Kind2',
        'uid': 'uid2', 'name': 'name2', 'namespace': 'ns2'}

EVENTS = Resource('', 'v1', 'events', namespaced=True)


@pytest.fixture(autouse=True)
def _settings_via_contextvar(settings_via_contextvar):
    pass


async def test_poster_polls_and_posts(mocker, settings):

    event1 = K8sEvent(type='type1', reason='reason1', message='message1', ref=REF1)
    event2 = K8sEvent(type='type2', reason='reason2', message='message2', ref=REF2)
    event_queue = asyncio.Queue()
    event_queue.put_nowait(event1)
    event_queue.put_nowait(event2)

    # A way to cancel `while True` cycle when we need it (ASAP).
    def _cancel(*args, **kwargs):
        if post.call_count >= 2:
            raise asyncio.CancelledError()
    post = mocker.patch('kopf._cogs.clients.api.post', side_effect=_cancel)

    backbone = Backbone()
    await backbone.fill(resources=[EVENTS])

    # A way to cancel a `while True` cycle by timing, even if the routines are not called.
    with pytest.raises(asyncio.CancelledError):
        await poster(event_queue=event_queue, backbone=backbone, settings=settings)

    assert post.call_count == 2
    assert post.call_args_list[0][1]['url'] == '/api/v1/namespaces/ns1/events'
    assert post.call_args_list[0][1]['payload']['type'] == 'type1'
    assert post.call_args_list[0][1]['payload']['reason'] == 'reason1'
    assert post.call_args_list[0][1]['payload']['message'] == 'message1'
    assert post.call_args_list[0][1]['payload']['involvedObject'] == REF1
    assert post.call_args_list[1][1]['url'] == '/api/v1/namespaces/ns2/events'
    assert post.call_args_list[1][1]['payload']['type'] == 'type2'
    assert post.call_args_list[1][1]['payload']['reason'] == 'reason2'
    assert post.call_args_list[1][1]['payload']['message'] == 'message2'
    assert post.call_args_list[1][1]['payload']['involvedObject'] == REF2


def test_queueing_fails_with_no_queue(event_queue_loop):
    # Prerequisite: the context-var should not be set by anything in advance.
    sentinel = object()
    assert event_queue_var.get(sentinel) is sentinel

    with pytest.raises(LookupError):
        event(OBJ1, type='type1', reason='reason1', message='message1')


def test_queueing_fails_with_no_loop(event_queue):
    # Prerequisite: the context-var should not be set by anything in advance.
    sentinel = object()
    assert event_queue_loop_var.get(sentinel) is sentinel

    with pytest.raises(LookupError):
        event(OBJ1, type='type1', reason='reason1', message='message1')


async def test_via_event_function(mocker, event_queue, event_queue_loop):
    post = mocker.patch('kopf._cogs.clients.api.post')

    event(OBJ1, type='type1', reason='reason1', message='message1')

    assert not post.called
    assert event_queue.qsize() == 1
    event1 = event_queue.get_nowait()

    assert isinstance(event1, K8sEvent)
    assert event1.ref == REF1
    assert event1.type == 'type1'
    assert event1.reason == 'reason1'
    assert event1.message == 'message1'


@pytest.mark.parametrize('event_fn, event_type, min_levelno', [
    pytest.param(info, "Normal", logging.INFO, id='info'),
    pytest.param(warn, "Warning", logging.WARNING, id='warn'),
    pytest.param(exception, "Error", logging.ERROR, id='exception'),
])
async def test_via_shortcut(settings, mocker, event_fn, event_type, min_levelno,
                            event_queue, event_queue_loop):
    post = mocker.patch('kopf._cogs.clients.api.post')

    settings.posting.level = min_levelno
    event_fn(OBJ1, reason='reason1', message='message1')  # posted
    settings.posting.level = min_levelno + 1
    event_fn(OBJ1, reason='reason2', message='message2')  # not posted

    assert not post.called
    assert event_queue.qsize() == 1
    event1 = event_queue.get_nowait()

    assert isinstance(event1, K8sEvent)
    assert event1.ref == REF1
    assert event1.type == event_type
    assert event1.reason == 'reason1'
    assert event1.message == 'message1'



================================================
FILE: tests/posting/test_threadsafety.py
================================================
"""
A little note on how these tests work:

Almost all asyncio objects are not thread-safe, as per the official doc.
This includes `asyncio.Queue`. This queue is used for k8s-event posting.

K8s-events are posted via ``kopf.event()`` and similar calls,
and also via ``logger.info()`` for per-object logging messages.

The calls originate from various threads:

* Main thread where the framework's event-loop runs.
* Thread-pool executors for sync handlers.
* Explicitly started threads for object monitoring
  (e.g. from ``@kopf.on.resume`` handlers).

In the main thread, there is an event-loop running, and it has an asyncio task
to get the k8s-event events from the queue and to post them to the K8s API.

In the non-thread-safe mode, putting an object via `queue.put_nowait()``
does **NOT** wake up the pending ``queue.get()`` in the `poster` task
until something happens on the event-loop (not necessary on the queue).

In the thread-safe mode, putting an an object via `queue.put()``
(which is a coroutine and must be executed in the loop)
wakes the pending ``queue.get()`` call immediately.

These tests ensure that the thread-safe calls are used for k8s-event posting
by artificially reproducing the described situation. The delayed no-op task
(awakener) is used to wake up the event-loop after some time if the k8s-event
posting is not thread-safe. Otherwise, it wakes up on ``queue.get()`` instantly.

If thread safety is not ensured, the operators get sporadic errors regarding
thread-unsafe calls, which are difficult to catch and reproduce.
"""

import asyncio
import contextvars
import functools
import threading
import time

import pytest

from kopf import event

OBJ1 = {'apiVersion': 'group1/version1', 'kind': 'Kind1',
        'metadata': {'uid': 'uid1', 'name': 'name1', 'namespace': 'ns1'}}


@pytest.fixture()
def awakener():
    handles = []

    def noop():
        pass

    def awaken_fn(delay, fn=noop):
        handle = asyncio.get_running_loop().call_later(delay, fn)
        handles.append(handle)

    try:
        yield awaken_fn
    finally:
        for handle in handles:
            handle.cancel()


@pytest.fixture()
def threader():
    threads = []

    def start_fn(delay, fn):
        def thread_fn():
            time.sleep(delay)
            fn()

        target = functools.partial(contextvars.copy_context().run, thread_fn)
        thread = threading.Thread(target=target)
        thread.start()
        threads.append(thread)

    try:
        yield start_fn
    finally:
        for thread in threads:
            thread.join()


async def test_nonthreadsafe_indeed_fails(timer, awakener, threader, event_queue, event_queue_loop):

    def thread_fn():
        event_queue.put_nowait(object())

    awakener(0.7)
    threader(0.3, thread_fn)

    with timer:
        await event_queue.get()

    assert 0.6 <= timer.seconds <= 0.8


async def test_threadsafe_indeed_works(timer, awakener, threader, event_queue, event_queue_loop):

    def thread_fn():
        asyncio.run_coroutine_threadsafe(event_queue.put(object()), loop=event_queue_loop)

    awakener(0.7)
    threader(0.3, thread_fn)

    with timer:
        await event_queue.get()

    assert 0.2 <= timer.seconds <= 0.4


async def test_queueing_is_threadsafe(timer, awakener, threader, event_queue, event_queue_loop,
                                      settings_via_contextvar):

    def thread_fn():
        event(OBJ1, type='type1', reason='reason1', message='message1')

    awakener(0.7)
    threader(0.3, thread_fn)

    with timer:
        await event_queue.get()

    assert 0.2 <= timer.seconds <= 0.4



================================================
FILE: tests/primitives/test_conditions.py
================================================
import asyncio

import pytest

from kopf._cogs.aiokits.aiobindings import condition_chain


async def test_no_triggering():
    source = asyncio.Condition()
    target = asyncio.Condition()
    task = asyncio.create_task(condition_chain(source, target))
    try:
        with pytest.raises(asyncio.TimeoutError):
            async with target:
                await asyncio.wait_for(target.wait(), timeout=0.1)
    finally:
        task.cancel()
        await asyncio.wait([task])


async def test_triggering(timer):
    source = asyncio.Condition()
    target = asyncio.Condition()
    task = asyncio.create_task(condition_chain(source, target))
    try:

        async def delayed_trigger():
            async with source:
                source.notify_all()

        loop = asyncio.get_running_loop()
        loop.call_later(0.1, asyncio.create_task, delayed_trigger())

        with timer:
            async with target:
                await target.wait()

        assert 0.1 <= timer.seconds <= 0.2

    finally:
        task.cancel()
        await asyncio.wait([task])



================================================
FILE: tests/primitives/test_containers.py
================================================
import asyncio

import pytest

from kopf._cogs.aiokits.aiovalues import Container


async def test_empty_by_default():
    container = Container()
    with pytest.raises(asyncio.TimeoutError):
        await asyncio.wait_for(container.wait(), timeout=0.1)


async def test_does_not_wake_up_when_reset(timer):
    container = Container()

    async def reset_it():
        await container.reset()

    loop = asyncio.get_running_loop()
    loop.call_later(0.05, asyncio.create_task, reset_it())

    with pytest.raises(asyncio.TimeoutError):
        await asyncio.wait_for(container.wait(), timeout=0.1)


async def test_wakes_up_when_preset(timer):
    container = Container()
    await container.set(123)

    with timer:
        result = await container.wait()

    assert timer.seconds <= 0.1
    assert result == 123


async def test_wakes_up_when_set(timer):
    container = Container()

    async def set_it():
        await container.set(123)

    loop = asyncio.get_running_loop()
    loop.call_later(0.1, asyncio.create_task, set_it())

    with timer:
        result = await container.wait()

    assert 0.1 <= timer.seconds <= 0.2
    assert result == 123


async def test_iterates_when_set(timer):
    container = Container()

    async def set_it(v):
        await container.set(v)

    loop = asyncio.get_running_loop()
    loop.call_later(0.1, asyncio.create_task, set_it(123))
    loop.call_later(0.2, asyncio.create_task, set_it(234))

    values = []
    with timer:
        async for value in container.as_changed():
            values.append(value)
            if value == 234:
                break

    assert 0.2 <= timer.seconds <= 0.3
    assert values == [123, 234]


async def test_iterates_when_preset(timer):
    container = Container()
    await container.set(123)

    values = []
    with timer:
        async for value in container.as_changed():
            values.append(value)
            break

    assert timer.seconds <= 0.1
    assert values == [123]



================================================
FILE: tests/primitives/test_flags.py
================================================
import asyncio
import concurrent.futures
import threading

import pytest

from kopf._cogs.aiokits.aioadapters import check_flag, raise_flag, wait_flag


@pytest.fixture(params=[
    pytest.param(asyncio.Event, id='asyncio-event'),
    pytest.param(asyncio.Future, id='asyncio-future'),
    pytest.param(threading.Event, id='threading-event'),
    pytest.param(concurrent.futures.Future, id='concurrent-future'),
])
async def flag(request):
    """
    Fulfil the flag's waiting expectations, so that a threaded call can finish.
    Otherwise, the test runs freeze at exit while waiting for the executors.
    """
    flag = request.param()
    try:
        yield flag
    finally:
        if hasattr(flag, 'cancel'):
            flag.cancel()
        if hasattr(flag, 'set'):
            flag.set()


async def test_checking_of_unsupported_raises_an_error():
    with pytest.raises(TypeError):
        check_flag(object())


async def test_checking_of_none_is_none():
    result = check_flag(None)
    assert result is None


async def test_checking_of_asyncio_event_when_raised():
    event = asyncio.Event()
    event.set()
    result = check_flag(event)
    assert result is True


async def test_checking_of_asyncio_event_when_unset():
    event = asyncio.Event()
    event.clear()
    result = check_flag(event)
    assert result is False


async def test_checking_of_asyncio_future_when_set():
    future = asyncio.Future()
    future.set_result(None)
    result = check_flag(future)
    assert result is True


async def test_checking_of_asyncio_future_when_empty():
    future = asyncio.Future()
    result = check_flag(future)
    assert result is False


async def test_checking_of_threading_event_when_set():
    event = threading.Event()
    event.set()
    result = check_flag(event)
    assert result is True


async def test_checking_of_threading_event_when_unset():
    event = threading.Event()
    event.clear()
    result = check_flag(event)
    assert result is False


async def test_checking_of_concurrent_future_when_set():
    future = concurrent.futures.Future()
    future.set_result(None)
    result = check_flag(future)
    assert result is True


async def test_checking_of_concurrent_future_when_unset():
    future = concurrent.futures.Future()
    result = check_flag(future)
    assert result is False


async def test_raising_of_unsupported_raises_an_error():
    with pytest.raises(TypeError):
        await raise_flag(object())


async def test_raising_of_none_does_nothing():
    await raise_flag(None)


async def test_raising_of_asyncio_event():
    event = asyncio.Event()
    await raise_flag(event)
    assert event.is_set()


async def test_raising_of_asyncio_future():
    future = asyncio.Future()
    await raise_flag(future)
    assert future.done()


async def test_raising_of_threading_event():
    event = threading.Event()
    await raise_flag(event)
    assert event.is_set()


async def test_raising_of_concurrent_future():
    future = concurrent.futures.Future()
    await raise_flag(future)
    assert future.done()


async def test_waiting_of_unsupported_raises_an_error():
    with pytest.raises(TypeError):
        await wait_flag(object())

async def test_waiting_of_none_does_nothing():
    await wait_flag(None)


async def test_waiting_for_unraised_times_out(flag, timer):
    with pytest.raises(asyncio.TimeoutError), timer:
        await asyncio.wait_for(wait_flag(flag), timeout=0.1)
    assert timer.seconds >= 0.099  # uvloop finishes it earlier than 0.1


async def test_waiting_for_preraised_is_instant(flag, timer):
    await raise_flag(flag)  # tested separately above
    with timer:
        await wait_flag(flag)
    assert timer.seconds < 0.5  # near-instant, plus code overhead


async def test_waiting_for_raised_during_the_wait(flag, timer):

    async def raise_delayed(delay: float) -> None:
        await asyncio.sleep(delay)
        await raise_flag(flag)  # tested separately above

    asyncio.create_task(raise_delayed(0.2))
    with timer:
        await wait_flag(flag)
    assert 0.2 <= timer.seconds < 0.5  # near-instant once raised



================================================
FILE: tests/primitives/test_toggles.py
================================================
import asyncio

import pytest

from kopf._cogs.aiokits.aiotoggles import Toggle


async def test_created_as_off():
    toggle = Toggle()
    assert not toggle.is_on()
    assert toggle.is_off()


async def test_initialised_as_off():
    toggle = Toggle(False)
    assert not toggle.is_on()
    assert toggle.is_off()


async def test_initialised_as_on():
    toggle = Toggle(True)
    assert toggle.is_on()
    assert not toggle.is_off()


async def test_turning_on():
    toggle = Toggle(False)
    await toggle.turn_to(True)
    assert toggle.is_on()
    assert not toggle.is_off()


async def test_turning_off():
    toggle = Toggle(True)
    await toggle.turn_to(False)
    assert not toggle.is_on()
    assert toggle.is_off()


async def test_waiting_until_on_fails_when_not_turned_on():
    toggle = Toggle(False)
    with pytest.raises(asyncio.TimeoutError):
        await asyncio.wait_for(toggle.wait_for(True), timeout=0.1)

    assert toggle.is_off()


async def test_waiting_until_off_fails_when_not_turned_off():
    toggle = Toggle(True)
    with pytest.raises(asyncio.TimeoutError):
        await asyncio.wait_for(toggle.wait_for(False), timeout=0.1)

    assert toggle.is_on()


async def test_waiting_until_on_wakes_when_turned_on(timer):
    toggle = Toggle(False)

    async def delayed_turning_on(delay: float):
        await asyncio.sleep(delay)
        await toggle.turn_to(True)

    with timer:
        asyncio.create_task(delayed_turning_on(0.05))
        await toggle.wait_for(True)

    assert toggle.is_on()
    assert timer.seconds < 0.5  # approx. 0.05 plus some code overhead


async def test_waiting_until_off_wakes_when_turned_off(timer):
    toggle = Toggle(True)

    async def delayed_turning_off(delay: float):
        await asyncio.sleep(delay)
        await toggle.turn_to(False)

    with timer:
        asyncio.create_task(delayed_turning_off(0.05))
        await toggle.wait_for(False)

    assert toggle.is_off()
    assert timer.seconds < 0.5  # approx. 0.05 plus some code overhead


async def test_secures_against_usage_as_a_boolean():
    toggle = Toggle()
    with pytest.raises(NotImplementedError):
        bool(toggle)


async def test_repr_when_unnamed_and_off():
    toggle = Toggle(False)
    assert toggle.name is None
    assert repr(toggle) == "<Toggle: off>"


async def test_repr_when_unnamed_and_on():
    toggle = Toggle(True)
    assert toggle.name is None
    assert repr(toggle) == "<Toggle: on>"


async def test_repr_when_named_and_off():
    toggle = Toggle(False, name='xyz')
    assert toggle.name == 'xyz'
    assert repr(toggle) == "<Toggle: xyz: off>"


async def test_repr_when_named_and_on():
    toggle = Toggle(True, name='xyz')
    assert toggle.name == 'xyz'
    assert repr(toggle) == "<Toggle: xyz: on>"



================================================
FILE: tests/primitives/test_togglesets.py
================================================
import asyncio

import pytest

from kopf._cogs.aiokits.aiotoggles import Toggle, ToggleSet


@pytest.mark.parametrize('fn, expected', [(all, True), (any, False)])
async def test_created_empty(fn, expected):
    toggleset = ToggleSet(fn)
    assert len(toggleset) == 0
    assert set(toggleset) == set()
    assert Toggle() not in toggleset
    assert toggleset.is_on() == expected
    assert toggleset.is_off() == (not expected)


@pytest.mark.parametrize('fn', [all, any])
async def test_making_a_default_toggle(fn):
    toggleset = ToggleSet(fn)
    toggle = await toggleset.make_toggle()
    assert len(toggleset) == 1
    assert set(toggleset) == {toggle}
    assert toggle in toggleset
    assert Toggle() not in toggleset
    assert toggleset.is_on() == False
    assert toggleset.is_off() == True


@pytest.mark.parametrize('fn', [all, any])
async def test_making_a_turned_off_toggle(fn):
    toggleset = ToggleSet(fn)
    toggle = await toggleset.make_toggle(False)
    assert len(toggleset) == 1
    assert set(toggleset) == {toggle}
    assert toggle in toggleset
    assert Toggle() not in toggleset
    assert toggleset.is_on() == False
    assert toggleset.is_off() == True


@pytest.mark.parametrize('fn', [all, any])
async def test_making_a_turned_on_toggle(fn):
    toggleset = ToggleSet(fn)
    toggle = await toggleset.make_toggle(True)
    assert len(toggleset) == 1
    assert set(toggleset) == {toggle}
    assert toggle in toggleset
    assert Toggle() not in toggleset
    assert toggleset.is_on() == True
    assert toggleset.is_off() == False


@pytest.mark.parametrize('fn, expected', [(all, True), (any, False)])
async def test_dropping_a_turned_off_toggle(fn, expected):
    toggleset = ToggleSet(fn)
    toggle = await toggleset.make_toggle(False)
    await toggle.turn_to(True)
    await toggleset.drop_toggle(toggle)
    assert len(toggleset) == 0
    assert set(toggleset) == set()
    assert toggle not in toggleset
    assert toggleset.is_on() == expected
    assert toggleset.is_off() == (not expected)


@pytest.mark.parametrize('fn, expected', [(all, True), (any, False)])
async def test_dropping_a_turned_on_toggle(fn, expected):
    toggleset = ToggleSet(fn)
    toggle = await toggleset.make_toggle(True)
    await toggleset.drop_toggle(toggle)
    assert len(toggleset) == 0
    assert set(toggleset) == set()
    assert toggle not in toggleset
    assert toggleset.is_on() == expected
    assert toggleset.is_off() == (not expected)


@pytest.mark.parametrize('fn, expected', [(all, True), (any, False)])
async def test_dropping_an_unexistent_toggle(fn, expected):
    toggleset = ToggleSet(fn)
    toggle = Toggle()
    await toggleset.drop_toggle(toggle)
    assert len(toggleset) == 0
    assert set(toggleset) == set()
    assert toggle not in toggleset
    assert toggleset.is_on() == expected
    assert toggleset.is_off() == (not expected)


@pytest.mark.parametrize('fn, expected', [(all, True), (any, False)])
async def test_dropping_multiple_toggles(fn, expected):
    toggleset = ToggleSet(fn)
    toggle1 = await toggleset.make_toggle(True)
    toggle2 = Toggle()
    await toggleset.drop_toggles([toggle1, toggle2])
    assert len(toggleset) == 0
    assert set(toggleset) == set()
    assert toggle1 not in toggleset
    assert toggle2 not in toggleset
    assert toggleset.is_on() == expected
    assert toggleset.is_off() == (not expected)


@pytest.mark.parametrize('fn', [all, any])
async def test_turning_a_toggle_on_turns_the_toggleset_on(fn):
    toggleset = ToggleSet(fn)
    toggle = await toggleset.make_toggle(False)
    assert toggleset.is_on() == False
    assert toggleset.is_off() == True

    await toggle.turn_to(True)
    assert toggleset.is_on() == True
    assert toggleset.is_off() == False


@pytest.mark.parametrize('fn', [all, any])
async def test_turning_a_toggle_off_turns_the_toggleset_off(fn):
    toggleset = ToggleSet(fn)
    toggle = await toggleset.make_toggle(True)
    assert toggleset.is_on() == True
    assert toggleset.is_off() == False

    await toggle.turn_to(False)
    assert toggleset.is_on() == False
    assert toggleset.is_off() == True


@pytest.mark.parametrize('fn', [all])
async def test_all_toggles_must_be_on_for_alltoggleset_to_be_on(fn):
    toggleset = ToggleSet(fn)
    toggle1 = await toggleset.make_toggle(False)
    toggle2 = await toggleset.make_toggle(False)
    assert toggleset.is_on() == False
    assert toggleset.is_off() == True

    await toggle1.turn_to(True)
    assert toggleset.is_on() == False
    assert toggleset.is_off() == True

    await toggle2.turn_to(True)
    assert toggleset.is_on() == True
    assert toggleset.is_off() == False


@pytest.mark.parametrize('fn', [all])
async def test_any_toggle_must_be_off_for_alltoggleset_to_be_off(fn):
    toggleset = ToggleSet(fn)
    toggle1 = await toggleset.make_toggle(True)
    toggle2 = await toggleset.make_toggle(True)
    assert toggleset.is_on() == True
    assert toggleset.is_off() == False

    await toggle1.turn_to(False)
    assert toggleset.is_on() == False
    assert toggleset.is_off() == True

    await toggle2.turn_to(False)
    assert toggleset.is_on() == False
    assert toggleset.is_off() == True


@pytest.mark.parametrize('fn', [any])
async def test_any_toggle_must_be_on_for_anytoggleset_to_be_on(fn):
    toggleset = ToggleSet(fn)
    toggle1 = await toggleset.make_toggle(False)
    toggle2 = await toggleset.make_toggle(False)
    assert toggleset.is_on() == False
    assert toggleset.is_off() == True

    await toggle1.turn_to(True)
    assert toggleset.is_on() == True
    assert toggleset.is_off() == False

    await toggle2.turn_to(True)
    assert toggleset.is_on() == True
    assert toggleset.is_off() == False


@pytest.mark.parametrize('fn', [any])
async def test_all_toggles_must_be_off_for_anytoggleset_to_be_off(fn):
    toggleset = ToggleSet(fn)
    toggle1 = await toggleset.make_toggle(True)
    toggle2 = await toggleset.make_toggle(True)
    assert toggleset.is_on() == True
    assert toggleset.is_off() == False

    await toggle1.turn_to(False)
    assert toggleset.is_on() == True
    assert toggleset.is_off() == False

    await toggle2.turn_to(False)
    assert toggleset.is_on() == False
    assert toggleset.is_off() == True


@pytest.mark.parametrize('fn', [all, any])
async def test_waiting_until_on_fails_when_not_turned_on(fn):
    toggleset = ToggleSet(fn)
    await toggleset.make_toggle(False)
    with pytest.raises(asyncio.TimeoutError):
        await asyncio.wait_for(toggleset.wait_for(True), timeout=0.1)
    assert toggleset.is_off()


@pytest.mark.parametrize('fn', [all, any])
async def test_waiting_until_off_fails_when_not_turned_off(fn):
    toggleset = ToggleSet(fn)
    await toggleset.make_toggle(True)
    with pytest.raises(asyncio.TimeoutError):
        await asyncio.wait_for(toggleset.wait_for(False), timeout=0.1)
    assert toggleset.is_on()


@pytest.mark.parametrize('fn', [all, any])
async def test_waiting_until_on_wakes_when_turned_on(fn, timer):
    toggleset = ToggleSet(fn)
    toggle = await toggleset.make_toggle(False)

    async def delayed_turning_on(delay: float):
        await asyncio.sleep(delay)
        await toggle.turn_to(True)

    with timer:
        asyncio.create_task(delayed_turning_on(0.05))
        await asyncio.wait_for(toggleset.wait_for(True), timeout=1.0)

    assert toggleset.is_on()
    assert timer.seconds < 0.5  # approx. 0.05 plus some code overhead


@pytest.mark.parametrize('fn', [all, any])
async def test_waiting_until_off_wakes_when_turned_off(fn, timer):
    toggleset = ToggleSet(fn)
    toggle = await toggleset.make_toggle(True)

    async def delayed_turning_off(delay: float):
        await asyncio.sleep(delay)
        await toggle.turn_to(False)

    with timer:
        asyncio.create_task(delayed_turning_off(0.05))
        await asyncio.wait_for(toggleset.wait_for(False), timeout=1.0)

    assert toggleset.is_off()
    assert timer.seconds < 0.5  # approx. 0.05 plus some code overhead


@pytest.mark.parametrize('fn', [all, any])
async def test_secures_against_usage_as_a_boolean(fn):
    toggle = ToggleSet(fn)
    with pytest.raises(NotImplementedError):
        bool(toggle)


@pytest.mark.parametrize('fn', [all, any])
async def test_repr_when_empty(fn):
    toggleset = ToggleSet(fn)
    assert repr(toggleset) == "set()"


@pytest.mark.parametrize('fn', [all, any])
async def test_repr_when_unnamed_and_off(fn):
    toggleset = ToggleSet(fn)
    await toggleset.make_toggle(False)
    assert repr(toggleset) == "{<Toggle: off>}"


@pytest.mark.parametrize('fn', [all, any])
async def test_repr_when_unnamed_and_on(fn):
    toggleset = ToggleSet(fn)
    await toggleset.make_toggle(True)
    assert repr(toggleset) == "{<Toggle: on>}"


@pytest.mark.parametrize('fn', [all, any])
async def test_repr_when_named_and_off(fn):
    toggleset = ToggleSet(fn)
    await toggleset.make_toggle(False, name='xyz')
    assert repr(toggleset) == "{<Toggle: xyz: off>}"


@pytest.mark.parametrize('fn', [all, any])
async def test_repr_when_named_and_on(fn):
    toggleset = ToggleSet(fn)
    await toggleset.make_toggle(True, name='xyz')
    assert repr(toggleset) == "{<Toggle: xyz: on>}"



================================================
FILE: tests/reactor/conftest.py
================================================
import asyncio
import functools
from unittest.mock import AsyncMock

import pytest

from kopf._cogs.clients.watching import infinite_watch
from kopf._core.reactor.queueing import watcher, worker as original_worker


@pytest.fixture(autouse=True)
def _autouse_resp_mocker(resp_mocker):
    pass


@pytest.fixture()
def processor():
    """ A mock for processor -- to be checked if the handler has been called. """
    return AsyncMock()


@pytest.fixture()
def worker_spy(mocker):
    """ Spy on the watcher: actually call it, but provide the mock-fields. """
    spy = AsyncMock(spec=original_worker, wraps=original_worker)
    return mocker.patch('kopf._core.reactor.queueing.worker', spy)


@pytest.fixture()
def worker_mock(mocker):
    """ Prevent the queue consumption, so that the queues could be checked. """
    return mocker.patch('kopf._core.reactor.queueing.worker')


@pytest.fixture()
def watcher_limited(mocker, settings):
    """ Make event streaming finite, watcher exits after depletion. """
    settings.watching.reconnect_backoff = 0
    mocker.patch('kopf._cogs.clients.watching.infinite_watch',
                 new=functools.partial(infinite_watch, _iterations=1))


@pytest.fixture()
async def watcher_in_background(settings, resource, worker_spy, stream):

    # Prevent remembering the streaming objects in the mocks.
    async def do_nothing(*args, **kwargs):
        pass

    # Prevent any real streaming for the very beginning, before it even starts.
    stream.feed([])

    # Spawn a watcher in the background.
    coro = watcher(
        namespace=None,
        resource=resource,
        settings=settings,
        processor=do_nothing,
    )
    task = asyncio.create_task(coro)

    try:
        # Go for a test.
        yield task
    finally:
        # Terminate the watcher to cleanup the loop.
        task.cancel()
        try:
            await task
        except asyncio.CancelledError:
            pass  # cancellations are expected at this point



================================================
FILE: tests/reactor/test_patching_inconsistencies.py
================================================
import logging

import aiohttp.web
import pytest

from kopf._cogs.structs.bodies import Body
from kopf._cogs.structs.patches import Patch
from kopf._core.actions.application import patch_and_check
from kopf._core.actions.loggers import LocalObjectLogger

# Assume that the underlying patch_obj() is already tested with/without status as a sub-resource.
# Assume that the underlying diff() is already tested with left/right/full scopes and all values.
# Test ONLY the logging/warning on patch-vs-response inconsistencies here.


@pytest.mark.parametrize('patch, response', [

    pytest.param({'spec': {'x': 'y'}, 'status': {'s': 't'}},
                 {'spec': {'x': 'y'}, 'status': {'s': 't'}},
                 id='response-exact'),

    pytest.param({'spec': {'x': 'y'}, 'status': {'s': 't'}},
                 {'spec': {'x': 'y'}, 'status': {'s': 't'}, 'metadata': '...'},
                 id='response-root-extra'),

    pytest.param({'spec': {'x': 'y'}, 'status': {'s': 't'}},
                 {'spec': {'x': 'y', 'extra': '...'}, 'status': {'s': 't'}},
                 id='response-spec-extra'),

    pytest.param({'spec': {'x': 'y'}, 'status': {'s': 't'}},
                 {'spec': {'x': 'y'}, 'status': {'s': 't', 'extra': '...'}},
                 id='response-status-extra'),

    pytest.param({'spec': {'x': None}, 'status': {'s': None}},
                 {'spec': {}, 'status': {}},
                 id='response-clean'),

    # False-positive inconsistencies for K8s-managed fields.
    pytest.param({'metadata': {'annotations': {}}}, {'metadata': {}}, id='false-annotations'),
    pytest.param({'metadata': {'finalizers': []}}, {'metadata': {}}, id='false-finalizers'),
    pytest.param({'metadata': {'labels': {}}}, {'metadata': {}}, id='false-labels'),

])
async def test_patching_without_inconsistencies(
        resource, namespace, settings, caplog, assert_logs, version_api,
        aresponses, hostname, resp_mocker,
        patch, response):
    caplog.set_level(logging.DEBUG)

    url = resource.get_url(namespace=namespace, name='name1')
    patch_mock = resp_mocker(return_value=aiohttp.web.json_response(response))
    aresponses.add(hostname, url, 'patch', patch_mock)

    body = Body({'metadata': {'namespace': namespace, 'name': 'name1'}})
    logger = LocalObjectLogger(body=body, settings=settings)
    await patch_and_check(
        settings=settings,
        resource=resource,
        body=body,
        patch=Patch(patch),
        logger=logger,
    )

    assert_logs([
        "Patching with:",
    ], prohibited=[
        "Patching failed with inconsistencies:",
    ])


@pytest.mark.parametrize('patch, response', [

    pytest.param({'spec': {'x': 'y'}, 'status': {'s': 't'}},
                 {},
                 id='response-empty'),

    pytest.param({'spec': {'x': 'y'}, 'status': {'s': 't'}},
                 {'spec': {'x': 'y'}},
                 id='response-status-lost'),

    pytest.param({'spec': {'x': 'y'}, 'status': {'s': 't'}},
                 {'status': {'s': 't'}},
                 id='response-spec-lost'),

    pytest.param({'spec': {'x': 'y'}, 'status': {'s': 't'}},
                 {'spec': {'x': 'not-y'}, 'status': {'s': 't'}},
                 id='response-spec-altered'),

    pytest.param({'spec': {'x': 'y'}, 'status': {'s': 't'}},
                 {'spec': {'x': 'y'}, 'status': {'s': 'not-t'}},
                 id='response-status-altered'),

    pytest.param({'spec': {'x': None}, 'status': {'s': None}},
                 {'spec': {'x': 'y'}, 'status': {}},
                 id='response-spec-undeleted'),

    pytest.param({'spec': {'x': None}, 'status': {'s': None}},
                 {'spec': {}, 'status': {'s': 't'}},
                 id='response-status-undeleted'),

    # True-positive inconsistencies for K8s-managed fields with possible false-positives.
    pytest.param({'metadata': {'annotations': {'x': 'y'}}}, {'metadata': {}}, id='true-annotations'),
    pytest.param({'metadata': {'finalizers': ['x', 'y']}}, {'metadata': {}}, id='true-finalizers'),
    pytest.param({'metadata': {'labels': {'x': 'y'}}}, {'metadata': {}}, id='true-labels'),

])
async def test_patching_with_inconsistencies(
        resource, namespace, settings, caplog, assert_logs, version_api,
        aresponses, hostname, resp_mocker,
        patch, response):
    caplog.set_level(logging.DEBUG)

    url = resource.get_url(namespace=namespace, name='name1')
    patch_mock = resp_mocker(return_value=aiohttp.web.json_response(response))
    aresponses.add(hostname, url, 'patch', patch_mock)

    body = Body({'metadata': {'namespace': namespace, 'name': 'name1'}})
    logger = LocalObjectLogger(body=body, settings=settings)
    await patch_and_check(
        settings=settings,
        resource=resource,
        body=body,
        patch=Patch(patch),
        logger=logger,
    )

    assert_logs([
        "Patching with:",
        "Patching failed with inconsistencies:",
    ])


async def test_patching_with_disappearance(
        resource, namespace, settings, caplog, assert_logs, version_api,
        aresponses, hostname, resp_mocker):
    caplog.set_level(logging.DEBUG)

    patch = {'spec': {'x': 'y'}, 'status': {'s': 't'}}  # irrelevant
    url = resource.get_url(namespace=namespace, name='name1')
    patch_mock = resp_mocker(return_value=aresponses.Response(status=404, reason='oops'))
    aresponses.add(hostname, url, 'patch', patch_mock)

    body = Body({'metadata': {'namespace': namespace, 'name': 'name1'}})
    logger = LocalObjectLogger(body=body, settings=settings)
    await patch_and_check(
        settings=settings,
        resource=resource,
        body=body,
        patch=Patch(patch),
        logger=logger,
    )

    assert_logs([
        "Patching with:",
        "Patching was skipped: the object does not exist anymore",
    ], prohibited=[
        "inconsistencies"
    ])



================================================
FILE: tests/reactor/test_queueing.py
================================================
"""
Only the tests from the watching (simulated) to the handling (substituted).

Excluded: the watching-streaming routines
(see ``tests_streaming.py`` and ``test_watching_*.py``).

Excluded: the causation and handling routines
(to be done later).

Used for internal control that the event queueing works are intended.
If the intentions change, the tests should be rewritten.
They are NOT part of the public interface of the framework.

NOTE: These tests also check that the bookmarks are ignored
by checking that they are not multiplexed into workers.
"""
import asyncio
import contextlib
import gc
import weakref

import pytest

from kopf._core.reactor.queueing import EOS, watcher


@pytest.mark.parametrize('uids, cnts, events', [

    pytest.param(['uid1'], [1], [
        {'type': 'ADDED', 'object': {'metadata': {'uid': 'uid1'}}},
    ], id='single'),

    pytest.param(['uid1'], [3], [
        {'type': 'ADDED', 'object': {'metadata': {'uid': 'uid1'}}},
        {'type': 'MODIFIED', 'object': {'metadata': {'uid': 'uid1'}}},
        {'type': 'DELETED', 'object': {'metadata': {'uid': 'uid1'}}},
    ], id='multiple'),

    pytest.param(['uid1', 'uid2'], [3, 2], [
        {'type': 'ADDED', 'object': {'metadata': {'uid': 'uid1'}}},
        {'type': 'ADDED', 'object': {'metadata': {'uid': 'uid2'}}},
        {'type': 'MODIFIED', 'object': {'metadata': {'uid': 'uid1'}}},
        {'type': 'MODIFIED', 'object': {'metadata': {'uid': 'uid2'}}},
        {'type': 'DELETED', 'object': {'metadata': {'uid': 'uid1'}}},
    ], id='mixed'),

])
@pytest.mark.usefixtures('watcher_limited')
async def test_watchevent_demultiplexing(worker_mock, timer, resource, processor,
                                         settings, stream, events, uids, cnts):
    """ Verify that every unique uid goes into its own queue+worker, which are never shared. """

    # Override the default timeouts to make the tests faster.
    settings.batching.idle_timeout = 100  # should not be involved, fail if it is
    settings.batching.exit_timeout = 100  # should exit instantly, fail if it didn't
    settings.batching.batch_window = 100  # should not be involved, fail if it is

    # Inject the events of unique objects - to produce a few streams/workers.
    stream.feed(events)
    stream.close()

    # Run the watcher (near-instantly and test-blocking).
    with timer:
        await watcher(
            namespace=None,
            resource=resource,
            settings=settings,
            processor=processor,
        )

    # Extra-check: verify that the real workers were not involved:
    # they would do batching, which is absent in the mocked workers.
    assert timer.seconds < settings.batching.batch_window

    # The processor must not be called by the watcher, only by the worker.
    # But the worker (even if mocked) must be called & awaited by the watcher.
    assert processor.call_count == 0
    assert processor.await_count == 0
    assert worker_mock.await_count > 0

    # Are the worker-streams created by the watcher? Populated as expected?
    # One stream per unique uid? All events are sequential? EOS marker appended?
    assert worker_mock.call_count == len(uids)
    assert worker_mock.call_count == len(cnts)
    for uid, cnt, (args, kwargs) in zip(uids, cnts, worker_mock.call_args_list):
        key = kwargs['key']
        streams = kwargs['streams']
        assert kwargs['processor'] is processor
        assert key == (resource, uid)
        assert key in streams

        queue_events = []
        while not streams[key].backlog.empty():
            queue_events.append(streams[key].backlog.get_nowait())

        assert len(queue_events) == cnt + 1
        assert queue_events[-1] is EOS.token
        assert all(queue_event['object']['metadata']['uid'] == uid
                   for queue_event in queue_events[:-1])


@pytest.mark.parametrize('uids, vals, events', [

    pytest.param(['uid1'], ['b'], [
        {'type': 'MODIFIED', 'object': {'metadata': {'uid': 'uid1'}, 'spec': 'a'}},
        {'type': 'MODIFIED', 'object': {'metadata': {'uid': 'uid1'}, 'spec': 'b'}},
    ], id='the same'),

    pytest.param(['uid1', 'uid2'], ['a', 'b'], [
        {'type': 'MODIFIED', 'object': {'metadata': {'uid': 'uid1'}, 'spec': 'a'}},
        {'type': 'MODIFIED', 'object': {'metadata': {'uid': 'uid2'}, 'spec': 'b'}},
    ], id='distinct'),

    pytest.param(['uid1', 'uid2', 'uid3'], ['e', 'd', 'f'], [
        {'type': 'ADDED', 'object': {'metadata': {'uid': 'uid1'}, 'spec': 'a'}},
        {'type': 'ADDED', 'object': {'metadata': {'uid': 'uid2'}, 'spec': 'b'}},
        {'type': 'MODIFIED', 'object': {'metadata': {'uid': 'uid1'}, 'spec': 'c'}},
        {'type': 'MODIFIED', 'object': {'metadata': {'uid': 'uid2'}, 'spec': 'd'}},
        {'type': 'DELETED', 'object': {'metadata': {'uid': 'uid1'}, 'spec': 'e'}},
        {'type': 'DELETED', 'object': {'metadata': {'uid': 'uid3'}, 'spec': 'f'}},
    ], id='mixed'),

])
@pytest.mark.usefixtures('watcher_limited')
async def test_watchevent_batching(settings, resource, processor, timer,
                                   stream, events, uids, vals):
    """ Verify that only the last event per uid is actually handled. """

    # Override the default timeouts to make the tests faster.
    settings.batching.idle_timeout = 100  # should not be involved, fail if it is
    settings.batching.exit_timeout = 100  # should exit instantly, fail if it didn't
    settings.batching.batch_window = 0.3  # the time period being tested (make bigger than overhead)

    # Inject the events of unique objects - to produce a few streams/workers.
    stream.feed(events)
    stream.close()

    # Run the watcher (near-instantly and test-blocking).
    with timer:
        await watcher(
            namespace=None,
            resource=resource,
            settings=settings,
            processor=processor,
        )

    # Should be batched strictly once (never twice). Note: multiple uids run concurrently,
    # so they all are batched in parallel, and the timing remains the same.
    assert timer.seconds > settings.batching.batch_window * 1
    assert timer.seconds < settings.batching.batch_window * 2

    # Was the processor called at all? Awaited as needed for async fns?
    assert processor.await_count > 0

    # Was it called only once per uid? Only with the latest event?
    # Note: the calls can be in arbitrary order, not as we expect then.
    assert processor.call_count == len(uids)
    assert processor.call_count == len(vals)
    expected_uid_val_pairs = set(zip(uids, vals))
    actual_uid_val_pairs = {(
            kwargs['raw_event']['object']['metadata']['uid'],
            kwargs['raw_event']['object']['spec'])
            for args, kwargs in processor.call_args_list}
    assert actual_uid_val_pairs == expected_uid_val_pairs


@pytest.mark.parametrize('unique, events', [

    pytest.param(1, [
        {'type': 'ADDED', 'object': {'metadata': {'uid': 'uid1'}}},
        {'type': 'MODIFIED', 'object': {'metadata': {'uid': 'uid1'}}},
        {'type': 'DELETED', 'object': {'metadata': {'uid': 'uid1'}}},
    ], id='the same'),

    pytest.param(2, [
        {'type': 'ADDED', 'object': {'metadata': {'uid': 'uid1'}}},
        {'type': 'ADDED', 'object': {'metadata': {'uid': 'uid2'}}},
    ], id='distinct'),

])
@pytest.mark.usefixtures('watcher_in_background')
async def test_garbage_collection_of_streams(settings, stream, events, unique, worker_spy):

    # Override the default timeouts to make the tests faster.
    settings.batching.exit_timeout = 100  # should exit instantly, fail if it didn't
    settings.batching.idle_timeout = .05  # finish workers faster, but not as fast as batching
    settings.batching.batch_window = .01  # minimize the effects of batching (not our interest)
    settings.watching.reconnect_backoff = 1.0  # to prevent src depletion

    # Inject the events of unique objects - to produce a few streams/workers.
    stream.feed(events)
    stream.close()

    # Give it a moment to populate the streams and spawn all the workers.
    # Intercept and remember _any_ seen dict of streams for further checks.
    while worker_spy.call_count < unique:
        await asyncio.sleep(0.001)  # give control to the loop
    streams = worker_spy.call_args_list[-1][1]['streams']
    signaller: asyncio.Condition = worker_spy.call_args_list[0][1]['signaller']

    # The mutable(!) streams dict is now populated with the objects' streams.
    assert len(streams) != 0  # usually 1, but can be 2+ if it is fast enough.

    # Weakly remember the stream's content to make sure it is gc'ed later.
    # Note: namedtuples are not referable due to __slots__/__weakref__ issues.
    refs = [weakref.ref(val) for wstream in streams.values() for val in wstream]
    assert all([ref() is not None for ref in refs])

    # Give the workers some time to finish waiting for the events.
    # After the idle timeout is reached, they will exit and gc their streams.
    allowed_timeout = (
        settings.batching.batch_window +  # depleting the queues.
        settings.batching.idle_timeout +  # idling on empty queues.
        1.0)  # the code itself takes time: add a max tolerable delay.
    with contextlib.suppress(asyncio.TimeoutError):
        async with signaller:
            await asyncio.wait_for(signaller.wait_for(lambda: not streams), timeout=allowed_timeout)

    # The mutable(!) streams dict is now empty, i.e. garbage-collected.
    assert len(streams) == 0

    # Let the workers to actually exit and gc their local scopes with variables.
    # The jobs can take a tiny moment more, but this is noticeable in the tests.
    await asyncio.sleep(0.1)

    # For PyPy: force the gc! (GC can be delayed in PyPy, unlike in CPython.)
    # https://doc.pypy.org/en/latest/cpython_differences.html#differences-related-to-garbage-collection-strategies
    gc.collect()

    # Truly garbage-collected? Memory freed?
    assert all([ref() is None for ref in refs])


# TODO: also add tests for the depletion of the workers pools on cancellation (+timing)



================================================
FILE: tests/reactor/test_uids.py
================================================
from kopf._core.reactor.queueing import get_uid


def test_uid_is_used_if_present():
    raw_event = {'type': ..., 'object': {'metadata': {'uid': '123'}}}
    uid = get_uid(raw_event)

    assert isinstance(uid, str)
    assert uid == '123'


def test_uid_is_simulated_if_absent():
    raw_event = {'type': ...,
                 'object': {
                     'apiVersion': 'group/v1',
                     'kind': 'Kind1',
                     'metadata': {
                         'name': 'name1',
                         'namespace': 'namespace1',
                         'creationTimestamp': 'created1',
                     }}}
    uid = get_uid(raw_event)

    # The exact order is irrelevant.
    assert isinstance(uid, str)
    assert 'created1' in uid
    assert 'name1' in uid
    assert 'namespace' in uid
    assert 'Kind1' in uid
    assert 'group/v1' in uid



================================================
FILE: tests/references/test_backbone.py
================================================
import asyncio

import pytest

from kopf._cogs.structs.references import CLUSTER_PEERINGS_K, CLUSTER_PEERINGS_Z, CRDS, EVENTS, \
                                          NAMESPACED_PEERINGS_K, NAMESPACED_PEERINGS_Z, \
                                          NAMESPACES, Backbone, Resource, Selector


@pytest.mark.parametrize('selector', [
    CRDS, EVENTS, NAMESPACES,
    CLUSTER_PEERINGS_K, NAMESPACED_PEERINGS_K,
    CLUSTER_PEERINGS_Z, NAMESPACED_PEERINGS_Z,
])
def test_empty_backbone(selector: Selector):
    backbone = Backbone()
    assert len(backbone) == 0
    assert set(backbone) == set()
    with pytest.raises(KeyError):
        assert backbone[selector]


@pytest.mark.parametrize('selector, resource', [
    (CRDS, Resource('apiextensions.k8s.io', 'v1beta1', 'customresourcedefinitions')),
    (CRDS, Resource('apiextensions.k8s.io', 'v1', 'customresourcedefinitions')),
    (CRDS, Resource('apiextensions.k8s.io', 'vX', 'customresourcedefinitions')),
    (EVENTS, Resource('', 'v1', 'events')),
    (NAMESPACES, Resource('', 'v1', 'namespaces')),
    (CLUSTER_PEERINGS_K, Resource('kopf.dev', 'v1', 'clusterkopfpeerings')),
    (NAMESPACED_PEERINGS_K, Resource('kopf.dev', 'v1', 'kopfpeerings')),
    (CLUSTER_PEERINGS_Z, Resource('zalando.org', 'v1', 'clusterkopfpeerings')),
    (NAMESPACED_PEERINGS_Z, Resource('zalando.org', 'v1', 'kopfpeerings')),
])
async def test_refill_populates_the_resources(selector: Selector, resource: Resource):
    backbone = Backbone()
    await backbone.fill(resources=[resource])
    assert len(backbone) == 1
    assert set(backbone) == {selector}
    assert backbone[selector] == resource


async def test_refill_is_cumulative_ie_does_not_reset():
    backbone = Backbone()
    await backbone.fill(resources=[Resource('', 'v1', 'namespaces')])
    await backbone.fill(resources=[Resource('', 'v1', 'events')])
    assert len(backbone) == 2
    assert set(backbone) == {NAMESPACES, EVENTS}


async def test_waiting_for_absent_resources_never_ends(timer):
    backbone = Backbone()
    with pytest.raises(asyncio.TimeoutError):
        await asyncio.wait_for(backbone.wait_for(NAMESPACES), timeout=0.1)


async def test_waiting_for_preexisting_resources_ends_instantly(timer):
    resource = Resource('', 'v1', 'namespaces')
    backbone = Backbone()
    await backbone.fill(resources=[resource])
    with timer:
        found_resource = await backbone.wait_for(NAMESPACES)
    assert timer.seconds < 0.1
    assert found_resource == resource


async def test_waiting_for_delayed_resources_ends_once_delivered(timer):
    resource = Resource('', 'v1', 'namespaces')
    backbone = Backbone()

    async def delayed_injection(delay: float):
        await asyncio.sleep(delay)
        await backbone.fill(resources=[resource])

    task = asyncio.create_task(delayed_injection(0.1))
    with timer:
        found_resource = await backbone.wait_for(NAMESPACES)
    await task
    assert 0.1 < timer.seconds < 0.11
    assert found_resource == resource



================================================
FILE: tests/references/test_namespace_matching.py
================================================
import re

import pytest

from kopf._cogs.structs.references import match_namespace


@pytest.mark.parametrize('name, pattern, expected', [
    ('ns1', 'ns1', True),
    ('ns1', 'ns', False),
    ('ns1', '', False),
])
def test_exact_values(name, pattern, expected):
    result = match_namespace(name=name, pattern=pattern)
    assert result == expected


@pytest.mark.parametrize('name, pattern, expected', [
    ('ns1', '*', True),
    ('ns1', 'ns*', True),
    ('ns1', 'ns?', True),
    ('ns1', 'ns??', False),
    ('ns1', 'n*1', True),
    ('ns1', 'n?1', True),
    ('ns1', 'n??1', False),
    ('ns1', '*1', True),
    ('ns1', '?1', False),
    ('ns1', '??1', True),
    ('ns1', 'ns*x', False),
])
def test_glob_patterns(name, pattern, expected):
    result = match_namespace(name=name, pattern=pattern)
    assert result == expected


@pytest.mark.parametrize('name, pattern, expected', [
    ('ns1', 'ns1,ns2', True),
    ('ns1', 'ns2,ns1', False),  # the follow-up patterns are for re-inclusion only
])
def test_multiple_patterns(name, pattern, expected):
    result = match_namespace(name=name, pattern=pattern)
    assert result == expected


@pytest.mark.parametrize('name, pattern, expected', [
    ('ns1', ' ns1', True),
    ('ns1', 'ns1 ', True),
    ('ns1', ' ns1 ', True),
    ('ns1', ' ns1 , ns2 ', True),
    ('ns1', ' ns2 , ns1 ', False),  # the follow-up patterns are for re-inclusion only
])
def test_spaces_are_ignored(name, pattern, expected):
    result = match_namespace(name=name, pattern=pattern)
    assert result == expected


@pytest.mark.parametrize('name, pattern, expected', [
    ('ns1', 'ns*,!ns1', False),
    ('ns1', 'ns*,!ns2', True),
    ('ns1', 'ns*,!ns2', True),
    ('ns1', 'ns*,!ns2', True),
])
def test_exclusions_after_inclusion(name, pattern, expected):
    result = match_namespace(name=name, pattern=pattern)
    assert result == expected


@pytest.mark.parametrize('name, pattern, expected', [
    ('ns1', '!ns1', False),
    ('ns1', '!ns2', True),
    ('ns1', '!ns*', False),
])
def test_initial_exclusion_implies_catchall_inclusion(name, pattern, expected):
    result = match_namespace(name=name, pattern=pattern)
    assert result == expected


@pytest.mark.parametrize('name, pattern, expected', [
    ('ns1', 'ns*,!ns1,ns1', True),
    ('ns1', 'ns*,!ns*,ns1', True),
    ('ns1', '!ns*,ns1', True),
    ('ns1', '!ns*,ns?', True),
])
def test_reinclusion_after_exclusion(name, pattern, expected):
    result = match_namespace(name=name, pattern=pattern)
    assert result == expected


@pytest.mark.parametrize('name, pattern, expected', [
    ('ns1', re.compile(r'ns1'), True),
    ('ns1', re.compile(r'ns'), False),
    ('ns1', re.compile(r's1'), False),
    ('ns1', re.compile(r'.+s1'), True),
    ('ns1', re.compile(r'ns.+'), True),

])
def test_regexps_with_full_matching(name, pattern, expected):
    result = match_namespace(name=name, pattern=pattern)
    assert result == expected



================================================
FILE: tests/references/test_namespace_selection.py
================================================
import re

from kopf._cogs.structs.references import select_specific_namespaces


def test_empty_pattern_list():
    names = select_specific_namespaces([])
    assert not names


def test_included_empty_string():
    names = select_specific_namespaces([''])
    assert names == {''}


def test_included_exact_strings():
    names = select_specific_namespaces(['ns2', 'ns1'])
    assert names == {'ns1', 'ns2'}


def test_excluded_multipatterns():
    names = select_specific_namespaces(['ns1,ns2'])
    assert not names


def test_excluded_globs():
    names = select_specific_namespaces(['n*s', 'n?s'])
    assert not names


def test_excluded_regexps():
    names = select_specific_namespaces([re.compile(r'ns1')])
    assert not names



================================================
FILE: tests/references/test_selector_matching.py
================================================
import pytest

from kopf._cogs.structs.references import EVERYTHING, Resource, Selector


@pytest.fixture()
def resource():
    return Resource(
        group='group1', version='version1', preferred=True,
        plural='plural1', singular='singular1', kind='kind1',
        shortcuts=['shortcut1', 'shortcut2'],
        categories=['category1', 'category2'],
    )


@pytest.fixture()
def v1_resource():
    return Resource(
        group='', version='v1', preferred=True,
        plural='plural1', singular='singular1', kind='kind1',
        shortcuts=['shortcut1', 'shortcut2'],
        categories=['category1', 'category2'],
    )


@pytest.mark.parametrize('group, version', [
    (None, None),
    ('group1', None),
    (None, 'version1'),
    ('group1', 'version1'),
])
def test_when_matches_everything(resource, group, version):
    selector = Selector(EVERYTHING, group=group, version=version)
    matches = selector.check(resource)
    assert matches


@pytest.mark.parametrize('kwarg, kwval', [
    ('kind', 'kind1'),
    ('plural', 'plural1'),
    ('singular', 'singular1'),
    ('shortcut', 'shortcut1'),
    ('shortcut', 'shortcut2'),
    ('category', 'category1'),
    ('category', 'category2'),
    ('any_name', 'kind1'),
    ('any_name', 'plural1'),
    ('any_name', 'singular1'),
    ('any_name', 'shortcut1'),
    ('any_name', 'shortcut2'),
])
@pytest.mark.parametrize('group, version', [
    (None, None),
    ('group1', None),
    (None, 'version1'),
    ('group1', 'version1'),
])
def test_when_matches_names(resource, kwarg, kwval, group, version):
    selector = Selector(group=group, version=version, **{kwarg: kwval})
    matches = selector.check(resource)
    assert matches


@pytest.mark.parametrize('kwarg, kwval', [
    ('kind', 'kind1'),
    ('plural', 'plural1'),
    ('singular', 'singular1'),
    ('shortcut', 'shortcut1'),
    ('shortcut', 'shortcut2'),
    ('category', 'category1'),
    ('category', 'category2'),
    ('any_name', 'kind1'),
    ('any_name', 'plural1'),
    ('any_name', 'singular1'),
    ('any_name', 'shortcut1'),
    ('any_name', 'shortcut2'),
])
@pytest.mark.parametrize('group, version', [
    ('group9', None),
    (None, 'version9'),
    ('group1', 'version9'),
    ('group9', 'version1'),
    ('group9', 'version9'),
])
def test_when_groupversion_mismatch_but_names_do_match(resource, kwarg, kwval, group, version):
    selector = Selector(group=group, version=version, **{kwarg: kwval})
    matches = selector.check(resource)
    assert not matches


@pytest.mark.parametrize('kwarg, kwval', [
    ('kind', 'kind9'),
    ('plural', 'plural9'),
    ('singular', 'singular9'),
    ('shortcut', 'shortcut9'),
    ('category', 'category9'),
    ('any_name', 'category9'),
    ('any_name', 'category1'),  # categories are not used with any_name, must be explicit.
    ('any_name', 'category2'),  # categories are not used with any_name, must be explicit.
])
@pytest.mark.parametrize('group, version', [
    (None, None),
    ('group1', None),
    (None, 'version1'),
    ('group1', 'version1'),
])
def test_when_groupversion_do_match_but_names_mismatch(resource, kwarg, kwval, group, version):
    selector = Selector(group=group, version=version, **{kwarg: kwval})
    matches = selector.check(resource)
    assert not matches


def test_catchall_versions_are_ignored_for_nonpreferred_resources():
    resource = Resource(
        group='group1', version='version1', preferred=False,
        plural='plural1', singular='singular1', kind='kind1',
        shortcuts=['shortcut1', 'shortcut2'],
        categories=['category1', 'category2'],
    )
    selector = Selector(EVERYTHING)
    matches = selector.check(resource)
    assert not matches


@pytest.mark.parametrize('selector_args', [
    pytest.param(['events'], id='only-name'),
    pytest.param(['v1', 'events'], id='with-version'),
    pytest.param(['', 'v1', 'events'], id='with-groupversion'),
])
def test_events_are_matched_when_explicitly_named(selector_args):
    resource = Resource('', 'v1', 'events')
    selector = Selector(*selector_args)
    matches = selector.check(resource)
    assert matches


@pytest.mark.parametrize('selector_args', [
    pytest.param([EVERYTHING], id='only-marker'),
    pytest.param(['v1', EVERYTHING], id='with-core-version'),
    pytest.param(['', 'v1', EVERYTHING], id='with-core-groupversion'),
    pytest.param(['events.k8s.io', EVERYTHING], id='with-k8sio-group'),
    pytest.param(['events.k8s.io', 'v1beta1', EVERYTHING], id='with-k8sio-groupversion'),
])
@pytest.mark.parametrize('resource_kwargs', [
    pytest.param(dict(group='', version='v1'), id='core-v1'),
    pytest.param(dict(group='events.k8s.io', version='v1'), id='k8sio-v1'),
    pytest.param(dict(group='events.k8s.io', version='v1beta1'), id='k8sio-v1beta1'),
])
def test_events_are_excluded_from_everything(resource_kwargs, selector_args):
    resource = Resource(**resource_kwargs, plural='events')
    selector = Selector(*selector_args)
    matches = selector.check(resource)
    assert not matches


@pytest.mark.parametrize('kwarg, kwval', [
    ('kind', 'kind1'),
    ('plural', 'plural1'),
    ('singular', 'singular1'),
    ('shortcut', 'shortcut1'),
    ('shortcut', 'shortcut2'),
    ('any_name', 'kind1'),
    ('any_name', 'plural1'),
    ('any_name', 'singular1'),
    ('any_name', 'shortcut1'),
    ('any_name', 'shortcut2'),
])
def test_selection_of_specific_resources(resource, kwarg, kwval):
    selector = Selector(**{kwarg: kwval})
    selected = selector.select([resource])
    assert selector.is_specific  # prerequisite
    assert selected == {resource}


@pytest.mark.parametrize('kwarg, kwval', [
    ('category', 'category1'),
    ('category', 'category2'),
    ('any_name', EVERYTHING),
])
def test_selection_of_nonspecific_resources(resource, kwarg, kwval):
    selector = Selector(**{kwarg: kwval})
    selected = selector.select([resource])
    assert not selector.is_specific  # prerequisite
    assert selected == {resource}


@pytest.mark.parametrize('kwarg, kwval', [
    ('kind', 'kind1'),
    ('plural', 'plural1'),
    ('singular', 'singular1'),
    ('shortcut', 'shortcut1'),
    ('shortcut', 'shortcut2'),
    ('any_name', 'kind1'),
    ('any_name', 'plural1'),
    ('any_name', 'singular1'),
    ('any_name', 'shortcut1'),
    ('any_name', 'shortcut2'),
])
def test_precedence_of_corev1_over_others_when_specific(resource, v1_resource, kwarg, kwval):
    selector = Selector(**{kwarg: kwval})
    selected = selector.select([resource, v1_resource])
    assert selector.is_specific  # prerequisite
    assert selected == {v1_resource}


@pytest.mark.parametrize('kwarg, kwval', [
    ('category', 'category1'),
    ('category', 'category2'),
    ('any_name', EVERYTHING),
])
def test_precedence_of_corev1_same_as_others_when_nonspecific(resource, v1_resource, kwarg, kwval):
    selector = Selector(**{kwarg: kwval})
    selected = selector.select([resource, v1_resource])
    assert not selector.is_specific  # prerequisite
    assert selected == {resource, v1_resource}



================================================
FILE: tests/references/test_selector_parsing.py
================================================
import pytest

from kopf._cogs.structs.references import EVERYTHING, Selector


def test_no_args():
    with pytest.raises(TypeError) as err:
        Selector()
    assert "Unspecific resource with no names" in str(err.value)


def test_no_name_but_group():
    with pytest.raises(TypeError) as err:
        Selector(group='group1')
    assert "Unspecific resource with no names" in str(err.value)


def test_no_name_but_version():
    with pytest.raises(TypeError) as err:
        Selector(version='version1')
    assert "Unspecific resource with no names" in str(err.value)


@pytest.mark.parametrize('name', ['name1', EVERYTHING])
def test_one_arg_with_only_name(name):
    selector = Selector(name)
    assert selector.group is None
    assert selector.version is None
    assert selector.kind is None
    assert selector.plural is None
    assert selector.singular is None
    assert selector.shortcut is None
    assert selector.category is None
    assert selector.any_name == name


@pytest.mark.parametrize('group', ['group1', 'group1.example.com', 'v1nonconventional'])
def test_one_arg_with_group(group):
    selector = Selector(f'name1.{group}')
    assert selector.group == group
    assert selector.version is None
    assert selector.kind is None
    assert selector.plural is None
    assert selector.singular is None
    assert selector.shortcut is None
    assert selector.category is None
    assert selector.any_name == 'name1'


@pytest.mark.parametrize('version', ['v1', 'v99', 'v99beta99', 'v99alpha99'])
@pytest.mark.parametrize('group', ['group1', 'group1.example.com', 'v1nonconventional'])
def test_one_arg_with_group_and_conventional_version(version, group):
    selector = Selector(f'name1.{version}.{group}')
    assert selector.group == group
    assert selector.version == version
    assert selector.kind is None
    assert selector.plural is None
    assert selector.singular is None
    assert selector.shortcut is None
    assert selector.category is None
    assert selector.any_name == 'name1'


@pytest.mark.parametrize('name', ['name1', EVERYTHING])
@pytest.mark.parametrize('group', ['group1', 'group1.example.com', 'v1nonconventional'])
def test_two_args_with_only_group(group, name):
    selector = Selector(f'{group}', name)
    assert selector.group == group
    assert selector.version is None
    assert selector.kind is None
    assert selector.plural is None
    assert selector.singular is None
    assert selector.shortcut is None
    assert selector.category is None
    assert selector.any_name == name


@pytest.mark.parametrize('name', ['name1', EVERYTHING])
@pytest.mark.parametrize('version', ['v1', 'v99', 'v99beta99', 'v99alpha99'])
@pytest.mark.parametrize('group', ['group1', 'group1.example.com', 'v1nonconventional'])
def test_two_args_with_group_and_conventional_version(version, group, name):
    selector = Selector(f'{group}/{version}', name)
    assert selector.group == group
    assert selector.version == version
    assert selector.kind is None
    assert selector.plural is None
    assert selector.singular is None
    assert selector.shortcut is None
    assert selector.category is None
    assert selector.any_name == name


@pytest.mark.parametrize('name', ['name1', EVERYTHING])
def test_two_args_and_corev1(name):
    selector = Selector('v1', name)
    assert selector.group == ''
    assert selector.version == 'v1'
    assert selector.kind is None
    assert selector.plural is None
    assert selector.singular is None
    assert selector.shortcut is None
    assert selector.category is None
    assert selector.any_name == name


@pytest.mark.parametrize('name', ['name1', EVERYTHING])
@pytest.mark.parametrize('version', ['v1', 'v99', 'v99beta99', 'v99alpha99'])
@pytest.mark.parametrize('group', ['group1', 'group1.example.com', 'v1nonconventional'])
def test_three_args(group, version, name):
    selector = Selector(group, version, name)
    assert selector.group == group
    assert selector.version == version
    assert selector.kind is None
    assert selector.plural is None
    assert selector.singular is None
    assert selector.shortcut is None
    assert selector.category is None
    assert selector.any_name == name


def test_too_many_args():
    with pytest.raises(TypeError) as err:
        Selector('group1', 'version1', 'name1', 'etc')
    assert "Too many positional arguments" in str(err.value)


def test_kwarg_group():
    selector = Selector(group='group1', any_name='whatever')
    assert selector.group == 'group1'
    assert selector.version is None
    assert selector.kind is None
    assert selector.plural is None
    assert selector.singular is None
    assert selector.shortcut is None
    assert selector.category is None
    assert selector.any_name == 'whatever'


def test_kwarg_version():
    selector = Selector(version='version1', any_name='whatever')
    assert selector.group is None
    assert selector.version == 'version1'
    assert selector.kind is None
    assert selector.plural is None
    assert selector.singular is None
    assert selector.shortcut is None
    assert selector.category is None
    assert selector.any_name == 'whatever'


def test_kwarg_kind():
    selector = Selector(kind='name1')
    assert selector.group is None
    assert selector.version is None
    assert selector.kind == 'name1'
    assert selector.plural is None
    assert selector.singular is None
    assert selector.shortcut is None
    assert selector.category is None
    assert selector.any_name is None


def test_kwarg_plural():
    selector = Selector(plural='name1')
    assert selector.group is None
    assert selector.version is None
    assert selector.kind is None
    assert selector.plural == 'name1'
    assert selector.singular is None
    assert selector.shortcut is None
    assert selector.category is None
    assert selector.any_name is None


def test_kwarg_singular():
    selector = Selector(singular='name1')
    assert selector.group is None
    assert selector.version is None
    assert selector.kind is None
    assert selector.plural is None
    assert selector.singular == 'name1'
    assert selector.shortcut is None
    assert selector.category is None
    assert selector.any_name is None


def test_kwarg_shortcut():
    selector = Selector(shortcut='name1')
    assert selector.group is None
    assert selector.version is None
    assert selector.kind is None
    assert selector.plural is None
    assert selector.singular is None
    assert selector.shortcut == 'name1'
    assert selector.category is None
    assert selector.any_name is None


def test_kwarg_category():
    selector = Selector(category='name1')
    assert selector.group is None
    assert selector.version is None
    assert selector.kind is None
    assert selector.plural is None
    assert selector.singular is None
    assert selector.shortcut is None
    assert selector.category == 'name1'
    assert selector.any_name is None


def test_kwarg_any_name():
    selector = Selector(any_name='name1')
    assert selector.group is None
    assert selector.version is None
    assert selector.kind is None
    assert selector.plural is None
    assert selector.singular is None
    assert selector.shortcut is None
    assert selector.category is None
    assert selector.any_name == 'name1'


@pytest.mark.parametrize('kwargs', [
    {kwarg1: 'name1', kwarg2: 'name2'}
    for kwarg1 in ['kind', 'plural', 'singular', 'shortcut', 'category', 'any_name']
    for kwarg2 in ['kind', 'plural', 'singular', 'shortcut', 'category', 'any_name']
    if kwarg1 != kwarg2
])
def test_conflicting_names_with_only_kwargs(kwargs):
    with pytest.raises(TypeError) as err:
        Selector(**kwargs)
    assert "Ambiguous resource specification with names" in str(err.value)


@pytest.mark.parametrize('kwarg', ['kind', 'plural', 'singular', 'shortcut', 'category'])
def test_conflicting_name_with_positional_name(kwarg):
    with pytest.raises(TypeError) as err:
        Selector('name1', **{kwarg: 'name2'})
    assert "Ambiguous resource specification with names" in str(err.value)


@pytest.mark.parametrize('kwarg', ['kind', 'plural', 'singular', 'shortcut', 'category'])
def test_empty_names_are_prohibited_with_kwargs(kwarg):
    with pytest.raises(TypeError) as err:
        Selector(**{kwarg: ''})
    assert "Names must not be empty strings" in str(err.value)


def test_empty_names_are_prohibited_with_positionals():
    with pytest.raises(TypeError) as err:
        Selector('')
    assert "Names must not be empty strings" in str(err.value)



================================================
FILE: tests/references/test_selector_properties.py
================================================
import pytest

from kopf._cogs.structs.references import EVERYTHING, Selector


@pytest.mark.parametrize('kw', ['kind', 'plural', 'singular', 'shortcut', 'category', 'any_name'])
def test_repr_with_names(kw):
    selector = Selector(**{kw: 'name1'})
    selector_repr = repr(selector)
    assert selector_repr == f"Selector({kw}='name1')"


@pytest.mark.parametrize('group', ['group1', 'group1.example.com', 'v1nonconventional'])
def test_repr_with_group(group):
    selector = Selector(group=group, any_name='name1')
    selector_repr = repr(selector)
    assert selector_repr == f"Selector(group='{group}', any_name='name1')"


@pytest.mark.parametrize('kw', ['kind', 'plural', 'singular', 'shortcut', 'any_name'])
def test_is_specific_with_names(kw):
    selector = Selector(**{kw: 'name1'})
    assert selector.is_specific


@pytest.mark.parametrize('kw', ['category'])
def test_is_not_specific_with_categories(kw):
    selector = Selector(**{kw: 'name1'})
    assert not selector.is_specific


@pytest.mark.parametrize('kw', ['category'])
def test_is_not_specific_with_categories(kw):
    selector = Selector(EVERYTHING)
    assert not selector.is_specific


@pytest.mark.parametrize('kw', ['category'])
def test_is_not_specific_with_categories(kw):
    selector = Selector(**{kw: 'name1'})
    assert not selector.is_specific



================================================
FILE: tests/registries/conftest.py
================================================
import logging

import pytest

from kopf._cogs.structs.bodies import Body
from kopf._cogs.structs.diffs import Diff, DiffItem
from kopf._cogs.structs.ephemera import Memo
from kopf._cogs.structs.ids import HandlerId
from kopf._cogs.structs.patches import Patch
from kopf._core.engines.indexing import OperatorIndexers
from kopf._core.intents.causes import ActivityCause, ChangingCause, ResourceCause, \
                                      SpawningCause, WatchingCause, WebhookCause
from kopf._core.intents.handlers import ChangingHandler
from kopf._core.intents.registries import ActivityRegistry, ChangingRegistry, OperatorRegistry, \
                                          ResourceRegistry, SpawningRegistry, WatchingRegistry, \
                                          WebhooksRegistry


@pytest.fixture(params=[
    pytest.param(ActivityRegistry, id='activity-registry'),
    pytest.param(WatchingRegistry, id='watching-registry'),
    pytest.param(ChangingRegistry, id='changing-registry'),
])
def generic_registry_cls(request):
    return request.param


@pytest.fixture(params=[
    pytest.param(ActivityRegistry, id='activity-registry'),
])
def activity_registry_cls(request):
    return request.param


@pytest.fixture(params=[
    pytest.param(WatchingRegistry, id='watching-registry'),
    pytest.param(ChangingRegistry, id='changing-registry'),
])
def resource_registry_cls(request):
    return request.param


@pytest.fixture(params=[
    pytest.param(OperatorRegistry, id='operator-registry'),
])
def operator_registry_cls(request):
    return request.param


@pytest.fixture()
def parent_handler(selector):

    def parent_fn(**_):
        pass

    return ChangingHandler(
        fn=parent_fn, id=HandlerId('parent_fn'), param=None,
        errors=None, retries=None, timeout=None, backoff=None,
        selector=selector, labels=None, annotations=None, when=None,
        field=None, value=None, old=None, new=None, field_needs_change=None,
        initial=None, deleted=None, requires_finalizer=None,
        reason=None,
    )


@pytest.fixture()
def cause_factory(resource):
    """
    A simplified factory of causes.

    It assumes most of the parameters to be unused defaults, which is sufficient
    for testing. Some parameters are of improper types (e.g. Nones), others are
    converted from built-in types to proper types, the rest are passed as is.

    The cause class is selected based on the passed ``cls``, which is either
    directly the cause class to use, or the registry class. For the latter
    case, the best matching cause class is used (hard-coded mapping). Classes
    are used here as equivalents of enums, in order not to create actual enums.

    All is done for simplicity of testing. This factory is not supposed to be
    used outside of Kopf's own tests, is not packaged, is not delivered, and
    is not available to the users.
    """
    def make_cause(
            cls=ChangingCause,
            *,
            resource=resource,
            event=None,
            type=None,
            body=None,
            diff=(),
            old=None,
            new=None,
            reason='some-reason',
            initial=False,
            activity=None,
            settings=None,
    ):
        if cls is ActivityCause or cls is ActivityRegistry:
            return ActivityCause(
                memo=Memo(),
                logger=logging.getLogger('kopf.test.fake.logger'),
                indices=OperatorIndexers().indices,
                activity=activity,
                settings=settings,
            )
        if cls is ResourceCause or cls is ResourceRegistry:
            return ResourceCause(
                logger=logging.getLogger('kopf.test.fake.logger'),
                indices=OperatorIndexers().indices,
                resource=resource,
                patch=Patch(),
                memo=Memo(),
                body=Body(body if body is not None else {}),
            )
        if cls is WatchingCause or cls is WatchingRegistry:
            return WatchingCause(
                logger=logging.getLogger('kopf.test.fake.logger'),
                indices=OperatorIndexers().indices,
                resource=resource,
                patch=Patch(),
                memo=Memo(),
                body=Body(body if body is not None else {}),
                type=type,
                event=event,
            )
        if cls is ChangingCause or cls is ChangingRegistry:
            return ChangingCause(
                logger=logging.getLogger('kopf.test.fake.logger'),
                indices=OperatorIndexers().indices,
                resource=resource,
                patch=Patch(),
                memo=Memo(),
                body=Body(body if body is not None else {}),
                diff=Diff(DiffItem(*d) for d in diff),
                old=old,
                new=new,
                initial=initial,
                reason=reason,
            )
        if cls is SpawningCause or cls is SpawningRegistry:
            return SpawningCause(
                logger=logging.getLogger('kopf.test.fake.logger'),
                indices=OperatorIndexers().indices,
                resource=resource,
                patch=Patch(),
                memo=Memo(),
                body=Body(body if body is not None else {}),
                reset=False,
            )
        if cls is WebhookCause or cls is WebhooksRegistry:
            return WebhookCause(
                logger=logging.getLogger('kopf.test.fake.logger'),
                indices=OperatorIndexers().indices,
                resource=resource,
                patch=Patch(),
                memo=Memo(),
                body=Body(body if body is not None else {}),
                dryrun=False,
                sslpeer={},
                headers={},
                userinfo={},
                warnings=[],
                reason=None,
                webhook=None,
                operation=None,
                subresource=None,
            )
        raise TypeError(f"Cause/registry type {cls} is not supported by this fixture.")
    return make_cause



================================================
FILE: tests/registries/test_creation.py
================================================
from kopf._core.intents.registries import ActivityRegistry, OperatorRegistry, ResourceRegistry


def test_activity_registry(activity_registry_cls):
    registry = activity_registry_cls()
    assert isinstance(registry, ActivityRegistry)


def test_resource_registry(resource_registry_cls):
    registry = resource_registry_cls()
    assert isinstance(registry, ResourceRegistry)


def test_operator_registry(operator_registry_cls):
    registry = operator_registry_cls()
    assert isinstance(registry, OperatorRegistry)
    assert not isinstance(registry, ResourceRegistry)



================================================
FILE: tests/registries/test_decorators.py
================================================
import pytest

import kopf
from kopf._core.actions.execution import ErrorsMode, handler_var
from kopf._core.actions.invocation import context
from kopf._core.intents.causes import HANDLER_REASONS, Activity, Reason, WebhookCause
from kopf._core.intents.registries import ChangingRegistry, OperatorRegistry
from kopf._core.reactor.subhandling import subregistry_var


def test_on_startup_minimal():
    registry = kopf.get_default_registry()

    @kopf.on.startup()
    def fn(**_):
        pass

    handlers = registry._activities.get_handlers(activity=Activity.STARTUP)
    assert len(handlers) == 1
    assert handlers[0].fn is fn
    assert handlers[0].activity == Activity.STARTUP
    assert handlers[0].errors is None
    assert handlers[0].timeout is None
    assert handlers[0].retries is None
    assert handlers[0].backoff is None


def test_on_cleanup_minimal():
    registry = kopf.get_default_registry()

    @kopf.on.cleanup()
    def fn(**_):
        pass

    handlers = registry._activities.get_handlers(activity=Activity.CLEANUP)
    assert len(handlers) == 1
    assert handlers[0].fn is fn
    assert handlers[0].activity == Activity.CLEANUP
    assert handlers[0].errors is None
    assert handlers[0].timeout is None
    assert handlers[0].retries is None
    assert handlers[0].backoff is None


def test_on_probe_minimal():
    registry = kopf.get_default_registry()

    @kopf.on.probe()
    def fn(**_):
        pass

    handlers = registry._activities.get_handlers(activity=Activity.PROBE)
    assert len(handlers) == 1
    assert handlers[0].fn is fn
    assert handlers[0].activity == Activity.PROBE
    assert handlers[0].errors is None
    assert handlers[0].timeout is None
    assert handlers[0].retries is None
    assert handlers[0].backoff is None


# Resume handlers are mixed-in into all resource-changing reactions with initial listing.
@pytest.mark.parametrize('reason', HANDLER_REASONS)
def test_on_resume_minimal(reason, cause_factory, resource):
    registry = kopf.get_default_registry()
    cause = cause_factory(resource=resource, reason=reason, initial=True)

    @kopf.on.resume(*resource)
    def fn(**_):
        pass

    handlers = registry._changing.get_handlers(cause)
    assert len(handlers) == 1
    assert handlers[0].fn is fn
    assert handlers[0].reason is None
    assert handlers[0].field is None
    assert handlers[0].errors is None
    assert handlers[0].timeout is None
    assert handlers[0].retries is None
    assert handlers[0].backoff is None
    assert handlers[0].labels is None
    assert handlers[0].annotations is None
    assert handlers[0].when is None
    assert handlers[0].field is None
    assert handlers[0].old is None
    assert handlers[0].new is None


def test_on_create_minimal(cause_factory, resource):
    registry = kopf.get_default_registry()
    cause = cause_factory(resource=resource, reason=Reason.CREATE)

    @kopf.on.create(*resource)
    def fn(**_):
        pass

    handlers = registry._changing.get_handlers(cause)
    assert len(handlers) == 1
    assert handlers[0].fn is fn
    assert handlers[0].reason == Reason.CREATE
    assert handlers[0].field is None
    assert handlers[0].errors is None
    assert handlers[0].timeout is None
    assert handlers[0].retries is None
    assert handlers[0].backoff is None
    assert handlers[0].labels is None
    assert handlers[0].annotations is None
    assert handlers[0].when is None
    assert handlers[0].field is None
    assert handlers[0].old is None
    assert handlers[0].new is None


def test_on_update_minimal(cause_factory, resource):
    registry = kopf.get_default_registry()
    cause = cause_factory(resource=resource, reason=Reason.UPDATE)

    @kopf.on.update(*resource)
    def fn(**_):
        pass

    handlers = registry._changing.get_handlers(cause)
    assert len(handlers) == 1
    assert handlers[0].fn is fn
    assert handlers[0].reason == Reason.UPDATE
    assert handlers[0].field is None
    assert handlers[0].errors is None
    assert handlers[0].timeout is None
    assert handlers[0].retries is None
    assert handlers[0].backoff is None
    assert handlers[0].labels is None
    assert handlers[0].annotations is None
    assert handlers[0].when is None
    assert handlers[0].field is None
    assert handlers[0].old is None
    assert handlers[0].new is None


def test_on_delete_minimal(cause_factory, resource):
    registry = kopf.get_default_registry()
    cause = cause_factory(resource=resource, reason=Reason.DELETE)

    @kopf.on.delete(*resource)
    def fn(**_):
        pass

    handlers = registry._changing.get_handlers(cause)
    assert len(handlers) == 1
    assert handlers[0].fn is fn
    assert handlers[0].reason == Reason.DELETE
    assert handlers[0].field is None
    assert handlers[0].errors is None
    assert handlers[0].timeout is None
    assert handlers[0].retries is None
    assert handlers[0].backoff is None
    assert handlers[0].labels is None
    assert handlers[0].annotations is None
    assert handlers[0].when is None
    assert handlers[0].field is None
    assert handlers[0].old is None
    assert handlers[0].new is None


def test_on_field_minimal(cause_factory, resource):
    registry = kopf.get_default_registry()
    old = {'field': {'subfield': 'old'}}
    new = {'field': {'subfield': 'new'}}
    cause = cause_factory(resource=resource, reason=Reason.UPDATE, old=old, new=new, body=new)

    @kopf.on.field(*resource, field='field.subfield')
    def fn(**_):
        pass

    handlers = registry._changing.get_handlers(cause)
    assert len(handlers) == 1
    assert handlers[0].fn is fn
    assert handlers[0].reason is None
    assert handlers[0].errors is None
    assert handlers[0].timeout is None
    assert handlers[0].retries is None
    assert handlers[0].backoff is None
    assert handlers[0].labels is None
    assert handlers[0].annotations is None
    assert handlers[0].when is None
    assert handlers[0].field == ('field', 'subfield')
    assert handlers[0].value is None
    assert handlers[0].old is None
    assert handlers[0].new is None


def test_on_field_fails_without_field(resource):
    with pytest.raises(TypeError):
        @kopf.on.field(*resource)
        def fn(**_):
            pass


def test_on_startup_with_all_kwargs():
    registry = OperatorRegistry()

    @kopf.on.startup(
        id='id', registry=registry,
        errors=ErrorsMode.PERMANENT, timeout=123, retries=456, backoff=78)
    def fn(**_):
        pass

    handlers = registry._activities.get_handlers(activity=Activity.STARTUP)
    assert len(handlers) == 1
    assert handlers[0].fn is fn
    assert handlers[0].activity == Activity.STARTUP
    assert handlers[0].id == 'id'
    assert handlers[0].errors == ErrorsMode.PERMANENT
    assert handlers[0].timeout == 123
    assert handlers[0].retries == 456
    assert handlers[0].backoff == 78


def test_on_cleanup_with_all_kwargs():
    registry = OperatorRegistry()

    @kopf.on.cleanup(
        id='id', registry=registry,
        errors=ErrorsMode.PERMANENT, timeout=123, retries=456, backoff=78)
    def fn(**_):
        pass

    handlers = registry._activities.get_handlers(activity=Activity.CLEANUP)
    assert len(handlers) == 1
    assert handlers[0].fn is fn
    assert handlers[0].activity == Activity.CLEANUP
    assert handlers[0].id == 'id'
    assert handlers[0].errors == ErrorsMode.PERMANENT
    assert handlers[0].timeout == 123
    assert handlers[0].retries == 456
    assert handlers[0].backoff == 78


def test_on_probe_with_all_kwargs():
    registry = OperatorRegistry()

    @kopf.on.probe(
        id='id', registry=registry,
        errors=ErrorsMode.PERMANENT, timeout=123, retries=456, backoff=78)
    def fn(**_):
        pass

    handlers = registry._activities.get_handlers(activity=Activity.PROBE)
    assert len(handlers) == 1
    assert handlers[0].fn is fn
    assert handlers[0].activity == Activity.PROBE
    assert handlers[0].id == 'id'
    assert handlers[0].errors == ErrorsMode.PERMANENT
    assert handlers[0].timeout == 123
    assert handlers[0].retries == 456
    assert handlers[0].backoff == 78


# Resume handlers are mixed-in into all resource-changing reactions with initial listing.
@pytest.mark.parametrize('reason', HANDLER_REASONS)
def test_on_resume_with_most_kwargs(mocker, reason, cause_factory, resource):
    registry = OperatorRegistry()
    cause = cause_factory(resource=resource, reason=reason, initial=True)
    mocker.patch('kopf._core.intents.registries.match', return_value=True)

    when = lambda **_: False

    @kopf.on.resume(*resource,
                    id='id', registry=registry,
                    errors=ErrorsMode.PERMANENT, timeout=123, retries=456, backoff=78,
                    deleted=True,
                    field='field.subfield', value=999,
                    labels={'somelabel': 'somevalue'},
                    annotations={'someanno': 'somevalue'},
                    when=when)
    def fn(**_):
        pass

    handlers = registry._changing.get_handlers(cause)
    assert len(handlers) == 1
    assert handlers[0].fn is fn
    assert handlers[0].reason is None
    assert handlers[0].id == 'id/field.subfield'
    assert handlers[0].errors == ErrorsMode.PERMANENT
    assert handlers[0].timeout == 123
    assert handlers[0].retries == 456
    assert handlers[0].backoff == 78
    assert handlers[0].deleted == True
    assert handlers[0].labels == {'somelabel': 'somevalue'}
    assert handlers[0].annotations == {'someanno': 'somevalue'}
    assert handlers[0].when == when
    assert handlers[0].field == ('field', 'subfield')
    assert handlers[0].value == 999
    assert handlers[0].old is None
    assert handlers[0].new is None


def test_on_create_with_most_kwargs(mocker, cause_factory, resource):
    registry = OperatorRegistry()
    cause = cause_factory(resource=resource, reason=Reason.CREATE)
    mocker.patch('kopf._core.intents.registries.match', return_value=True)

    when = lambda **_: False

    @kopf.on.create(*resource,
                    id='id', registry=registry,
                    errors=ErrorsMode.PERMANENT, timeout=123, retries=456, backoff=78,
                    field='field.subfield', value=999,
                    labels={'somelabel': 'somevalue'},
                    annotations={'someanno': 'somevalue'},
                    when=when)
    def fn(**_):
        pass

    handlers = registry._changing.get_handlers(cause)
    assert len(handlers) == 1
    assert handlers[0].fn is fn
    assert handlers[0].reason == Reason.CREATE
    assert handlers[0].id == 'id/field.subfield'
    assert handlers[0].errors == ErrorsMode.PERMANENT
    assert handlers[0].timeout == 123
    assert handlers[0].retries == 456
    assert handlers[0].backoff == 78
    assert handlers[0].labels == {'somelabel': 'somevalue'}
    assert handlers[0].annotations == {'someanno': 'somevalue'}
    assert handlers[0].when == when
    assert handlers[0].field == ('field', 'subfield')
    assert handlers[0].value == 999
    assert handlers[0].old is None
    assert handlers[0].new is None


def test_on_update_with_most_kwargs(mocker, cause_factory, resource):
    registry = OperatorRegistry()
    cause = cause_factory(resource=resource, reason=Reason.UPDATE)
    mocker.patch('kopf._core.intents.registries.match', return_value=True)

    when = lambda **_: False

    @kopf.on.update(*resource,
                    id='id', registry=registry,
                    errors=ErrorsMode.PERMANENT, timeout=123, retries=456, backoff=78,
                    field='field.subfield', value=999,
                    labels={'somelabel': 'somevalue'},
                    annotations={'someanno': 'somevalue'},
                    when=when)
    def fn(**_):
        pass

    handlers = registry._changing.get_handlers(cause)
    assert len(handlers) == 1
    assert handlers[0].fn is fn
    assert handlers[0].reason == Reason.UPDATE
    assert handlers[0].id == 'id/field.subfield'
    assert handlers[0].errors == ErrorsMode.PERMANENT
    assert handlers[0].timeout == 123
    assert handlers[0].retries == 456
    assert handlers[0].backoff == 78
    assert handlers[0].labels == {'somelabel': 'somevalue'}
    assert handlers[0].annotations == {'someanno': 'somevalue'}
    assert handlers[0].when == when
    assert handlers[0].field == ('field', 'subfield')
    assert handlers[0].value == 999
    assert handlers[0].old is None
    assert handlers[0].new is None


@pytest.mark.parametrize('optional', [
    pytest.param(True, id='optional'),
    pytest.param(False, id='mandatory'),
])
def test_on_delete_with_most_kwargs(mocker, cause_factory, optional, resource):
    registry = OperatorRegistry()
    cause = cause_factory(resource=resource, reason=Reason.DELETE)
    mocker.patch('kopf._core.intents.registries.match', return_value=True)

    when = lambda **_: False

    @kopf.on.delete(*resource,
                    id='id', registry=registry,
                    errors=ErrorsMode.PERMANENT, timeout=123, retries=456, backoff=78,
                    optional=optional,
                    field='field.subfield', value=999,
                    labels={'somelabel': 'somevalue'},
                    annotations={'someanno': 'somevalue'},
                    when=when)
    def fn(**_):
        pass

    handlers = registry._changing.get_handlers(cause)
    assert len(handlers) == 1
    assert handlers[0].fn is fn
    assert handlers[0].reason == Reason.DELETE
    assert handlers[0].id == 'id/field.subfield'
    assert handlers[0].errors == ErrorsMode.PERMANENT
    assert handlers[0].timeout == 123
    assert handlers[0].retries == 456
    assert handlers[0].backoff == 78
    assert handlers[0].labels == {'somelabel': 'somevalue'}
    assert handlers[0].annotations == {'someanno': 'somevalue'}
    assert handlers[0].when == when
    assert handlers[0].field == ('field', 'subfield')
    assert handlers[0].value == 999
    assert handlers[0].old is None
    assert handlers[0].new is None


def test_on_field_with_most_kwargs(mocker, cause_factory, resource):
    registry = OperatorRegistry()
    old = {'field': {'subfield': 'old'}}
    new = {'field': {'subfield': 'new'}}
    cause = cause_factory(resource=resource, reason=Reason.UPDATE, old=old, new=new, body=new)
    mocker.patch('kopf._core.intents.registries.match', return_value=True)

    when = lambda **_: False

    @kopf.on.field(*resource,
                   id='id', registry=registry,
                   errors=ErrorsMode.PERMANENT, timeout=123, retries=456, backoff=78,
                   field='field.subfield', value=999,
                   labels={'somelabel': 'somevalue'},
                   annotations={'someanno': 'somevalue'},
                   when=when)
    def fn(**_):
        pass

    handlers = registry._changing.get_handlers(cause)
    assert len(handlers) == 1
    assert handlers[0].fn is fn
    assert handlers[0].reason is None
    assert handlers[0].id == 'id/field.subfield'
    assert handlers[0].errors == ErrorsMode.PERMANENT
    assert handlers[0].timeout == 123
    assert handlers[0].retries == 456
    assert handlers[0].backoff == 78
    assert handlers[0].labels == {'somelabel': 'somevalue'}
    assert handlers[0].annotations == {'someanno': 'somevalue'}
    assert handlers[0].when == when
    assert handlers[0].field == ('field', 'subfield')
    assert handlers[0].value == 999
    assert handlers[0].old is None
    assert handlers[0].new is None


def test_explicit_instance_methods(resource, cause_factory):
    registry = OperatorRegistry()
    cause = cause_factory(resource=resource, reason=Reason.CREATE)

    class TestCls:
        def fn(self, **_):
            pass

    obj1 = TestCls()
    kopf.on.create(*resource, registry=registry)(obj1.fn)

    handlers = registry._changing.get_handlers(cause)
    assert len(handlers) == 1
    assert handlers[0].fn == obj1.fn  # a bound method


def test_explicit_subclass_methods(resource, cause_factory):
    registry = OperatorRegistry()
    cause = cause_factory(resource=resource, reason=Reason.CREATE)

    class BaseCls:
        @classmethod
        def fn(cls, **_):
            pass

    class TestCls(BaseCls):
        pass

    kopf.on.create(*resource, registry=registry)(TestCls.fn)

    handlers = registry._changing.get_handlers(cause)
    assert len(handlers) == 1
    assert handlers[0].fn != BaseCls.fn  # an improperly bound method
    assert handlers[0].fn == TestCls.fn  # a properly bound method


def test_explicit_static_methods(resource, cause_factory):
    registry = OperatorRegistry()
    cause = cause_factory(resource=resource, reason=Reason.CREATE)

    class BaseCls:
        @staticmethod
        def fn(cls, **_):
            pass

    class TestCls(BaseCls):
        pass

    obj = TestCls()
    kopf.on.create(*resource, registry=registry)(obj.fn)

    handlers = registry._changing.get_handlers(cause)
    assert len(handlers) == 1
    assert handlers[0].fn == BaseCls.fn  # an improperly bound method
    assert handlers[0].fn == TestCls.fn  # a properly bound method
    assert handlers[0].fn == obj.fn  # a properly bound method


def test_subhandler_fails_with_no_parent_handler():

    registry = ChangingRegistry()
    subregistry_var.set(registry)

    # Check if the contextvar is indeed not set (as a prerequisite).
    with pytest.raises(LookupError):
        handler_var.get()

    # Check the actual behaviour of the decorator.
    with pytest.raises(LookupError):
        @kopf.subhandler()
        def fn(**_):
            pass


def test_subhandler_declaratively(parent_handler, cause_factory):
    cause = cause_factory(reason=Reason.UPDATE)

    registry = ChangingRegistry()
    subregistry_var.set(registry)

    with context([(handler_var, parent_handler)]):
        @kopf.subhandler()
        def fn(**_):
            pass

    handlers = registry.get_handlers(cause)
    assert len(handlers) == 1
    assert handlers[0].fn is fn


def test_subhandler_imperatively(parent_handler, cause_factory):
    cause = cause_factory(reason=Reason.UPDATE)

    registry = ChangingRegistry()
    subregistry_var.set(registry)

    def fn(**_):
        pass

    with context([(handler_var, parent_handler)]):
        kopf.register(fn)

    handlers = registry.get_handlers(cause)
    assert len(handlers) == 1
    assert handlers[0].fn is fn


@pytest.mark.parametrize('decorator, kwargs', [
    (kopf.index, {}),
    (kopf.on.event, {}),
    (kopf.on.mutate, {}),
    (kopf.on.validate, {}),
    (kopf.on.resume, {}),
    (kopf.on.create, {}),
    (kopf.on.update, {}),
    (kopf.on.delete, {}),
    (kopf.on.field, dict(field='x')),
])
def test_labels_filter_with_nones(resource, decorator, kwargs):

    with pytest.raises(ValueError):
        @decorator(*resource, **kwargs, labels={'x': None})
        def fn(**_):
            pass


@pytest.mark.parametrize('decorator, kwargs', [
    (kopf.index, {}),
    (kopf.on.event, {}),
    (kopf.on.mutate, {}),
    (kopf.on.validate, {}),
    (kopf.on.resume, {}),
    (kopf.on.create, {}),
    (kopf.on.update, {}),
    (kopf.on.delete, {}),
    (kopf.on.field, dict(field='f')),
])
def test_annotations_filter_with_nones(resource, decorator, kwargs):

    with pytest.raises(ValueError):
        @decorator(*resource, **kwargs, annotations={'x': None})
        def fn(**_):
            pass


@pytest.mark.parametrize('decorator, causeargs, handlers_prop', [
    pytest.param(kopf.index, dict(), '_indexing', id='on-index'),
    pytest.param(kopf.on.event, dict(), '_watching', id='on-event'),
    pytest.param(kopf.on.mutate, dict(cls=WebhookCause), '_webhooks', id='on-mutation'),
    pytest.param(kopf.on.validate, dict(cls=WebhookCause), '_webhooks', id='on-validation'),
    pytest.param(kopf.on.resume, dict(reason=None, initial=True), '_changing', id='on-resume'),
    pytest.param(kopf.on.create, dict(reason=Reason.CREATE), '_changing', id='on-create'),
    pytest.param(kopf.on.update, dict(reason=Reason.UPDATE), '_changing', id='on-update'),
    pytest.param(kopf.on.delete, dict(reason=Reason.DELETE), '_changing', id='on-delete'),
    pytest.param(kopf.on.field, dict(reason=Reason.UPDATE), '_changing', id='on-field'),
    pytest.param(kopf.daemon, dict(), '_spawning', id='on-daemon'),
    pytest.param(kopf.timer, dict(), '_spawning', id='on-timer'),
])
def test_field_with_value(mocker, cause_factory, decorator, causeargs, handlers_prop, resource, registry):
    old = {'field': {'subfield': 'old'}}
    new = {'field': {'subfield': 'new'}}
    cause = cause_factory(resource=resource, old=old, new=new, body=new, **causeargs)
    mocker.patch('kopf._core.intents.registries.match', return_value=True)

    @decorator(*resource, field='spec.field', value='value')
    def fn(**_):
        pass

    handlers_registry = getattr(registry, handlers_prop)
    handlers = handlers_registry.get_handlers(cause)
    assert len(handlers) == 1
    assert handlers[0].field == ('spec', 'field')
    assert handlers[0].value == 'value'


@pytest.mark.parametrize('decorator, causeargs, handlers_prop', [
    pytest.param(kopf.on.update, dict(reason=Reason.UPDATE), '_changing', id='on-update'),
    pytest.param(kopf.on.field, dict(reason=Reason.UPDATE), '_changing', id='on-field'),
])
def test_field_with_oldnew(mocker, cause_factory, decorator, causeargs, handlers_prop, resource, registry):
    cause = cause_factory(resource=resource, **causeargs)
    mocker.patch('kopf._core.intents.registries.match', return_value=True)

    @decorator(*resource, field='spec.field', old='old', new='new')
    def fn(**_):
        pass

    handlers_registry = getattr(registry, handlers_prop)
    handlers = handlers_registry.get_handlers(cause)
    assert len(handlers) == 1
    assert handlers[0].field == ('spec', 'field')
    assert handlers[0].value is None
    assert handlers[0].old == 'old'
    assert handlers[0].new == 'new'


@pytest.mark.parametrize('decorator', [
    pytest.param(kopf.index, id='on-index'),
    pytest.param(kopf.on.event, id='on-event'),
    pytest.param(kopf.on.mutate, id='on-mutation'),
    pytest.param(kopf.on.validate, id='on-validation'),
    pytest.param(kopf.on.resume, id='on-resume'),
    pytest.param(kopf.on.create, id='on-create'),
    pytest.param(kopf.on.update, id='on-update'),
    pytest.param(kopf.on.delete, id='on-delete'),
    pytest.param(kopf.daemon, id='on-daemon'),
    pytest.param(kopf.timer, id='on-timer'),
])
def test_missing_field_with_specified_value(resource, decorator):
    with pytest.raises(TypeError, match="without a mandatory field"):
        @decorator(*resource, value='v')
        def fn(**_):
            pass


@pytest.mark.parametrize('kwargs', [
    pytest.param(dict(field='f', value='v', old='x'), id='value-vs-old'),
    pytest.param(dict(field='f', value='v', new='x'), id='value-vs-new'),
])
@pytest.mark.parametrize('decorator', [
    pytest.param(kopf.on.update, id='on-update'),
    pytest.param(kopf.on.field, id='on-field'),
])
def test_conflicts_of_values_vs_oldnew(resource, decorator, kwargs):
    with pytest.raises(TypeError, match="Either value= or old=/new="):
        @decorator(*resource, **kwargs)
        def fn(**_):
            pass


@pytest.mark.parametrize('decorator', [
    pytest.param(kopf.on.resume, id='on-resume'),
    pytest.param(kopf.on.create, id='on-create'),
    pytest.param(kopf.on.delete, id='on-delete'),
])
def test_invalid_oldnew_for_inappropriate_subhandlers(resource, decorator, registry):

    @decorator(*resource)
    def fn(**_):
        @kopf.subhandler(field='f', old='x')
        def fn2(**_):
            pass

    subregistry = ChangingRegistry()
    handler = registry._changing.get_all_handlers()[0]
    with context([(handler_var, handler), (subregistry_var, subregistry)]):
        with pytest.raises(TypeError, match="can only be used in update handlers"):
            handler.fn()



================================================
FILE: tests/registries/test_default_registry.py
================================================
import kopf


def test_getting_default_registry():
    registry = kopf.get_default_registry()
    assert isinstance(registry, kopf.OperatorRegistry)


def test_setting_default_registry():
    registry_expected = kopf.OperatorRegistry()
    kopf.set_default_registry(registry_expected)
    registry_actual = kopf.get_default_registry()
    assert registry_actual is registry_expected



================================================
FILE: tests/registries/test_handler_getting.py
================================================
import collections.abc

import pytest

import kopf
from kopf._cogs.structs.references import Selector
from kopf._core.intents.causes import Activity, ChangingCause, WatchingCause


# Used in the tests. Must be global-scoped, or its qualname will be affected.
def some_fn():
    pass


def test_generic_registry_via_iter(
        generic_registry_cls, cause_factory):

    cause = cause_factory(generic_registry_cls)
    registry = generic_registry_cls()
    iterator = registry.iter_handlers(cause)

    assert isinstance(iterator, collections.abc.Iterator)
    assert not isinstance(iterator, collections.abc.Collection)
    assert not isinstance(iterator, collections.abc.Container)
    assert not isinstance(iterator, (list, tuple))

    handlers = list(iterator)
    assert not handlers


def test_generic_registry_via_list(
        generic_registry_cls, cause_factory):

    cause = cause_factory(generic_registry_cls)
    registry = generic_registry_cls()
    handlers = registry.get_handlers(cause)

    assert isinstance(handlers, collections.abc.Iterable)
    assert isinstance(handlers, collections.abc.Container)
    assert isinstance(handlers, collections.abc.Collection)
    assert not handlers


@pytest.mark.parametrize('activity', list(Activity))
def test_operator_registry_with_activity_via_iter(
        operator_registry_cls, activity):

    registry = operator_registry_cls()
    iterator = registry._activities.iter_handlers(activity=activity)

    assert isinstance(iterator, collections.abc.Iterator)
    assert not isinstance(iterator, collections.abc.Collection)
    assert not isinstance(iterator, collections.abc.Container)
    assert not isinstance(iterator, (list, tuple))

    handlers = list(iterator)
    assert not handlers


def test_operator_registry_watching_handlers_via_iter(
        operator_registry_cls, cause_factory):

    cause = cause_factory(WatchingCause)
    registry = operator_registry_cls()
    iterator = registry._watching.iter_handlers(cause)

    assert isinstance(iterator, collections.abc.Iterator)
    assert not isinstance(iterator, collections.abc.Collection)
    assert not isinstance(iterator, collections.abc.Container)
    assert not isinstance(iterator, (list, tuple))

    handlers = list(iterator)
    assert not handlers


def test_operator_registry_changing_handlers_via_iter(
        operator_registry_cls, cause_factory):

    cause = cause_factory(ChangingCause)
    registry = operator_registry_cls()
    iterator = registry._changing.iter_handlers(cause)

    assert isinstance(iterator, collections.abc.Iterator)
    assert not isinstance(iterator, collections.abc.Collection)
    assert not isinstance(iterator, collections.abc.Container)
    assert not isinstance(iterator, (list, tuple))

    handlers = list(iterator)
    assert not handlers


@pytest.mark.parametrize('activity', list(Activity))
def test_operator_registry_with_activity_via_list(
        operator_registry_cls, activity):

    registry = operator_registry_cls()
    handlers = registry._activities.get_handlers(activity=activity)

    assert isinstance(handlers, collections.abc.Iterable)
    assert isinstance(handlers, collections.abc.Container)
    assert isinstance(handlers, collections.abc.Collection)
    assert not handlers


def test_operator_registry_watching_handlers_via_list(
        operator_registry_cls, cause_factory):

    cause = cause_factory(WatchingCause)
    registry = operator_registry_cls()
    handlers = registry._watching.get_handlers(cause)

    assert isinstance(handlers, collections.abc.Iterable)
    assert isinstance(handlers, collections.abc.Container)
    assert isinstance(handlers, collections.abc.Collection)
    assert not handlers


def test_operator_registry_changing_handlers_via_list(
        operator_registry_cls, cause_factory):

    cause = cause_factory(ChangingCause)
    registry = operator_registry_cls()
    handlers = registry._changing.get_handlers(cause)

    assert isinstance(handlers, collections.abc.Iterable)
    assert isinstance(handlers, collections.abc.Container)
    assert isinstance(handlers, collections.abc.Collection)
    assert not handlers


def test_all_handlers(operator_registry_cls):
    registry = operator_registry_cls()

    @kopf.index('resource', registry=registry)
    @kopf.on.event('resource', registry=registry)
    @kopf.on.event('resource', registry=registry)
    @kopf.on.create('resource', registry=registry)
    @kopf.on.update('resource', registry=registry)
    @kopf.on.delete('resource', registry=registry)
    @kopf.on.resume('resource', registry=registry)
    @kopf.on.timer('resource', registry=registry)
    @kopf.on.daemon('resource', registry=registry)
    def fn(**_): pass

    assert len(registry._indexing.get_all_handlers()) == 1
    assert len(registry._watching.get_all_handlers()) == 2
    assert len(registry._spawning.get_all_handlers()) == 2
    assert len(registry._changing.get_all_handlers()) == 4


def test_all_selectors(operator_registry_cls):
    registry = operator_registry_cls()

    @kopf.index('resource0', registry=registry)
    @kopf.on.event('resource1', registry=registry)
    @kopf.on.event('resource2', registry=registry)
    @kopf.on.create('resource1', registry=registry)
    @kopf.on.update('resource2', registry=registry)
    @kopf.on.delete('resource3', registry=registry)
    @kopf.on.resume('resource4', registry=registry)
    @kopf.on.timer('resource5', registry=registry)
    @kopf.on.daemon('resource6', registry=registry)
    def fn(**_): pass

    assert registry._indexing.get_all_selectors() == {Selector('resource0')}
    assert registry._watching.get_all_selectors() == {Selector('resource1'), Selector('resource2')}
    assert registry._spawning.get_all_selectors() == {Selector('resource5'), Selector('resource6')}
    assert registry._changing.get_all_selectors() == {Selector('resource1'), Selector('resource2'),
                                                      Selector('resource3'), Selector('resource4')}



================================================
FILE: tests/registries/test_id_detection.py
================================================
import functools

from kopf._core.intents.registries import get_callable_id


# Used in the tests. Must be global-scoped, or its qualname will be affected.
def some_fn():
    pass


def test_id_of_simple_function():
    fn_id = get_callable_id(some_fn)
    assert fn_id == 'some_fn'


def test_id_of_single_partial():
    partial_fn = functools.partial(some_fn)

    fn_id = get_callable_id(partial_fn)
    assert fn_id == 'some_fn'


def test_id_of_double_partial():
    partial1_fn = functools.partial(some_fn)
    partial2_fn = functools.partial(partial1_fn)

    fn_id = get_callable_id(partial2_fn)
    assert fn_id == 'some_fn'


def test_id_of_single_wrapper():

    @functools.wraps(some_fn)
    def wrapped_fn():
        pass

    fn_id = get_callable_id(wrapped_fn)
    assert fn_id == 'some_fn'


def test_id_of_double_wrapper():

    @functools.wraps(some_fn)
    def wrapped1_fn():
        pass

    @functools.wraps(wrapped1_fn)
    def wrapped2_fn():
        pass

    fn_id = get_callable_id(wrapped2_fn)
    assert fn_id == 'some_fn'


def test_id_of_lambda():
    some_lambda = lambda: None

    fn_id = get_callable_id(some_lambda)
    assert fn_id.startswith(f'lambda:{__file__}:')



================================================
FILE: tests/registries/test_matching_for_changing.py
================================================
import copy

import pytest

import kopf
from kopf._cogs.structs.bodies import Body
from kopf._cogs.structs.dicts import parse_field
from kopf._core.intents.causes import ALL_REASONS, Reason
from kopf._core.intents.filters import ABSENT, PRESENT
from kopf._core.intents.handlers import ChangingHandler


# Used in the tests. Must be global-scoped, or its qualname will be affected.
def some_fn(x=None):
    pass


def _never(*_, **__):
    return False


def _always(*_, **__):
    return True


matching_reason_and_decorator = pytest.mark.parametrize('reason, decorator', [
    (Reason.CREATE, kopf.on.create),
    (Reason.UPDATE, kopf.on.update),
    (Reason.DELETE, kopf.on.delete),
])

matching_reason_and_decorator_with_field = pytest.mark.parametrize('reason, decorator', [
    (Reason.CREATE, kopf.on.create),
    (Reason.UPDATE, kopf.on.update),
    (Reason.DELETE, kopf.on.delete),
    (Reason.CREATE, kopf.on.field),
    (Reason.UPDATE, kopf.on.field),
    (Reason.DELETE, kopf.on.field),
])

mismatching_reason_and_decorator = pytest.mark.parametrize('reason, decorator', [
    (Reason.CREATE, kopf.on.update),
    (Reason.CREATE, kopf.on.delete),
    (Reason.UPDATE, kopf.on.create),
    (Reason.UPDATE, kopf.on.delete),
    (Reason.DELETE, kopf.on.create),
    (Reason.DELETE, kopf.on.update),
])


@pytest.fixture()
def handler_factory(registry, selector):
    def factory(**kwargs):
        handler = ChangingHandler(**dict(dict(
            fn=some_fn, id='a', param=None,
            errors=None, timeout=None, retries=None, backoff=None,
            initial=None, deleted=None, requires_finalizer=None,
            selector=selector, annotations=None, labels=None, when=None,
            field=None, value=None, old=None, new=None, field_needs_change=None,
            reason=None,
        ), **kwargs))
        registry._changing.append(handler)
        return handler
    return factory


@pytest.fixture(params=[
    # The original no-diff was equivalent to no-field until body/old/new were added to the check.
    pytest.param(dict(old={}, new={}, body={}, diff=[]), id='with-empty-diff'),
])
def cause_no_diff(request, cause_factory):
    kwargs = copy.deepcopy(request.param)
    kwargs['body'] |= {'metadata': {'labels': {'known': 'value'},
                                    'annotations': {'known': 'value'}}}
    cause = cause_factory(**kwargs)
    return cause


@pytest.fixture(params=[
    pytest.param(dict(old={'known-field': 'old'},
                      new={'known-field': 'new'},
                      body={'known-field': 'new'},
                      diff=[('op', ('known-field',), 'old', 'new')]), id='with-field-diff'),
])
def cause_with_diff(request, cause_factory):
    kwargs = copy.deepcopy(request.param)
    kwargs['body'] |= {'metadata': {'labels': {'known': 'value'},
                                    'annotations': {'known': 'value'}}}
    cause = cause_factory(**kwargs)
    return cause


@pytest.fixture(params=[
    # The original no-diff was equivalent to no-field until body/old/new were added to the check.
    pytest.param(dict(old={}, new={}, body={}, diff=[]), id='with-empty-diff'),
    pytest.param(dict(old={'known-field': 'old'},
                      new={'known-field': 'new'},
                      body={'known-field': 'new'},
                      diff=[('op', ('known-field',), 'old', 'new')]), id='with-field-diff'),
])
def cause_any_diff(request, cause_factory):
    kwargs = copy.deepcopy(request.param)
    kwargs['body'] |= {'metadata': {'labels': {'known': 'value'},
                                    'annotations': {'known': 'value'}}}
    cause = cause_factory(**kwargs)
    return cause


#
# "Catch-all" handlers are those with event == None.
#

def test_catchall_handlers_without_field_found(
        cause_any_diff, registry, handler_factory):
    cause = cause_any_diff
    handler_factory(reason=None, field=None)
    handlers = registry._changing.get_handlers(cause)
    assert handlers


def test_catchall_handlers_with_field_found(
        cause_with_diff, registry, handler_factory):
    cause = cause_with_diff
    handler_factory(reason=None, field=parse_field('known-field'))
    handlers = registry._changing.get_handlers(cause)
    assert handlers


def test_catchall_handlers_with_field_ignored(
        cause_no_diff, registry, handler_factory):
    cause = cause_no_diff
    handler_factory(reason=None, field=parse_field('known-field'))
    handlers = registry._changing.get_handlers(cause)
    assert not handlers


@pytest.mark.parametrize('labels', [
    pytest.param({'known': 'value'}, id='with-label'),
    pytest.param({'known': 'value', 'extra': 'other'}, id='with-extra-label'),
])
def test_catchall_handlers_with_exact_labels_satisfied(
        cause_factory, registry, handler_factory, labels):
    cause = cause_factory(body={'metadata': {'labels': labels}})
    handler_factory(reason=None, labels={'known': 'value'})
    handlers = registry._changing.get_handlers(cause)
    assert handlers


@pytest.mark.parametrize('labels', [
    pytest.param({}, id='without-label'),
    pytest.param({'known': 'other'}, id='with-other-value'),
    pytest.param({'extra': 'other'}, id='with-other-label'),
])
def test_catchall_handlers_with_exact_labels_not_satisfied(
        cause_factory, registry, handler_factory, labels):
    cause = cause_factory(body={'metadata': {'labels': labels}})
    handler_factory(reason=None, labels={'known': 'value'})
    handlers = registry._changing.get_handlers(cause)
    assert not handlers


@pytest.mark.parametrize('labels', [
    pytest.param({'known': 'value'}, id='with-label'),
    pytest.param({'known': 'other'}, id='with-other-value'),
])
def test_catchall_handlers_with_desired_labels_present(
        cause_factory, registry, handler_factory, labels):
    cause = cause_factory(body={'metadata': {'labels': labels}})
    handler_factory(reason=None, labels={'known': PRESENT})
    handlers = registry._changing.get_handlers(cause)
    assert handlers


@pytest.mark.parametrize('labels', [
    pytest.param({}, id='without-label'),
    pytest.param({'extra': 'other'}, id='with-other-label'),
])
def test_catchall_handlers_with_desired_labels_absent(
        cause_factory, registry, handler_factory, labels):
    cause = cause_factory(body={'metadata': {'labels': labels}})
    handler_factory(reason=None, labels={'known': PRESENT})
    handlers = registry._changing.get_handlers(cause)
    assert not handlers


@pytest.mark.parametrize('labels', [
    pytest.param({'known': 'value'}, id='with-label'),
    pytest.param({'known': 'other'}, id='with-other-value'),
])
def test_catchall_handlers_with_undesired_labels_present(
        cause_factory, registry, handler_factory, labels):
    cause = cause_factory(body={'metadata': {'labels': labels}})
    handler_factory(reason=None, labels={'known': ABSENT})
    handlers = registry._changing.get_handlers(cause)
    assert not handlers


@pytest.mark.parametrize('labels', [
    pytest.param({}, id='without-label'),
    pytest.param({'extra': 'other'}, id='with-other-label'),
])
def test_catchall_handlers_with_undesired_labels_absent(
        cause_factory, registry, handler_factory, labels):
    cause = cause_factory(body={'metadata': {'labels': labels}})
    handler_factory(reason=None, labels={'known': ABSENT})
    handlers = registry._changing.get_handlers(cause)
    assert handlers


@pytest.mark.parametrize('labels', [
    pytest.param({}, id='without-label'),
    pytest.param({'known': 'value'}, id='with-label'),
    pytest.param({'known': 'other'}, id='with-other-value'),
])
def test_catchall_handlers_with_labels_callback_says_true(
        cause_factory, registry, handler_factory, labels):
    cause = cause_factory(body={'metadata': {'labels': labels}})
    handler_factory(reason=None, labels={'known': _always})
    handlers = registry._changing.get_handlers(cause)
    assert handlers


@pytest.mark.parametrize('labels', [
    pytest.param({}, id='without-label'),
    pytest.param({'known': 'value'}, id='with-label'),
    pytest.param({'known': 'other'}, id='with-other-value'),
])
def test_catchall_handlers_with_labels_callback_says_false(
        cause_factory, registry, handler_factory, labels):
    cause = cause_factory(body={'metadata': {'labels': labels}})
    handler_factory(reason=None, labels={'known': _never})
    handlers = registry._changing.get_handlers(cause)
    assert not handlers


@pytest.mark.parametrize('labels', [
    pytest.param({}, id='without-label'),
    pytest.param({'known': 'value'}, id='with-label'),
    pytest.param({'known': 'other'}, id='with-other-value'),
    pytest.param({'extra': 'other'}, id='with-other-label'),
    pytest.param({'known': 'value', 'extra': 'other'}, id='with-extra-label'),
])
def test_catchall_handlers_without_labels(
        cause_factory, registry, handler_factory, labels):
    cause = cause_factory(body={'metadata': {'labels': labels}})
    handler_factory(reason=None, labels=None)
    handlers = registry._changing.get_handlers(cause)
    assert handlers


@pytest.mark.parametrize('annotations', [
    pytest.param({'known': 'value'}, id='with-annotation'),
    pytest.param({'known': 'value', 'extra': 'other'}, id='with-extra-annotation'),
])
def test_catchall_handlers_with_exact_annotations_satisfied(
        cause_factory, registry, handler_factory, annotations):
    cause = cause_factory(body={'metadata': {'annotations': annotations}})
    handler_factory(reason=None, annotations={'known': 'value'})
    handlers = registry._changing.get_handlers(cause)
    assert handlers


@pytest.mark.parametrize('annotations', [
    pytest.param({}, id='without-annotation'),
    pytest.param({'known': 'other'}, id='with-other-value'),
    pytest.param({'extra': 'other'}, id='with-other-annotation'),
])
def test_catchall_handlers_with_exact_annotations_not_satisfied(
        cause_factory, registry, handler_factory, annotations):
    cause = cause_factory(body={'metadata': {'annotations': annotations}})
    handler_factory(reason=None, annotations={'known': 'value'})
    handlers = registry._changing.get_handlers(cause)
    assert not handlers


@pytest.mark.parametrize('annotations', [
    pytest.param({'known': 'value'}, id='with-annotation'),
    pytest.param({'known': 'other'}, id='with-other-value'),
])
def test_catchall_handlers_with_desired_annotations_present(
        cause_factory, registry, handler_factory, annotations):
    cause = cause_factory(body={'metadata': {'annotations': annotations}})
    handler_factory(reason=None, annotations={'known': PRESENT})
    handlers = registry._changing.get_handlers(cause)
    assert handlers


@pytest.mark.parametrize('annotations', [
    pytest.param({}, id='without-annotation'),
    pytest.param({'extra': 'other'}, id='with-other-annotation'),
])
def test_catchall_handlers_with_desired_annotations_absent(
        cause_factory, registry, handler_factory, annotations):
    cause = cause_factory(body={'metadata': {'annotations': annotations}})
    handler_factory(reason=None, annotations={'known': PRESENT})
    handlers = registry._changing.get_handlers(cause)
    assert not handlers


@pytest.mark.parametrize('annotations', [
    pytest.param({'known': 'value'}, id='with-annotation'),
    pytest.param({'known': 'other'}, id='with-other-value'),
])
def test_catchall_handlers_with_undesired_annotations_present(
        cause_factory, registry, handler_factory, annotations):
    cause = cause_factory(body={'metadata': {'annotations': annotations}})
    handler_factory(reason=None, annotations={'known': ABSENT})
    handlers = registry._changing.get_handlers(cause)
    assert not handlers


@pytest.mark.parametrize('annotations', [
    pytest.param({}, id='without-annotation'),
    pytest.param({'extra': 'other'}, id='with-other-annotation'),
])
def test_catchall_handlers_with_undesired_annotations_absent(
        cause_factory, registry, handler_factory, annotations):
    cause = cause_factory(body={'metadata': {'annotations': annotations}})
    handler_factory(reason=None, annotations={'known': ABSENT})
    handlers = registry._changing.get_handlers(cause)
    assert handlers


@pytest.mark.parametrize('annotations', [
    pytest.param({}, id='without-annotation'),
    pytest.param({'known': 'value'}, id='with-annotation'),
    pytest.param({'known': 'other'}, id='with-other-value'),
])
def test_catchall_handlers_with_annotations_callback_says_true(
        cause_factory, registry, handler_factory, annotations):
    cause = cause_factory(body={'metadata': {'annotations': annotations}})
    handler_factory(reason=None, annotations={'known': _always})
    handlers = registry._changing.get_handlers(cause)
    assert handlers


@pytest.mark.parametrize('annotations', [
    pytest.param({}, id='without-annotation'),
    pytest.param({'known': 'value'}, id='with-annotation'),
    pytest.param({'known': 'other'}, id='with-other-value'),
])
def test_catchall_handlers_with_annotations_callback_says_false(
        cause_factory, registry, handler_factory, annotations):
    cause = cause_factory(body={'metadata': {'annotations': annotations}})
    handler_factory(reason=None, annotations={'known': _never})
    handlers = registry._changing.get_handlers(cause)
    assert not handlers


@pytest.mark.parametrize('annotations', [
    pytest.param({}, id='without-annotation'),
    pytest.param({'known': 'value'}, id='with-annotation'),
    pytest.param({'known': 'other'}, id='with-other-value'),
    pytest.param({'extra': 'other'}, id='with-other-annotation'),
    pytest.param({'known': 'value', 'extra': 'other'}, id='with-extra-annotation'),
])
def test_catchall_handlers_without_annotations(
        cause_factory, registry, handler_factory, annotations):
    cause = cause_factory(body={'metadata': {'annotations': annotations}})
    handler_factory(reason=None)
    handlers = registry._changing.get_handlers(cause)
    assert handlers


@pytest.mark.parametrize('labels, annotations', [
    pytest.param({'known': 'value'}, {'known': 'value'}, id='with-label-annotation'),
    pytest.param({'known': 'value', 'extra': 'other'}, {'known': 'value'}, id='with-extra-label-annotation'),
    pytest.param({'known': 'value'}, {'known': 'value', 'extra': 'other'}, id='with-label-extra-annotation'),
    pytest.param({'known': 'value', 'extra': 'other'}, {'known': 'value', 'extra': 'other'}, id='with-extra-label-extra-annotation'),
])
def test_catchall_handlers_with_labels_and_annotations_satisfied(
        cause_factory, registry, handler_factory, labels, annotations):
    cause = cause_factory(body={'metadata': {'labels': labels, 'annotations': annotations}})
    handler_factory(reason=None, labels={'known': 'value'}, annotations={'known': 'value'})
    handlers = registry._changing.get_handlers(cause)
    assert handlers


@pytest.mark.parametrize('labels', [
    pytest.param({}, id='without-label'),
    pytest.param({'known': 'value'}, id='with-label'),
    pytest.param({'known': 'other'}, id='with-other-value'),
    pytest.param({'extra': 'other'}, id='with-other-label'),
    pytest.param({'known': 'value', 'extra': 'other'}, id='with-extra-label'),
])
def test_catchall_handlers_with_labels_and_annotations_not_satisfied(
        cause_factory, registry, handler_factory, labels):
    cause = cause_factory(body={'metadata': {'labels': labels}})
    handler_factory(reason=None, labels={'known': 'value'}, annotations={'known': 'value'})
    handlers = registry._changing.get_handlers(cause)
    assert not handlers


@pytest.mark.parametrize('reason', ALL_REASONS)
@pytest.mark.parametrize('when', [
    pytest.param(None, id='without-when'),
    pytest.param(lambda body=None, **_: body['spec']['name'] == 'test', id='with-when'),
    pytest.param(lambda **_: True, id='with-other-when'),
])
def test_catchall_handlers_with_when_callback_matching(
        cause_factory, registry, handler_factory, reason, when):
    cause = cause_factory(body={'spec': {'name': 'test'}})
    handler_factory(reason=None, when=when)
    handlers = registry._changing.get_handlers(cause)
    assert handlers


@pytest.mark.parametrize('when', [
    pytest.param(lambda body=None, **_: body['spec']['name'] != "test", id='with-when'),
    pytest.param(lambda **_: False, id='with-other-when'),
])
def test_catchall_handlers_with_when_callback_mismatching(
        cause_factory, registry, handler_factory, when):
    cause = cause_factory(body={'spec': {'name': 'test'}})
    handler_factory(reason=None, when=when)
    handlers = registry._changing.get_handlers(cause)
    assert not handlers


#
# Relevant handlers are those with reason matching the cause's reason.
# In the per-field handlers, also with field == 'known-field' (not 'extra-field').
# In the label filtered handlers, the relevant handlers are those that ask for 'known'.
# In the annotation filtered handlers, the relevant handlers are those that ask for 'known'.
#


@matching_reason_and_decorator_with_field
def test_relevant_handlers_without_field_found(
        cause_any_diff, registry, resource, reason, decorator):

    @decorator(*resource, field=None)
    def some_fn(**_): pass

    cause = cause_any_diff
    cause.reason = reason
    handlers = registry._changing.get_handlers(cause)
    assert handlers


@matching_reason_and_decorator_with_field
def test_relevant_handlers_with_field_found(
        cause_with_diff, registry, resource, reason, decorator):

    @decorator(*resource, field='known-field')
    def some_fn(**_): pass

    cause = cause_with_diff
    cause.reason = reason
    handlers = registry._changing.get_handlers(cause)
    assert handlers


@matching_reason_and_decorator_with_field
def test_relevant_handlers_with_field_ignored(
        cause_no_diff, registry, resource, reason, decorator):

    @decorator(*resource, field='known-field')
    def some_fn(**_): pass

    cause = cause_no_diff
    cause.reason = reason
    handlers = registry._changing.get_handlers(cause)
    assert not handlers


@matching_reason_and_decorator
def test_relevant_handlers_with_labels_satisfied(
        cause_any_diff, registry, resource, reason, decorator):

    @decorator(*resource, labels={'known': PRESENT})
    def some_fn(**_): pass

    cause = cause_any_diff
    cause.reason = reason
    handlers = registry._changing.get_handlers(cause)
    assert handlers


@matching_reason_and_decorator
def test_relevant_handlers_with_labels_not_satisfied(
        cause_any_diff, registry, resource, reason, decorator):

    @decorator(*resource, labels={'extra': PRESENT})
    def some_fn(**_): pass

    cause = cause_any_diff
    cause.reason = reason
    handlers = registry._changing.get_handlers(cause)
    assert not handlers


@matching_reason_and_decorator
def test_relevant_handlers_with_annotations_satisfied(
        cause_any_diff, registry, resource, reason, decorator):

    @decorator(*resource, annotations={'known': PRESENT})
    def some_fn(**_): pass

    cause = cause_any_diff
    cause.reason = reason
    handlers = registry._changing.get_handlers(cause)
    assert handlers


@matching_reason_and_decorator
def test_relevant_handlers_with_annotations_not_satisfied(
        cause_any_diff, registry, resource, reason, decorator):

    @decorator(*resource, annotations={'extra': PRESENT})
    def some_fn(**_): pass

    cause = cause_any_diff
    cause.reason = reason
    handlers = registry._changing.get_handlers(cause)
    assert not handlers


@matching_reason_and_decorator
def test_relevant_handlers_with_filter_satisfied(
        cause_any_diff, registry, resource, reason, decorator):

    @decorator(*resource, when=_always)
    def some_fn(**_): pass

    cause = cause_any_diff
    cause.reason = reason
    handlers = registry._changing.get_handlers(cause)
    assert handlers


@matching_reason_and_decorator
def test_relevant_handlers_with_filter_not_satisfied(
        cause_any_diff, registry, resource, reason, decorator):

    @decorator(*resource, when=_never)
    def some_fn(**_): pass

    cause = cause_any_diff
    cause.reason = reason
    handlers = registry._changing.get_handlers(cause)
    assert not handlers


@mismatching_reason_and_decorator
def test_irrelevant_handlers_without_field_ignored(
        cause_any_diff, registry, resource, reason, decorator):

    @decorator(*resource)
    def some_fn(**_): pass

    cause = cause_any_diff
    cause.reason = reason
    handlers = registry._changing.get_handlers(cause)
    assert not handlers


@matching_reason_and_decorator_with_field
def test_irrelevant_handlers_with_field_ignored(
        cause_any_diff, registry, resource, reason, decorator):

    @decorator(*resource, field='extra-field')
    def some_fn(**_): pass

    cause = cause_any_diff
    cause.reason = reason
    handlers = registry._changing.get_handlers(cause)
    assert not handlers


@mismatching_reason_and_decorator
def test_irrelevant_handlers_with_labels_satisfied(
        cause_any_diff, registry, resource, reason, decorator):

    @decorator(*resource, labels={'known': PRESENT})
    def some_fn(**_): pass

    cause = cause_any_diff
    cause.reason = reason
    handlers = registry._changing.get_handlers(cause)
    assert not handlers


@mismatching_reason_and_decorator
def test_irrelevant_handlers_with_labels_not_satisfied(
        cause_any_diff, registry, resource, reason, decorator):

    @decorator(*resource, labels={'extra': PRESENT})
    def some_fn(**_): pass

    cause = cause_any_diff
    cause.reason = reason
    handlers = registry._changing.get_handlers(cause)
    assert not handlers


@mismatching_reason_and_decorator
def test_irrelevant_handlers_with_annotations_satisfied(
        cause_any_diff, registry, resource, reason, decorator):

    @decorator(*resource, annotations={'known': PRESENT})
    def some_fn(**_): pass

    cause = cause_any_diff
    cause.reason = reason
    handlers = registry._changing.get_handlers(cause)
    assert not handlers


@mismatching_reason_and_decorator
def test_irrelevant_handlers_with_annotations_not_satisfied(
        cause_any_diff, registry, resource, reason, decorator):

    @decorator(*resource, annotations={'extra': PRESENT})
    def some_fn(**_): pass

    cause = cause_any_diff
    cause.reason = reason
    handlers = registry._changing.get_handlers(cause)
    assert not handlers


@mismatching_reason_and_decorator
def test_irrelevant_handlers_with_when_callback_satisfied(
        cause_any_diff, registry, resource, reason, decorator):

    @decorator(*resource, when=_always)
    def some_fn(**_): pass

    cause = cause_any_diff
    cause.reason = reason
    handlers = registry._changing.get_handlers(cause)
    assert not handlers


@mismatching_reason_and_decorator
def test_irrelevant_handlers_with_when_callback_not_satisfied(
        cause_any_diff, registry, resource, reason, decorator):

    @decorator(*resource, when=_never)
    def some_fn(**_): pass

    cause = cause_any_diff
    cause.reason = reason
    handlers = registry._changing.get_handlers(cause)
    assert not handlers


#
# Special case for nested fields with shorter/longer diffs.
#

@matching_reason_and_decorator_with_field
def test_field_same_as_diff(cause_with_diff, registry, resource, reason, decorator):

    @decorator(*resource, field='level1.level2')
    def some_fn(**_): pass

    cause = cause_with_diff
    cause.reason = reason
    cause.old = {'level1': {'level2': 'old'}}
    cause.new = {'level1': {'level2': 'new'}}
    cause.body = Body({'level1': {'level2': 'new'}})
    handlers = registry._changing.get_handlers(cause)
    assert handlers


@matching_reason_and_decorator_with_field
def test_field_shorter_than_diff(cause_with_diff, registry, resource, reason, decorator):

    @decorator(*resource, field='level1')
    def some_fn(**_): pass

    cause = cause_with_diff
    cause.reason = reason
    cause.old = {'level1': {'level2': 'old'}}
    cause.new = {'level1': {'level2': 'new'}}
    cause.body = Body({'level1': {'level2': 'new'}})
    handlers = registry._changing.get_handlers(cause)
    assert handlers


@matching_reason_and_decorator_with_field
def test_field_longer_than_diff_for_wrong_field(cause_with_diff, registry, resource, reason, decorator):

    @decorator(*resource, field='level1.level2.level3')
    def some_fn(**_): pass

    cause = cause_with_diff
    cause.reason = reason
    cause.old = {'level1': {'level2': 'old'}}
    cause.new = {'level1': {'level2': 'new'}}
    cause.body = Body({'level1': {'level2': 'new'}})
    handlers = registry._changing.get_handlers(cause)
    assert not handlers


@pytest.mark.parametrize('old, new', [
    pytest.param({'level3': 'old'}, {'level3': 'new'}, id='struct2struct'),
    pytest.param({'level3': 'old'}, 'new', id='struct2scalar'),
    pytest.param('old', {'level3': 'new'}, id='scalar2struct'),
    pytest.param(None, {'level3': 'new'}, id='none2struct'),
    pytest.param({'level3': 'old'}, None, id='struct2none'),
    pytest.param({}, {'level3': 'new'}, id='empty2struct'),
    pytest.param({'level3': 'old'}, {}, id='struct2empty'),
])
@matching_reason_and_decorator_with_field
def test_field_longer_than_diff_for_right_field(cause_with_diff, registry, resource, old, new, reason, decorator):

    @decorator(*resource, field='level1.level2.level3')
    def some_fn(**_): pass

    cause = cause_with_diff
    cause.reason = reason
    cause.old = {'level1': {'level2': old}} if old is not None else {'level1': {'level2': {}}}
    cause.new = {'level1': {'level2': new}} if new is not None else {'level1': {'level2': {}}}
    cause.body = Body(cause.new)
    handlers = registry._changing.get_handlers(cause)
    assert handlers


#
# The handlers must be returned in order of registration,
# even if they are mixed with-/without- * -event/-field handlers.
#

def test_order_persisted_a(cause_with_diff, registry, resource):

    @kopf.on.create(*resource)
    def some_fn_1(**_): pass  # used

    @kopf.on.update(*resource)
    def some_fn_2(**_): pass  # filtered out

    @kopf.on.create(*resource)
    def some_fn_3(**_): pass  # used

    @kopf.on.field(*resource, field='filtered-out-field')
    def some_fn_4(**_): pass  # filtered out

    @kopf.on.field(*resource, field='known-field')
    def some_fn_5(**_): pass  # used

    cause = cause_with_diff
    cause.reason = Reason.CREATE
    handlers = registry._changing.get_handlers(cause)

    # Order must be preserved -- same as registered.
    assert len(handlers) == 3
    assert handlers[0].fn is some_fn_1
    assert handlers[1].fn is some_fn_3
    assert handlers[2].fn is some_fn_5


def test_order_persisted_b(cause_with_diff, registry, resource):

    @kopf.on.field(*resource, field='known-field')
    def some_fn_1(**_): pass  # used

    @kopf.on.field(*resource, field='filtered-out-field')
    def some_fn_2(**_): pass  # filtered out

    @kopf.on.create(*resource)
    def some_fn_3(**_): pass  # used

    @kopf.on.update(*resource)
    def some_fn_4(**_): pass  # filtered out

    @kopf.on.create(*resource)
    def some_fn_5(**_): pass  # used

    cause = cause_with_diff
    cause.reason = Reason.CREATE
    handlers = registry._changing.get_handlers(cause)

    # Order must be preserved -- same as registered.
    assert len(handlers) == 3
    assert handlers[0].fn is some_fn_1
    assert handlers[1].fn is some_fn_3
    assert handlers[2].fn is some_fn_5


#
# Same function should not be returned twice for the same event/cause.
# Only actual for the cases when the event/cause can match multiple handlers.
#
@matching_reason_and_decorator
def test_deduplication_by_fn_and_id(
        cause_with_diff, registry, resource, reason, decorator):

    # Note: the decorators are applied bottom-up -- hence, the order of ids:
    @decorator(*resource, id='a')
    @decorator(*resource, id='a')
    def some_fn(**_): pass

    cause = cause_with_diff
    cause.reason = reason
    handlers = registry._changing.get_handlers(cause)

    assert len(handlers) == 1
    assert handlers[0].id == 'a'  # the first found one is returned


@matching_reason_and_decorator
def test_deduplication_distinguishes_different_fns(
        cause_with_diff, registry, resource, reason, decorator):

    # Note: the decorators are applied bottom-up -- hence, the order of ids:
    @decorator(*resource, id='a')
    def some_fn1(**_): pass

    @decorator(*resource, id='a')
    def some_fn2(**_): pass

    cause = cause_with_diff
    cause.reason = reason
    handlers = registry._changing.get_handlers(cause)

    assert len(handlers) == 2


@matching_reason_and_decorator
def test_deduplication_distinguishes_different_ids(
        cause_with_diff, registry, resource, reason, decorator):

    # Note: the decorators are applied bottom-up -- hence, the order of ids:
    @decorator(*resource, id='b')
    @decorator(*resource, id='a')
    def some_fn(**_): pass

    cause = cause_with_diff
    cause.reason = reason
    handlers = registry._changing.get_handlers(cause)

    assert len(handlers) == 2



================================================
FILE: tests/registries/test_matching_for_indexing.py
================================================
import copy

import pytest

import kopf
from kopf._cogs.structs.dicts import parse_field
from kopf._core.intents.causes import WatchingCause
from kopf._core.intents.filters import ABSENT, PRESENT
from kopf._core.intents.handlers import IndexingHandler


# Used in the tests. Must be global-scoped, or its qualname will be affected.
def some_fn(x=None):
    pass


def _never(*_, **__):
    return False


def _always(*_, **__):
    return True


@pytest.fixture()
def handler_factory(registry, selector):
    def factory(**kwargs):
        handler = IndexingHandler(**dict(dict(
            fn=some_fn, id='a', param=None,
            errors=None, timeout=None, retries=None, backoff=None,
            selector=selector, annotations=None, labels=None, when=None,
            field=None, value=None,
        ), **kwargs))
        registry._indexing.append(handler)
        return handler
    return factory


@pytest.fixture(params=[
    pytest.param(dict(body={}), id='no-field'),
])
def cause_no_field(request, cause_factory):
    kwargs = copy.deepcopy(request.param)
    kwargs['body'] |= {'metadata': {'labels': {'known': 'value'},
                                    'annotations': {'known': 'value'}}}
    cause = cause_factory(cls=WatchingCause, **kwargs)
    return cause


@pytest.fixture(params=[
    pytest.param(dict(body={'known-field': 'new'}), id='with-field'),
])
def cause_with_field(request, cause_factory):
    kwargs = copy.deepcopy(request.param)
    kwargs['body'] |= {'metadata': {'labels': {'known': 'value'},
                                    'annotations': {'known': 'value'}}}
    cause = cause_factory(cls=WatchingCause, **kwargs)
    return cause


@pytest.fixture(params=[
    # The original no-diff was equivalent to no-field until body/old/new were added to the check.
    pytest.param(dict(body={}, diff=[]), id='no-field'),
    pytest.param(dict(body={'known-field': 'new'}), id='with-field'),
])
def cause_any_field(request, cause_factory):
    kwargs = copy.deepcopy(request.param)
    kwargs['body'] |= {'metadata': {'labels': {'known': 'value'},
                                    'annotations': {'known': 'value'}}}
    cause = cause_factory(cls=WatchingCause, **kwargs)
    return cause


#
# "Catch-all" handlers.
#

def test_catchall_handlers_without_field_found(
        cause_any_field, registry, handler_factory):
    cause = cause_any_field
    handler_factory(field=None)
    handlers = registry._indexing.get_handlers(cause)
    assert handlers


def test_catchall_handlers_with_field_found(
        cause_with_field, registry, handler_factory):
    cause = cause_with_field
    handler_factory(field=parse_field('known-field'))
    handlers = registry._indexing.get_handlers(cause)
    assert handlers


def test_catchall_handlers_with_field_ignored(
        cause_no_field, registry, handler_factory):
    cause = cause_no_field
    handler_factory(field=parse_field('known-field'))
    handlers = registry._indexing.get_handlers(cause)
    assert not handlers


@pytest.mark.parametrize('labels', [
    pytest.param({'known': 'value'}, id='with-label'),
    pytest.param({'known': 'value', 'extra': 'other'}, id='with-extra-label'),
])
def test_catchall_handlers_with_exact_labels_satisfied(
        cause_factory, registry, handler_factory, labels):
    cause = cause_factory(body={'metadata': {'labels': labels}})
    handler_factory(labels={'known': 'value'})
    handlers = registry._indexing.get_handlers(cause)
    assert handlers


@pytest.mark.parametrize('labels', [
    pytest.param({}, id='without-label'),
    pytest.param({'known': 'other'}, id='with-other-value'),
    pytest.param({'extra': 'other'}, id='with-other-label'),
])
def test_catchall_handlers_with_exact_labels_not_satisfied(
        cause_factory, registry, handler_factory, labels):
    cause = cause_factory(body={'metadata': {'labels': labels}})
    handler_factory(labels={'known': 'value'})
    handlers = registry._indexing.get_handlers(cause)
    assert not handlers


@pytest.mark.parametrize('labels', [
    pytest.param({'known': 'value'}, id='with-label'),
    pytest.param({'known': 'other'}, id='with-other-value'),
])
def test_catchall_handlers_with_desired_labels_present(
        cause_factory, registry, handler_factory, labels):
    cause = cause_factory(body={'metadata': {'labels': labels}})
    handler_factory(labels={'known': PRESENT})
    handlers = registry._indexing.get_handlers(cause)
    assert handlers


@pytest.mark.parametrize('labels', [
    pytest.param({}, id='without-label'),
    pytest.param({'extra': 'other'}, id='with-other-label'),
])
def test_catchall_handlers_with_desired_labels_absent(
        cause_factory, registry, handler_factory, labels):
    cause = cause_factory(body={'metadata': {'labels': labels}})
    handler_factory(labels={'known': PRESENT})
    handlers = registry._indexing.get_handlers(cause)
    assert not handlers


@pytest.mark.parametrize('labels', [
    pytest.param({'known': 'value'}, id='with-label'),
    pytest.param({'known': 'other'}, id='with-other-value'),
])
def test_catchall_handlers_with_undesired_labels_present(
        cause_factory, registry, handler_factory, labels):
    cause = cause_factory(body={'metadata': {'labels': labels}})
    handler_factory(labels={'known': ABSENT})
    handlers = registry._indexing.get_handlers(cause)
    assert not handlers


@pytest.mark.parametrize('labels', [
    pytest.param({}, id='without-label'),
    pytest.param({'extra': 'other'}, id='with-other-label'),
])
def test_catchall_handlers_with_undesired_labels_absent(
        cause_factory, registry, handler_factory, labels):
    cause = cause_factory(body={'metadata': {'labels': labels}})
    handler_factory(labels={'known': ABSENT})
    handlers = registry._indexing.get_handlers(cause)
    assert handlers


@pytest.mark.parametrize('labels', [
    pytest.param({}, id='without-label'),
    pytest.param({'known': 'value'}, id='with-label'),
    pytest.param({'known': 'other'}, id='with-other-value'),
])
def test_catchall_handlers_with_labels_callback_says_true(
        cause_factory, registry, handler_factory, labels):
    cause = cause_factory(body={'metadata': {'labels': labels}})
    handler_factory(labels={'known': _always})
    handlers = registry._indexing.get_handlers(cause)
    assert handlers


@pytest.mark.parametrize('labels', [
    pytest.param({}, id='without-label'),
    pytest.param({'known': 'value'}, id='with-label'),
    pytest.param({'known': 'other'}, id='with-other-value'),
])
def test_catchall_handlers_with_labels_callback_says_false(
        cause_factory, registry, handler_factory, labels):
    cause = cause_factory(body={'metadata': {'labels': labels}})
    handler_factory(labels={'known': _never})
    handlers = registry._indexing.get_handlers(cause)
    assert not handlers


@pytest.mark.parametrize('labels', [
    pytest.param({}, id='without-label'),
    pytest.param({'known': 'value'}, id='with-label'),
    pytest.param({'known': 'other'}, id='with-other-value'),
    pytest.param({'extra': 'other'}, id='with-other-label'),
    pytest.param({'known': 'value', 'extra': 'other'}, id='with-extra-label'),
])
def test_catchall_handlers_without_labels(
        cause_factory, registry, handler_factory, labels):
    cause = cause_factory(body={'metadata': {'labels': labels}})
    handler_factory(labels=None)
    handlers = registry._indexing.get_handlers(cause)
    assert handlers


@pytest.mark.parametrize('annotations', [
    pytest.param({'known': 'value'}, id='with-annotation'),
    pytest.param({'known': 'value', 'extra': 'other'}, id='with-extra-annotation'),
])
def test_catchall_handlers_with_exact_annotations_satisfied(
        cause_factory, registry, handler_factory, annotations):
    cause = cause_factory(body={'metadata': {'annotations': annotations}})
    handler_factory(annotations={'known': 'value'})
    handlers = registry._indexing.get_handlers(cause)
    assert handlers


@pytest.mark.parametrize('annotations', [
    pytest.param({}, id='without-annotation'),
    pytest.param({'known': 'other'}, id='with-other-value'),
    pytest.param({'extra': 'other'}, id='with-other-annotation'),
])
def test_catchall_handlers_with_exact_annotations_not_satisfied(
        cause_factory, registry, handler_factory, annotations):
    cause = cause_factory(body={'metadata': {'annotations': annotations}})
    handler_factory(annotations={'known': 'value'})
    handlers = registry._indexing.get_handlers(cause)
    assert not handlers


@pytest.mark.parametrize('annotations', [
    pytest.param({'known': 'value'}, id='with-annotation'),
    pytest.param({'known': 'other'}, id='with-other-value'),
])
def test_catchall_handlers_with_desired_annotations_present(
        cause_factory, registry, handler_factory, annotations):
    cause = cause_factory(body={'metadata': {'annotations': annotations}})
    handler_factory(annotations={'known': PRESENT})
    handlers = registry._indexing.get_handlers(cause)
    assert handlers


@pytest.mark.parametrize('annotations', [
    pytest.param({}, id='without-annotation'),
    pytest.param({'extra': 'other'}, id='with-other-annotation'),
])
def test_catchall_handlers_with_desired_annotations_absent(
        cause_factory, registry, handler_factory, annotations):
    cause = cause_factory(body={'metadata': {'annotations': annotations}})
    handler_factory(annotations={'known': PRESENT})
    handlers = registry._indexing.get_handlers(cause)
    assert not handlers


@pytest.mark.parametrize('annotations', [
    pytest.param({'known': 'value'}, id='with-annotation'),
    pytest.param({'known': 'other'}, id='with-other-value'),
])
def test_catchall_handlers_with_undesired_annotations_present(
        cause_factory, registry, handler_factory, annotations):
    cause = cause_factory(body={'metadata': {'annotations': annotations}})
    handler_factory(annotations={'known': ABSENT})
    handlers = registry._indexing.get_handlers(cause)
    assert not handlers


@pytest.mark.parametrize('annotations', [
    pytest.param({}, id='without-annotation'),
    pytest.param({'extra': 'other'}, id='with-other-annotation'),
])
def test_catchall_handlers_with_undesired_annotations_absent(
        cause_factory, registry, handler_factory, annotations):
    cause = cause_factory(body={'metadata': {'annotations': annotations}})
    handler_factory(annotations={'known': ABSENT})
    handlers = registry._indexing.get_handlers(cause)
    assert handlers


@pytest.mark.parametrize('annotations', [
    pytest.param({}, id='without-annotation'),
    pytest.param({'known': 'value'}, id='with-annotation'),
    pytest.param({'known': 'other'}, id='with-other-value'),
])
def test_catchall_handlers_with_annotations_callback_says_true(
        cause_factory, registry, handler_factory, annotations):
    cause = cause_factory(body={'metadata': {'annotations': annotations}})
    handler_factory(annotations={'known': _always})
    handlers = registry._indexing.get_handlers(cause)
    assert handlers


@pytest.mark.parametrize('annotations', [
    pytest.param({}, id='without-annotation'),
    pytest.param({'known': 'value'}, id='with-annotation'),
    pytest.param({'known': 'other'}, id='with-other-value'),
])
def test_catchall_handlers_with_annotations_callback_says_false(
        cause_factory, registry, handler_factory, annotations):
    cause = cause_factory(body={'metadata': {'annotations': annotations}})
    handler_factory(annotations={'known': _never})
    handlers = registry._indexing.get_handlers(cause)
    assert not handlers


@pytest.mark.parametrize('annotations', [
    pytest.param({}, id='without-annotation'),
    pytest.param({'known': 'value'}, id='with-annotation'),
    pytest.param({'known': 'other'}, id='with-other-value'),
    pytest.param({'extra': 'other'}, id='with-other-annotation'),
    pytest.param({'known': 'value', 'extra': 'other'}, id='with-extra-annotation'),
])
def test_catchall_handlers_without_annotations(
        cause_factory, registry, handler_factory, annotations):
    cause = cause_factory(body={'metadata': {'annotations': annotations}})
    handler_factory()
    handlers = registry._indexing.get_handlers(cause)
    assert handlers


@pytest.mark.parametrize('labels, annotations', [
    pytest.param({'known': 'value'}, {'known': 'value'}, id='with-label-annotation'),
    pytest.param({'known': 'value', 'extra': 'other'}, {'known': 'value'}, id='with-extra-label-annotation'),
    pytest.param({'known': 'value'}, {'known': 'value', 'extra': 'other'}, id='with-label-extra-annotation'),
    pytest.param({'known': 'value', 'extra': 'other'}, {'known': 'value', 'extra': 'other'}, id='with-extra-label-extra-annotation'),
])
def test_catchall_handlers_with_labels_and_annotations_satisfied(
        cause_factory, registry, handler_factory, labels, annotations):
    cause = cause_factory(body={'metadata': {'labels': labels, 'annotations': annotations}})
    handler_factory(labels={'known': 'value'}, annotations={'known': 'value'})
    handlers = registry._indexing.get_handlers(cause)
    assert handlers


@pytest.mark.parametrize('labels', [
    pytest.param({}, id='without-label'),
    pytest.param({'known': 'value'}, id='with-label'),
    pytest.param({'known': 'other'}, id='with-other-value'),
    pytest.param({'extra': 'other'}, id='with-other-label'),
    pytest.param({'known': 'value', 'extra': 'other'}, id='with-extra-label'),
])
def test_catchall_handlers_with_labels_and_annotations_not_satisfied(
        cause_factory, registry, handler_factory, labels):
    cause = cause_factory(body={'metadata': {'labels': labels}})
    handler_factory(labels={'known': 'value'}, annotations={'known': 'value'})
    handlers = registry._indexing.get_handlers(cause)
    assert not handlers


@pytest.mark.parametrize('when', [
    pytest.param(None, id='without-when'),
    pytest.param(lambda body=None, **_: body['spec']['name'] == 'test', id='with-when'),
    pytest.param(lambda **_: True, id='with-other-when'),
])
def test_catchall_handlers_with_when_callback_matching(
        cause_factory, registry, handler_factory, when):
    cause = cause_factory(body={'spec': {'name': 'test'}})
    handler_factory(when=when)
    handlers = registry._indexing.get_handlers(cause)
    assert handlers


@pytest.mark.parametrize('when', [
    pytest.param(lambda body=None, **_: body['spec']['name'] != "test", id='with-when'),
    pytest.param(lambda **_: False, id='with-other-when'),
])
def test_catchall_handlers_with_when_callback_mismatching(
        cause_factory, registry, handler_factory, when):
    cause = cause_factory(body={'spec': {'name': 'test'}})
    handler_factory(when=when)
    handlers = registry._indexing.get_handlers(cause)
    assert not handlers


def test_decorator_without_field_found(
        cause_any_field, registry, resource):

    @kopf.index(*resource, field=None)
    def some_fn(**_): pass

    cause = cause_any_field
    handlers = registry._indexing.get_handlers(cause)
    assert handlers


def test_decorator_with_field_found(
        cause_with_field, registry, resource):

    @kopf.index(*resource, field='known-field')
    def some_fn(**_): pass

    cause = cause_with_field
    handlers = registry._indexing.get_handlers(cause)
    assert handlers


def test_decorator_with_field_ignored(
        cause_no_field, registry, resource):

    @kopf.index(*resource, field='known-field')
    def some_fn(**_): pass

    cause = cause_no_field
    handlers = registry._indexing.get_handlers(cause)
    assert not handlers


def test_decorator_with_labels_satisfied(
        cause_any_field, registry, resource):

    @kopf.index(*resource, labels={'known': PRESENT})
    def some_fn(**_): pass

    cause = cause_any_field
    handlers = registry._indexing.get_handlers(cause)
    assert handlers


def test_decorator_with_labels_not_satisfied(
        cause_any_field, registry, resource):

    @kopf.index(*resource, labels={'extra': PRESENT})
    def some_fn(**_): pass

    cause = cause_any_field
    handlers = registry._indexing.get_handlers(cause)
    assert not handlers


def test_decorator_with_annotations_satisfied(
        cause_any_field, registry, resource):

    @kopf.index(*resource, annotations={'known': PRESENT})
    def some_fn(**_): pass

    cause = cause_any_field
    handlers = registry._indexing.get_handlers(cause)
    assert handlers


def test_decorator_with_annotations_not_satisfied(
        cause_any_field, registry, resource):

    @kopf.index(*resource, annotations={'extra': PRESENT})
    def some_fn(**_): pass

    cause = cause_any_field
    handlers = registry._indexing.get_handlers(cause)
    assert not handlers


def test_decorator_with_filter_satisfied(
        cause_any_field, registry, resource):

    @kopf.index(*resource, when=_always)
    def some_fn(**_): pass

    cause = cause_any_field
    handlers = registry._indexing.get_handlers(cause)
    assert handlers


def test_decorator_with_filter_not_satisfied(
        cause_any_field, registry, resource):

    @kopf.index(*resource, when=_never)
    def some_fn(**_): pass

    cause = cause_any_field
    handlers = registry._indexing.get_handlers(cause)
    assert not handlers



================================================
FILE: tests/registries/test_matching_for_spawning.py
================================================
import copy

import pytest

import kopf
from kopf._cogs.structs.dicts import parse_field
from kopf._core.intents.causes import SpawningCause
from kopf._core.intents.filters import ABSENT, PRESENT
from kopf._core.intents.handlers import SpawningHandler


# Used in the tests. Must be global-scoped, or its qualname will be affected.
def some_fn(x=None):
    pass


def _never(*_, **__):
    return False


def _always(*_, **__):
    return True


spawning_decorators = pytest.mark.parametrize('decorator', [
    (kopf.timer),
    (kopf.daemon),
])


@pytest.fixture()
def handler_factory(registry, selector):
    def factory(**kwargs):
        handler = SpawningHandler(**dict(dict(
            fn=some_fn, id='a', param=None,
            errors=None, timeout=None, retries=None, backoff=None,
            selector=selector, annotations=None, labels=None, when=None,
            field=None, value=None,
            requires_finalizer=None, initial_delay=None,
        ), **kwargs))
        registry._spawning.append(handler)
        return handler
    return factory


@pytest.fixture(params=[
    pytest.param(dict(body={}), id='no-field'),
])
def cause_no_field(request, cause_factory):
    kwargs = copy.deepcopy(request.param)
    kwargs['body'] |= {'metadata': {'labels': {'known': 'value'},
                                    'annotations': {'known': 'value'}}}
    cause = cause_factory(cls=SpawningCause, **kwargs)
    return cause


@pytest.fixture(params=[
    pytest.param(dict(body={'known-field': 'new'}), id='with-field'),
])
def cause_with_field(request, cause_factory):
    kwargs = copy.deepcopy(request.param)
    kwargs['body'] |= {'metadata': {'labels': {'known': 'value'},
                                    'annotations': {'known': 'value'}}}
    cause = cause_factory(cls=SpawningCause, **kwargs)
    return cause


@pytest.fixture(params=[
    # The original no-diff was equivalent to no-field until body/old/new were added to the check.
    pytest.param(dict(body={}, diff=[]), id='no-field'),
    pytest.param(dict(body={'known-field': 'new'}), id='with-field'),
])
def cause_any_field(request, cause_factory):
    kwargs = copy.deepcopy(request.param)
    kwargs['body'] |= {'metadata': {'labels': {'known': 'value'},
                                    'annotations': {'known': 'value'}}}
    cause = cause_factory(cls=SpawningCause, **kwargs)
    return cause


#
# "Catch-all" handlers are those with event == None.
#

def test_catchall_handlers_without_field_found(
        cause_any_field, registry, handler_factory):
    cause = cause_any_field
    handler_factory(field=None)
    handlers = registry._spawning.get_handlers(cause)
    assert handlers


def test_catchall_handlers_with_field_found(
        cause_with_field, registry, handler_factory):
    cause = cause_with_field
    handler_factory(field=parse_field('known-field'))
    handlers = registry._spawning.get_handlers(cause)
    assert handlers


def test_catchall_handlers_with_field_ignored(
        cause_no_field, registry, handler_factory):
    cause = cause_no_field
    handler_factory(field=parse_field('known-field'))
    handlers = registry._spawning.get_handlers(cause)
    assert not handlers


@pytest.mark.parametrize('labels', [
    pytest.param({'known': 'value'}, id='with-label'),
    pytest.param({'known': 'value', 'extra': 'other'}, id='with-extra-label'),
])
def test_catchall_handlers_with_exact_labels_satisfied(
        cause_factory, registry, handler_factory, labels):
    cause = cause_factory(body={'metadata': {'labels': labels}})
    handler_factory(labels={'known': 'value'})
    handlers = registry._spawning.get_handlers(cause)
    assert handlers


@pytest.mark.parametrize('labels', [
    pytest.param({}, id='without-label'),
    pytest.param({'known': 'other'}, id='with-other-value'),
    pytest.param({'extra': 'other'}, id='with-other-label'),
])
def test_catchall_handlers_with_exact_labels_not_satisfied(
        cause_factory, registry, handler_factory, labels):
    cause = cause_factory(body={'metadata': {'labels': labels}})
    handler_factory(labels={'known': 'value'})
    handlers = registry._spawning.get_handlers(cause)
    assert not handlers


@pytest.mark.parametrize('labels', [
    pytest.param({'known': 'value'}, id='with-label'),
    pytest.param({'known': 'other'}, id='with-other-value'),
])
def test_catchall_handlers_with_desired_labels_present(
        cause_factory, registry, handler_factory, labels):
    cause = cause_factory(body={'metadata': {'labels': labels}})
    handler_factory(labels={'known': PRESENT})
    handlers = registry._spawning.get_handlers(cause)
    assert handlers


@pytest.mark.parametrize('labels', [
    pytest.param({}, id='without-label'),
    pytest.param({'extra': 'other'}, id='with-other-label'),
])
def test_catchall_handlers_with_desired_labels_absent(
        cause_factory, registry, handler_factory, labels):
    cause = cause_factory(body={'metadata': {'labels': labels}})
    handler_factory(labels={'known': PRESENT})
    handlers = registry._spawning.get_handlers(cause)
    assert not handlers


@pytest.mark.parametrize('labels', [
    pytest.param({'known': 'value'}, id='with-label'),
    pytest.param({'known': 'other'}, id='with-other-value'),
])
def test_catchall_handlers_with_undesired_labels_present(
        cause_factory, registry, handler_factory, labels):
    cause = cause_factory(body={'metadata': {'labels': labels}})
    handler_factory(labels={'known': ABSENT})
    handlers = registry._spawning.get_handlers(cause)
    assert not handlers


@pytest.mark.parametrize('labels', [
    pytest.param({}, id='without-label'),
    pytest.param({'extra': 'other'}, id='with-other-label'),
])
def test_catchall_handlers_with_undesired_labels_absent(
        cause_factory, registry, handler_factory, labels):
    cause = cause_factory(body={'metadata': {'labels': labels}})
    handler_factory(labels={'known': ABSENT})
    handlers = registry._spawning.get_handlers(cause)
    assert handlers


@pytest.mark.parametrize('labels', [
    pytest.param({}, id='without-label'),
    pytest.param({'known': 'value'}, id='with-label'),
    pytest.param({'known': 'other'}, id='with-other-value'),
])
def test_catchall_handlers_with_labels_callback_says_true(
        cause_factory, registry, handler_factory, labels):
    cause = cause_factory(body={'metadata': {'labels': labels}})
    handler_factory(labels={'known': _always})
    handlers = registry._spawning.get_handlers(cause)
    assert handlers


@pytest.mark.parametrize('labels', [
    pytest.param({}, id='without-label'),
    pytest.param({'known': 'value'}, id='with-label'),
    pytest.param({'known': 'other'}, id='with-other-value'),
])
def test_catchall_handlers_with_labels_callback_says_false(
        cause_factory, registry, handler_factory, labels):
    cause = cause_factory(body={'metadata': {'labels': labels}})
    handler_factory(labels={'known': _never})
    handlers = registry._spawning.get_handlers(cause)
    assert not handlers


@pytest.mark.parametrize('labels', [
    pytest.param({}, id='without-label'),
    pytest.param({'known': 'value'}, id='with-label'),
    pytest.param({'known': 'other'}, id='with-other-value'),
    pytest.param({'extra': 'other'}, id='with-other-label'),
    pytest.param({'known': 'value', 'extra': 'other'}, id='with-extra-label'),
])
def test_catchall_handlers_without_labels(
        cause_factory, registry, handler_factory, labels):
    cause = cause_factory(body={'metadata': {'labels': labels}})
    handler_factory(labels=None)
    handlers = registry._spawning.get_handlers(cause)
    assert handlers


@pytest.mark.parametrize('annotations', [
    pytest.param({'known': 'value'}, id='with-annotation'),
    pytest.param({'known': 'value', 'extra': 'other'}, id='with-extra-annotation'),
])
def test_catchall_handlers_with_exact_annotations_satisfied(
        cause_factory, registry, handler_factory, annotations):
    cause = cause_factory(body={'metadata': {'annotations': annotations}})
    handler_factory(annotations={'known': 'value'})
    handlers = registry._spawning.get_handlers(cause)
    assert handlers


@pytest.mark.parametrize('annotations', [
    pytest.param({}, id='without-annotation'),
    pytest.param({'known': 'other'}, id='with-other-value'),
    pytest.param({'extra': 'other'}, id='with-other-annotation'),
])
def test_catchall_handlers_with_exact_annotations_not_satisfied(
        cause_factory, registry, handler_factory, annotations):
    cause = cause_factory(body={'metadata': {'annotations': annotations}})
    handler_factory(annotations={'known': 'value'})
    handlers = registry._spawning.get_handlers(cause)
    assert not handlers


@pytest.mark.parametrize('annotations', [
    pytest.param({'known': 'value'}, id='with-annotation'),
    pytest.param({'known': 'other'}, id='with-other-value'),
])
def test_catchall_handlers_with_desired_annotations_present(
        cause_factory, registry, handler_factory, annotations):
    cause = cause_factory(body={'metadata': {'annotations': annotations}})
    handler_factory(annotations={'known': PRESENT})
    handlers = registry._spawning.get_handlers(cause)
    assert handlers


@pytest.mark.parametrize('annotations', [
    pytest.param({}, id='without-annotation'),
    pytest.param({'extra': 'other'}, id='with-other-annotation'),
])
def test_catchall_handlers_with_desired_annotations_absent(
        cause_factory, registry, handler_factory, annotations):
    cause = cause_factory(body={'metadata': {'annotations': annotations}})
    handler_factory(annotations={'known': PRESENT})
    handlers = registry._spawning.get_handlers(cause)
    assert not handlers


@pytest.mark.parametrize('annotations', [
    pytest.param({'known': 'value'}, id='with-annotation'),
    pytest.param({'known': 'other'}, id='with-other-value'),
])
def test_catchall_handlers_with_undesired_annotations_present(
        cause_factory, registry, handler_factory, annotations):
    cause = cause_factory(body={'metadata': {'annotations': annotations}})
    handler_factory(annotations={'known': ABSENT})
    handlers = registry._spawning.get_handlers(cause)
    assert not handlers


@pytest.mark.parametrize('annotations', [
    pytest.param({}, id='without-annotation'),
    pytest.param({'extra': 'other'}, id='with-other-annotation'),
])
def test_catchall_handlers_with_undesired_annotations_absent(
        cause_factory, registry, handler_factory, annotations):
    cause = cause_factory(body={'metadata': {'annotations': annotations}})
    handler_factory(annotations={'known': ABSENT})
    handlers = registry._spawning.get_handlers(cause)
    assert handlers


@pytest.mark.parametrize('annotations', [
    pytest.param({}, id='without-annotation'),
    pytest.param({'known': 'value'}, id='with-annotation'),
    pytest.param({'known': 'other'}, id='with-other-value'),
])
def test_catchall_handlers_with_annotations_callback_says_true(
        cause_factory, registry, handler_factory, annotations):
    cause = cause_factory(body={'metadata': {'annotations': annotations}})
    handler_factory(annotations={'known': _always})
    handlers = registry._spawning.get_handlers(cause)
    assert handlers


@pytest.mark.parametrize('annotations', [
    pytest.param({}, id='without-annotation'),
    pytest.param({'known': 'value'}, id='with-annotation'),
    pytest.param({'known': 'other'}, id='with-other-value'),
])
def test_catchall_handlers_with_annotations_callback_says_false(
        cause_factory, registry, handler_factory, annotations):
    cause = cause_factory(body={'metadata': {'annotations': annotations}})
    handler_factory(annotations={'known': _never})
    handlers = registry._spawning.get_handlers(cause)
    assert not handlers


@pytest.mark.parametrize('annotations', [
    pytest.param({}, id='without-annotation'),
    pytest.param({'known': 'value'}, id='with-annotation'),
    pytest.param({'known': 'other'}, id='with-other-value'),
    pytest.param({'extra': 'other'}, id='with-other-annotation'),
    pytest.param({'known': 'value', 'extra': 'other'}, id='with-extra-annotation'),
])
def test_catchall_handlers_without_annotations(
        cause_factory, registry, handler_factory, annotations):
    cause = cause_factory(body={'metadata': {'annotations': annotations}})
    handler_factory()
    handlers = registry._spawning.get_handlers(cause)
    assert handlers


@pytest.mark.parametrize('labels, annotations', [
    pytest.param({'known': 'value'}, {'known': 'value'}, id='with-label-annotation'),
    pytest.param({'known': 'value', 'extra': 'other'}, {'known': 'value'}, id='with-extra-label-annotation'),
    pytest.param({'known': 'value'}, {'known': 'value', 'extra': 'other'}, id='with-label-extra-annotation'),
    pytest.param({'known': 'value', 'extra': 'other'}, {'known': 'value', 'extra': 'other'}, id='with-extra-label-extra-annotation'),
])
def test_catchall_handlers_with_labels_and_annotations_satisfied(
        cause_factory, registry, handler_factory, labels, annotations):
    cause = cause_factory(body={'metadata': {'labels': labels, 'annotations': annotations}})
    handler_factory(labels={'known': 'value'}, annotations={'known': 'value'})
    handlers = registry._spawning.get_handlers(cause)
    assert handlers


@pytest.mark.parametrize('labels', [
    pytest.param({}, id='without-label'),
    pytest.param({'known': 'value'}, id='with-label'),
    pytest.param({'known': 'other'}, id='with-other-value'),
    pytest.param({'extra': 'other'}, id='with-other-label'),
    pytest.param({'known': 'value', 'extra': 'other'}, id='with-extra-label'),
])
def test_catchall_handlers_with_labels_and_annotations_not_satisfied(
        cause_factory, registry, handler_factory, labels):
    cause = cause_factory(body={'metadata': {'labels': labels}})
    handler_factory(labels={'known': 'value'}, annotations={'known': 'value'})
    handlers = registry._spawning.get_handlers(cause)
    assert not handlers


@pytest.mark.parametrize('when', [
    pytest.param(None, id='without-when'),
    pytest.param(lambda body=None, **_: body['spec']['name'] == 'test', id='with-when'),
    pytest.param(lambda **_: True, id='with-other-when'),
])
def test_catchall_handlers_with_when_callback_matching(
        cause_factory, registry, handler_factory, when):
    cause = cause_factory(body={'spec': {'name': 'test'}})
    handler_factory(when=when)
    handlers = registry._spawning.get_handlers(cause)
    assert handlers


@pytest.mark.parametrize('when', [
    pytest.param(lambda body=None, **_: body['spec']['name'] != "test", id='with-when'),
    pytest.param(lambda **_: False, id='with-other-when'),
])
def test_catchall_handlers_with_when_callback_mismatching(
        cause_factory, registry, handler_factory, when):
    cause = cause_factory(body={'spec': {'name': 'test'}})
    handler_factory(when=when)
    handlers = registry._spawning.get_handlers(cause)
    assert not handlers


@spawning_decorators
def test_decorator_without_field_found(
        cause_any_field, registry, resource, decorator):

    @decorator(*resource, field=None)
    def some_fn(**_): pass

    cause = cause_any_field
    handlers = registry._spawning.get_handlers(cause)
    assert handlers


@spawning_decorators
def test_decorator_with_field_found(
        cause_with_field, registry, resource, decorator):

    @decorator(*resource, field='known-field')
    def some_fn(**_): pass

    cause = cause_with_field
    handlers = registry._spawning.get_handlers(cause)
    assert handlers


@spawning_decorators
def test_decorator_with_field_ignored(
        cause_no_field, registry, resource, decorator):

    @decorator(*resource, field='known-field')
    def some_fn(**_): pass

    cause = cause_no_field
    handlers = registry._spawning.get_handlers(cause)
    assert not handlers


@spawning_decorators
def test_decorator_with_labels_satisfied(
        cause_any_field, registry, resource, decorator):

    @decorator(*resource, labels={'known': PRESENT})
    def some_fn(**_): pass

    cause = cause_any_field
    handlers = registry._spawning.get_handlers(cause)
    assert handlers


@spawning_decorators
def test_decorator_with_labels_not_satisfied(
        cause_any_field, registry, resource, decorator):

    @decorator(*resource, labels={'extra': PRESENT})
    def some_fn(**_): pass

    cause = cause_any_field
    handlers = registry._spawning.get_handlers(cause)
    assert not handlers


@spawning_decorators
def test_decorator_with_annotations_satisfied(
        cause_any_field, registry, resource, decorator):

    @decorator(*resource, annotations={'known': PRESENT})
    def some_fn(**_): pass

    cause = cause_any_field
    handlers = registry._spawning.get_handlers(cause)
    assert handlers


@spawning_decorators
def test_decorator_with_annotations_not_satisfied(
        cause_any_field, registry, resource, decorator):

    @decorator(*resource, annotations={'extra': PRESENT})
    def some_fn(**_): pass

    cause = cause_any_field
    handlers = registry._spawning.get_handlers(cause)
    assert not handlers


@spawning_decorators
def test_decorator_with_filter_satisfied(
        cause_any_field, registry, resource, decorator):

    @decorator(*resource, when=_always)
    def some_fn(**_): pass

    cause = cause_any_field
    handlers = registry._spawning.get_handlers(cause)
    assert handlers


@spawning_decorators
def test_decorator_with_filter_not_satisfied(
        cause_any_field, registry, resource, decorator):

    @decorator(*resource, when=_never)
    def some_fn(**_): pass

    cause = cause_any_field
    handlers = registry._spawning.get_handlers(cause)
    assert not handlers



================================================
FILE: tests/registries/test_matching_for_watching.py
================================================
import copy

import pytest

import kopf
from kopf._cogs.structs.dicts import parse_field
from kopf._core.intents.causes import WatchingCause
from kopf._core.intents.filters import ABSENT, PRESENT
from kopf._core.intents.handlers import WatchingHandler


# Used in the tests. Must be global-scoped, or its qualname will be affected.
def some_fn(x=None):
    pass


def _never(*_, **__):
    return False


def _always(*_, **__):
    return True


@pytest.fixture()
def handler_factory(registry, selector):
    def factory(**kwargs):
        handler = WatchingHandler(**dict(dict(
            fn=some_fn, id='a', param=None,
            errors=None, timeout=None, retries=None, backoff=None,
            selector=selector, annotations=None, labels=None, when=None,
            field=None, value=None,
        ), **kwargs))
        registry._watching.append(handler)
        return handler
    return factory


@pytest.fixture(params=[
    pytest.param(dict(body={}), id='no-field'),
])
def cause_no_field(request, cause_factory):
    kwargs = copy.deepcopy(request.param)
    kwargs['body'] |= {'metadata': {'labels': {'known': 'value'},
                                    'annotations': {'known': 'value'}}}
    cause = cause_factory(cls=WatchingCause, **kwargs)
    return cause


@pytest.fixture(params=[
    pytest.param(dict(body={'known-field': 'new'}), id='with-field'),
])
def cause_with_field(request, cause_factory):
    kwargs = copy.deepcopy(request.param)
    kwargs['body'] |= {'metadata': {'labels': {'known': 'value'},
                                    'annotations': {'known': 'value'}}}
    cause = cause_factory(cls=WatchingCause, **kwargs)
    return cause


@pytest.fixture(params=[
    # The original no-diff was equivalent to no-field until body/old/new were added to the check.
    pytest.param(dict(body={}, diff=[]), id='no-field'),
    pytest.param(dict(body={'known-field': 'new'}), id='with-field'),
])
def cause_any_field(request, cause_factory):
    kwargs = copy.deepcopy(request.param)
    kwargs['body'] |= {'metadata': {'labels': {'known': 'value'},
                                    'annotations': {'known': 'value'}}}
    cause = cause_factory(cls=WatchingCause, **kwargs)
    return cause


#
# "Catch-all" handlers are those with event == None.
#

def test_catchall_handlers_without_field_found(
        cause_any_field, registry, handler_factory):
    cause = cause_any_field
    handler_factory(field=None)
    handlers = registry._watching.get_handlers(cause)
    assert handlers


def test_catchall_handlers_with_field_found(
        cause_with_field, registry, handler_factory):
    cause = cause_with_field
    handler_factory(field=parse_field('known-field'))
    handlers = registry._watching.get_handlers(cause)
    assert handlers


def test_catchall_handlers_with_field_ignored(
        cause_no_field, registry, handler_factory):
    cause = cause_no_field
    handler_factory(field=parse_field('known-field'))
    handlers = registry._watching.get_handlers(cause)
    assert not handlers


@pytest.mark.parametrize('labels', [
    pytest.param({'known': 'value'}, id='with-label'),
    pytest.param({'known': 'value', 'extra': 'other'}, id='with-extra-label'),
])
def test_catchall_handlers_with_exact_labels_satisfied(
        cause_factory, registry, handler_factory, labels):
    cause = cause_factory(body={'metadata': {'labels': labels}})
    handler_factory(labels={'known': 'value'})
    handlers = registry._watching.get_handlers(cause)
    assert handlers


@pytest.mark.parametrize('labels', [
    pytest.param({}, id='without-label'),
    pytest.param({'known': 'other'}, id='with-other-value'),
    pytest.param({'extra': 'other'}, id='with-other-label'),
])
def test_catchall_handlers_with_exact_labels_not_satisfied(
        cause_factory, registry, handler_factory, labels):
    cause = cause_factory(body={'metadata': {'labels': labels}})
    handler_factory(labels={'known': 'value'})
    handlers = registry._watching.get_handlers(cause)
    assert not handlers


@pytest.mark.parametrize('labels', [
    pytest.param({'known': 'value'}, id='with-label'),
    pytest.param({'known': 'other'}, id='with-other-value'),
])
def test_catchall_handlers_with_desired_labels_present(
        cause_factory, registry, handler_factory, labels):
    cause = cause_factory(body={'metadata': {'labels': labels}})
    handler_factory(labels={'known': PRESENT})
    handlers = registry._watching.get_handlers(cause)
    assert handlers


@pytest.mark.parametrize('labels', [
    pytest.param({}, id='without-label'),
    pytest.param({'extra': 'other'}, id='with-other-label'),
])
def test_catchall_handlers_with_desired_labels_absent(
        cause_factory, registry, handler_factory, labels):
    cause = cause_factory(body={'metadata': {'labels': labels}})
    handler_factory(labels={'known': PRESENT})
    handlers = registry._watching.get_handlers(cause)
    assert not handlers


@pytest.mark.parametrize('labels', [
    pytest.param({'known': 'value'}, id='with-label'),
    pytest.param({'known': 'other'}, id='with-other-value'),
])
def test_catchall_handlers_with_undesired_labels_present(
        cause_factory, registry, handler_factory, labels):
    cause = cause_factory(body={'metadata': {'labels': labels}})
    handler_factory(labels={'known': ABSENT})
    handlers = registry._watching.get_handlers(cause)
    assert not handlers


@pytest.mark.parametrize('labels', [
    pytest.param({}, id='without-label'),
    pytest.param({'extra': 'other'}, id='with-other-label'),
])
def test_catchall_handlers_with_undesired_labels_absent(
        cause_factory, registry, handler_factory, labels):
    cause = cause_factory(body={'metadata': {'labels': labels}})
    handler_factory(labels={'known': ABSENT})
    handlers = registry._watching.get_handlers(cause)
    assert handlers


@pytest.mark.parametrize('labels', [
    pytest.param({}, id='without-label'),
    pytest.param({'known': 'value'}, id='with-label'),
    pytest.param({'known': 'other'}, id='with-other-value'),
])
def test_catchall_handlers_with_labels_callback_says_true(
        cause_factory, registry, handler_factory, labels):
    cause = cause_factory(body={'metadata': {'labels': labels}})
    handler_factory(labels={'known': _always})
    handlers = registry._watching.get_handlers(cause)
    assert handlers


@pytest.mark.parametrize('labels', [
    pytest.param({}, id='without-label'),
    pytest.param({'known': 'value'}, id='with-label'),
    pytest.param({'known': 'other'}, id='with-other-value'),
])
def test_catchall_handlers_with_labels_callback_says_false(
        cause_factory, registry, handler_factory, labels):
    cause = cause_factory(body={'metadata': {'labels': labels}})
    handler_factory(labels={'known': _never})
    handlers = registry._watching.get_handlers(cause)
    assert not handlers


@pytest.mark.parametrize('labels', [
    pytest.param({}, id='without-label'),
    pytest.param({'known': 'value'}, id='with-label'),
    pytest.param({'known': 'other'}, id='with-other-value'),
    pytest.param({'extra': 'other'}, id='with-other-label'),
    pytest.param({'known': 'value', 'extra': 'other'}, id='with-extra-label'),
])
def test_catchall_handlers_without_labels(
        cause_factory, registry, handler_factory, labels):
    cause = cause_factory(body={'metadata': {'labels': labels}})
    handler_factory(labels=None)
    handlers = registry._watching.get_handlers(cause)
    assert handlers


@pytest.mark.parametrize('annotations', [
    pytest.param({'known': 'value'}, id='with-annotation'),
    pytest.param({'known': 'value', 'extra': 'other'}, id='with-extra-annotation'),
])
def test_catchall_handlers_with_exact_annotations_satisfied(
        cause_factory, registry, handler_factory, annotations):
    cause = cause_factory(body={'metadata': {'annotations': annotations}})
    handler_factory(annotations={'known': 'value'})
    handlers = registry._watching.get_handlers(cause)
    assert handlers


@pytest.mark.parametrize('annotations', [
    pytest.param({}, id='without-annotation'),
    pytest.param({'known': 'other'}, id='with-other-value'),
    pytest.param({'extra': 'other'}, id='with-other-annotation'),
])
def test_catchall_handlers_with_exact_annotations_not_satisfied(
        cause_factory, registry, handler_factory, annotations):
    cause = cause_factory(body={'metadata': {'annotations': annotations}})
    handler_factory(annotations={'known': 'value'})
    handlers = registry._watching.get_handlers(cause)
    assert not handlers


@pytest.mark.parametrize('annotations', [
    pytest.param({'known': 'value'}, id='with-annotation'),
    pytest.param({'known': 'other'}, id='with-other-value'),
])
def test_catchall_handlers_with_desired_annotations_present(
        cause_factory, registry, handler_factory, annotations):
    cause = cause_factory(body={'metadata': {'annotations': annotations}})
    handler_factory(annotations={'known': PRESENT})
    handlers = registry._watching.get_handlers(cause)
    assert handlers


@pytest.mark.parametrize('annotations', [
    pytest.param({}, id='without-annotation'),
    pytest.param({'extra': 'other'}, id='with-other-annotation'),
])
def test_catchall_handlers_with_desired_annotations_absent(
        cause_factory, registry, handler_factory, annotations):
    cause = cause_factory(body={'metadata': {'annotations': annotations}})
    handler_factory(annotations={'known': PRESENT})
    handlers = registry._watching.get_handlers(cause)
    assert not handlers


@pytest.mark.parametrize('annotations', [
    pytest.param({'known': 'value'}, id='with-annotation'),
    pytest.param({'known': 'other'}, id='with-other-value'),
])
def test_catchall_handlers_with_undesired_annotations_present(
        cause_factory, registry, handler_factory, annotations):
    cause = cause_factory(body={'metadata': {'annotations': annotations}})
    handler_factory(annotations={'known': ABSENT})
    handlers = registry._watching.get_handlers(cause)
    assert not handlers


@pytest.mark.parametrize('annotations', [
    pytest.param({}, id='without-annotation'),
    pytest.param({'extra': 'other'}, id='with-other-annotation'),
])
def test_catchall_handlers_with_undesired_annotations_absent(
        cause_factory, registry, handler_factory, annotations):
    cause = cause_factory(body={'metadata': {'annotations': annotations}})
    handler_factory(annotations={'known': ABSENT})
    handlers = registry._watching.get_handlers(cause)
    assert handlers


@pytest.mark.parametrize('annotations', [
    pytest.param({}, id='without-annotation'),
    pytest.param({'known': 'value'}, id='with-annotation'),
    pytest.param({'known': 'other'}, id='with-other-value'),
])
def test_catchall_handlers_with_annotations_callback_says_true(
        cause_factory, registry, handler_factory, annotations):
    cause = cause_factory(body={'metadata': {'annotations': annotations}})
    handler_factory(annotations={'known': _always})
    handlers = registry._watching.get_handlers(cause)
    assert handlers


@pytest.mark.parametrize('annotations', [
    pytest.param({}, id='without-annotation'),
    pytest.param({'known': 'value'}, id='with-annotation'),
    pytest.param({'known': 'other'}, id='with-other-value'),
])
def test_catchall_handlers_with_annotations_callback_says_false(
        cause_factory, registry, handler_factory, annotations):
    cause = cause_factory(body={'metadata': {'annotations': annotations}})
    handler_factory(annotations={'known': _never})
    handlers = registry._watching.get_handlers(cause)
    assert not handlers


@pytest.mark.parametrize('annotations', [
    pytest.param({}, id='without-annotation'),
    pytest.param({'known': 'value'}, id='with-annotation'),
    pytest.param({'known': 'other'}, id='with-other-value'),
    pytest.param({'extra': 'other'}, id='with-other-annotation'),
    pytest.param({'known': 'value', 'extra': 'other'}, id='with-extra-annotation'),
])
def test_catchall_handlers_without_annotations(
        cause_factory, registry, handler_factory, annotations):
    cause = cause_factory(body={'metadata': {'annotations': annotations}})
    handler_factory()
    handlers = registry._watching.get_handlers(cause)
    assert handlers


@pytest.mark.parametrize('labels, annotations', [
    pytest.param({'known': 'value'}, {'known': 'value'}, id='with-label-annotation'),
    pytest.param({'known': 'value', 'extra': 'other'}, {'known': 'value'}, id='with-extra-label-annotation'),
    pytest.param({'known': 'value'}, {'known': 'value', 'extra': 'other'}, id='with-label-extra-annotation'),
    pytest.param({'known': 'value', 'extra': 'other'}, {'known': 'value', 'extra': 'other'}, id='with-extra-label-extra-annotation'),
])
def test_catchall_handlers_with_labels_and_annotations_satisfied(
        cause_factory, registry, handler_factory, labels, annotations):
    cause = cause_factory(body={'metadata': {'labels': labels, 'annotations': annotations}})
    handler_factory(labels={'known': 'value'}, annotations={'known': 'value'})
    handlers = registry._watching.get_handlers(cause)
    assert handlers


@pytest.mark.parametrize('labels', [
    pytest.param({}, id='without-label'),
    pytest.param({'known': 'value'}, id='with-label'),
    pytest.param({'known': 'other'}, id='with-other-value'),
    pytest.param({'extra': 'other'}, id='with-other-label'),
    pytest.param({'known': 'value', 'extra': 'other'}, id='with-extra-label'),
])
def test_catchall_handlers_with_labels_and_annotations_not_satisfied(
        cause_factory, registry, handler_factory, labels):
    cause = cause_factory(body={'metadata': {'labels': labels}})
    handler_factory(labels={'known': 'value'}, annotations={'known': 'value'})
    handlers = registry._watching.get_handlers(cause)
    assert not handlers


@pytest.mark.parametrize('when', [
    pytest.param(None, id='without-when'),
    pytest.param(lambda body=None, **_: body['spec']['name'] == 'test', id='with-when'),
    pytest.param(lambda **_: True, id='with-other-when'),
])
def test_catchall_handlers_with_when_callback_matching(
        cause_factory, registry, handler_factory, when):
    cause = cause_factory(body={'spec': {'name': 'test'}})
    handler_factory(when=when)
    handlers = registry._watching.get_handlers(cause)
    assert handlers


@pytest.mark.parametrize('when', [
    pytest.param(lambda body=None, **_: body['spec']['name'] != "test", id='with-when'),
    pytest.param(lambda **_: False, id='with-other-when'),
])
def test_catchall_handlers_with_when_callback_mismatching(
        cause_factory, registry, handler_factory, when):
    cause = cause_factory(body={'spec': {'name': 'test'}})
    handler_factory(when=when)
    handlers = registry._watching.get_handlers(cause)
    assert not handlers


def test_decorator_without_field_found(
        cause_any_field, registry, resource):

    @kopf.on.event(*resource, field=None)
    def some_fn(**_): pass

    cause = cause_any_field
    handlers = registry._watching.get_handlers(cause)
    assert handlers


def test_decorator_with_field_found(
        cause_with_field, registry, resource):

    @kopf.on.event(*resource, field='known-field')
    def some_fn(**_): pass

    cause = cause_with_field
    handlers = registry._watching.get_handlers(cause)
    assert handlers


def test_decorator_with_field_ignored(
        cause_no_field, registry, resource):

    @kopf.on.event(*resource, field='known-field')
    def some_fn(**_): pass

    cause = cause_no_field
    handlers = registry._watching.get_handlers(cause)
    assert not handlers


def test_decorator_with_labels_satisfied(
        cause_any_field, registry, resource):

    @kopf.on.event(*resource, labels={'known': PRESENT})
    def some_fn(**_): pass

    cause = cause_any_field
    handlers = registry._watching.get_handlers(cause)
    assert handlers


def test_decorator_with_labels_not_satisfied(
        cause_any_field, registry, resource):

    @kopf.on.event(*resource, labels={'extra': PRESENT})
    def some_fn(**_): pass

    cause = cause_any_field
    handlers = registry._watching.get_handlers(cause)
    assert not handlers


def test_decorator_with_annotations_satisfied(
        cause_any_field, registry, resource):

    @kopf.on.event(*resource, annotations={'known': PRESENT})
    def some_fn(**_): pass

    cause = cause_any_field
    handlers = registry._watching.get_handlers(cause)
    assert handlers


def test_decorator_with_annotations_not_satisfied(
        cause_any_field, registry, resource):

    @kopf.on.event(*resource, annotations={'extra': PRESENT})
    def some_fn(**_): pass

    cause = cause_any_field
    handlers = registry._watching.get_handlers(cause)
    assert not handlers


def test_decorator_with_filter_satisfied(
        cause_any_field, registry, resource):

    @kopf.on.event(*resource, when=_always)
    def some_fn(**_): pass

    cause = cause_any_field
    handlers = registry._watching.get_handlers(cause)
    assert handlers


def test_decorator_with_filter_not_satisfied(
        cause_any_field, registry, resource):

    @kopf.on.event(*resource, when=_never)
    def some_fn(**_): pass

    cause = cause_any_field
    handlers = registry._watching.get_handlers(cause)
    assert not handlers



================================================
FILE: tests/registries/test_matching_of_callbacks.py
================================================
import dataclasses
from unittest.mock import Mock

import pytest

from kopf._cogs.structs.bodies import Body
from kopf._cogs.structs.dicts import parse_field
from kopf._cogs.structs.references import Resource
from kopf._core.intents.causes import WatchingCause
from kopf._core.intents.handlers import WatchingHandler
from kopf._core.intents.registries import match, prematch


# Used in the tests. Must be global-scoped, or its qualname will be affected.
def some_fn(x=None):
    pass


@pytest.fixture()
def callback():
    mock = Mock()
    mock.return_value = True
    return mock


@pytest.fixture(params=['annotations', 'labels', 'value', 'when'])
def handler(request, callback, selector):
    handler = WatchingHandler(
        selector=selector,
        annotations={'known': 'value'},
        labels={'known': 'value'},
        field=parse_field('spec.field'),
        value='value',
        when=None,
        fn=some_fn, id='a', param=None, errors=None, timeout=None, retries=None, backoff=None,
    )
    if request.param in ['annotations', 'labels']:
        handler = dataclasses.replace(handler, **{request.param: {'known': callback}})
    else:
        handler = dataclasses.replace(handler, **{request.param: callback})
    return handler


@pytest.fixture()
def cause(cause_factory, callback):
    return cause_factory(
        cls=WatchingCause,
        body=Body(dict(
            metadata=dict(
                labels={'known': 'value'},
                annotations={'known': 'value'},
            ),
            spec=dict(
                field='value',
            ),
        )))


@pytest.mark.parametrize('match_fn', [match, prematch])
def test_callback_is_called_with_matching_resource(
        match_fn, callback, handler, cause,
):
    result = match_fn(handler=handler, cause=cause)
    assert result
    assert callback.called


@pytest.mark.parametrize('match_fn', [match, prematch])
def test_callback_is_not_called_with_mismatching_resource(
        match_fn, callback, handler, cause,
):
    cause = dataclasses.replace(cause, resource=Resource(group='x', version='y', plural='z'))
    result = match_fn(handler=handler, cause=cause)
    assert not result
    assert not callback.called



================================================
FILE: tests/registries/test_matching_of_resources.py
================================================
from unittest.mock import Mock

from kopf._cogs.structs.references import Resource, Selector
from kopf._core.intents.registries import _matches_resource


def test_different_resource():
    selector = Selector('group1', 'version1', 'plural1')
    resource = Resource('group2', 'version2', 'plural2')
    handler = Mock(selector=selector)
    matches = _matches_resource(handler, resource)
    assert not matches


def test_equivalent_resources():
    selector = Selector('group1', 'version1', 'plural1')
    resource = Resource('group1', 'version1', 'plural1')
    handler = Mock(selector=selector)
    matches = _matches_resource(handler, resource)
    assert matches


def test_catchall_with_none():
    resource = Resource('group2', 'version2', 'plural2')
    handler = Mock(selector=None)
    matches = _matches_resource(handler, resource)
    assert matches



================================================
FILE: tests/registries/test_requires_finalizer.py
================================================
import pytest

import kopf
from kopf._core.intents.filters import PRESENT

OBJECT_BODY = {
    'apiVersion': 'group/version',
    'kind': 'singular',
    'metadata': {
        'name': 'test',
        'labels': {
            'key': 'value',
        },
        'annotations': {
            'key': 'value',
        }
    }
}


@pytest.mark.parametrize('optional, expected', [
    pytest.param(True, False, id='optional'),
    pytest.param(False, True, id='mandatory'),
])
def test_requires_finalizer_deletion_handler(
        optional, expected, cause_factory, resource, registry):
    cause = cause_factory(resource=resource, body=OBJECT_BODY)

    @kopf.on.delete(*resource, optional=optional)
    def fn(**_):
        pass

    requires_finalizer = registry._changing.requires_finalizer(cause)
    assert requires_finalizer == expected


@pytest.mark.parametrize('optional, expected', [
    pytest.param(True, False, id='optional'),
    pytest.param(False, True, id='mandatory'),
])
def test_requires_finalizer_multiple_handlers(
        optional, expected, cause_factory, resource, registry):
    cause = cause_factory(resource=resource, body=OBJECT_BODY)

    @kopf.on.create(*resource)
    def fn1(**_):
        pass

    @kopf.on.delete(*resource, optional=optional)
    def fn2(**_):
        pass

    requires_finalizer = registry._changing.requires_finalizer(cause)
    assert requires_finalizer == expected


def test_requires_finalizer_no_deletion_handler(
        cause_factory, resource, registry):
    cause = cause_factory(resource=resource, body=OBJECT_BODY)

    @kopf.on.create(*resource)
    def fn1(**_):
        pass

    requires_finalizer = registry._changing.requires_finalizer(cause)
    assert requires_finalizer is False


@pytest.mark.parametrize('optional, expected', [
    pytest.param(True, False, id='optional'),
    pytest.param(False, True, id='mandatory'),
])
@pytest.mark.parametrize('labels', [
    pytest.param({'key': 'value'}, id='value-matches'),
    pytest.param({'key': PRESENT}, id='key-exists'),
])
def test_requires_finalizer_deletion_handler_matches_labels(
        labels, optional, expected, cause_factory, resource, registry):
    cause = cause_factory(resource=resource, body=OBJECT_BODY)

    @kopf.on.delete(*resource, labels=labels, optional=optional)
    def fn(**_):
        pass

    requires_finalizer = registry._changing.requires_finalizer(cause)
    assert requires_finalizer == expected


@pytest.mark.parametrize('optional, expected', [
    pytest.param(True, False, id='optional'),
    pytest.param(False, False, id='mandatory'),
])
@pytest.mark.parametrize('labels', [
    pytest.param({'key': 'othervalue'}, id='value-mismatch'),
    pytest.param({'otherkey': PRESENT}, id='key-doesnt-exist'),
])
def test_requires_finalizer_deletion_handler_mismatches_labels(
        labels, optional, expected, cause_factory, resource, registry):
    cause = cause_factory(resource=resource, body=OBJECT_BODY)

    @kopf.on.delete(*resource, labels=labels, optional=optional)
    def fn(**_):
        pass

    requires_finalizer = registry._changing.requires_finalizer(cause)
    assert requires_finalizer == expected


@pytest.mark.parametrize('optional, expected', [
    pytest.param(True, False, id='optional'),
    pytest.param(False, True, id='mandatory'),
])
@pytest.mark.parametrize('annotations', [
    pytest.param({'key': 'value'}, id='value-matches'),
    pytest.param({'key': PRESENT}, id='key-exists'),
])
def test_requires_finalizer_deletion_handler_matches_annotations(
        annotations, optional, expected, cause_factory, resource, registry):
    cause = cause_factory(resource=resource, body=OBJECT_BODY)

    @kopf.on.delete(*resource, annotations=annotations, optional=optional)
    def fn(**_):
        pass

    requires_finalizer = registry._changing.requires_finalizer(cause)
    assert requires_finalizer == expected


@pytest.mark.parametrize('optional, expected', [
    pytest.param(True, False, id='optional'),
    pytest.param(False, False, id='mandatory'),
])
@pytest.mark.parametrize('annotations', [
    pytest.param({'key': 'othervalue'}, id='value-mismatch'),
    pytest.param({'otherkey': PRESENT}, id='key-doesnt-exist'),
])
def test_requires_finalizer_deletion_handler_mismatches_annotations(
        annotations, optional, expected, cause_factory, resource, registry):
    cause = cause_factory(resource=resource, body=OBJECT_BODY)

    @kopf.on.delete(*resource, annotations=annotations, optional=optional)
    def fn(**_):
        pass

    requires_finalizer = registry._changing.requires_finalizer(cause)
    assert requires_finalizer == expected



================================================
FILE: tests/registries/test_resumes_mixed_in.py
================================================
import pytest

import kopf
from kopf._core.intents.causes import HANDLER_REASONS, Reason


@pytest.mark.parametrize('deleted', [True, False, None])
@pytest.mark.parametrize('reason', HANDLER_REASONS)
def test_resumes_ignored_for_non_initial_causes(
        reason, deleted, cause_factory, resource):

    registry = kopf.get_default_registry()
    cause = cause_factory(resource=resource, reason=reason, initial=False,
                          body={'metadata': {'deletionTimestamp': '...'} if deleted else {}})

    @kopf.on.resume(*resource)
    def fn(**_):
        pass

    handlers = registry._changing.get_handlers(cause)
    assert len(handlers) == 0


@pytest.mark.parametrize('reason', list(set(HANDLER_REASONS) - {Reason.DELETE}))
def test_resumes_selected_for_initial_non_deletions(
        reason, cause_factory, resource):

    registry = kopf.get_default_registry()
    cause = cause_factory(resource=resource, reason=reason, initial=True)

    @kopf.on.resume(*resource)
    def fn(**_):
        pass

    handlers = registry._changing.get_handlers(cause)
    assert len(handlers) == 1
    assert handlers[0].fn is fn


@pytest.mark.parametrize('reason', [Reason.DELETE])
def test_resumes_ignored_for_initial_deletions_by_default(
        reason, cause_factory, resource):

    registry = kopf.get_default_registry()
    cause = cause_factory(resource=resource, reason=reason, initial=True,
                          body={'metadata': {'deletionTimestamp': '...'}})

    @kopf.on.resume(*resource)
    def fn(**_):
        pass

    handlers = registry._changing.get_handlers(cause)
    assert len(handlers) == 0


@pytest.mark.parametrize('reason', [Reason.DELETE])
def test_resumes_selected_for_initial_deletions_when_explicitly_marked(
        reason, cause_factory, resource):

    registry = kopf.get_default_registry()
    cause = cause_factory(resource=resource, reason=reason, initial=True,
                          body={'metadata': {'deletionTimestamp': '...'}})

    @kopf.on.resume(*resource, deleted=True)
    def fn(**_):
        pass

    handlers = registry._changing.get_handlers(cause)
    assert len(handlers) == 1
    assert handlers[0].fn is fn



================================================
FILE: tests/registries/test_subhandlers_ids.py
================================================
import kopf
from kopf._core.actions.execution import handler_var
from kopf._core.actions.invocation import context
from kopf._core.reactor.subhandling import subregistry_var


# Used in the tests. Must be global-scoped, or its qualname will be affected.
def child_fn(**_):
    pass


def test_with_parent(
        parent_handler, resource_registry_cls, cause_factory):

    cause = cause_factory(resource_registry_cls)
    registry = resource_registry_cls()

    with context([(handler_var, parent_handler), (subregistry_var, registry)]):
        kopf.subhandler()(child_fn)

    handlers = registry.get_handlers(cause)
    assert len(handlers) == 1
    assert handlers[0].fn is child_fn
    assert handlers[0].id == 'parent_fn/child_fn'



================================================
FILE: tests/settings/test_defaults.py
================================================
import logging

import kopf


async def test_declared_public_interface_and_promised_defaults():
    settings = kopf.OperatorSettings()
    assert settings.posting.level == logging.INFO
    assert settings.peering.name == "default"
    assert settings.peering.stealth == False
    assert settings.peering.priority == 0
    assert settings.peering.lifetime == 60
    assert settings.peering.mandatory == False
    assert settings.peering.standalone == False
    assert settings.peering.namespaced == True
    assert settings.peering.clusterwide == False
    assert settings.watching.reconnect_backoff == 0.1
    assert settings.watching.connect_timeout is None
    assert settings.watching.server_timeout is None
    assert settings.watching.client_timeout is None
    assert settings.batching.worker_limit is None
    assert settings.batching.idle_timeout == 5.0
    assert settings.batching.exit_timeout == 2.0
    assert settings.batching.batch_window == 0.1
    assert settings.batching.error_delays == (1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610)
    assert settings.scanning.disabled == False
    assert settings.admission.server is None
    assert settings.admission.managed is None
    assert settings.execution.executor is not None
    assert settings.execution.max_workers is None
    assert settings.networking.request_timeout == 5 * 60
    assert settings.networking.connect_timeout is None


async def test_peering_namespaced_is_modified_by_clusterwide():
    settings = kopf.OperatorSettings()
    assert settings.peering.namespaced == True
    settings.peering.clusterwide = not settings.peering.clusterwide
    assert settings.peering.namespaced == False


async def test_peering_clusterwide_is_modified_by_namespaced():
    settings = kopf.OperatorSettings()
    assert settings.peering.clusterwide == False
    settings.peering.namespaced = not settings.peering.namespaced
    assert settings.peering.clusterwide == True



================================================
FILE: tests/settings/test_executor.py
================================================
import concurrent.futures
import threading
from unittest.mock import MagicMock

import kopf
from kopf._core.actions.invocation import invoke


class CatchyExecutor(concurrent.futures.ThreadPoolExecutor):

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.calls = []

    def submit(self, fn, *args, **kwargs):
        self.calls.append(fn)
        return super().submit(fn, *args, **kwargs)


async def test_synchronous_calls_are_threaded():
    settings = kopf.OperatorSettings()
    thread = None

    def fn():
        nonlocal thread
        thread = threading.current_thread()

    mock = MagicMock(wraps=fn)
    await invoke(fn=mock, settings=settings)

    assert mock.called
    assert thread is not None  # remembered from inside fn()
    assert thread is not threading.current_thread()  # not in the main thread


async def test_synchronous_calls_use_replaced_executor():
    settings = kopf.OperatorSettings()
    executor = CatchyExecutor()
    settings.execution.executor = executor

    mock = MagicMock()
    await invoke(fn=mock, settings=settings)

    assert mock.called
    assert len(executor.calls) == 1


async def test_synchronous_executor_limit_is_applied():
    settings = kopf.OperatorSettings()
    assert hasattr(settings.execution.executor, '_max_workers')  # prerequisite

    assert settings.execution.max_workers is None  # as in "unset by us, assume defaults"
    assert settings.execution.executor._max_workers is not None  # usually CPU count * N.

    settings.execution.max_workers = 123456

    assert settings.execution.max_workers == 123456
    assert settings.execution.executor._max_workers == 123456



================================================
FILE: tests/testing/test_runner.py
================================================
import asyncio

import pytest

from kopf._cogs.configs.configuration import OperatorSettings
from kopf._core.intents.registries import OperatorRegistry
from kopf.testing import KopfRunner


@pytest.fixture(autouse=True)
def no_config_needed(login_mocks):
    pass


@pytest.fixture(autouse=True, params=['default', 'uvloop'])
def _event_loop_policy(request):
    original_policy = asyncio.get_event_loop_policy()
    if request.param == 'default':
        policy = asyncio.DefaultEventLoopPolicy()
    elif request.param == 'uvloop':
        uvloop = pytest.importorskip('uvloop')
        policy = uvloop.EventLoopPolicy()
    else:
        raise RuntimeError(f"Unknown event loop type {request.param!r}")

    asyncio.set_event_loop_policy(policy)
    yield
    asyncio.set_event_loop_policy(original_policy)


def test_command_invocation_works():
    with KopfRunner(['--help']) as runner:
        pass
    assert runner.exc_info
    assert runner.exc_info[0] is SystemExit
    assert runner.exc_info[1].code == 0
    assert runner.exit_code == 0
    assert runner.exception is None
    assert runner.output.startswith("Usage:")
    assert runner.stdout.startswith("Usage:")
    assert runner.stdout_bytes.startswith(b"Usage:")
    # Note: stderr is not captured, it is mixed with stdout.


def test_registry_and_settings_are_propagated(mocker):
    operator_mock = mocker.patch('kopf._core.reactor.running.operator')
    registry = OperatorRegistry()
    settings = OperatorSettings()
    with KopfRunner(['run', '--standalone'], registry=registry, settings=settings) as runner:
        pass
    assert runner.exit_code == 0
    assert runner.exception is None
    assert operator_mock.called
    assert operator_mock.call_args[1]['registry'] is registry
    assert operator_mock.call_args[1]['settings'] is settings


def test_exception_from_runner_suppressed_with_no_reraise():
    with KopfRunner(['run', 'non-existent.py', '--standalone'], reraise=False) as runner:
        pass
    assert runner.exception is not None
    assert isinstance(runner.exception, FileNotFoundError)


def test_exception_from_runner_escalates_with_reraise():
    with pytest.raises(FileNotFoundError):
        with KopfRunner(['run', 'non-existent.py', '--standalone'], reraise=True):
            pass


def test_exception_from_runner_escalates_by_default():
    with pytest.raises(FileNotFoundError):
        with KopfRunner(['run', 'non-existent.py', '--standalone']):
            pass


@pytest.mark.parametrize('kwargs',[
    dict(reraise=False),
    dict(reraise=True),
    dict(),
], ids=['reraise=False', 'reraise=True', 'no-reraise'])
def test_exception_from_invoke_escalates(mocker, kwargs):
    class SampleError(Exception): pass
    mocker.patch('click.testing.CliRunner.invoke', side_effect=SampleError)

    with pytest.raises(SampleError):
        with KopfRunner(['run', 'non-existent.py', '--standalone'], **kwargs):
            pass


def test_wrong_command_fails():
    with pytest.raises(SystemExit) as e:
        with KopfRunner(['unexistent-command']):
            pass
    assert e.value.code == 2


def test_absent_file_fails():
    with pytest.raises(FileNotFoundError):
        with KopfRunner(['run', 'non-existent.py', '--standalone']):
            pass


def test_bad_syntax_file_fails(tmpdir):
    path = tmpdir.join('handlers.py')
    path.write("""This is a Python syntax error!""")
    with pytest.raises((IndentationError, SyntaxError)):
        with KopfRunner(['run', str(path), '--standalone']):
            pass



================================================
FILE: tests/timing/test_sleeping.py
================================================
import asyncio

import pytest

from kopf._cogs.aiokits.aiotime import sleep


async def test_the_only_delay_is_awaited(timer):
    with timer:
        unslept = await sleep(0.10)
    assert 0.10 <= timer.seconds < 0.11
    assert unslept is None


async def test_the_shortest_delay_is_awaited(timer):
    with timer:
        unslept = await sleep([0.10, 0.20])
    assert 0.10 <= timer.seconds < 0.11
    assert unslept is None


async def test_specific_delays_only_are_awaited(timer):
    with timer:
        unslept = await sleep([0.10, None])
    assert 0.10 <= timer.seconds < 0.11
    assert unslept is None


@pytest.mark.parametrize('delays', [
    pytest.param([1000, -10], id='mixed-signs'),
    pytest.param([-100, -10], id='all-negative'),
    pytest.param(-10, id='alone'),
])
async def test_negative_delays_skip_sleeping(timer, delays):
    with timer:
        unslept = await sleep(delays)
    assert timer.seconds < 0.01
    assert unslept is None


@pytest.mark.parametrize('delays', [
    pytest.param([], id='empty-list'),
    pytest.param([None], id='list-of-none'),
])
async def test_no_delays_skip_sleeping(timer, delays):
    with timer:
        unslept = await sleep(delays)
    assert timer.seconds < 0.01
    assert unslept is None


async def test_by_event_set_before_time_comes(timer):
    event = asyncio.Event()
    asyncio.get_running_loop().call_later(0.07, event.set)
    with timer:
        unslept = await sleep(0.10, event)
    assert unslept is not None
    assert 0.02 <= unslept <= 0.04
    assert 0.06 <= timer.seconds <= 0.08


async def test_with_zero_time_and_event_initially_cleared(timer):
    event = asyncio.Event()
    event.clear()
    with timer:
        unslept = await sleep(0, event)
    assert timer.seconds <= 0.01
    assert unslept is None


async def test_with_zero_time_and_event_initially_set(timer):
    event = asyncio.Event()
    event.set()
    with timer:
        unslept = await sleep(0, event)
    assert timer.seconds <= 0.01
    assert not unslept  # 0/None; undefined for such case: both goals reached.



================================================
FILE: tests/timing/test_throttling.py
================================================
import asyncio
import logging
from unittest.mock import call

import pytest

from kopf._core.actions.throttlers import Throttler, throttled


@pytest.fixture(autouse=True)
def clock(mocker):
    return mocker.patch('time.monotonic', return_value=0)


@pytest.fixture(autouse=True)
def sleep(mocker):
    return mocker.patch('kopf._cogs.aiokits.aiotime.sleep', return_value=None)


async def test_remains_inactive_on_success():
    logger = logging.getLogger()
    throttler = Throttler()
    async with throttled(throttler=throttler, logger=logger, delays=[123]):
        pass
    assert throttler.source_of_delays is None
    assert throttler.last_used_delay is None


@pytest.mark.parametrize('exc_cls, kwargs', [
    pytest.param(BaseException, dict(), id='none'),
    pytest.param(BaseException, dict(errors=BaseException), id='base'),
    pytest.param(Exception, dict(errors=ValueError), id='child'),
    pytest.param(RuntimeError, dict(errors=ValueError), id='sibling'),
    pytest.param(RuntimeError, dict(errors=(ValueError, TypeError)), id='tuple'),
    pytest.param(asyncio.CancelledError, dict(), id='cancelled'),
])
async def test_escalates_unexpected_errors(exc_cls, kwargs):
    logger = logging.getLogger()
    throttler = Throttler()
    with pytest.raises(exc_cls):
        async with throttled(throttler=throttler, logger=logger, delays=[123], **kwargs):
            raise exc_cls()


@pytest.mark.parametrize('exc_cls, kwargs', [
    pytest.param(Exception, dict(), id='none'),
    pytest.param(RuntimeError, dict(errors=Exception), id='parent'),
    pytest.param(RuntimeError, dict(errors=(RuntimeError, EnvironmentError)), id='tuple'),
])
async def test_activates_on_expected_errors(exc_cls, kwargs):
    logger = logging.getLogger()
    throttler = Throttler()
    async with throttled(throttler=throttler, logger=logger, delays=[123], **kwargs):
        raise exc_cls()
    assert throttler.source_of_delays is not None
    assert throttler.last_used_delay is not None


async def test_sleeps_for_the_first_delay_when_inactive(sleep):
    logger = logging.getLogger()
    throttler = Throttler()

    async with throttled(throttler=throttler, logger=logger, delays=[123, 234]):
        raise Exception()

    assert throttler.last_used_delay == 123
    assert throttler.source_of_delays is not None
    assert next(throttler.source_of_delays) == 234

    assert throttler.active_until is None  # means: no sleep time left
    assert sleep.mock_calls == [call(123, wakeup=None)]


async def test_sleeps_for_the_next_delay_when_active(sleep):
    logger = logging.getLogger()
    throttler = Throttler()

    async with throttled(throttler=throttler, logger=logger, delays=[123, 234]):
        raise Exception()

    sleep.reset_mock()
    async with throttled(throttler=throttler, logger=logger, delays=[...]):
        raise Exception()

    assert throttler.last_used_delay == 234
    assert throttler.source_of_delays is not None
    assert next(throttler.source_of_delays, 999) == 999

    assert throttler.active_until is None  # means: no sleep time left
    assert sleep.mock_calls == [call(234, wakeup=None)]


async def test_sleeps_for_the_last_known_delay_when_depleted(sleep):
    logger = logging.getLogger()
    throttler = Throttler()

    async with throttled(throttler=throttler, logger=logger, delays=[123, 234]):
        raise Exception()

    async with throttled(throttler=throttler, logger=logger, delays=[...]):
        raise Exception()

    sleep.reset_mock()
    async with throttled(throttler=throttler, logger=logger, delays=[...]):
        raise Exception()

    assert throttler.last_used_delay == 234
    assert throttler.source_of_delays is not None
    assert next(throttler.source_of_delays, 999) == 999

    assert throttler.active_until is None  # means: no sleep time left
    assert sleep.mock_calls == [call(234, wakeup=None)]


async def test_resets_on_success(sleep):
    logger = logging.getLogger()
    throttler = Throttler()

    async with throttled(throttler=throttler, logger=logger, delays=[123]):
        raise Exception()

    sleep.reset_mock()
    async with throttled(throttler=throttler, logger=logger, delays=[...]):
        pass

    assert throttler.last_used_delay is None
    assert throttler.source_of_delays is None
    assert throttler.active_until is None
    assert sleep.mock_calls == []


async def test_skips_on_no_delays(sleep):
    logger = logging.getLogger()
    throttler = Throttler()

    async with throttled(throttler=throttler, logger=logger, delays=[]):
        raise Exception()

    assert throttler.last_used_delay is None
    assert throttler.source_of_delays is not None
    assert next(throttler.source_of_delays, 999) == 999

    assert throttler.active_until is None  # means: no sleep time left
    assert sleep.mock_calls == []


async def test_renews_on_repeated_failure(sleep):
    logger = logging.getLogger()
    throttler = Throttler()

    async with throttled(throttler=throttler, logger=logger, delays=[123]):
        raise Exception()

    async with throttled(throttler=throttler, logger=logger, delays=[...]):
        pass

    sleep.reset_mock()
    async with throttled(throttler=throttler, logger=logger, delays=[234]):
        raise Exception()

    assert throttler.last_used_delay == 234
    assert throttler.source_of_delays is not None
    assert throttler.active_until is None
    assert sleep.mock_calls == [call(234, wakeup=None)]


async def test_interruption(clock, sleep):
    wakeup = asyncio.Event()
    logger = logging.getLogger()
    throttler = Throttler()

    clock.return_value = 1000  # simulated "now"
    sleep.return_value = 55  # simulated sleep time left
    async with throttled(throttler=throttler, logger=logger, delays=[123, 234], wakeup=wakeup):
        raise Exception()

    assert throttler.last_used_delay == 123
    assert throttler.source_of_delays is not None
    assert throttler.active_until == 1123  # means: some sleep time is left
    assert sleep.mock_calls == [call(123, wakeup=wakeup)]


async def test_continuation_with_success(clock, sleep):
    wakeup = asyncio.Event()
    logger = logging.getLogger()
    throttler = Throttler()

    clock.return_value = 1000  # simulated "now"
    sleep.return_value = 55  # simulated sleep time left
    async with throttled(throttler=throttler, logger=logger, delays=[123, 234], wakeup=wakeup):
        raise Exception()

    sleep.reset_mock()
    clock.return_value = 1077  # simulated "now"
    sleep.return_value = None  # simulated sleep time left
    async with throttled(throttler=throttler, logger=logger, delays=[...], wakeup=wakeup):
        pass

    assert throttler.last_used_delay is None
    assert throttler.source_of_delays is None
    assert throttler.active_until is None  # means: no sleep time is left
    assert sleep.mock_calls == [call(123 - 77, wakeup=wakeup)]


async def test_continuation_with_error(clock, sleep):
    wakeup = asyncio.Event()
    logger = logging.getLogger()
    throttler = Throttler()

    clock.return_value = 1000  # simulated "now"
    sleep.return_value = 55  # simulated sleep time left
    async with throttled(throttler=throttler, logger=logger, delays=[123, 234], wakeup=wakeup):
        raise Exception()

    sleep.reset_mock()
    clock.return_value = 1077  # simulated "now"
    sleep.return_value = None  # simulated sleep time left
    async with throttled(throttler=throttler, logger=logger, delays=[...], wakeup=wakeup):
        raise Exception()

    assert throttler.last_used_delay == 234
    assert throttler.source_of_delays is not None
    assert throttler.active_until is None  # means: no sleep time is left
    assert sleep.mock_calls == [call(123 - 77, wakeup=wakeup), call(234, wakeup=wakeup)]


async def test_continuation_when_overdue(clock, sleep):
    wakeup = asyncio.Event()
    logger = logging.getLogger()
    throttler = Throttler()

    clock.return_value = 1000  # simulated "now"
    sleep.return_value = 55  # simulated sleep time left
    async with throttled(throttler=throttler, logger=logger, delays=[123, 234], wakeup=wakeup):
        raise Exception()

    sleep.reset_mock()
    clock.return_value = 2000  # simulated "now"
    sleep.return_value = None  # simulated sleep time left
    async with throttled(throttler=throttler, logger=logger, delays=[...], wakeup=wakeup):
        raise Exception()

    assert throttler.last_used_delay == 234
    assert throttler.source_of_delays is not None
    assert throttler.active_until is None  # means: no sleep time is left
    assert sleep.mock_calls == [call(123 - 1000, wakeup=wakeup), call(234, wakeup=wakeup)]


async def test_recommends_running_initially():
    logger = logging.getLogger()
    throttler = Throttler()
    async with throttled(throttler=throttler, logger=logger, delays=[123]) as should_run:
        remembered_should_run = should_run
    assert remembered_should_run is True


async def test_recommends_skipping_immediately_after_interrupted_error(sleep):
    logger = logging.getLogger()
    throttler = Throttler()

    sleep.return_value = 33  # simulated sleep time left
    async with throttled(throttler=throttler, logger=logger, delays=[123]):
        raise Exception()

    sleep.return_value = 33  # simulated sleep time left
    async with throttled(throttler=throttler, logger=logger, delays=[...]) as should_run:
        remembered_should_run = should_run

    assert remembered_should_run is False


async def test_recommends_running_immediately_after_continued(sleep):
    logger = logging.getLogger()
    throttler = Throttler()

    sleep.return_value = 33  # simulated sleep time left
    async with throttled(throttler=throttler, logger=logger, delays=[123]):
        raise Exception()

    sleep.return_value = None  # simulated sleep time left
    async with throttled(throttler=throttler, logger=logger, delays=[...]) as should_run:
        remembered_should_run = should_run

    assert remembered_should_run is True


async def test_logging_when_deactivates_immediately(caplog):
    caplog.set_level(0)
    logger = logging.getLogger()
    throttler = Throttler()

    async with throttled(throttler=throttler, logger=logger, delays=[123]):
        raise Exception("boo!")

    assert caplog.messages == [
        "Throttling for 123 seconds due to an unexpected error: Exception('boo!')",
        "Throttling is over. Switching back to normal operations.",
    ]


async def test_logging_when_deactivates_on_reentry(sleep, caplog):
    caplog.set_level(0)
    logger = logging.getLogger()
    throttler = Throttler()

    sleep.return_value = 55  # simulated sleep time left
    async with throttled(throttler=throttler, logger=logger, delays=[123]):
        raise Exception("boo!")

    sleep.return_value = None  # simulated sleep time left
    async with throttled(throttler=throttler, logger=logger, delays=[...]):
        pass

    assert caplog.messages == [
        "Throttling for 123 seconds due to an unexpected error: Exception('boo!')",
        "Throttling is over. Switching back to normal operations.",
    ]



================================================
FILE: tests/utilities/aiotasks/test_coro_cancellation.py
================================================
import asyncio
import gc
import warnings
from unittest.mock import AsyncMock, Mock

import pytest

from kopf._cogs.aiokits.aiotasks import cancel_coro


async def f(mock):
    return mock()


# Kwargs are accepted to match the signatures, but are unused/not passed through due to no need.
# Usually those are `name` & `context`, as in `asyncio.create_task(…)`.
def factory(loop, coro_or_mock, **_):
    coro = coro_or_mock._mock_wraps if isinstance(coro_or_mock, AsyncMock) else coro_or_mock
    return asyncio.Task(coro, loop=loop)


@pytest.fixture(autouse=True)
async def coromock_task_factory():
    factory_spy = Mock(wraps=factory)
    asyncio.get_running_loop().set_task_factory(factory_spy)
    yield factory_spy
    asyncio.get_running_loop().set_task_factory(None)


async def test_coro_issues_a_warning_normally(coromock_task_factory):
    with warnings.catch_warnings(record=True) as w:
        warnings.simplefilter('default')
        mock = Mock()
        coro = f(mock)

        # The warnings come only from the garbage collection, so dereference it.
        del coro
        gc.collect()

    # The 1st coro is the test function itself; the 2nd coro would be the coro-under-test.
    assert coromock_task_factory.call_count == 1  # i.e. the task was NOT created.
    assert not mock.called
    assert len(w) == 1
    assert issubclass(w[0].category, RuntimeWarning)
    assert str(w[0].message) == "coroutine 'f' was never awaited"


async def test_coro_is_closed_via_a_hack_with_no_warning(coromock_task_factory):
    with warnings.catch_warnings(record=True) as w:
        warnings.simplefilter('default')
        mock = Mock()
        coro = f(mock)
        await cancel_coro(coro)

        # The warnings come only from the garbage collection, so dereference it.
        del coro
        gc.collect()

    # The 1st coro is the test function itself; the 2nd coro would be the coro-under-test.
    assert coromock_task_factory.call_count == 1  # i.e. the task was NOT created.
    assert not mock.called
    assert not w


async def test_coro_is_awaited_via_a_task_with_no_warning(coromock_task_factory):
    with warnings.catch_warnings(record=True) as w:
        warnings.simplefilter('default')
        mock = Mock()
        coro = AsyncMock(wraps=f(mock))
        del coro.close
        await cancel_coro(coro)

        # The warnings come only from the garbage collection, so dereference it.
        del coro
        gc.collect()

    # The 1st coro is the test function itself; the 2nd coro is the coro-under-test.
    assert coromock_task_factory.call_count == 2  # i.e. the task WAS created.
    assert not mock.called
    assert not w



================================================
FILE: tests/utilities/aiotasks/test_scheduler.py
================================================
import asyncio
from unittest.mock import Mock

import pytest

from kopf._cogs.aiokits.aiotasks import Scheduler

CODE_OVERHEAD = 0.01


async def f(mock, *args):
    try:
        mock('started')
        for arg in args:
            if isinstance(arg, asyncio.Event):
                arg.set()
            elif isinstance(arg, float):
                await asyncio.sleep(arg)
            elif callable(arg):
                arg()
    except asyncio.CancelledError:
        mock('cancelled')
    else:
        mock('finished')


async def test_empty_scheduler_lifecycle(timer):
    with timer:
        scheduler = Scheduler()
        assert scheduler.empty()
        await scheduler.wait()
        assert scheduler.empty()
        await scheduler.close()
        assert scheduler.empty()
    assert timer.seconds < CODE_OVERHEAD


async def test_task_spawning_and_graceful_finishing(timer):
    mock = Mock()
    flag1 = asyncio.Event()
    flag2 = asyncio.Event()
    scheduler = Scheduler()

    result = await scheduler.spawn(f(mock, flag1, 0.1, flag2))
    assert result is None

    with timer:
        await flag1.wait()
    assert timer.seconds < CODE_OVERHEAD
    assert mock.call_args[0][0] == 'started'

    with timer:
        await flag2.wait()
    assert timer.seconds > 0.1
    assert timer.seconds < 0.1 + CODE_OVERHEAD
    assert mock.call_args[0][0] == 'finished'

    await scheduler.close()


async def test_task_spawning_and_cancellation(timer):
    mock = Mock()
    flag1 = asyncio.Event()
    flag2 = asyncio.Event()
    scheduler = Scheduler()

    result = await scheduler.spawn(f(mock, flag1, 1.0, flag2))
    assert result is None

    with timer:
        await flag1.wait()
    assert timer.seconds < CODE_OVERHEAD
    assert mock.call_args[0][0] == 'started'

    with timer:
        await scheduler.close()
    assert timer.seconds < CODE_OVERHEAD  # near-instant
    assert mock.call_args[0][0] == 'cancelled'


async def test_no_tasks_are_accepted_after_closing():
    scheduler = Scheduler()
    await scheduler.close()

    assert scheduler._closed
    assert scheduler._spawning_task.done()
    assert scheduler._cleaning_task.done()

    with pytest.raises(RuntimeError, match=r"Cannot add new coroutines"):
        await scheduler.spawn(f(Mock(), 1.0))


async def test_successes_are_not_reported():
    exception_handler = Mock()
    scheduler = Scheduler(exception_handler=exception_handler)
    await scheduler.spawn(f(Mock()))
    await scheduler.wait()
    await scheduler.close()
    assert exception_handler.call_count == 0


async def test_cancellations_are_not_reported():
    exception_handler = Mock()
    mock = Mock(side_effect=asyncio.CancelledError())
    scheduler = Scheduler(exception_handler=exception_handler)
    await scheduler.spawn(f(mock, 1))
    await scheduler.wait()
    await scheduler.close()
    assert exception_handler.call_count == 0


async def test_exceptions_are_reported():
    exception = ValueError('hello')
    exception_handler = Mock()
    mock = Mock(side_effect=exception)
    scheduler = Scheduler(exception_handler=exception_handler)
    await scheduler.spawn(f(mock))
    await scheduler.wait()
    await scheduler.close()
    assert exception_handler.call_count == 1
    assert exception_handler.call_args[0][0] is exception


async def test_tasks_are_parallel_if_limit_is_not_reached(timer):
    """
    time:  ////////----------------------0.1s------------------0.2s--///
    task1: ->spawn->start->sleep->finish->|
    task2: ->spawn->start->sleep->finish->|
    """
    task1_started = asyncio.Event()
    task1_finished = asyncio.Event()
    task2_started = asyncio.Event()
    task2_finished = asyncio.Event()
    scheduler = Scheduler(limit=2)

    with timer:
        await scheduler.spawn(f(Mock(), task1_started, 0.1, task1_finished))
        await scheduler.spawn(f(Mock(), task2_started, 0.1, task2_finished))
    assert timer.seconds < CODE_OVERHEAD  # i.e. spawning is not not blocking

    with timer:
        await task1_finished.wait()
        assert task2_started.is_set()
        await task2_finished.wait()

    assert timer.seconds > 0.1
    assert timer.seconds < 0.1 + CODE_OVERHEAD

    await scheduler.close()


async def test_tasks_are_pending_if_limit_is_reached(timer):
    """
    time:  ////////----------------------0.1s------------------0.2s--///
    task1: ->spawn->start->sleep->finish->|
    task2: ->spawn->.....(pending)......->start->sleep->finish->|
    """
    task1_started = asyncio.Event()
    task1_finished = asyncio.Event()
    task2_started = asyncio.Event()
    task2_finished = asyncio.Event()
    scheduler = Scheduler(limit=1)

    with timer:
        await scheduler.spawn(f(Mock(), task1_started, 0.1, task1_finished))
        await scheduler.spawn(f(Mock(), task2_started, 0.1, task2_finished))
    assert timer.seconds < CODE_OVERHEAD  # i.e. spawning is not not blocking

    with timer:
        await task1_finished.wait()
        assert not task2_started.is_set()
        await task2_finished.wait()

    assert timer.seconds > 0.2
    assert timer.seconds < 0.2 + CODE_OVERHEAD * 2

    await scheduler.close()



================================================
FILE: tests/utilities/aiotasks/test_task_guarding.py
================================================
import asyncio
import logging

import pytest

from kopf._cogs.aiokits.aiotasks import create_guarded_task, reraise


class Error(Exception):
    pass


async def fail(msg: str) -> None:
    raise Error(msg)


async def delay(n: float) -> None:
    await asyncio.sleep(n)


async def sample() -> None:
    pass


async def test_guard_logs_on_exit(assert_logs, caplog):
    caplog.set_level(0)
    logger = logging.getLogger()
    await create_guarded_task(sample(), name='this task', logger=logger)
    assert_logs(["This task has finished unexpectedly"])


async def test_guard_logs_on_failure(assert_logs, caplog):
    caplog.set_level(0)
    logger = logging.getLogger()
    task = create_guarded_task(coro=fail("boo!"), name='this task', logger=logger)
    await asyncio.wait([task], timeout=0.01)  # let it start & react
    assert_logs(["This task has failed: boo!"])


async def test_guard_logs_on_cancellation(assert_logs, caplog):
    caplog.set_level(0)
    logger = logging.getLogger()
    task = create_guarded_task(coro=delay(1), name='this task', logger=logger)
    await asyncio.wait([task], timeout=0.01)  # let it start
    task.cancel()
    await asyncio.wait([task], timeout=0.01)  # let it react
    assert_logs(["This task is cancelled"])


async def test_guard_is_silent_when_finishable(assert_logs, caplog):
    caplog.set_level(0)
    logger = logging.getLogger()
    await create_guarded_task(sample(), name='this task', logger=logger, finishable=True)
    assert_logs([], prohibited=["This task has finished unexpectedly"])
    assert not caplog.messages


async def test_guard_is_silent_when_cancellable(assert_logs, caplog):
    caplog.set_level(0)
    logger = logging.getLogger()
    task = create_guarded_task(coro=delay(1), name='this task', logger=logger, cancellable=True)
    await asyncio.wait([task], timeout=0.01)  # let it start
    task.cancel()
    await asyncio.wait([task], timeout=0.01)  # let it react
    assert_logs([], prohibited=["This task is cancelled"])
    assert not caplog.messages


@pytest.mark.parametrize('finishable', [True, False])
async def test_guard_escalates_on_failure(finishable):
    task = create_guarded_task(coro=fail("boo!"), name='this task', finishable=finishable)
    await asyncio.wait([task], timeout=0.01)  # let it start & react
    with pytest.raises(Error):
        await task


@pytest.mark.parametrize('cancellable', [True, False])
async def test_guard_escalates_on_cancellation(cancellable):
    task = create_guarded_task(coro=delay(1), name='this task', cancellable=cancellable)
    await asyncio.wait([task], timeout=0.01)  # let it start
    task.cancel()
    await asyncio.wait([task], timeout=0.01)  # let it react
    with pytest.raises(asyncio.CancelledError):
        await task


async def test_guard_waits_for_the_flag():
    flag = asyncio.Event()

    task = create_guarded_task(coro=sample(), name='this task', flag=flag)
    await asyncio.wait([task], timeout=0.01)  # let it start
    assert not task.done()

    flag.set()
    await asyncio.wait([task], timeout=0.01)  # let it react
    assert task.done()


async def test_reraise_escalates_errors():
    task = asyncio.create_task(fail("boo!"))
    await asyncio.wait([task], timeout=0.01)  # let it start & react
    with pytest.raises(Error):
        await reraise([task])


async def test_reraise_skips_cancellations():
    task = asyncio.create_task(asyncio.Event().wait())
    done, pending = await asyncio.wait([task], timeout=0.01)  # let it start
    assert not done
    task.cancel()
    done, pending = await asyncio.wait([task], timeout=0.01)  # let it react
    assert done
    await reraise([task])



================================================
FILE: tests/utilities/aiotasks/test_task_selection.py
================================================
import asyncio

from kopf._cogs.aiokits.aiotasks import all_tasks


async def test_alltasks_exclusion():
    flag = asyncio.Event()
    task1 = asyncio.create_task(flag.wait())
    task2 = asyncio.create_task(flag.wait())
    done, pending = await asyncio.wait([task1, task2], timeout=0.01)
    assert not done

    tasks = await all_tasks(ignored=[task2])
    assert task1 in tasks
    assert task2 not in tasks
    assert asyncio.current_task() not in tasks

    flag.set()
    await task1
    await task2



================================================
FILE: tests/utilities/aiotasks/test_task_stopping.py
================================================
import asyncio
import logging

import pytest

from kopf._cogs.aiokits.aiotasks import stop


async def simple() -> None:
    await asyncio.Event().wait()


async def stuck() -> None:
    try:
        await asyncio.Event().wait()
    except asyncio.CancelledError:
        await asyncio.Event().wait()


async def test_stop_with_no_tasks(assert_logs, caplog):
    logger = logging.getLogger()
    caplog.set_level(0)
    done, pending = await stop([], title='sample', logger=logger)
    assert not done
    assert not pending
    assert_logs(["Sample tasks stopping is skipped: no tasks given."])


async def test_stop_with_no_tasks_when_quiet(assert_logs, caplog):
    logger = logging.getLogger()
    caplog.set_level(0)
    done, pending = await stop([], title='sample', logger=logger, quiet=True)
    assert not done
    assert not pending
    assert not caplog.messages


async def test_stop_immediately_with_finishing(assert_logs, caplog):
    logger = logging.getLogger()
    caplog.set_level(0)
    task1 = asyncio.create_task(simple())
    task2 = asyncio.create_task(simple())
    done, pending = await stop([task1, task2], title='sample', logger=logger, cancelled=False)
    assert done
    assert not pending
    assert_logs(["Sample tasks are stopped: finishing normally"])
    assert task1.cancelled()
    assert task2.cancelled()


async def test_stop_immediately_with_cancelling(assert_logs, caplog):
    logger = logging.getLogger()
    caplog.set_level(0)
    task1 = asyncio.create_task(simple())
    task2 = asyncio.create_task(simple())
    done, pending = await stop([task1, task2], title='sample', logger=logger, cancelled=True)
    assert done
    assert not pending
    assert_logs(["Sample tasks are stopped: cancelling normally"])
    assert task1.cancelled()
    assert task2.cancelled()


@pytest.mark.parametrize('cancelled', [False, True])
async def test_stop_iteratively(assert_logs, caplog, cancelled):
    logger = logging.getLogger()
    caplog.set_level(0)
    task1 = asyncio.create_task(simple())
    task2 = asyncio.create_task(stuck())
    stask = asyncio.create_task(stop([task1, task2], title='sample', logger=logger, interval=0.01, cancelled=cancelled))

    done, pending = await asyncio.wait({stask}, timeout=0.011)
    assert not done
    assert task1.done()
    assert not task2.done()

    task2.cancel()

    done, pending = await asyncio.wait({stask}, timeout=0.011)
    assert done
    assert task1.done()
    assert task2.done()

    assert_logs([
        r"Sample tasks are not stopped: (finishing|cancelling) normally; tasks left: \{<Task",
        r"Sample tasks are stopped: (finishing|cancelling) normally; tasks left: set\(\)",
    ])


@pytest.mark.parametrize('cancelled', [False, True])
async def test_stop_itself_is_cancelled(assert_logs, caplog, cancelled):
    logger = logging.getLogger()
    caplog.set_level(0)
    task1 = asyncio.create_task(simple())
    task2 = asyncio.create_task(stuck())
    stask = asyncio.create_task(stop([task1, task2], title='sample', logger=logger, interval=0.01, cancelled=cancelled))

    done, pending = await asyncio.wait({stask}, timeout=0.011)
    assert not done
    assert task1.done()
    assert not task2.done()

    stask.cancel()

    done, pending = await asyncio.wait({stask}, timeout=0.011)
    assert done
    assert task1.done()
    assert not task2.done()

    assert_logs([
        r"Sample tasks are not stopped: (finishing|cancelling) normally; tasks left: \{<Task",
        r"Sample tasks are not stopped: (cancelling|double-cancelling) at stopping; tasks left: \{<Task",
    ], prohibited=[
        r"Sample tasks are stopped",
    ])

    task2.cancel()
    done, pending = await asyncio.wait({task1, task2})
    assert done
    assert task1.done()
    assert task2.done()



================================================
FILE: tests/utilities/aiotasks/test_task_waiting.py
================================================
import asyncio

from kopf._cogs.aiokits.aiotasks import wait


async def test_wait_with_no_tasks():
    done, pending = await wait([])
    assert not done
    assert not pending


async def test_wait_with_timeout():
    flag = asyncio.Event()
    task = asyncio.create_task(flag.wait())
    done, pending = await wait([task], timeout=0.01)
    assert not done
    assert pending == {task}
    flag.set()
    await task



================================================
FILE: tools/install-kind.sh
================================================
#!/bin/bash
# Install K8s via KinD (Kubernetes-in-Docker).
#
# Spin-up times previously detected:
# * k3d -- 20 seconds.
# * kind -- 90 seconds.
# * minikube -- 110-120 seconds.
#
# Not all of the latest K8s versions are available as the Kind versions.
# Care should be taken when upgrading. Check the available versions at:
# https://hub.docker.com/r/kindest/node/tags
#
set -eu
set -x

: ${KIND:=latest}
: ${K8S:=latest}
if [[ "$K8S" == latest ]] ; then
    K8S="$( curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt )"
fi

curl -Lo ./kind https://kind.sigs.k8s.io/dl/"$KIND"/kind-linux-amd64
chmod +x ./kind
sudo mv ./kind /usr/local/bin/

kind create cluster --image=kindest/node:"$K8S"



================================================
FILE: tools/install-kubectl.sh
================================================
#!/bin/bash
# Install kubectl.
#
# Use the latest client version always, ignore the requested K8s version.
# Kubectl is not a system-under-tests, it is a environment configuring tool.
#
set -eu
set -x

: ${K8S:=latest}

if [[ "$K8S" == latest ]] ; then
    K8S="$( curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt )"
fi

curl -Lo kubectl https://storage.googleapis.com/kubernetes-release/release/"$K8S"/bin/linux/amd64/kubectl
chmod +x kubectl
sudo mv kubectl /usr/local/bin/



================================================
FILE: tools/install-minikube.sh
================================================
#!/bin/bash
# Install K8s via Minikube.
#
# Minikube is heavy, but reliable, can run almost any version of K8s.
# Spin-up times previously detected:
# * k3d -- 20 seconds.
# * kind -- 90 seconds.
# * minikube -- 110-120 seconds.
#
# Based on https://github.com/LiliC/travis-minikube.
#
set -eu
set -x

: ${K8S:=latest}
if [[ "$K8S" == latest ]] ; then
    K8S="$( curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt )"
fi

curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
chmod +x minikube
sudo mv minikube /usr/local/bin/

mkdir -p $HOME/.kube $HOME/.minikube
touch $HOME/.kube/config

sudo apt-get update -y
sudo apt-get install -y conntrack  # see #334

minikube config set driver docker
minikube start \
    --extra-config=apiserver.authorization-mode=Node,RBAC \
    --extra-config=apiserver.runtime-config=events.k8s.io/v1beta1=false \
    --kubernetes-version="$K8S"



================================================
FILE: .github/CODEOWNERS
================================================
# Who is automatically assigned to the new PRs based on their content.
* @nolar



================================================
FILE: .github/FUNDING.yml
================================================
github: nolar



================================================
FILE: .github/ISSUE_TEMPLATE.md
================================================
## Long story short

<!-- Please describe the issue in 1-3 sentences. -->


## Description

<!-- Please provide as much information as possible.
     Lack of information may result in a delayed response. Thank you! -->



================================================
FILE: .github/PULL_REQUEST_TEMPLATE.md
================================================
<!--
Thank you for contributing! Please make sure that your code changes
are covered with tests. And in case of new features or big changes
remember to adjust the documentation.

Feel free to ping maintainers for the review!

In case of existing issue, reference it using one of the following:

closes: #ISSUE
related: #ISSUE

How to write a good git commit message:
http://chris.beams.io/posts/git-commit/
-->



================================================
FILE: .github/ISSUE_TEMPLATE/bug-report.yaml
================================================
name: Bug Report
description: File a bug report
labels: [bug, triage]
body:

  - type: markdown
    attributes:
      value: >
        Please provide as much information as possible.
        All fields are optional, but a lack of information
        may result in a delayed response and time-consuming iterations.

  - type: markdown
    attributes:
      value: >
        _If you feel confident with English, please use English.
        If not, feel free to use your native or preferred language
        (avoid metaphors and idioms — they do not auto-translate well).
        The answers will be in English._

  - type: textarea
    id: summary
    attributes:
      label: Long story short
      description: >
        Please describe your problem in 1-3 sentences.
        What has happened? What has not happened, but should have?
      placeholder: >
        A feature X behaves "this" way, but expected to behave "that" way.
        The misbehaviour leads to unexpected results or side-effects A, B, C.

  - type: input
    id: kopf-version
    attributes:
      label: Kopf version
      placeholder: e.g. 1.31.2
  - type: input
    id: kubernetes-version
    attributes:
      label: Kubernetes version
      placeholder: e.g. 1.22 or 1.22.0
  - type: input
    id: python-version
    attributes:
      label: Python version
      placeholder: e.g. 3.10 or pypy-3.10-7.3.13

  - type: textarea
    id: code
    attributes:
      label: Code
      description: >
        The code snippet of the operator to reproduce the issue.
        (No backticks — the code will be formatted automatically.)
      placeholder: |
        # For example:
        import kopf

        @kopf.on.create('kopfexamples')
        def create_fn(**_):
            pass
      render: python

  - type: textarea
    id: logs
    attributes:
      label: Logs
      description: >
        The output that highlights the failure of the operator
        and shows what happened immediately before and after.
        (No backticks — the logs will be formatted automatically.)
      placeholder: |
        [2020-01-01 12:34:56,789] [DEBUG   ] Starting Kopf 1.31.1.
        [2020-01-01 12:34:56,890] [DEBUG   ] ...
      render: none

  - type: textarea
    id: extra
    attributes:
      label: Additional information
      description: >
        Everything you would like to add that can help to identify the issue.



================================================
FILE: .github/ISSUE_TEMPLATE/config.yaml
================================================
blank_issues_enabled: true
contact_links:
  - name: Community Support
    url: https://github.com/nolar/kopf/discussions/categories/q-a
    about: Please ask and answer questions here.



================================================
FILE: .github/ISSUE_TEMPLATE/feature-request.yaml
================================================
name: Feature Request
description: Suggest an idea for this project
labels: [enhancement, triage]
body:

  - type: markdown
    attributes:
      value: >
        Please provide as much information as possible.
        All fields are optional, but a lack of information
        may result in a delayed response and time-consumung iterations.

  - type: markdown
    attributes:
      value: >
        _If you feel confident with English, please use English.
        If not, feel free to use your native or preferred language
        (avoid metaphors and idioms — they do not auto-translate well).
        The answers will be in English._

  - type: textarea
    id: problem
    attributes:
      label: Problem
      description: >
        What problem do you currently face so that you want this feature?
        Are there existing features or tools close to solving this problem?
        Why don't they work?
      placeholder: >
        E.g.: I want to access several in-memory indicies anywhere in the code
        nested in multiple levels of function calls. Currently, I have to pass
        the indicies from the handlers down the stack in every call,
        which complicates the code and makes it too wordy.

  - type: textarea
    id: proposal
    attributes:
      label: Proposal
      description: >
        Describe the solution you would like to have.
        Are there any other ways of achieving the same goal?
        Why is this proposal better than those alternatives?
      placeholder: >
        E.g.: Either store the indicies in global variables,
        or pass a single kwarg with all indicies at once,
        not as separate kwargs.

  - type: textarea
    id: code
    attributes:
      label: Code
      description: >
        A code snippet showing the new feature in action, at least as an idea.
        (No backticks — the code will be formatted automatically.)
      placeholder: |
        # E.g.:
        import kopf

        my_index = kopf.Index()

        @kopf.index('pods', target=my_index)
        def fn(**_):
            ...

        def any_function():
            for key, val in my_index.items():
                ...

  - type: textarea
    id: extra
    attributes:
      label: Additional information
      description: >
        Additional information in free form — everything you would like to add.



================================================
FILE: .github/ISSUE_TEMPLATE/question.yaml
================================================
name: Question
description: Ask an advice on how to do something
labels: [question, triage]
body:

  - type: markdown
    attributes:
      value: >
        Please provide as much information as possible.
        All fields are optional, but a lack of information
        may result in a delayed response and time-consuming iterations.

  - type: markdown
    attributes:
      value: >
        _If you feel confident with English, please use English.
        If not, feel free to use your native or preferred language
        (avoid metaphors and idioms — they do not auto-translate well).
        The answers will be in English._

  - type: markdown
    attributes:
      value: >
        Before asking the question, please first try looking for the answers
        in the [documentation](https://kopf.readthedocs.io/en/stable/)
        and similar [issues & questions](https://github.com/nolar/kopf/issues).

  - type: input
    id: keywords
    attributes:
      label: Keywords
      description: >
        Which keywords did you search for in the documentation/issues
        for this problem? (Space or comma separated.)

  - type: textarea
    id: problem
    attributes:
      label: Problem
      description: >
        What problem do you currently face and see no solution for it?
        If possible, explain what other ways did you try to solve the problem?



================================================
FILE: .github/workflows/ci.yaml
================================================
# The quick CI tests run on every push to a PR. They perform a quick check
# if the feature set and codebase are stable in general, but only for
# a representative selection of environments.
name: CI
on:
  pull_request:
    branches:
      - main
      - release/**
  workflow_dispatch: {}

jobs:
  linters:
    name: Linting and static analysis
    runs-on: ubuntu-24.04
    timeout-minutes: 5  # usually 1-2, rarely 3 mins (because of installations)
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: "3.13"
      - run: pip install -r requirements.txt
      - run: pre-commit run --all-files
      - run: mypy kopf --strict
      - run: |
          # Mypying the examples
          exit_codes=0
          for d in $(find examples -maxdepth 1 -mindepth 1 -type d)
          do
            echo "Checking ${d}"
            mypy $d
            exit_codes=$[${exit_codes} + $?]
          done
          exit ${exit_codes}

  unit-tests:
    strategy:
      fail-fast: false
      matrix:
        install-extras: [ "", "full-auth" ]
        python-version: [ "3.9", "3.10", "3.11", "3.12", "3.13" ]
        include:
          - install-extras: "uvloop"
            python-version: "3.13"
    name: Python ${{ matrix.python-version }}${{ matrix.install-extras && ' ' || '' }}${{ matrix.install-extras }}
    runs-on: ubuntu-24.04
    timeout-minutes: 5  # usually 2-3 mins
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - run: pip install -r requirements.txt
      - run: pip install -e .[${{ matrix.install-extras }}]
        if: ${{ matrix.install-extras }}
      - run: pytest --color=yes --timeout=2 --cov=kopf --cov-branch

      - name: Publish coverage to Coveralls.io
        if: success()
        run: coveralls --service=github
        env:
          GITHUB_TOKEN: ${{ secrets.github_token }}
        continue-on-error: true
      - name: Publish coverage to CodeCov.io
        uses: codecov/codecov-action@v3
        if: success()
        env:
          PYTHON: ${{ matrix.python-version }}
        with:
          flags: unit
          env_vars: PYTHON
        continue-on-error: true

  # Only the core functionality is tested: no e2e or functional tests (for simplicity).
  # No coverage: PyPy performs extremely poorly with tracing/coverage (13 mins vs. 3 mins).
  # Extra time: 2-3 mins for building the dependencies (since no binary wheels are available).
  # Extra time: PyPy is good with JIT for repetitive code; tests are too unique for JIT.
  pypy-tests:
    strategy:
      fail-fast: false
      matrix:
        install-extras: [ "", "full-auth" ]
        python-version: [ "pypy-3.9", "pypy-3.10" ]
    name: Python ${{ matrix.python-version }}${{ matrix.install-extras && ' ' || '' }}${{ matrix.install-extras }}
    runs-on: ubuntu-24.04
    timeout-minutes: 10
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - run: sudo apt-get update && sudo apt-get install libxml2-dev libxslt-dev
      - run: pip install --upgrade pip wheel setuptools

      - run: pip install -r requirements.txt
      - run: pip install -e .[${{ matrix.install-extras }}]
        if: ${{ matrix.install-extras }}
      - run: pytest --color=yes --timeout=2 --no-cov

  functional:
    strategy:
      fail-fast: false
      matrix:
        k3s: [latest, v1.31, v1.30, v1.29]
    name: K3s ${{matrix.k3s}}
    runs-on: ubuntu-24.04
    timeout-minutes: 10  # usually 4-5 mins
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: "3.13"
      - uses: nolar/setup-k3d-k3s@v1
        with:
          version: ${{ matrix.k3s }}
          github-token: ${{ secrets.GITHUB_TOKEN }}
      - run: pip install -r requirements.txt -r examples/requirements.txt
      - run: pytest --color=yes --timeout=30 --only-e2e

  coveralls-finish:
    name: Finalize coveralls.io
    needs: [unit-tests, pypy-tests, functional]
    runs-on: ubuntu-24.04
    steps:
      - uses: actions/setup-python@v5
      - run: pip install coveralls
      - run: coveralls --service=github --finish
        env:
          GITHUB_TOKEN: ${{ secrets.github_token }}
        continue-on-error: true



================================================
FILE: .github/workflows/codeql.yml
================================================
name: "CodeQL"

on:
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]
  schedule:
    - cron: "6 2 * * 3"

jobs:
  analyze:
    name: Analyze
    runs-on: ubuntu-latest
    permissions:
      actions: read
      contents: read
      security-events: write

    strategy:
      fail-fast: false
      matrix:
        language: [ python ]

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Initialize CodeQL
        uses: github/codeql-action/init@v2
        with:
          languages: ${{ matrix.language }}
          queries: +security-and-quality

      - name: Autobuild
        uses: github/codeql-action/autobuild@v2

      - name: Perform CodeQL Analysis
        uses: github/codeql-action/analyze@v2
        with:
          category: "/language:${{ matrix.language }}"



================================================
FILE: .github/workflows/publish.yaml
================================================
name: Publish to PyPI
on:
  release:
    types:
      - published
#  push:
#    tags:
#      - "[0-9]+.[0-9]+*"
  workflow_dispatch: {}

jobs:
  publish:
    name: Build and publish
    runs-on: ubuntu-24.04
    permissions:
      id-token: write  # for trusted publishing
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: "3.13"
      - run: pip install --upgrade pip setuptools wheel twine
      - run: python setup.py sdist bdist_wheel
      - uses: pypa/gh-action-pypi-publish@release/v1
        with:
          skip_existing: true



================================================
FILE: .github/workflows/thorough.yaml
================================================
# The thorough tests run only on the main branch after everything is merged,
# and regularly by time —- on order to detect bugs and incompatibility with
# the new versions of freshly released software (e.g. K8s, K3s, Python libs).
# The first part fully includes the CI workflow, with more versions of K3d/K3s.
# The second part is unique to the thorough tests.
name: Thorough tests
on:
  push:
    branches:
      - main
      - release/**
  schedule:
    - cron: "13 3 * * 6"
  workflow_dispatch: {}

jobs:
  linters:
    name: Linting and static analysis
    runs-on: ubuntu-24.04
    timeout-minutes: 5  # usually 1-2, rarely 3 mins (because of installations)
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: "3.13"
      - run: pip install -r requirements.txt
      - run: pre-commit run --all-files
      - run: mypy kopf --strict
      - run: |
          # Mypying the examples
          exit_codes=0
          for d in $(find examples -maxdepth 1 -mindepth 1 -type d)
          do
            echo "Checking ${d}"
            mypy $d
            exit_codes=$[${exit_codes} + $?]
          done
          exit ${exit_codes}

  unit-tests:
    strategy:
      fail-fast: false
      matrix:
        install-extras: [ "", "full-auth" ]
        python-version: [ "3.9", "3.10", "3.11", "3.12", "3.13" ]
        include:
          - install-extras: "uvloop"
            python-version: "3.13"
    name: Python ${{ matrix.python-version }}${{ matrix.install-extras && ' ' || '' }}${{ matrix.install-extras }}
    runs-on: ubuntu-24.04
    timeout-minutes: 5  # usually 2-3 mins
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - run: pip install -r requirements.txt
      - run: pip install -e .[${{ matrix.install-extras }}]
        if: ${{ matrix.install-extras }}
      - run: pytest --color=yes --timeout=2 --cov=kopf --cov-branch

      - name: Publish coverage to Coveralls.io
        if: success()
        run: coveralls --service=github
        env:
          GITHUB_TOKEN: ${{ secrets.github_token }}
        continue-on-error: true
      - name: Publish coverage to CodeCov.io
        uses: codecov/codecov-action@v3
        if: success()
        env:
          PYTHON: ${{ matrix.python-version }}
        with:
          flags: unit
          env_vars: PYTHON
        continue-on-error: true

  # Only the core functionality is tested: no e2e or functional tests (for simplicity).
  # No coverage: PyPy performs extremely poorly with tracing/coverage (13 mins vs. 3 mins).
  # Extra time: 2-3 mins for building the dependencies (since no binary wheels are available).
  # Extra time: PyPy is good with JIT for repetitive code; tests are too unique for JIT.
  pypy-tests:
    strategy:
      fail-fast: false
      matrix:
        install-extras: [ "", "full-auth" ]
        python-version: [ "pypy-3.9", "pypy-3.10" ]
    name: Python ${{ matrix.python-version }}${{ matrix.install-extras && ' ' || '' }}${{ matrix.install-extras }}
    runs-on: ubuntu-24.04
    timeout-minutes: 10
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - run: sudo apt-get update && sudo apt-get install libxml2-dev libxslt-dev
      - run: pip install --upgrade pip wheel setuptools

      - run: pip install -r requirements.txt
      - run: pip install -e .[${{ matrix.install-extras }}]
        if: ${{ matrix.install-extras }}
      - run: pytest --color=yes --timeout=2 --no-cov

  functional:
    strategy:
      fail-fast: false
      matrix:
        k3s: [latest, v1.31, v1.30, v1.29]
    name: K3s ${{matrix.k3s}}
    runs-on: ubuntu-24.04
    timeout-minutes: 10  # usually 4-5 mins
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: "3.13"
      - uses: nolar/setup-k3d-k3s@v1
        with:
          version: ${{ matrix.k3s }}
          github-token: ${{ secrets.GITHUB_TOKEN }}
      - run: pip install -r requirements.txt -r examples/requirements.txt
      - run: pytest --color=yes --timeout=30 --only-e2e

  full-scale:
    strategy:
      fail-fast: false
      matrix:
        k8s: [latest, v1.31.2, v1.30.6, v1.29.10]
    name: K8s ${{matrix.k8s}}
    runs-on: ubuntu-24.04
    timeout-minutes: 10  # usually 4-5 mins
    env:
      K8S: ${{ matrix.k8s }}
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: "3.13"
      - run: tools/install-minikube.sh
      - run: pip install -r requirements.txt -r examples/requirements.txt
      - run: pytest --color=yes --timeout=30 --only-e2e

  coveralls-finish:
    name: Finalize coveralls.io
    needs: [unit-tests, pypy-tests, functional]
    runs-on: ubuntu-24.04
    steps:
      - uses: actions/setup-python@v5
      - run: pip install coveralls
      - run: coveralls --service=github --finish
        env:
          GITHUB_TOKEN: ${{ secrets.github_token }}
        continue-on-error: true


